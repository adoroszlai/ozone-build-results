Attaching to ha_dn3_1, ha_scm2_1, ha_s3g_1, ha_dn4_1, ha_om2_1, ha_dn5_1, ha_om1_1, ha_dn1_1, ha_dn2_1, ha_scm3_1, ha_recon_1, ha_scm1_1, ha_om3_1
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-07-19 07:34:10,515 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = e710b1935f2e/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn2_1    | STARTUP_MSG:   java = 11.0.19
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-07-19 07:34:15,976 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 751115a4ca9e/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | ************************************************************/
dn2_1    | 2023-07-19 07:34:10,576 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-07-19 07:34:11,110 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-07-19 07:34:12,416 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-07-19 07:34:15,318 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-07-19 07:34:15,318 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-07-19 07:34:17,487 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e710b1935f2e ip:10.9.0.18
dn2_1    | 2023-07-19 07:34:20,150 [main] INFO reflections.Reflections: Reflections took 2227 ms to scan 2 urls, producing 107 keys and 232 values 
dn2_1    | 2023-07-19 07:34:24,647 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn2_1    | 2023-07-19 07:34:25,186 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-07-19 07:34:27,164 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 8192 at 2023-07-19T07:33:37.639Z
dn2_1    | 2023-07-19 07:34:27,337 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-07-19 07:34:27,357 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-07-19 07:34:27,358 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-07-19 07:34:27,625 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-07-19 07:34:27,764 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-07-19 07:34:14,812 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 69ed6f01c113/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn3_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn3_1    | ************************************************************/
dn3_1    | 2023-07-19 07:34:14,944 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-07-19 07:34:15,423 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-07-19 07:34:16,827 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-07-19 07:34:19,136 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-07-19 07:34:19,137 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-07-19 07:34:21,421 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:69ed6f01c113 ip:10.9.0.19
dn3_1    | 2023-07-19 07:34:24,155 [main] INFO reflections.Reflections: Reflections took 2161 ms to scan 2 urls, producing 107 keys and 232 values 
dn3_1    | 2023-07-19 07:34:28,621 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn3_1    | 2023-07-19 07:34:29,614 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-07-19 07:34:30,961 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-07-19T07:33:37.489Z
dn3_1    | 2023-07-19 07:34:31,148 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-07-19 07:34:31,164 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-07-19 07:34:31,179 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-07-19 07:34:31,621 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-07-19 07:34:31,870 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-19 07:34:27,781 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-19T07:33:37.758Z
dn2_1    | 2023-07-19 07:34:27,805 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-07-19 07:34:27,805 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-07-19 07:34:27,815 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-07-19 07:34:28,857 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-ed4ac980-2861-4966-a424-56480ba2033a/container.db to cache
dn2_1    | 2023-07-19 07:34:28,857 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-ed4ac980-2861-4966-a424-56480ba2033a/container.db for volume DS-ed4ac980-2861-4966-a424-56480ba2033a
dn2_1    | 2023-07-19 07:34:29,059 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn2_1    | 2023-07-19 07:34:29,088 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-07-19 07:34:29,088 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-07-19 07:34:48,131 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-07-19 07:34:49,570 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-19 07:34:50,150 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-07-19 07:34:51,925 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-19 07:34:51,983 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-07-19 07:34:51,994 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-07-19 07:34:52,008 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-07-19 07:34:52,050 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-07-19 07:34:52,050 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-07-19 07:34:52,068 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-07-19 07:34:52,069 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:34:52,081 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-07-19 07:34:52,090 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-19 07:34:52,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-07-19 07:34:52,440 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-07-19 07:34:52,474 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-07-19 07:34:56,802 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-07-19 07:34:56,856 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-07-19 07:34:56,894 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-07-19 07:34:56,898 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:34:56,898 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-19 07:34:56,950 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-19 07:34:56,969 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: found a subdirectory /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a
dn2_1    | 2023-07-19 07:34:57,067 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: addNew group-944115277F4A:[] returns group-944115277F4A:java.util.concurrent.CompletableFuture@3b96dd38[Not completed]
dn2_1    | 2023-07-19 07:34:57,437 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-07-19 07:34:57,949 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: new RaftServerImpl for group-944115277F4A:[] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-19 07:34:57,999 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-07-19 07:34:58,012 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-19 07:34:58,012 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-19 07:34:58,014 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:34:58,014 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-19 07:34:58,014 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-19 07:34:58,166 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-19 07:34:58,174 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-19 07:34:58,195 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn2_1    | 2023-07-19 07:34:58,251 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-19 07:34:58,309 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-19 07:34:58,653 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:34:58,787 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-19 07:34:58,817 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-19 07:34:58,939 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-19 07:34:59,444 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-19 07:34:59,893 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-19 07:35:00,029 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-19 07:35:00,029 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-19 07:35:00,118 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-19 07:35:00,219 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-19 07:35:00,220 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-19 07:35:00,425 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-07-19 07:35:00,824 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-07-19 07:35:01,746 [main] INFO util.log: Logging initialized @67653ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-07-19 07:35:03,296 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-07-19 07:35:03,377 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-07-19 07:35:03,463 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-07-19 07:35:03,493 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-07-19 07:35:03,496 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-07-19 07:35:03,497 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-07-19 07:35:03,928 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn2_1    | 2023-07-19 07:35:03,995 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-07-19 07:35:04,006 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn2_1    | 2023-07-19 07:35:04,237 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-07-19 07:35:04,242 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-07-19 07:35:04,250 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-07-19 07:35:04,354 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2801827a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-07-19 07:35:04,366 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d0cd23c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-07-19 07:35:05,211 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@359ceb13{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-9731588391765791597/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn2_1    | 2023-07-19 07:35:05,287 [main] INFO server.AbstractConnector: Started ServerConnector@28768e25{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-07-19 07:35:05,304 [main] INFO server.Server: Started @71209ms
dn2_1    | 2023-07-19 07:35:05,316 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-07-19 07:35:05,322 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-07-19 07:35:05,381 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-07-19 07:35:06,102 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn2_1    | 2023-07-19 07:35:06,404 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn2_1    | 2023-07-19 07:35:06,418 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn2_1    | 2023-07-19 07:35:07,977 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn2_1    | 2023-07-19 07:35:08,000 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn2_1    | 2023-07-19 07:35:08,004 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn2_1    | 2023-07-19 07:35:08,005 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn2_1    | 2023-07-19 07:35:08,078 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-07-19 07:35:08,239 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn2_1    | 2023-07-19 07:35:08,241 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn2_1    | 2023-07-19 07:35:08,583 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-07-19 07:35:08,751 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-07-19 07:35:11,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:11,722 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:11,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:12,717 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:12,740 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:12,741 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:34:31,975 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-19T07:33:37.750Z
dn3_1    | 2023-07-19 07:34:32,042 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-07-19 07:34:32,047 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-07-19 07:34:32,062 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-07-19 07:34:36,904 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-48897640-b98d-4fe9-a8c5-1f3221eda432/container.db to cache
dn3_1    | 2023-07-19 07:34:36,905 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-48897640-b98d-4fe9-a8c5-1f3221eda432/container.db for volume DS-48897640-b98d-4fe9-a8c5-1f3221eda432
dn3_1    | 2023-07-19 07:34:37,138 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn3_1    | 2023-07-19 07:34:41,776 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-07-19 07:34:41,787 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 4s
dn3_1    | 2023-07-19 07:34:55,027 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-07-19 07:34:55,851 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-07-19 07:34:56,849 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-07-19 07:34:56,907 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-19 07:34:56,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-07-19 07:34:56,948 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-07-19 07:34:56,952 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-07-19 07:34:56,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-07-19 07:34:56,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-07-19 07:34:56,972 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-07-19 07:34:56,978 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:34:56,992 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-07-19 07:34:56,994 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:34:57,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-19 07:34:57,204 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-07-19 07:34:57,228 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-07-19 07:35:00,722 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-07-19 07:35:00,753 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-07-19 07:35:00,761 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-07-19 07:35:00,764 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:00,767 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-19 07:35:00,815 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:35:00,850 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: found a subdirectory /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef
dn3_1    | 2023-07-19 07:35:00,886 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-B30FA8B5BAEF:[] returns group-B30FA8B5BAEF:java.util.concurrent.CompletableFuture@4d42ff2[Not completed]
dn3_1    | 2023-07-19 07:35:00,891 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: found a subdirectory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn3_1    | 2023-07-19 07:35:00,892 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-8563B54DD732:[] returns group-8563B54DD732:java.util.concurrent.CompletableFuture@6cf57cda[Not completed]
dn3_1    | 2023-07-19 07:35:00,892 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: found a subdirectory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn3_1    | 2023-07-19 07:35:00,905 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-03BEFB15C8DE:[] returns group-03BEFB15C8DE:java.util.concurrent.CompletableFuture@50a54e42[Not completed]
dn3_1    | 2023-07-19 07:35:01,387 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-07-19 07:35:01,718 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-B30FA8B5BAEF:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:35:01,824 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:35:02,026 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:35:02,027 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-19 07:35:02,027 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:02,030 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-19 07:35:02,032 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-19 07:35:02,259 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-19 07:35:02,321 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:35:02,379 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:35:02,418 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-19 07:35:02,579 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn3_1    | 2023-07-19 07:35:02,798 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:02,978 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:03,106 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-19 07:35:03,134 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:35:03,819 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-19 07:35:04,348 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:35:04,460 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-19 07:35:04,467 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-19 07:35:04,468 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-19 07:35:04,530 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-19 07:35:04,531 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-8563B54DD732:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-19 07:35:04,533 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-19 07:35:04,608 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:35:04,611 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:35:04,611 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-19 07:35:04,618 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn1_1    | STARTUP_MSG:   java = 11.0.19
dn2_1    | 2023-07-19 07:35:13,718 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:13,742 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:13,743 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:14,719 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:14,743 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:14,744 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:15,720 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:15,744 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:15,910 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From e710b1935f2e/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:56388 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:56388 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-19 07:35:16,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:16,745 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:17,722 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:17,746 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:18,724 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:18,747 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:19,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:19,748 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:19,752 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From e710b1935f2e/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:54722 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:54722 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn2_1    | 2023-07-19 07:35:20,727 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:20,749 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:21,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:21,750 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:22,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:22,751 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:23,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:23,752 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:24,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:24,753 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:25,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:25,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From e710b1935f2e/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 2023-07-19 07:35:04,618 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:04,626 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-19 07:35:04,626 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:35:04,630 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-19 07:35:04,867 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:35:04,867 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-19 07:35:04,870 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-19 07:35:04,871 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-19 07:35:04,871 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-19 07:35:04,871 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:35:04,873 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-03BEFB15C8DE:[] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:35:04,904 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:35:04,905 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:35:04,905 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-19 07:35:04,906 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:04,907 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-19 07:35:04,907 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-19 07:35:04,908 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-07-19 07:35:04,910 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:35:04,957 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:35:04,980 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn1_1    | ************************************************************/
dn1_1    | 2023-07-19 07:34:16,048 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-07-19 07:34:16,952 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-07-19 07:34:18,022 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-07-19 07:34:19,667 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-07-19 07:34:19,667 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-07-19 07:34:21,927 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:751115a4ca9e ip:10.9.0.17
dn1_1    | 2023-07-19 07:34:24,565 [main] INFO reflections.Reflections: Reflections took 1928 ms to scan 2 urls, producing 107 keys and 232 values 
dn1_1    | 2023-07-19 07:34:28,985 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn1_1    | 2023-07-19 07:34:29,768 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-07-19 07:34:31,198 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4096 at 2023-07-19T07:33:37.642Z
dn1_1    | 2023-07-19 07:34:31,335 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-07-19 07:34:31,380 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-07-19 07:34:31,467 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-07-19 07:34:31,736 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-07-19 07:34:32,040 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-19 07:34:32,056 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-19T07:33:37.814Z
dn1_1    | 2023-07-19 07:34:32,175 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-07-19 07:34:32,182 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-07-19 07:34:32,183 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-07-19 07:34:33,400 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-83922423-1378-44d4-b082-70eb1c170396/container.db to cache
dn1_1    | 2023-07-19 07:34:33,400 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-83922423-1378-44d4-b082-70eb1c170396/container.db for volume DS-83922423-1378-44d4-b082-70eb1c170396
dn1_1    | 2023-07-19 07:34:33,571 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn1_1    | 2023-07-19 07:34:33,572 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-07-19 07:34:33,572 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-07-19 07:34:52,101 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-07-19 07:34:53,341 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-07-19 07:34:54,092 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-07-19 07:34:55,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-19 07:34:55,091 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-07-19 07:34:55,112 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-07-19 07:34:55,122 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-07-19 07:34:55,136 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-07-19 07:34:55,139 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-07-19 07:34:55,140 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-07-19 07:34:55,159 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-19 07:34:55,162 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-07-19 07:34:55,169 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-19 07:34:55,344 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-07-19 07:34:55,380 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-07-19 07:34:55,405 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-07-19 07:34:59,535 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-07-19 07:34:59,566 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-07-19 07:34:59,583 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-07-19 07:34:59,598 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-19 07:34:59,604 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-19 07:34:59,650 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-19 07:34:59,675 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer: a9b83a7e-59b8-4455-b30a-c01eee264fbd: found a subdirectory /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61
dn1_1    | 2023-07-19 07:34:59,707 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer: a9b83a7e-59b8-4455-b30a-c01eee264fbd: addNew group-AA6EE9A1AF61:[] returns group-AA6EE9A1AF61:java.util.concurrent.CompletableFuture@354a33e2[Not completed]
dn1_1    | 2023-07-19 07:34:59,964 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-07-19 07:35:00,509 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd: new RaftServerImpl for group-AA6EE9A1AF61:[] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-19 07:35:00,534 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-19 07:35:00,567 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-07-19 07:35:00,568 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-19 07:35:00,571 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-07-19 07:35:00,571 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-07-19 07:35:00,572 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-07-19 07:35:00,764 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-19 07:35:00,780 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-07-19 07:35:00,868 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-19 07:35:01,061 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-19 07:35:01,158 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-07-19 07:35:01,366 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-19 07:35:01,520 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-19 07:35:01,725 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-19 07:35:01,735 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-19 07:35:02,344 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn1_1    | 2023-07-19 07:35:02,929 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-19 07:35:03,072 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-19 07:35:03,164 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-19 07:35:03,180 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-19 07:35:03,194 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-07-19 07:35:03,214 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-07-19 07:35:03,496 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-07-19 07:35:03,837 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-07-19 07:35:04,344 [main] INFO util.log: Logging initialized @66217ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-07-19 07:35:06,194 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-07-19 07:35:06,286 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-07-19 07:35:06,372 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-07-19 07:35:06,400 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-07-19 07:35:06,400 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-07-19 07:35:06,400 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-07-19 07:35:06,794 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn1_1    | 2023-07-19 07:35:06,869 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-07-19 07:35:06,890 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn1_1    | 2023-07-19 07:35:07,245 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-07-19 07:35:07,262 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-07-19 07:35:07,270 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-07-19 07:35:07,434 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@10b8b900{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-07-19 07:35:07,450 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@64f4f12{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-07-19 07:35:08,519 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4dd2ef54{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-5652436010725697957/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn1_1    | 2023-07-19 07:35:08,571 [main] INFO server.AbstractConnector: Started ServerConnector@595f9916{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-07-19 07:35:08,575 [main] INFO server.Server: Started @70444ms
dn1_1    | 2023-07-19 07:35:08,594 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-07-19 07:35:08,594 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-07-19 07:35:08,596 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-07-19 07:35:08,809 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn1_1    | 2023-07-19 07:35:09,069 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn1_1    | 2023-07-19 07:35:09,101 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-07-19 07:35:11,413 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn1_1    | 2023-07-19 07:35:11,413 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn1_1    | 2023-07-19 07:35:11,418 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn1_1    | 2023-07-19 07:35:11,463 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn1_1    | 2023-07-19 07:35:11,491 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-07-19 07:35:11,600 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn1_1    | 2023-07-19 07:35:11,608 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn1_1    | 2023-07-19 07:35:12,075 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-07-19 07:35:12,255 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-07-19 07:35:14,949 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:14,965 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:15,950 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:15,967 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:16,951 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:16,968 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
dn2_1    | 	... 12 more
dn2_1    | 2023-07-19 07:35:25,754 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:25,755 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From e710b1935f2e/10.9.0.18 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
dn2_1    | 	... 12 more
dn2_1    | 2023-07-19 07:35:26,737 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:26,756 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:27,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-07-19 07:34:15,378 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 6cb42cec657e/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-07-19 07:34:15,753 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 26bb3b489ab7/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--upgrade]
om1_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:44Z
om1_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | 2023-07-19 07:35:04,982 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:35:04,990 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:04,990 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-19 07:35:04,992 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:35:04,998 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-19 07:35:05,003 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:35:05,012 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-19 07:35:05,020 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-19 07:35:05,022 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-07-19 07:35:05,046 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-19 07:35:05,050 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:35:05,080 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-07-19 07:35:05,195 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-07-19 07:35:05,602 [main] INFO util.log: Logging initialized @67513ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-07-19 07:35:06,803 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-07-19 07:35:06,809 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-07-19 07:35:06,865 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-07-19 07:35:06,888 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-07-19 07:35:06,889 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-07-19 07:35:06,889 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-07-19 07:35:07,242 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn3_1    | 2023-07-19 07:35:07,288 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-07-19 07:35:07,295 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn3_1    | 2023-07-19 07:35:07,492 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-07-19 07:35:07,502 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-07-19 07:35:07,511 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-07-19 07:35:07,660 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7e5efcab{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-07-19 07:35:07,673 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@8c43966{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-07-19 07:35:08,689 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@124eb83d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1067360716120222840/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn3_1    | 2023-07-19 07:35:08,765 [main] INFO server.AbstractConnector: Started ServerConnector@191a0351{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-07-19 07:35:08,766 [main] INFO server.Server: Started @70677ms
dn3_1    | 2023-07-19 07:35:08,770 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-07-19 07:35:08,774 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-07-19 07:35:08,817 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-07-19 07:35:09,019 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn3_1    | 2023-07-19 07:35:09,325 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn3_1    | 2023-07-19 07:35:09,368 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn3_1    | 2023-07-19 07:35:11,152 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn3_1    | 2023-07-19 07:35:11,152 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn3_1    | 2023-07-19 07:35:11,176 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 2023-07-19 07:35:11,179 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn3_1    | 2023-07-19 07:35:11,274 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-07-19 07:35:11,382 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
dn3_1    | 2023-07-19 07:35:11,388 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn3_1    | 2023-07-19 07:35:11,935 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-07-19 07:35:12,048 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-07-19 07:35:14,945 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:14,955 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:17,952 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:17,969 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:18,953 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:18,969 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:19,327 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 751115a4ca9e/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:35880 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:35880 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-19 07:35:19,327 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 751115a4ca9e/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:59290 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-07-19 07:34:14,719 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = aff5372d1efd/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om1_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om1_1    | ************************************************************/
om1_1    | 2023-07-19 07:34:15,847 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-07-19 07:34:30,401 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-07-19 07:34:36,152 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-07-19 07:34:36,886 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-07-19 07:34:36,918 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-07-19 07:34:37,018 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-07-19 07:34:38,399 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = QUOTA (version = 6)
om1_1    | 2023-07-19 07:34:42,271 [main] INFO reflections.Reflections: Reflections took 2680 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om1_1    | 2023-07-19 07:34:42,606 [main] INFO upgrade.OMLayoutVersionManager: Registering Upgrade Action : QuotaRepairUpgradeAction
om1_1    | 2023-07-19 07:34:43,228 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-07-19 07:34:47,156 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-07-19 07:34:47,606 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-07-19 07:34:55,151 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om1_1    | 2023-07-19 07:34:57,153 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om1_1    | 2023-07-19 07:34:59,159 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
dn3_1    | 2023-07-19 07:35:14,972 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:59290 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 2023-07-19 07:35:27,757 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:28,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn5_1    | STARTUP_MSG:   java = 11.0.19
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn4_1    | STARTUP_MSG:   java = 11.0.19
dn5_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn5_1    | ************************************************************/
dn5_1    | 2023-07-19 07:34:14,815 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-07-19 07:34:15,375 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-07-19 07:34:16,501 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-07-19 07:34:18,826 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-07-19 07:34:18,834 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-07-19 07:34:20,968 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:aff5372d1efd ip:10.9.0.21
dn5_1    | 2023-07-19 07:34:23,511 [main] INFO reflections.Reflections: Reflections took 1980 ms to scan 2 urls, producing 107 keys and 232 values 
dn5_1    | 2023-07-19 07:34:28,057 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn5_1    | 2023-07-19 07:34:28,767 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-07-19 07:34:30,886 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-07-19T07:33:37.456Z
dn5_1    | 2023-07-19 07:34:31,081 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-07-19 07:34:31,084 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-07-19 07:34:31,094 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-07-19 07:34:31,416 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-07-19 07:34:31,723 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-07-19 07:34:31,752 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-19T07:33:37.443Z
dn5_1    | 2023-07-19 07:34:31,812 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-07-19 07:34:31,842 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-07-19 07:34:31,885 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-07-19 07:34:37,082 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-60df3697-66a9-490b-9346-c4173742dcbd/container.db to cache
dn5_1    | 2023-07-19 07:34:37,082 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-60df3697-66a9-490b-9346-c4173742dcbd/container.db for volume DS-60df3697-66a9-490b-9346-c4173742dcbd
dn5_1    | 2023-07-19 07:34:37,493 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn5_1    | 2023-07-19 07:34:41,760 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-07-19 07:34:41,782 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 4s
dn5_1    | 2023-07-19 07:34:54,724 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-07-19 07:34:55,171 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-07-19 07:34:55,623 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-07-19 07:34:56,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-07-19 07:34:56,186 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-07-19 07:34:56,235 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-07-19 07:34:56,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-07-19 07:34:56,306 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-07-19 07:34:56,328 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-07-19 07:34:56,338 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-07-19 07:34:56,418 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:34:56,419 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-07-19 07:34:56,441 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:34:56,747 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-07-19 07:34:56,874 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-07-19 07:34:56,886 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-07-19 07:35:00,589 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-07-19 07:35:00,592 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-07-19 07:35:00,615 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-07-19 07:35:00,615 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:35:00,626 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-07-19 07:35:00,646 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-07-19 07:35:00,660 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: found a subdirectory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn5_1    | 2023-07-19 07:35:00,750 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-8563B54DD732:[] returns group-8563B54DD732:java.util.concurrent.CompletableFuture@2a401295[Not completed]
dn4_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | ************************************************************/
dn4_1    | 2023-07-19 07:34:15,477 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-07-19 07:34:15,918 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-07-19 07:34:17,290 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-07-19 07:34:19,444 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-07-19 07:34:19,444 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-07-19 07:34:21,556 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6cb42cec657e ip:10.9.0.20
dn4_1    | 2023-07-19 07:34:24,176 [main] INFO reflections.Reflections: Reflections took 2212 ms to scan 2 urls, producing 107 keys and 232 values 
dn4_1    | 2023-07-19 07:34:27,656 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
dn4_1    | 2023-07-19 07:34:28,216 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-07-19 07:34:29,958 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/hdds/scmUsed: 4651 at 2023-07-19T07:33:37.621Z
dn4_1    | 2023-07-19 07:34:30,172 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-07-19 07:34:30,175 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-07-19 07:34:30,252 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-07-19 07:34:30,516 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-07-19 07:34:30,667 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-07-19 07:34:30,700 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info found in /data/metadata/ratis/scmUsed: 4096 at 2023-07-19T07:33:37.680Z
dn4_1    | 2023-07-19 07:34:30,735 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-07-19 07:34:30,744 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-07-19 07:34:30,754 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-07-19 07:34:36,486 [main] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-6db5d1ea-5257-417c-a8d8-154d91a1a46b/container.db to cache
dn4_1    | 2023-07-19 07:34:36,487 [main] INFO volume.HddsVolume: SchemaV3 db is loaded at /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/DS-6db5d1ea-5257-417c-a8d8-154d91a1a46b/container.db for volume DS-6db5d1ea-5257-417c-a8d8-154d91a1a46b
dn4_1    | 2023-07-19 07:34:36,763 [Thread-3] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
dn4_1    | 2023-07-19 07:34:41,488 [Thread-3] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-07-19 07:34:41,514 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 5s
dn4_1    | 2023-07-19 07:34:54,157 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-07-19 07:34:54,881 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-07-19 07:34:55,364 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-07-19 07:34:55,501 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-07-19 07:34:55,514 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-07-19 07:34:55,520 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-07-19 07:34:55,540 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-07-19 07:34:55,544 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-07-19 07:34:55,544 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-07-19 07:34:55,545 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-07-19 07:34:55,551 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:34:55,563 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-07-19 07:34:55,571 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-07-19 07:34:55,675 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-07-19 07:34:55,711 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-07-19 07:34:55,770 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-07-19 07:35:00,515 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-07-19 07:35:00,560 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-07-19 07:35:00,618 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-07-19 07:35:00,630 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:00,630 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:35:00,633 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-07-19 07:35:00,788 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: found a subdirectory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn4_1    | 2023-07-19 07:35:00,986 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: addNew group-8563B54DD732:[] returns group-8563B54DD732:java.util.concurrent.CompletableFuture@7fd8437[Not completed]
dn4_1    | 2023-07-19 07:35:00,987 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: found a subdirectory /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd
dn4_1    | 2023-07-19 07:35:01,001 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: addNew group-20996759F8FD:[] returns group-20996759F8FD:java.util.concurrent.CompletableFuture@76deb719[Not completed]
dn4_1    | 2023-07-19 07:35:01,001 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: found a subdirectory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn4_1    | 2023-07-19 07:35:01,002 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: addNew group-03BEFB15C8DE:[] returns group-03BEFB15C8DE:java.util.concurrent.CompletableFuture@2dcc7b2b[Not completed]
dn4_1    | 2023-07-19 07:35:01,330 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-07-19 07:35:01,657 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: new RaftServerImpl for group-8563B54DD732:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-07-19 07:35:01,705 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-07-19 07:35:01,746 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-19 07:35:19,955 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:19,971 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:20,956 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:20,972 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:21,957 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:21,973 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:22,958 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:22,974 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:23,960 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:23,976 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:24,961 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:24,977 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:25,962 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:25,979 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:26,965 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:26,980 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:27,966 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:27,982 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:28,967 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:28,983 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:28,984 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 751115a4ca9e/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn5_1    | 2023-07-19 07:35:00,750 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: found a subdirectory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn5_1    | 2023-07-19 07:35:00,752 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-03BEFB15C8DE:[] returns group-03BEFB15C8DE:java.util.concurrent.CompletableFuture@781a85d4[Not completed]
dn5_1    | 2023-07-19 07:35:00,752 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: found a subdirectory /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3
dn5_1    | 2023-07-19 07:35:00,770 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-49D8FC4B32B3:[] returns group-49D8FC4B32B3:java.util.concurrent.CompletableFuture@1940eac1[Not completed]
dn5_1    | 2023-07-19 07:35:01,200 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-07-19 07:35:01,736 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-8563B54DD732:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-07-19 07:35:01,781 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-07-19 07:35:01,849 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-07-19 07:35:01,857 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-07-19 07:35:01,876 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:35:01,891 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-07-19 07:35:01,895 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-07-19 07:35:02,222 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-07-19 07:35:02,266 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-07-19 07:35:02,359 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-07-19 07:35:02,363 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-07-19 07:35:02,515 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om1_1    | 2023-07-19 07:35:01,167 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om1_1    | 2023-07-19 07:35:03,170 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om1_1    | 2023-07-19 07:35:05,175 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om1_1    | 2023-07-19 07:35:20,690 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om1_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om1_1    | 2023-07-19 07:35:22,700 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om1_1    | 2023-07-19 07:35:24,702 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om1_1    | 2023-07-19 07:35:26,731 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om1_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om1_1    | 2023-07-19 07:35:33,299 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b28076e1-4ec3-4254-8902-2272d74360c6 is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
dn1_1    | 	... 12 more
dn1_1    | 2023-07-19 07:35:29,986 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:30,988 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:31,989 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:32,990 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:33,982 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 751115a4ca9e/10.9.0.17 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:52090 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:52090 remote=scm2/10.9.0.15:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-07-19 07:35:28,758 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:29,740 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:30,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:31,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:32,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:33,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:34,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:35,747 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:36,748 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:37,542 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-07-19 07:35:37,551 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn2_1    | 2023-07-19 07:35:37,748 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:38,017 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn2_1    | 2023-07-19 07:35:38,020 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn2_1    | 2023-07-19 07:35:38,167 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/in_use.lock acquired by nodename 7@e710b1935f2e
dn2_1    | 2023-07-19 07:35:38,227 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef} from /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/raft-meta
dn2_1    | 2023-07-19 07:35:38,746 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: set configuration 3: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:38,751 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:38,963 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO ratis.ContainerStateMachine: group-944115277F4A: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-07-19 07:35:39,763 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:40,769 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:40,879 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-19 07:35:41,000 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-19 07:35:41,003 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:35:41,016 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-19 07:35:41,032 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-19 07:35:41,074 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:35:41,175 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-19 07:35:41,190 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-19 07:35:41,195 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:35:41,326 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a
dn2_1    | 2023-07-19 07:35:41,339 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-19 07:35:41,350 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:35:41,370 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:35:41,381 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-19 07:35:41,385 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-19 07:35:41,415 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-19 07:35:41,416 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-19 07:35:41,418 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-19 07:35:41,608 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-19 07:35:41,619 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:35:41,806 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:41,839 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:35:41,844 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:35:41,859 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-19 07:35:42,467 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: set configuration 0: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:42,522 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_0-0
dn2_1    | 2023-07-19 07:35:42,550 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: set configuration 1: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:42,595 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_1-2
dn2_1    | 2023-07-19 07:35:42,630 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: set configuration 3: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:42,638 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_inprogress_3
dn2_1    | 2023-07-19 07:35:42,718 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn2_1    | 2023-07-19 07:35:42,718 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn2_1    | 2023-07-19 07:35:42,818 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-07-19 07:34:14,032 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 756d9e25a1e4/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:44Z
recon_1  | STARTUP_MSG:   java = 11.0.19
dn4_1    | 2023-07-19 07:35:01,756 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-07-19 07:35:01,770 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:01,774 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:35:01,804 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-07-19 07:35:02,159 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-07-19 07:35:02,187 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-07-19 07:35:02,206 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-07-19 07:35:02,209 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-07-19 07:35:02,325 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:02,346 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn4_1    | 2023-07-19 07:35:02,393 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-07-19 07:35:02,490 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-07-19 07:35:02,492 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-07-19 07:35:03,010 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-07-19 07:35:03,914 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-07-19 07:35:03,965 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:35:03,974 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-07-19 07:35:03,986 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,013 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-07-19 07:35:04,026 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,063 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: new RaftServerImpl for group-20996759F8FD:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-07-19 07:35:04,069 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-07-19 07:35:04,073 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-07-19 07:35:04,074 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-07-19 07:35:04,075 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:04,076 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:35:04,077 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-07-19 07:35:04,086 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-07-19 07:35:04,087 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-07-19 07:35:04,090 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-07-19 07:35:04,094 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-07-19 07:35:04,101 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:04,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 2023-07-19 07:35:04,109 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-07-19 07:35:04,110 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-07-19 07:35:04,111 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-07-19 07:35:04,131 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-07-19 07:35:04,135 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:35:04,141 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-07-19 07:35:04,143 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,143 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-07-19 07:35:04,145 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,196 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: new RaftServerImpl for group-03BEFB15C8DE:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-07-19 07:35:04,205 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-07-19 07:35:04,207 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-07-19 07:35:04,218 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-07-19 07:35:43,823 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:44,833 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:45,453 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: start as a follower, conf=3: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:45,457 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn2_1    | 2023-07-19 07:35:45,566 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState
dn2_1    | 2023-07-19 07:35:45,646 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-19 07:35:45,647 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-19 07:35:45,787 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-944115277F4A,id=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn2_1    | 2023-07-19 07:35:45,796 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-19 07:35:45,803 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-19 07:35:45,812 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-19 07:35:45,813 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-19 07:35:45,834 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:45,913 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start RPC server
dn2_1    | 2023-07-19 07:35:45,974 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: GrpcService started, listening on 9858
dn2_1    | 2023-07-19 07:35:45,983 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: GrpcService started, listening on 9856
dn2_1    | 2023-07-19 07:35:45,992 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: GrpcService started, listening on 9857
dn2_1    | 2023-07-19 07:35:46,037 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: Started
dn2_1    | 2023-07-19 07:35:46,059 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef is started using port 9858 for RATIS
dn2_1    | 2023-07-19 07:35:46,060 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-07-19 07:35:46,060 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-07-19 07:35:46,099 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-07-19 07:35:46,186 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-19 07:35:46,839 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:47,840 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:48,842 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:49,843 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:50,701 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139257528ns, electionTimeout:5052ms
dn2_1    | 2023-07-19 07:35:50,703 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState
dn2_1    | 2023-07-19 07:35:50,703 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn2_1    | 2023-07-19 07:35:50,708 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-19 07:35:50,709 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1
dn2_1    | 2023-07-19 07:35:50,733 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:50,737 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn2_1    | 2023-07-19 07:35:50,825 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:50,826 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn2_1    | 2023-07-19 07:35:50,826 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1
dn2_1    | 2023-07-19 07:35:50,826 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn2_1    | 2023-07-19 07:35:50,827 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-944115277F4A with new leaderId: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn2_1    | 2023-07-19 07:35:50,848 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: change Leader from null to 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef at term 4 for becomeLeader, leader elected after 52174ms
dn2_1    | 2023-07-19 07:35:50,859 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:51,007 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-07-19 07:35:51,060 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:35:51,068 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-07-19 07:35:51,121 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-07-19 07:35:51,167 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-07-19 07:35:51,169 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-07-19 07:35:51,391 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:35:51,461 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-07-19 07:35:51,535 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderStateImpl
dn2_1    | 2023-07-19 07:35:51,733 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn2_1    | 2023-07-19 07:35:51,862 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:51,865 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_inprogress_3 to /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_3-4
dn2_1    | 2023-07-19 07:35:52,117 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderElection1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: set configuration 5: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:35:52,121 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/current/log_inprogress_5
dn2_1    | 2023-07-19 07:35:52,862 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:53,867 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:54,868 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:55,869 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:56,871 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:57,872 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:58,873 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:35:59,874 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:36:00,875 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:36:01,876 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:36:02,877 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:36:16,174 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-07-19 07:36:46,186 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-19 07:37:21,851 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn2_1    | 2023-07-19 07:37:21,853 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn2_1    | 2023-07-19 07:37:21,854 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-07-19 07:37:21,854 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn2_1    | 2023-07-19 07:37:21,857 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-07-19 07:37:21,857 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-07-19 07:37:21,858 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-07-19 07:37:21,858 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn2_1    | 2023-07-19 07:37:21,859 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn2_1    | 2023-07-19 07:37:21,859 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn2_1    | 2023-07-19 07:37:21,860 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn2_1    | 2023-07-19 07:37:46,187 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-19 07:37:51,851 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn1_1    | 2023-07-19 07:35:33,991 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:34,993 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:35,995 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:36,996 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:37,582 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-07-19 07:35:37,594 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn1_1    | 2023-07-19 07:35:37,979 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn1_1    | 2023-07-19 07:35:37,981 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a9b83a7e-59b8-4455-b30a-c01eee264fbd
dn1_1    | 2023-07-19 07:35:37,997 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:38,104 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/in_use.lock acquired by nodename 7@751115a4ca9e
dn1_1    | 2023-07-19 07:35:38,186 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=a9b83a7e-59b8-4455-b30a-c01eee264fbd} from /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/raft-meta
dn1_1    | 2023-07-19 07:35:38,674 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: set configuration 3: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:38,786 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO ratis.ContainerStateMachine: group-AA6EE9A1AF61: Setting the last applied index to (t:3, i:4)
dn1_1    | 2023-07-19 07:35:39,000 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:39,812 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-19 07:35:39,925 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-19 07:35:39,933 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-19 07:35:39,942 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-19 07:35:39,955 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-19 07:35:39,985 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-19 07:35:40,105 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:40,149 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-19 07:35:40,154 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-07-19 07:35:40,158 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-19 07:35:40,188 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61
dn1_1    | 2023-07-19 07:35:40,190 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-07-19 07:35:40,192 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-07-19 07:35:40,197 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-19 07:35:40,200 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-07-19 07:35:40,201 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-19 07:35:40,243 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-07-19 07:35:40,248 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-07-19 07:35:40,256 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-07-19 07:35:40,420 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-07-19 07:35:40,426 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-19 07:35:40,511 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-19 07:35:40,516 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-19 07:35:40,517 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-19 07:35:40,855 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: set configuration 0: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:40,862 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_0-0
dn1_1    | 2023-07-19 07:35:40,913 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: set configuration 1: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:40,930 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_1-2
dn1_1    | 2023-07-19 07:35:40,944 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: set configuration 3: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:40,944 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_inprogress_3
dn1_1    | 2023-07-19 07:35:40,952 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn1_1    | 2023-07-19 07:35:40,952 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn1_1    | 2023-07-19 07:35:41,108 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:41,387 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: start as a follower, conf=3: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:41,390 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn1_1    | 2023-07-19 07:35:41,403 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState
dn1_1    | 2023-07-19 07:35:41,483 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-19 07:35:41,484 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-19 07:35:41,484 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AA6EE9A1AF61,id=a9b83a7e-59b8-4455-b30a-c01eee264fbd
dn1_1    | 2023-07-19 07:35:41,501 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-07-19 07:35:41,526 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-19 07:35:41,530 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-07-19 07:35:41,569 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om1_1    | , while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om1_1    | 2023-07-19 07:35:35,301 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 26bb3b489ab7/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om1_1    | 2023-07-19 07:35:37,512 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Suggested leader is Server:scm2:9863.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om1_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om1_1    | 2023-07-19 07:35:46,323 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om1_1    | 2023-07-19 07:35:46,507 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-07-19 07:35:48,808 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om1_1    | 2023-07-19 07:35:53,602 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-07-19 07:35:53,967 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om1_1    | 2023-07-19 07:35:54,018 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-07-19 07:35:54,479 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om1_1    | 2023-07-19 07:35:54,503 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om1_1    | 2023-07-19 07:35:54,700 [main] WARN om.OzoneManager: Prepare marker file index 112 does not match DB prepare index 111. Writing DB index to prepare file and maintaining prepared state.
om1_1    | 2023-07-19 07:35:54,711 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 111 to file /data/metadata/current/prepareMarker
om1_1    | 2023-07-19 07:35:55,220 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-07-19 07:35:55,224 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-07-19 07:35:55,389 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
om1_1    | 2023-07-19 07:35:55,405 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-07-19 07:35:58,305 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-07-19 07:35:58,399 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-07-19 07:35:58,953 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-07-19 07:35:59,107 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:112)
om1_1    | 2023-07-19 07:36:00,112 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-07-19 07:36:00,313 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-07-19 07:36:00,357 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-07-19 07:36:00,366 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-07-19 07:36:00,370 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-07-19 07:36:00,370 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-07-19 07:36:00,381 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-07-19 07:36:00,398 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-07-19 07:36:00,459 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-07-19 07:36:00,470 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-07-19 07:36:00,483 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
recon_1  | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn4_1    | 2023-07-19 07:35:04,219 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:35:02,529 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
dn1_1    | 2023-07-19 07:35:41,600 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start RPC server
dn3_1    | 2023-07-19 07:35:15,948 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | 2023-07-19 07:36:00,775 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-07-19 07:36:00,840 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-07-19 07:36:00,841 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-07-19 07:36:03,593 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-07-19 07:37:51,867 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: remove    LEADER 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A:t4, leader=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, voted=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, raftlog=Memoized:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLog:OPENED:c6, conf=5: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn2_1    | 2023-07-19 07:37:51,870 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: shutdown
dn2_1    | 2023-07-19 07:37:51,870 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-944115277F4A,id=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn3_1    | 2023-07-19 07:35:15,975 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:16,950 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:04,219 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:35:04,219 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-07-19 07:35:04,221 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-07-19 07:34:15,924 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
dn5_1    | 2023-07-19 07:35:02,665 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-19 07:37:51,871 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-LeaderStateImpl
dn2_1    | 2023-07-19 07:37:51,876 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-07-19 07:35:04,224 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-07-19 07:35:02,751 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-07-19 07:35:02,832 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:35:16,976 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:17,614 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
dn4_1    | 2023-07-19 07:35:04,228 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-19 07:37:51,880 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-944115277F4A: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/sm/snapshot.4_6
dn2_1    | 2023-07-19 07:37:51,883 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater: set stopIndex = 6
dn2_1    | 2023-07-19 07:37:51,884 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-944115277F4A: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a/sm/snapshot.4_6 took: 5 ms
dn2_1    | 2023-07-19 07:37:51,886 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater: Took a snapshot at index 6
dn1_1    | 2023-07-19 07:35:41,664 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: a9b83a7e-59b8-4455-b30a-c01eee264fbd: GrpcService started, listening on 9858
dn4_1    | 2023-07-19 07:35:04,229 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-19 07:35:41,700 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: a9b83a7e-59b8-4455-b30a-c01eee264fbd: GrpcService started, listening on 9856
dn1_1    | 2023-07-19 07:35:41,715 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: a9b83a7e-59b8-4455-b30a-c01eee264fbd: GrpcService started, listening on 9857
dn1_1    | 2023-07-19 07:35:41,761 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a9b83a7e-59b8-4455-b30a-c01eee264fbd: Started
dn1_1    | 2023-07-19 07:35:41,768 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9b83a7e-59b8-4455-b30a-c01eee264fbd is started using port 9858 for RATIS
dn1_1    | 2023-07-19 07:35:41,768 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9b83a7e-59b8-4455-b30a-c01eee264fbd is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-07-19 07:35:41,768 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a9b83a7e-59b8-4455-b30a-c01eee264fbd is started using port 9856 for RATIS_SERVER
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | ************************************************************/
recon_1  | 2023-07-19 07:34:14,196 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | 2023-07-19 07:34:21,778 [main] INFO reflections.Reflections: Reflections took 624 ms to scan 1 urls, producing 20 keys and 75 values 
recon_1  | 2023-07-19 07:34:29,476 [main] INFO reflections.Reflections: Reflections took 1351 ms to scan 3 urls, producing 132 keys and 288 values 
recon_1  | 2023-07-19 07:34:30,066 [main] INFO recon.ReconServer: Initializing Recon server...
om1_1    | 2023-07-19 07:36:03,615 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-07-19 07:36:03,624 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-07-19 07:36:03,624 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-07-19 07:34:10,812 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 2e956d7203d7/10.9.0.13
dn5_1    | 2023-07-19 07:35:03,391 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:35:04,319 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | STARTUP_MSG:   args = [--upgrade]
dn5_1    | 2023-07-19 07:35:04,360 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:35:04,361 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-07-19 07:35:04,361 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-07-19 07:35:04,449 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | STARTUP_MSG:   host = 05bec1501e17/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--upgrade]
dn2_1    | 2023-07-19 07:37:51,887 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater] INFO impl.StateMachineUpdater: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn1_1    | 2023-07-19 07:35:41,940 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | Caused by: java.util.concurrent.TimeoutException
om3_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1  | 2023-07-19 07:34:30,312 [main] INFO impl.ReconDBProvider: Last known Recon DB : /data/metadata/recon/recon-container-key.db_1689751158478
recon_1  | 2023-07-19 07:34:33,146 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-07-19 07:34:43,555 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1  | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn5_1    | 2023-07-19 07:35:04,477 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-07-19 07:35:04,650 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-03BEFB15C8DE:[] with ContainerStateMachine:uninitialized
dn4_1    | 2023-07-19 07:35:04,231 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:35:04,232 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1    | 2023-07-19 07:36:03,625 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-07-19 07:36:03,635 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-07-19 07:36:03,654 [om1-impl-thread1] INFO server.RaftServer: om1: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-07-19 07:36:03,713 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@6459f4ea[Not completed]
om1_1    | 2023-07-19 07:36:03,714 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-07-19 07:36:03,740 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-07-19 07:36:03,848 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-07-19 07:36:03,878 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-19 07:35:42,083 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-19 07:35:42,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:44Z
dn2_1    | 2023-07-19 07:37:51,892 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: closes. applyIndex: 6
dn2_1    | 2023-07-19 07:37:52,229 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A-SegmentedRaftLogWorker close()
dn2_1    | 2023-07-19 07:37:52,240 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-944115277F4A: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/7b7ee2aa-396d-4f71-b4bf-944115277f4a
dn2_1    | 2023-07-19 07:37:52,241 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a command on datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef.
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
dn1_1    | 2023-07-19 07:35:43,150 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:44,151 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 2023-07-19 07:35:04,653 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-07-19 07:35:04,655 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | STARTUP_MSG:   java = 11.0.19
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-07-19 07:34:48,345 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
dn2_1    | 2023-07-19 07:38:21,908 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: new RaftServerImpl for group-14D0CFE0E993:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-07-19 07:35:44,806 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-07-19 07:34:08,595 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
dn4_1    | 2023-07-19 07:35:04,233 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:44Z
om3_1    | STARTUP_MSG:   java = 11.0.19
recon_1  | 2023-07-19 07:34:48,364 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
scm3_1   | Waiting for the service scm2:9894
dn5_1    | 2023-07-19 07:35:04,659 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1  | 2023-07-19 07:34:48,364 [main] INFO recon.ReconServer: Creating Recon Schema.
om3_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-07-19 07:38:21,911 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: ConfigurationManager, init=-1: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-07-19 07:38:21,913 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-19 07:38:21,914 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: addNew group-14D0CFE0E993:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-14D0CFE0E993:java.util.concurrent.CompletableFuture@6e9e8b52[Not completed]
dn2_1    | 2023-07-19 07:38:21,924 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-07-19 07:36:03,897 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-07-19 07:36:03,897 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | 2023-07-19 07:35:04,661 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 2023-07-19 07:34:54,441 [main] INFO codegen.SqlDbUtils: FILE_COUNT_BY_SIZE table already exists, skipping creation.
om3_1    | ************************************************************/
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 2023-07-19 07:38:21,924 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-19 07:38:21,924 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:38:21,924 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:21,924 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-07-19 07:38:21,925 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-19 07:38:21,925 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:35:04,661 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-07-19 07:34:54,516 [main] INFO codegen.SqlDbUtils: CLUSTER_GROWTH_DAILY table already exists, skipping creation.
recon_1  | 2023-07-19 07:34:54,597 [main] INFO codegen.SqlDbUtils: CONTAINER_COUNT_BY_SIZE table already exists, skipping creation.
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 2023-07-19 07:38:21,930 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-19 07:38:21,930 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-19 07:38:21,930 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-19 07:38:21,931 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,238 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-07-19 07:35:04,245 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 2023-07-19 07:35:04,263 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:35:04,661 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-07-19 07:34:54,775 [main] INFO codegen.SqlDbUtils: GLOBAL_STATS table already exists, skipping creation.
om3_1    | 2023-07-19 07:34:11,027 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
scm2_1   | Waiting for the service scm1:9894
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om2_1    | ************************************************************/
dn2_1    | 2023-07-19 07:38:21,931 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-07-19 07:35:04,274 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:35:04,277 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-07-19 07:35:04,661 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 2023-07-19 07:34:54,959 [main] INFO codegen.SqlDbUtils: UNHEALTHY_CONTAINERS table already exists, skipping creation.
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 	... 1 more
dn3_1    | 2023-07-19 07:35:17,951 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:17,977 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:34:16,032 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-07-19 07:38:21,931 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-19 07:38:21,932 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3937008a-fbf0-499b-90a2-14d0cfe0e993 does not exist. Creating ...
dn2_1    | 2023-07-19 07:38:21,937 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3937008a-fbf0-499b-90a2-14d0cfe0e993/in_use.lock acquired by nodename 7@e710b1935f2e
scm3_1   | 2023-07-19 07:35:34,461 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
dn5_1    | 2023-07-19 07:35:04,662 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 2023-07-19 07:34:55,072 [main] INFO codegen.SqlDbUtils: RECON_TASK_STATUS table already exists, skipping creation.
om3_1    | 2023-07-19 07:34:26,367 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-07-19 07:36:03,897 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-07-19 07:36:03,898 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-07-19 07:34:30,595 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-07-19 07:34:36,084 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | 2023-07-19 07:38:21,939 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3937008a-fbf0-499b-90a2-14d0cfe0e993 has been successfully formatted.
dn2_1    | 2023-07-19 07:38:21,988 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO ratis.ContainerStateMachine: group-14D0CFE0E993: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-19 07:38:21,989 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-19 07:38:21,990 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:35:04,662 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-07-19 07:35:04,663 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-07-19 07:35:04,666 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om1_1    | 2023-07-19 07:36:03,902 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-07-19 07:36:04,105 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
s3g_1    | 2023-07-19 07:34:14,293 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
dn2_1    | 2023-07-19 07:38:21,990 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:21,990 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-19 07:38:21,990 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-19 07:38:21,990 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | /************************************************************
recon_1  | 2023-07-19 07:34:55,583 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-07-19 07:34:55,707 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-07-19 07:35:04,666 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:18,952 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:18,978 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:34:36,672 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-07-19 07:34:36,678 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
dn2_1    | 2023-07-19 07:38:21,998 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-07-19 07:38:21,998 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-19 07:38:21,998 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:21,998 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3937008a-fbf0-499b-90a2-14d0cfe0e993
recon_1  | 2023-07-19 07:34:56,192 [main] INFO util.log: Logging initialized @60570ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-07-19 07:34:57,534 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-07-19 07:34:57,636 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
dn5_1    | 2023-07-19 07:35:04,666 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-19 07:35:19,044 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 69ed6f01c113/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42972 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om2_1    | 2023-07-19 07:34:36,743 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:34:37,306 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = QUOTA (version = 6)
scm1_1   | STARTUP_MSG:   host = 95d08cfc10eb/10.9.0.14
scm1_1   | STARTUP_MSG:   args = []
scm1_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
recon_1  | 2023-07-19 07:34:57,706 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-07-19 07:35:04,668 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-07-19 07:35:04,669 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1    | 2023-07-19 07:36:04,119 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-07-19 07:36:04,185 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om2_1    | 2023-07-19 07:34:41,674 [main] INFO reflections.Reflections: Reflections took 3218 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
s3g_1    | 2023-07-19 07:34:14,306 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | STARTUP_MSG:   java = 11.0.19
scm1_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm1_1   | ************************************************************/
scm1_1   | 2023-07-19 07:34:08,789 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-07-19 07:35:04,278 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | STARTUP_MSG:   host = 43575e796980/10.9.0.16
recon_1  | 2023-07-19 07:34:57,730 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-07-19 07:34:57,744 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 2023-07-19 07:36:04,186 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-07-19 07:36:04,362 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-07-19 07:34:42,060 [main] INFO upgrade.OMLayoutVersionManager: Registering Upgrade Action : QuotaRepairUpgradeAction
om2_1    | 2023-07-19 07:34:42,386 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:34:32,402 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-07-19 07:34:33,183 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-07-19 07:34:33,183 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-07-19 07:34:33,291 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:34:33,859 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = QUOTA (version = 6)
om3_1    | 2023-07-19 07:34:39,083 [main] INFO reflections.Reflections: Reflections took 3735 ms to scan 1 urls, producing 139 keys and 398 values [using 2 cores]
om3_1    | 2023-07-19 07:34:39,278 [main] INFO upgrade.OMLayoutVersionManager: Registering Upgrade Action : QuotaRepairUpgradeAction
scm3_1   | STARTUP_MSG:   args = []
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-07-19 07:35:45,157 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:46,159 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:46,548 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO impl.FollowerState: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5145320589ns, electionTimeout:5054ms
dn1_1    | 2023-07-19 07:35:46,550 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: shutdown a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState
dn1_1    | 2023-07-19 07:35:46,551 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
scm3_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn1_1    | 2023-07-19 07:35:46,554 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn1_1    | 2023-07-19 07:35:46,560 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-FollowerState] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1
dn1_1    | 2023-07-19 07:35:46,610 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:46,616 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1 PRE_VOTE round 0: result PASSED (term=3)
dn1_1    | 2023-07-19 07:35:46,832 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1 ELECTION round 0: submit vote requests at term 4 for 3: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:35:46,833 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1 ELECTION round 0: result PASSED (term=4)
dn1_1    | 2023-07-19 07:35:46,833 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: shutdown a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1
dn1_1    | 2023-07-19 07:35:46,845 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn1_1    | 2023-07-19 07:35:46,845 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-AA6EE9A1AF61 with new leaderId: a9b83a7e-59b8-4455-b30a-c01eee264fbd
dn1_1    | 2023-07-19 07:35:46,853 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: change Leader from null to a9b83a7e-59b8-4455-b30a-c01eee264fbd at term 4 for becomeLeader, leader elected after 45643ms
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn1_1    | 2023-07-19 07:35:47,016 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-07-19 07:35:04,287 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-07-19 07:35:04,287 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-07-19 07:35:04,358 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-07-19 07:35:04,502 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-07-19 07:35:04,729 [main] INFO util.log: Logging initialized @66550ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-07-19 07:35:05,661 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-07-19 07:35:05,710 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
recon_1  | 2023-07-19 07:34:57,750 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-07-19 07:34:58,192 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1  | 2023-07-19 07:34:58,225 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om2_1    | 2023-07-19 07:34:47,160 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-07-19 07:34:48,794 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-07-19 07:34:55,851 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om1_1    | 2023-07-19 07:36:04,437 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om1_1    | 2023-07-19 07:36:04,543 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-07-19 07:36:04,580 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-07-19 07:36:05,297 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om1_1    | 2023-07-19 07:36:05,961 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-07-19 07:36:06,006 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-07-19 07:36:06,019 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-07-19 07:36:06,025 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-07-19 07:36:06,028 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn4_1    | 2023-07-19 07:35:05,787 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-07-19 07:34:58,984 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
om1_1    | 2023-07-19 07:36:06,034 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | STARTUP_MSG:   java = 11.0.19
om3_1    | 2023-07-19 07:34:39,468 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:34:43,712 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-07-19 07:34:44,436 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-07-19 07:34:52,296 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
dn5_1    | 2023-07-19 07:35:04,676 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:35:04,686 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:35:04,686 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
recon_1  | 2023-07-19 07:34:59,167 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
om1_1    | 2023-07-19 07:36:08,040 [main] INFO reflections.Reflections: Reflections took 3912 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
om2_1    | 2023-07-19 07:34:57,852 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
scm1_1   | 2023-07-19 07:34:09,564 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-07-19 07:34:12,947 [main] INFO reflections.Reflections: Reflections took 2644 ms to scan 3 urls, producing 132 keys and 288 values 
scm1_1   | 2023-07-19 07:34:13,749 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn5_1    | 2023-07-19 07:35:04,687 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-07-19 07:35:04,692 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-07-19 07:35:04,694 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
recon_1  | 2023-07-19 07:34:59,226 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
om1_1    | 2023-07-19 07:36:09,036 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-07-19 07:34:59,857 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om2_1    | 2023-07-19 07:35:20,712 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 2023-07-19 07:34:54,298 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om3_1    | 2023-07-19 07:34:56,315 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om3_1    | 2023-07-19 07:34:58,318 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om3_1    | 2023-07-19 07:35:00,320 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
dn4_1    | 2023-07-19 07:35:05,809 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
om1_1    | 2023-07-19 07:36:09,106 [main] INFO ipc.Server: Listener at om1:9862
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn4_1    | 2023-07-19 07:35:05,826 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1    | 2023-07-19 07:36:09,129 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
scm1_1   | 2023-07-19 07:34:13,796 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-07-19 07:34:14,495 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
dn5_1    | 2023-07-19 07:35:04,749 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-49D8FC4B32B3:[] with ContainerStateMachine:uninitialized
dn5_1    | 2023-07-19 07:35:04,754 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-07-19 07:35:04,756 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-07-19 07:35:04,763 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-07-19 07:35:04,765 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:35:04,767 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-07-19 07:35:04,769 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-07-19 07:35:04,780 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om1_1    | 2023-07-19 07:36:12,300 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
scm1_1   | 2023-07-19 07:34:14,513 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-07-19 07:34:24,670 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-07-19 07:35:05,829 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-07-19 07:35:06,442 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn4_1    | 2023-07-19 07:35:06,509 [main] INFO http.HttpServer2: Jetty bound to port 9882
om3_1    | 2023-07-19 07:35:02,322 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om3_1    | 2023-07-19 07:35:20,701 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
recon_1  | 2023-07-19 07:34:59,700 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-07-19 07:36:13,110 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-07-19 07:34:27,357 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-07-19 07:34:30,148 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm1_1   | 2023-07-19 07:34:30,255 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-07-19 07:34:31,215 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-07-19 07:34:33,645 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:36a933ad-cfeb-4d3b-aa37-2c29c320331e
scm1_1   | 2023-07-19 07:34:35,685 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-07-19 07:34:35,939 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-07-19 07:34:35,947 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-07-19 07:34:35,953 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-07-19 07:34:35,956 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1  | 2023-07-19 07:34:59,730 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-07-19 07:36:13,198 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-07-19 07:36:13,199 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-07-19 07:36:13,855 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-07-19 07:36:13,859 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-07-19 07:36:13,886 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@26bb3b489ab7
om1_1    | 2023-07-19 07:36:13,897 [om1-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
dn1_1    | 2023-07-19 07:35:47,171 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:17,496 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1  | 2023-07-19 07:35:05,424 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-19 07:35:06,713 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | /************************************************************
dn4_1    | 2023-07-19 07:35:06,541 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn4_1    | 2023-07-19 07:35:06,872 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-07-19 07:35:06,872 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-07-19 07:35:06,873 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-07-19 07:35:07,024 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1b52699c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-07-19 07:36:14,004 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:36:14,056 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-07-19 07:36:14,152 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-19 07:35:47,202 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 2023-07-19 07:35:07,410 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1  | 2023-07-19 07:35:07,419 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-07-19 07:35:08,241 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-07-19 07:35:04,782 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-07-19 07:35:04,811 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-07-19 07:35:04,828 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-07-19 07:38:21,998 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn1_1    | 2023-07-19 07:35:47,203 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-07-19 07:35:47,307 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-07-19 07:35:47,320 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-07-19 07:35:47,322 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn4_1    | 2023-07-19 07:35:07,032 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1d2fb82{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | 2023-07-19 07:34:14,760 [main] INFO util.log: Logging initialized @17381ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-07-19 07:34:16,788 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-07-19 07:34:17,219 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-07-19 07:34:17,277 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-07-19 07:34:17,296 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-07-19 07:34:17,309 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-07-19 07:34:17,312 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | STARTUP_MSG:   host = de73b91882ca/10.9.0.15
dn1_1    | 2023-07-19 07:35:47,484 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-19 07:35:47,506 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-07-19 07:35:47,536 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderStateImpl
recon_1  | 2023-07-19 07:35:08,701 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
om2_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
dn4_1    | 2023-07-19 07:35:07,987 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@49c1e294{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-17425267236405301442/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
dn4_1    | 2023-07-19 07:35:08,042 [main] INFO server.AbstractConnector: Started ServerConnector@5a4d4f9c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-07-19 07:35:08,051 [main] INFO server.Server: Started @69872ms
dn4_1    | 2023-07-19 07:35:08,087 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-07-19 07:35:08,214 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-07-19 07:35:08,220 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-07-19 07:35:08,376 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn4_1    | 2023-07-19 07:35:08,574 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
scm2_1   | STARTUP_MSG:   args = []
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | 2023-07-19 07:35:47,641 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
recon_1  | 2023-07-19 07:35:08,851 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-07-19 07:35:09,048 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om2_1    | 2023-07-19 07:35:22,714 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
scm2_1   | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | 2023-07-19 07:35:08,589 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn1_1    | 2023-07-19 07:35:47,730 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_inprogress_3 to /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_3-4
recon_1  | 2023-07-19 07:35:09,909 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/140718b2-320a-4020-b7ea-662533776c74
recon_1  | 2023-07-19 07:35:09,929 [main] INFO node.SCMNodeManager: Registered Data node : 140718b2-320a-4020-b7ea-662533776c74{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-07-19 07:35:24,716 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
om1_1    | 2023-07-19 07:36:14,163 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-07-19 07:36:14,182 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-07-19 07:35:47,793 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderElection1] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: set configuration 5: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:35:26,741 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 2023-07-19 07:35:09,959 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn5_1    | 2023-07-19 07:35:04,850 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:35:04,854 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-07-19 07:35:04,855 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-07-19 07:35:04,883 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-07-19 07:35:04,897 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:35:04,947 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:35:04,956 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-07-19 07:38:22,000 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:38:22,000 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:22,002 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-19 07:38:22,002 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-19 07:38:22,002 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 2023-07-19 07:35:09,973 [main] INFO node.SCMNodeManager: Registered Data node : 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-19 07:35:09,975 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9b83a7e-59b8-4455-b30a-c01eee264fbd
recon_1  | 2023-07-19 07:35:09,976 [main] INFO node.SCMNodeManager: Registered Data node : a9b83a7e-59b8-4455-b30a-c01eee264fbd{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-19 07:35:09,988 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adc6845d-6cb4-4b43-88ca-47ca3f1a71df
recon_1  | 2023-07-19 07:35:09,995 [main] INFO node.SCMNodeManager: Registered Data node : adc6845d-6cb4-4b43-88ca-47ca3f1a71df{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-19 07:35:09,997 [main] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02af6ab-c12d-469b-a775-f6b30900ff13
recon_1  | 2023-07-19 07:35:10,014 [main] INFO node.SCMNodeManager: Registered Data node : f02af6ab-c12d-469b-a775-f6b30900ff13{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-07-19 07:35:10,014 [main] INFO scm.ReconNodeManager: Loaded 5 nodes from node DB.
recon_1  | 2023-07-19 07:35:10,269 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 2023-07-19 07:35:13,348 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-07-19 07:35:13,598 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-07-19 07:35:13,939 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1  | 2023-07-19 07:35:13,974 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om3_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om3_1    | 2023-07-19 07:35:22,703 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om3_1    | 2023-07-19 07:35:24,705 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om3_1    | 2023-07-19 07:35:26,717 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Could not determine the leader node.
dn2_1    | 2023-07-19 07:38:22,002 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:22,002 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-19 07:38:22,018 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:22,019 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:22,096 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:38:22,101 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:38:22,101 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-19 07:38:22,104 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-19 07:38:22,105 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-19 07:38:22,108 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: start as a follower, conf=-1: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:22,109 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-07-19 07:34:35,988 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-07-19 07:34:36,028 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
s3g_1    | 2023-07-19 07:34:17,755 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir3307324086937849326
s3g_1    | 2023-07-19 07:34:18,999 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
dn5_1    | 2023-07-19 07:35:04,957 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-07-19 07:35:04,994 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-07-19 07:35:05,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-07-19 07:35:05,008 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-07-19 07:35:05,254 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42972 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
scm1_1   | 2023-07-19 07:34:36,030 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-07-19 07:34:36,113 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:34:36,132 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-07-19 07:34:36,213 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-07-19 07:34:36,292 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-07-19 07:34:36,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-07-19 07:34:36,434 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-07-19 07:34:39,993 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-07-19 07:34:40,076 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-07-19 07:34:40,108 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-07-19 07:34:40,108 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-07-19 07:34:40,122 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-07-19 07:34:40,250 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-07-19 07:34:40,403 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: found a subdirectory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
om1_1    | 2023-07-19 07:36:14,199 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-07-19 07:36:14,229 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
s3g_1    | STARTUP_MSG:   host = c8e8ca26bc39/10.9.0.23
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
dn4_1    | 2023-07-19 07:35:10,565 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn4_1    | 2023-07-19 07:35:10,565 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
dn4_1    | 2023-07-19 07:35:10,668 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 2023-07-19 07:35:14,942 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1  | 2023-07-19 07:35:14,967 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-07-19 07:35:14,968 [main] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-07-19 07:35:15,464 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-07-19 07:35:15,513 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-07-19 07:35:15,513 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-07-19 07:35:17,950 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-07-19 07:35:17,962 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1  | 2023-07-19 07:35:18,124 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-07-19 07:35:18,124 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-07-19 07:35:18,139 [main] INFO server.session: node0 Scavenging every 660000ms
om1_1    | 2023-07-19 07:36:14,292 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-07-19 07:36:14,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-07-19 07:36:14,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-07-19 07:36:14,379 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-07-19 07:36:14,387 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-07-19 07:36:14,398 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-07-19 07:36:14,411 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn5_1    | 2023-07-19 07:35:05,397 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-07-19 07:35:05,874 [main] INFO util.log: Logging initialized @67327ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-07-19 07:35:07,407 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-07-19 07:35:07,560 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-07-19 07:35:07,645 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-07-19 07:35:07,700 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-07-19 07:35:07,716 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-07-19 07:35:07,717 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-07-19 07:35:08,183 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
dn5_1    | 2023-07-19 07:35:08,310 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-07-19 07:35:08,312 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn5_1    | 2023-07-19 07:35:08,684 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-07-19 07:35:08,684 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-07-19 07:35:08,728 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-07-19 07:35:08,844 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@250d440{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jgraphx-2.0.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk8-1.8.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/okio-3.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/jgraph-5.13.0.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/antlr4-runtime-4.5.3.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/jgrapht-ext-1.0.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jgrapht-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.61.Final.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-jdk7-1.8.0.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.61.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/okio-jvm-3.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.94.Final.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:44Z
s3g_1    | STARTUP_MSG:   java = 11.0.19
s3g_1    | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir3307324086937849326, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1    | ************************************************************/
s3g_1    | 2023-07-19 07:34:19,068 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-07-19 07:34:19,355 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-07-19 07:34:20,972 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-07-19 07:34:22,963 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-07-19 07:34:22,964 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
dn4_1    | 2023-07-19 07:35:10,669 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-07-19 07:35:10,742 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-07-19 07:35:10,876 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm1_1   | 2023-07-19 07:34:40,714 [main] INFO server.RaftServer: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: addNew group-05548E1F8073:[] returns group-05548E1F8073:java.util.concurrent.CompletableFuture@1174a305[Not completed]
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone/96333857ee921f43d327609f117047fbb3a7b745 ; compiled by 'runner' on 2023-07-19T06:43Z
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn2_1    | 2023-07-19 07:38:22,109 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState
dn2_1    | 2023-07-19 07:38:22,110 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-07-19 07:34:41,039 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 7#78 with transactionInfo term andIndex
recon_1  | 2023-07-19 07:35:18,219 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7fd4e815{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-07-19 07:35:47,880 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/current/log_inprogress_5
s3g_1    | 2023-07-19 07:34:23,506 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
dn5_1    | 2023-07-19 07:35:08,845 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3db65c0d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm2_1   | STARTUP_MSG:   java = 11.0.19
dn4_1    | 2023-07-19 07:35:10,889 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
dn4_1    | 2023-07-19 07:35:11,223 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-07-19 07:35:11,293 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-07-19 07:35:14,326 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:14,327 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:34:41,047 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: new RaftServerImpl for group-05548E1F8073:[] with SCMStateMachine:uninitialized
recon_1  | 2023-07-19 07:35:18,222 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e1ae763{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-07-19 07:35:48,252 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-07-19 07:34:23,572 [main] INFO http.HttpServer2: Jetty bound to port 9878
dn5_1    | 2023-07-19 07:35:10,196 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@50e5032c{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-18368555131260583964/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
scm3_1   | ************************************************************/
scm3_1   | 2023-07-19 07:35:34,471 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm1_1   | 2023-07-19 07:34:41,077 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-07-19 07:34:41,126 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1  | 2023-07-19 07:35:26,554 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@46aea9f7{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-4512923292482834755/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
dn1_1    | 2023-07-19 07:35:49,255 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-07-19 07:34:23,601 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn5_1    | 2023-07-19 07:35:10,505 [main] INFO server.AbstractConnector: Started ServerConnector@6c9b44bf{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
scm3_1   | 2023-07-19 07:35:34,527 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-07-19 07:35:14,336 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5s, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=10m, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.ack.timeout=PT15M, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.enabled=false, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.rootca.certificate.polling.interval=PT2h, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=6, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=0.0.0.0:9862, ozone.om.address.omservice.om1=om1, ozone.om.address.omservice.om2=om2, ozone.om.address.omservice.om3=om3, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=0.0.0.0:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.nodes.omservice=om1,om2,om3, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.ratis.snapshot.max.total.sst.size=100000000, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.service.ids=omservice, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.disable.native.libs=false, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=1m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.address.scmservice.scm1=scm1, ozone.scm.address.scmservice.scm2=scm2, ozone.scm.address.scmservice.scm3=scm3, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=1GB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.nodes.scmservice=scm1,scm2,scm3, ozone.scm.pipeline.allocated.timeout=2m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=30s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=1m, ozone.scm.primordial.node.id=scm1, ozone.scm.ratis.enable=true, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.service.ids=scmservice, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1  | 2023-07-19 07:35:26,596 [main] INFO server.AbstractConnector: Started ServerConnector@370ef50b{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
dn1_1    | 2023-07-19 07:35:50,256 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:51,257 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:52,258 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:34,760 [main] INFO reflections.Reflections: Reflections took 180 ms to scan 3 urls, producing 132 keys and 288 values 
dn5_1    | 2023-07-19 07:35:10,508 [main] INFO server.Server: Started @71961ms
scm2_1   | ************************************************************/
dn4_1    | 2023-07-19 07:35:15,327 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:15,329 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:16,328 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:16,330 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:38:22,113 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-19 07:38:22,114 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-14D0CFE0E993,id=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn2_1    | 2023-07-19 07:38:22,114 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-07-19 07:36:14,426 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-07-19 07:36:14,436 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-07-19 07:36:14,448 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-07-19 07:35:34,842 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-07-19 07:34:41,133 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-07-19 07:35:10,524 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-07-19 07:35:17,525 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-07-19 07:35:17,651 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-07-19 07:35:18,136 [main] INFO reflections.Reflections: Reflections took 355 ms to scan 3 urls, producing 132 keys and 288 values 
scm2_1   | 2023-07-19 07:35:18,286 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-07-19 07:35:18,298 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn3_1    | 2023-07-19 07:35:19,617 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 2023-07-19 07:38:22,115 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-19 07:38:22,115 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-07-19 07:38:22,116 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-07-19 07:34:41,147 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-07-19 07:35:34,850 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-07-19 07:35:34,875 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-07-19 07:35:34,876 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1   | 2023-07-19 07:35:36,404 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-07-19 07:35:36,867 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-07-19 07:35:37,585 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:18,362 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm1_1   | 2023-07-19 07:34:41,150 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-07-19 07:34:41,151 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-07-19 07:35:26,597 [main] INFO server.Server: Started @90974ms
recon_1  | 2023-07-19 07:35:26,611 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-07-19 07:35:26,611 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-07-19 07:35:26,617 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-07-19 07:35:26,617 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-07-19 07:35:26,647 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-07-19 07:35:37,602 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-07-19 07:35:18,363 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-07-19 07:35:20,497 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-07-19 07:35:21,127 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-07-19 07:35:21,661 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
om1_1    | 2023-07-19 07:36:14,473 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-07-19 07:36:14,474 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-07-19 07:36:14,575 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-07-19 07:36:14,582 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-07-19 07:36:14,685 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-07-19 07:36:14,688 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-07-19 07:36:14,694 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-07-19 07:36:14,756 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 112
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm2_1   | 2023-07-19 07:35:21,699 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-07-19 07:34:41,341 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-07-19 07:34:41,432 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om2_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om2_1    | 2023-07-19 07:35:33,306 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b28076e1-4ec3-4254-8902-2272d74360c6 is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om3_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om3_1    | 2023-07-19 07:35:33,298 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:b28076e1-4ec3-4254-8902-2272d74360c6 is not the leader. Could not determine the leader node.
scm2_1   | 2023-07-19 07:35:21,980 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | 2023-07-19 07:35:22,509 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:b28076e1-4ec3-4254-8902-2272d74360c6
dn2_1    | 2023-07-19 07:38:22,127 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3937008a-fbf0-499b-90a2-14d0cfe0e993
dn2_1    | 2023-07-19 07:38:22,128 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3937008a-fbf0-499b-90a2-14d0cfe0e993.
s3g_1    | 2023-07-19 07:34:23,995 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-07-19 07:35:53,259 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:54,260 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:17,329 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:17,331 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:18,330 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:18,332 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:18,425 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 6cb42cec657e/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:42424 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm2_1   | 2023-07-19 07:35:22,691 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-07-19 07:34:41,549 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 2023-07-19 07:38:22,129 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: addNew group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-075378E90876:java.util.concurrent.CompletableFuture@5402aa0b[Not completed]
s3g_1    | 2023-07-19 07:34:23,998 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-07-19 07:34:24,024 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1    | 2023-07-19 07:34:24,307 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@545b995e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-07-19 07:34:24,319 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@77102b91{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1    | 2023-07-19 07:34:38,400 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.186s without any GCs.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1_1   | 2023-07-19 07:34:41,574 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-07-19 07:34:41,765 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn2_1    | 2023-07-19 07:38:22,136 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: new RaftServerImpl for group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om1_1    | 2023-07-19 07:36:14,757 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-07-19 07:36:14,777 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:36:14,779 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om1_1    | 2023-07-19 07:36:14,810 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-07-19 07:36:14,813 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-07-19 07:36:14,815 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-07-19 07:35:55,262 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:56,263 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:57,264 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:58,266 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:35:59,267 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:36:00,269 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:38:22,137 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-07-19 07:34:41,829 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1  | 2023-07-19 07:35:26,647 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-07-19 07:35:26,651 [main] INFO recovery.ReconOmMetadataManagerImpl: Last known snapshot for OM : /data/metadata/om.snapshot.db_1689751269948
recon_1  | 2023-07-19 07:35:26,714 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-19 07:35:27,023 [main] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1689751269948.
recon_1  | 2023-07-19 07:35:27,091 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-07-19 07:35:27,093 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-07-19 07:35:37,814 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Suggested leader is Server:scm2:9860.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
dn5_1    | 2023-07-19 07:35:10,524 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-07-19 07:35:10,598 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-07-19 07:35:11,028 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn5_1    | 2023-07-19 07:35:11,567 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
dn2_1    | 2023-07-19 07:38:22,138 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-07-19 07:34:41,891 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-07-19 07:34:41,949 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-07-19 07:34:42,335 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm1_1   | 2023-07-19 07:34:44,351 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-07-19 07:34:44,361 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om1_1    | 2023-07-19 07:36:14,829 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-07-19 07:36:14,832 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-07-19 07:36:14,834 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
dn2_1    | 2023-07-19 07:38:22,138 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
s3g_1    | WARNING: An illegal reflective access operation has occurred
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm1_1   | 2023-07-19 07:34:44,394 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-07-19 07:36:14,835 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-07-19 07:36:14,838 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-07-19 07:36:14,885 [main] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-07-19 07:36:15,454 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-07-19 07:36:15,507 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn5_1    | 2023-07-19 07:35:11,600 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
dn5_1    | 2023-07-19 07:35:13,480 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
dn5_1    | 2023-07-19 07:35:13,480 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om1_1    | 2023-07-19 07:36:15,522 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-07-19 07:36:15,523 [main] INFO upgrade.UpgradeFinalizer: Skipping action QuotaRepairUpgradeAction since it has already been run.
dn2_1    | 2023-07-19 07:38:22,139 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:38:22,142 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-07-19 07:38:22,142 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 2023-07-19 07:35:13,511 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-07-19 07:34:44,397 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-07-19 07:38:22,142 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-07-19 07:35:38,215 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | 2023-07-19 07:35:22,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 2023-07-19 07:36:01,270 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
s3g_1    | WARNING: Illegal reflective access by com.sun.xml.bind.v2.runtime.reflect.opt.Injector (file:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1    | WARNING: Please consider reporting this to the maintainers of com.sun.xml.bind.v2.runtime.reflect.opt.Injector
dn2_1    | 2023-07-19 07:38:22,142 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-07-19 07:38:22,143 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
scm1_1   | 2023-07-19 07:34:44,398 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-07-19 07:36:02,271 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:36:03,272 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:36:16,184 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-07-19 07:36:42,083 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-19 07:37:42,084 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-07-19 07:34:44,418 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | 2023-07-19 07:35:07,806 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@64dfb31d{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir3307324086937849326/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-12596951870729489529/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1    | 2023-07-19 07:35:07,873 [main] INFO server.AbstractConnector: Started ServerConnector@16fdec90{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om2_1    | , while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om2_1    | 2023-07-19 07:35:35,307 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 05bec1501e17/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om2_1    | 2023-07-19 07:35:37,565 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Suggested leader is Server:scm2:9863.
scm1_1   | 2023-07-19 07:34:44,454 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
om1_1    | 2023-07-19 07:36:15,523 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:42424 remote=scm1/10.9.0.14:9861]
s3g_1    | 2023-07-19 07:35:07,875 [main] INFO server.Server: Started @70497ms
s3g_1    | 2023-07-19 07:35:07,886 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-07-19 07:35:07,886 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-07-19 07:35:07,890 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-07-19 07:39:16,461 [qtp2112233878-22] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-07-19 07:39:16,537 [qtp2112233878-22] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm1_1   | 2023-07-19 07:34:44,454 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-07-19 07:36:15,531 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
dn5_1    | 2023-07-19 07:35:13,549 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
dn5_1    | 2023-07-19 07:35:13,559 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-07-19 07:35:13,583 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn1_1    | 2023-07-19 07:37:47,852 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:35:19,953 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:19,979 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:19,987 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 69ed6f01c113/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:39440 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
scm1_1   | 2023-07-19 07:34:44,455 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-07-19 07:36:16,151 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-07-19 07:36:16,151 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-07-19 07:35:13,584 [Datanode State Machine Daemon Thread] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om3_1    | , while invoking $Proxy34.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om3_1    | 2023-07-19 07:35:35,303 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 2e956d7203d7/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy34.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om3_1    | 2023-07-19 07:35:37,483 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:36a933ad-cfeb-4d3b-aa37-2c29c320331e is not the leader. Suggested leader is Server:scm2:9863.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
dn1_1    | 2023-07-19 07:37:47,854 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn1_1    | 2023-07-19 07:37:47,858 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn1_1    | 2023-07-19 07:37:47,859 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm1_1   | 2023-07-19 07:34:44,759 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn5_1    | 2023-07-19 07:35:14,098 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-07-19 07:35:14,192 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-07-19 07:35:16,865 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:37:47,862 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-07-19 07:37:47,863 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn1_1    | 2023-07-19 07:37:47,864 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-07-19 07:37:47,864 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn1_1    | 2023-07-19 07:37:47,865 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn1_1    | 2023-07-19 07:37:47,865 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm1_1   | 2023-07-19 07:34:45,457 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1_1   | 2023-07-19 07:34:46,826 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
s3g_1    | 2023-07-19 07:39:16,560 [qtp2112233878-22] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-07-19 07:38:22,144 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-19 07:38:22,147 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-07-19 07:38:22,147 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-07-19 07:34:46,848 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
dn5_1    | 2023-07-19 07:35:16,889 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:17,866 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om1_1    | 2023-07-19 07:36:16,551 [main] INFO util.log: Logging initialized @138209ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-07-19 07:39:16,560 [qtp2112233878-22] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-07-19 07:39:18,136 [qtp2112233878-22] INFO protocolPB.GrpcOmTransport: GrpcOmTransport: started
s3g_1    | 2023-07-19 07:39:18,872 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.159s with 0.021s GC time.
s3g_1    | GC pool 'ParNew' had collection(s): count=1 time=21ms
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
dn5_1    | 2023-07-19 07:35:17,891 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:18,868 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:106)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
s3g_1    | 2023-07-19 07:39:20,034 [qtp2112233878-22] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
dn2_1    | 2023-07-19 07:38:22,148 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-19 07:38:22,149 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-19 07:37:47,865 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn1_1    | 2023-07-19 07:37:47,887 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a9b83a7e-59b8-4455-b30a-c01eee264fbd: remove    LEADER a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61:t4, leader=a9b83a7e-59b8-4455-b30a-c01eee264fbd, voted=a9b83a7e-59b8-4455-b30a-c01eee264fbd, raftlog=Memoized:a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLog:OPENED:c6, conf=5: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn1_1    | 2023-07-19 07:37:47,890 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: shutdown
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
dn5_1    | 2023-07-19 07:35:18,893 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:19,869 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:22,716 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-07-19 07:35:22,717 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
s3g_1    | 2023-07-19 07:40:40,453 [qtp2112233878-21] INFO rpc.RpcClient: Creating Bucket: s3v/new2-bucket, with bucket layout OBJECT_STORE, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false, Replication Type set to server-side default replication type, Namespace Quota set to -1, Space Quota set to -1 
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14238)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
scm3_1   | 2023-07-19 07:35:39,728 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:57eda1b0-9276-42ed-8a05-f1155bfae1c0
scm3_1   | 2023-07-19 07:35:41,351 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3_1   | 2023-07-19 07:35:41,472 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-07-19 07:35:41,488 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
dn5_1    | 2023-07-19 07:35:19,895 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:20,870 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:20,896 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:21,009 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn2_1    | 2023-07-19 07:38:22,149 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-07-19 07:38:22,149 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-07-19 07:38:22,150 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
s3g_1    | 2023-07-19 07:40:41,877 [qtp2112233878-25] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-07-19 07:36:17,749 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-07-19 07:35:22,719 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-07-19 07:35:22,720 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-07-19 07:35:22,721 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om1_1    | 2023-07-19 07:36:17,787 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-07-19 07:36:17,830 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-07-19 07:36:17,836 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
scm3_1   | 2023-07-19 07:35:41,495 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-07-19 07:35:41,500 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
dn2_1    | 2023-07-19 07:38:22,152 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/in_use.lock acquired by nodename 7@e710b1935f2e
dn2_1    | 2023-07-19 07:38:22,156 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 has been successfully formatted.
dn2_1    | 2023-07-19 07:38:22,157 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO ratis.ContainerStateMachine: group-075378E90876: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-19 07:38:22,158 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-19 07:38:22,158 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-07-19 07:37:47,891 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-AA6EE9A1AF61,id=a9b83a7e-59b8-4455-b30a-c01eee264fbd
dn1_1    | 2023-07-19 07:37:47,892 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: shutdown a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-LeaderStateImpl
scm2_1   | 2023-07-19 07:35:22,757 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1  | 2023-07-19 07:35:44,011 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-07-19 07:35:44,030 [main] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn5_1    | java.net.SocketTimeoutException: Call From aff5372d1efd/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:46808 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
scm3_1   | 2023-07-19 07:35:41,505 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-07-19 07:37:47,901 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-PendingRequests: sendNotLeaderResponses
dn1_1    | 2023-07-19 07:37:47,907 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-AA6EE9A1AF61: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/sm/snapshot.4_6
dn1_1    | 2023-07-19 07:37:47,908 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater: set stopIndex = 6
dn1_1    | 2023-07-19 07:37:47,909 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-AA6EE9A1AF61: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61/sm/snapshot.4_6 took: 2 ms
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1    | 2023-07-19 07:36:17,839 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-07-19 07:35:41,507 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-07-19 07:34:46,875 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
scm2_1   | 2023-07-19 07:35:22,768 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 2023-07-19 07:35:44,484 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn2_1    | 2023-07-19 07:38:22,163 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:22,163 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
om1_1    | 2023-07-19 07:36:17,840 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm3_1   | 2023-07-19 07:35:41,515 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-07-19 07:34:47,399 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2_1   | 2023-07-19 07:35:22,770 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
recon_1  | 2023-07-19 07:35:44,496 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn2_1    | 2023-07-19 07:38:22,163 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-07-19 07:37:47,913 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater] INFO impl.StateMachineUpdater: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om1_1    | 2023-07-19 07:36:18,264 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm3_1   | 2023-07-19 07:35:41,537 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:34:47,411 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-07-19 07:34:47,473 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1_1   | 2023-07-19 07:34:47,480 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-07-19 07:34:47,507 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-07-19 07:38:22,164 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-19 07:37:47,913 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater] INFO impl.StateMachineUpdater: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn1_1    | 2023-07-19 07:37:47,916 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: closes. applyIndex: 6
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om1_1    | 2023-07-19 07:36:18,285 [main] INFO http.HttpServer2: Jetty bound to port 9874
scm3_1   | 2023-07-19 07:35:41,556 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1   | 2023-07-19 07:35:22,771 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-07-19 07:38:22,175 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-07-19 07:37:48,058 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61-SegmentedRaftLogWorker close()
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
om1_1    | 2023-07-19 07:36:18,341 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3_1   | 2023-07-19 07:35:41,565 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-07-19 07:35:22,786 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm1_1   | 2023-07-19 07:34:47,517 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-07-19 07:34:47,562 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | 2023-07-19 07:34:47,568 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om3_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om3_1    | 2023-07-19 07:35:47,208 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om3_1    | 2023-07-19 07:35:47,659 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
scm3_1   | 2023-07-19 07:35:41,727 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-07-19 07:35:22,792 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
scm2_1   | 2023-07-19 07:35:22,793 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 2023-07-19 07:35:44,514 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-07-19 07:35:44,538 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
om1_1    | 2023-07-19 07:36:18,748 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om2_1    | , while invoking $Proxy34.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om2_1    | 2023-07-19 07:35:46,746 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:39440 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
scm2_1   | 2023-07-19 07:35:23,277 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1  | 2023-07-19 07:35:45,487 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:35880 / 10.9.0.17:35880
recon_1  | 2023-07-19 07:35:45,488 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:45342 / 10.9.0.20:45342
om1_1    | 2023-07-19 07:36:18,748 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn4_1    | 2023-07-19 07:35:19,332 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-07-19 07:35:48,305 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:57552 / 10.9.0.21:57552: output error
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
om1_1    | 2023-07-19 07:36:18,769 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-07-19 07:37:48,067 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-AA6EE9A1AF61: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/64e66587-f45d-4b2d-8462-aa6ee9a1af61
dn1_1    | 2023-07-19 07:37:48,069 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 command on datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd.
dn1_1    | 2023-07-19 07:38:17,852 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn1_1    | 2023-07-19 07:38:42,085 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-19 07:38:47,899 [PipelineCommandHandlerThread-0] INFO server.RaftServer: a9b83a7e-59b8-4455-b30a-c01eee264fbd: addNew group-08992B8D070F:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-08992B8D070F:java.util.concurrent.CompletableFuture@654f7219[Not completed]
scm1_1   | 2023-07-19 07:34:47,988 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-07-19 07:34:48,023 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-07-19 07:34:48,436 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
dn4_1    | 2023-07-19 07:35:19,333 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:23,283 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 2023-07-19 07:35:48,329 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:44628 / 10.9.0.20:44628: output error
recon_1  | 2023-07-19 07:35:48,346 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:51758 / 10.9.0.19:51758: output error
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn2_1    | 2023-07-19 07:38:22,176 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-19 07:38:22,177 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876
scm3_1   | 2023-07-19 07:35:41,782 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3_1   | 2023-07-19 07:35:41,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3_1   | 2023-07-19 07:35:45,026 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-07-19 07:35:19,379 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
scm2_1   | 2023-07-19 07:35:23,284 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-07-19 07:36:18,949 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@38848217{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 26 on default port 9891] WARN ipc.Server: IPC Server handler 26 on default port 9891, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:43420 / 10.9.0.17:43420: output error
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 20 on default port 9891] WARN ipc.Server: IPC Server handler 20 on default port 9891, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:44634 / 10.9.0.20:44634: output error
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 18 on default port 9891] WARN ipc.Server: IPC Server handler 18 on default port 9891, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:40156 / 10.9.0.21:40156: output error
om2_1    | 2023-07-19 07:35:47,161 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:35:48,310 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-07-19 07:35:49,271 [Thread-22] WARN rocksdiff.RocksDBCheckpointDiffer: Snapshot info table column family handle is not set!
om2_1    | 2023-07-19 07:35:49,297 [Thread-25] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] and output files: [/data/metadata/om.db/000082.sst, /data/metadata/om.db/000073.sst, /data/metadata/om.db/000051.sst, /data/metadata/om.db/000047.sst] are same.
dn4_1    | java.net.SocketTimeoutException: Call From 6cb42cec657e/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:45342 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
scm1_1   | 2023-07-19 07:34:49,429 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1   | 2023-07-19 07:34:49,663 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1_1   | 2023-07-19 07:34:49,666 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | WARNING: An illegal reflective access operation has occurred
scm1_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm1_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm1_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-07-19 07:35:23,286 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 2023-07-19 07:35:49,152 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-07-19 07:35:53,938 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 2023-07-19 07:38:47,910 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd: new RaftServerImpl for group-08992B8D070F:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-07-19 07:38:47,910 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm2_1   | 2023-07-19 07:35:23,287 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-07-19 07:35:23,296 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 2023-07-19 07:35:51,047 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-07-19 07:35:54,231 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om3_1    | 2023-07-19 07:35:54,377 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:35:54,787 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om3_1    | 2023-07-19 07:35:54,803 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om3_1    | 2023-07-19 07:35:54,968 [main] WARN om.OzoneManager: Prepare marker file index 112 does not match DB prepare index 111. Writing DB index to prepare file and maintaining prepared state.
om3_1    | 2023-07-19 07:35:54,976 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 111 to file /data/metadata/current/prepareMarker
scm2_1   | 2023-07-19 07:35:23,307 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer: b28076e1-4ec3-4254-8902-2272d74360c6: found a subdirectory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
scm2_1   | 2023-07-19 07:35:23,323 [main] INFO server.RaftServer: b28076e1-4ec3-4254-8902-2272d74360c6: addNew group-05548E1F8073:[] returns group-05548E1F8073:java.util.concurrent.CompletableFuture@1174a305[Not completed]
om2_1    | 2023-07-19 07:35:51,325 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om2_1    | 2023-07-19 07:35:51,465 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-07-19 07:36:18,954 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a78d380{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-07-19 07:36:19,977 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5167209542ns, electionTimeout:5155ms
om1_1    | 2023-07-19 07:36:19,983 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
scm3_1   | 2023-07-19 07:35:45,031 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-07-19 07:35:45,067 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-07-19 07:35:45,068 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-07-19 07:35:45,071 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om1_1    | 2023-07-19 07:36:19,990 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
recon_1  | 2023-07-19 07:35:48,350 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:54722 / 10.9.0.18:54722: output error
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om2_1    | 2023-07-19 07:35:52,002 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | WARNING: All illegal access operations will be denied in a future release
om3_1    | 2023-07-19 07:35:55,743 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:46808 remote=recon/10.9.0.22:9891]
scm2_1   | 2023-07-19 07:35:23,401 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 7#78 with transactionInfo term andIndex
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-07-19 07:36:20,024 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om1_1    | 2023-07-19 07:36:20,029 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-07-19 07:36:20,072 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:35:45,099 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
om2_1    | 2023-07-19 07:35:52,050 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
scm2_1   | 2023-07-19 07:35:23,403 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6: new RaftServerImpl for group-05548E1F8073:[] with SCMStateMachine:uninitialized
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:40494 / 10.9.0.19:40494: output error
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | 2023-07-19 07:35:45,147 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: found a subdirectory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
om2_1    | 2023-07-19 07:35:52,320 [main] WARN om.OzoneManager: Prepare marker file index 112 does not match DB prepare index 111. Writing DB index to prepare file and maintaining prepared state.
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-07-19 07:35:23,411 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:34212 / 10.9.0.17:34212: output error
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
om2_1    | 2023-07-19 07:35:52,353 [main] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 111 to file /data/metadata/current/prepareMarker
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:22,182 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om3_1    | 2023-07-19 07:35:55,761 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:60982 / 10.9.0.19:60982: output error
dn3_1    | 2023-07-19 07:35:20,954 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:45,232 [main] INFO server.RaftServer: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: addNew group-05548E1F8073:[] returns group-05548E1F8073:java.util.concurrent.CompletableFuture@5f117b3d[Not completed]
scm3_1   | 2023-07-19 07:35:45,685 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO ha.SCMStateMachine: Updated lastAppliedTermIndex 7#78 with transactionInfo term andIndex
scm3_1   | 2023-07-19 07:35:45,692 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: new RaftServerImpl for group-05548E1F8073:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-07-19 07:35:45,740 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-07-19 07:34:49,772 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1_1   | 2023-07-19 07:34:49,814 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
om3_1    | 2023-07-19 07:35:56,098 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
scm2_1   | 2023-07-19 07:35:23,413 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-07-19 07:35:23,414 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-07-19 07:35:23,414 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-07-19 07:35:23,414 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-07-19 07:35:23,415 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-07-19 07:35:23,457 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-07-19 07:35:52,729 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-07-19 07:36:20,601 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-07-19 07:36:20,601 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-07-19 07:34:49,873 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-07-19 07:34:50,622 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
om3_1    | 2023-07-19 07:35:56,111 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-07-19 07:35:58,932 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1  | 2023-07-19 07:35:48,345 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:58004 / 10.9.0.18:58004: output error
scm2_1   | 2023-07-19 07:35:23,459 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-07-19 07:35:23,529 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: ConfigurationManager, init=-1: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-07-19 07:38:47,911 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-07-19 07:35:52,729 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-07-19 07:35:45,762 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-07-19 07:36:20,691 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-07-19 07:36:20,695 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-07-19 07:36:20,994 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5f5a33ed{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-4403962168768693460/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
scm1_1   | 2023-07-19 07:34:58,671 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-07-19 07:35:00,045 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-07-19 07:35:00,684 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm1_1   | 2023-07-19 07:35:00,727 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-07-19 07:35:01,525 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om2_1    | 2023-07-19 07:35:52,804 [main] INFO utils.RDBSnapshotProvider: Cleaning up the candidate dir: /data/metadata/snapshot/om.db.candidate
dn3_1    | 2023-07-19 07:35:20,980 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:21,955 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:38:22,185 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:22,190 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
recon_1  | 2023-07-19 07:35:48,340 [IPC Server handler 28 on default port 9891] WARN ipc.Server: IPC Server handler 28 on default port 9891, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:34218 / 10.9.0.17:34218: output error
recon_1  | 2023-07-19 07:35:48,340 [IPC Server handler 27 on default port 9891] WARN ipc.Server: IPC Server handler 27 on default port 9891, call Call#16 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:34128 / 10.9.0.18:34128: output error
recon_1  | 2023-07-19 07:35:48,336 [IPC Server handler 24 on default port 9891] WARN ipc.Server: IPC Server handler 24 on default port 9891, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:40160 / 10.9.0.21:40160: output error
om2_1    | 2023-07-19 07:35:52,809 [main] INFO ratis_snapshot.OmRatisSnapshotProvider: Initializing OM Snapshot Provider
dn2_1    | 2023-07-19 07:38:22,230 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-07-19 07:35:59,115 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-07-19 07:35:59,768 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-07-19 07:35:59,951 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:112)
om3_1    | 2023-07-19 07:36:00,578 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
recon_1  | 2023-07-19 07:35:48,336 [IPC Server handler 23 on default port 9891] WARN ipc.Server: IPC Server handler 23 on default port 9891, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:57566 / 10.9.0.21:57566: output error
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
om2_1    | 2023-07-19 07:35:54,681 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-07-19 07:35:54,823 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-19 07:38:22,230 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-07-19 07:38:47,930 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-07-19 07:38:47,930 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-07-19 07:38:47,931 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-07-19 07:38:47,931 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
recon_1  | 2023-07-19 07:35:48,336 [IPC Server handler 22 on default port 9891] WARN ipc.Server: IPC Server handler 22 on default port 9891, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:41816 / 10.9.0.18:41816: output error
recon_1  | 2023-07-19 07:35:48,336 [IPC Server handler 21 on default port 9891] WARN ipc.Server: IPC Server handler 21 on default port 9891, call Call#14 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:46300 / 10.9.0.20:46300: output error
scm1_1   | 2023-07-19 07:35:01,621 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-07-19 07:35:01,632 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm1_1   | 2023-07-19 07:35:01,659 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
om2_1    | 2023-07-19 07:35:55,133 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
dn2_1    | 2023-07-19 07:38:22,231 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm2_1   | 2023-07-19 07:35:23,531 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-07-19 07:35:23,598 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-07-19 07:35:23,635 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-07-19 07:35:23,669 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
scm3_1   | 2023-07-19 07:35:45,762 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-19 07:35:21,981 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:35:55,257 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:4, i:112)
dn2_1    | 2023-07-19 07:38:22,231 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-19 07:38:22,231 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-19 07:38:22,232 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:22,233 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-07-19 07:38:22,233 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:48508 / 10.9.0.17:48508: output error
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#11 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:41804 / 10.9.0.18:41804: output error
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
scm3_1   | 2023-07-19 07:35:45,762 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn3_1    | 2023-07-19 07:35:22,956 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:35:56,083 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-07-19 07:36:21,129 [main] INFO server.AbstractConnector: Started ServerConnector@1fc5c0b2{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
dn1_1    | 2023-07-19 07:38:47,931 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-07-19 07:38:47,931 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-07-19 07:38:47,935 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-07-19 07:36:01,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-07-19 07:36:01,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-07-19 07:36:01,120 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-07-19 07:35:45,762 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-07-19 07:35:45,762 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-07-19 07:35:56,230 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-07-19 07:36:21,132 [main] INFO server.Server: Started @142796ms
om1_1    | 2023-07-19 07:36:21,203 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-07-19 07:36:21,204 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-07-19 07:36:21,212 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm2_1   | 2023-07-19 07:35:23,674 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-07-19 07:35:23,744 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2_1   | 2023-07-19 07:35:24,102 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-07-19 07:35:24,108 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
scm3_1   | 2023-07-19 07:35:45,919 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-07-19 07:35:45,931 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-07-19 07:35:45,994 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-07-19 07:35:56,247 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
scm1_1   | 2023-07-19 07:35:02,231 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om3_1    | 2023-07-19 07:36:01,121 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-07-19 07:36:01,121 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:57406 / 10.9.0.20:57406: output error
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:39440 / 10.9.0.19:39440: output error
scm3_1   | 2023-07-19 07:35:45,995 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-07-19 07:35:46,259 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn1_1    | 2023-07-19 07:38:47,938 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-07-19 07:38:47,938 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-19 07:38:47,938 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-07-19 07:38:47,938 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-07-19 07:38:47,938 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-07-19 07:35:56,261 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-07-19 07:36:01,130 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-07-19 07:36:01,150 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-07-19 07:36:01,192 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-07-19 07:36:01,195 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn3_1    | 2023-07-19 07:35:22,982 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:23,958 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:38:47,940 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-07-19 07:35:02,388 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-07-19 07:35:02,389 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm1_1   | 2023-07-19 07:35:02,420 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-07-19 07:35:03,326 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-07-19 07:35:24,113 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-07-19 07:35:24,114 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-07-19 07:35:24,117 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-07-19 07:35:24,117 [b28076e1-4ec3-4254-8902-2272d74360c6-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-07-19 07:35:24,124 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
dn3_1    | 2023-07-19 07:35:23,983 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:46,349 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn1_1    | 2023-07-19 07:38:47,940 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/301e375a-65de-4355-b5a7-08992b8d070f does not exist. Creating ...
dn2_1    | 2023-07-19 07:38:22,234 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-075378E90876,id=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn2_1    | 2023-07-19 07:38:22,234 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-07-19 07:38:22,234 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-07-19 07:38:22,234 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn5_1    | 2023-07-19 07:35:21,010 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From aff5372d1efd/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51734 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 2023-07-19 07:35:24,959 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:24,984 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:25,961 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:38:47,945 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/301e375a-65de-4355-b5a7-08992b8d070f/in_use.lock acquired by nodename 7@751115a4ca9e
dn1_1    | 2023-07-19 07:38:47,949 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/301e375a-65de-4355-b5a7-08992b8d070f has been successfully formatted.
om1_1    | 2023-07-19 07:36:21,223 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-07-19 07:36:21,234 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-07-19 07:36:21,737 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn2_1    | 2023-07-19 07:38:22,235 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:45342 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 2023-07-19 07:35:25,986 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:26,962 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:46816 / 10.9.0.21:46816: output error
dn1_1    | 2023-07-19 07:38:47,950 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO ratis.ContainerStateMachine: group-08992B8D070F: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-07-19 07:38:47,950 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-19 07:38:47,972 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-07-19 07:35:03,334 [main] INFO server.StorageContainerManager: 
dn2_1    | 2023-07-19 07:38:22,236 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:35:24,127 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-07-19 07:36:01,203 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-07-19 07:36:01,336 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-07-19 07:35:56,269 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-07-19 07:35:56,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-07-19 07:35:26,987 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-07-19 07:35:48,332 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:48516 / 10.9.0.17:48516: output error
recon_1  | 2023-07-19 07:35:48,329 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:46808 / 10.9.0.21:46808: output error
recon_1  | 2023-07-19 07:35:48,434 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
om1_1    | 2023-07-19 07:36:23,168 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
dn1_1    | 2023-07-19 07:38:47,979 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:22,242 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=502632ea-08d6-4150-a7b0-075378e90876
dn2_1    | 2023-07-19 07:38:22,246 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-07-19 07:36:01,403 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-07-19 07:35:56,272 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
scm3_1   | 2023-07-19 07:35:46,430 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 2023-07-19 07:35:27,963 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:27,988 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-07-19 07:36:24,339 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm1_1   | Container Balancer status:
recon_1  | java.nio.channels.ClosedChannelException
dn1_1    | 2023-07-19 07:38:47,979 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-19 07:38:23,541 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876.
scm2_1   | 2023-07-19 07:35:24,128 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-07-19 07:36:01,412 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-07-19 07:36:03,230 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-07-19 07:35:46,437 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
scm1_1   | Key                            Value
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-07-19 07:38:23,543 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: addNew group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-649965091133:java.util.concurrent.CompletableFuture@71bb8e84[Not completed]
scm2_1   | 2023-07-19 07:35:24,193 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm2_1   | 2023-07-19 07:35:24,323 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
om3_1    | 2023-07-19 07:36:03,258 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-07-19 07:35:46,967 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
om1_1    | 2023-07-19 07:36:24,341 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t4
om1_1    | 2023-07-19 07:36:24,341 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
dn1_1    | 2023-07-19 07:38:47,980 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-19 07:38:23,546 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: new RaftServerImpl for group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om2_1    | 2023-07-19 07:35:56,279 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
scm2_1   | 2023-07-19 07:35:24,555 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-07-19 07:35:24,597 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm2_1   | 2023-07-19 07:35:24,610 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm1_1   | Running                        false
om1_1    | 2023-07-19 07:36:24,351 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn3_1    | 2023-07-19 07:35:28,965 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
scm3_1   | 2023-07-19 07:35:48,625 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | Container Balancer Configuration values:
scm1_1   | Key                                                Value
om1_1    | 2023-07-19 07:36:24,389 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-07-19 07:38:47,980 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:23,546 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:35:28,989 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:28,990 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
om3_1    | 2023-07-19 07:36:03,271 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-07-19 07:36:03,277 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-07-19 07:36:03,278 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-07-19 07:35:48,660 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-07-19 07:35:56,312 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:35:56,322 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
scm2_1   | 2023-07-19 07:35:24,813 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | Threshold                                          10
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | Max Size to Move per Iteration                     500GB
om1_1    | 2023-07-19 07:36:24,404 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-07-19 07:36:24,488 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:112))
om1_1    | 2023-07-19 07:36:24,494 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
om1_1    | 2023-07-19 07:36:24,495 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om1#0:OK-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:35:56,330 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-07-19 07:35:56,462 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-07-19 07:35:48,666 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
scm2_1   | 2023-07-19 07:35:24,817 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn3_1    | java.net.ConnectException: Call From 69ed6f01c113/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om3_1    | 2023-07-19 07:36:03,301 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-07-19 07:35:56,509 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-07-19 07:35:56,516 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-07-19 07:35:59,399 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
scm3_1   | 2023-07-19 07:35:48,678 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
om1_1    | 2023-07-19 07:36:24,504 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-07-19 07:36:24,505 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om2#0:OK-t5
om1_1    | 2023-07-19 07:36:24,505 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
scm2_1   | 2023-07-19 07:35:24,838 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn1_1    | 2023-07-19 07:38:47,991 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn4_1    | 2023-07-19 07:35:20,333 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-07-19 07:35:24,841 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn1_1    | 2023-07-19 07:38:47,998 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-07-19 07:35:20,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:21,334 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:21,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 2023-07-19 07:38:23,547 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
om1_1    | 2023-07-19 07:36:24,505 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
scm2_1   | 2023-07-19 07:35:24,855 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn1_1    | 2023-07-19 07:38:47,998 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:35:22,335 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-07-19 07:36:03,331 [om3-impl-thread1] INFO server.RaftServer: om3: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-07-19 07:36:03,399 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@6459f4ea[Not completed]
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 2023-07-19 07:38:23,548 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | Max Size Entering Target per Iteration             26GB
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn1_1    | 2023-07-19 07:38:47,998 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO segmented.SegmentedRaftLogWorker: new a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/301e375a-65de-4355-b5a7-08992b8d070f
dn4_1    | 2023-07-19 07:35:22,336 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:23,337 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:48,680 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-07-19 07:38:23,548 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-07-19 07:35:59,430 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-07-19 07:35:59,456 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
scm2_1   | 2023-07-19 07:35:24,865 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 2023-07-19 07:36:03,403 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
dn4_1    | 2023-07-19 07:35:23,338 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:48,691 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 2023-07-19 07:38:23,548 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-07-19 07:35:59,456 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-07-19 07:35:59,460 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-07-19 07:36:24,505 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm1_1   | 
om3_1    | 2023-07-19 07:36:03,424 [main] INFO om.OzoneManager: Creating RPC Server
dn4_1    | 2023-07-19 07:35:24,338 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:35:48,693 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-07-19 07:35:48,693 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-07-19 07:38:23,550 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm2_1   | 2023-07-19 07:35:24,878 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
om1_1    | 2023-07-19 07:36:24,510 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om1 at term 5 for becomeLeader, leader elected after 20145ms
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn1_1    | 2023-07-19 07:38:47,998 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 2023-07-19 07:35:48,693 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 2023-07-19 07:38:23,550 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
om2_1    | 2023-07-19 07:35:59,512 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 2023-07-19 07:35:24,887 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om1_1    | 2023-07-19 07:36:24,590 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:112))
om1_1    | 2023-07-19 07:36:24,594 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | 2023-07-19 07:36:03,564 [om3-groupManagement] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
dn1_1    | 2023-07-19 07:38:47,999 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | 2023-07-19 07:35:03,536 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-07-19 07:35:03,617 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm3_1   | 2023-07-19 07:35:49,040 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm3_1   | 2023-07-19 07:35:49,555 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-07-19 07:38:23,551 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
scm2_1   | 2023-07-19 07:35:25,016 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om1_1    | 2023-07-19 07:36:24,632 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-07-19 07:36:24,637 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-07-19 07:36:24,693 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om3_1    | 2023-07-19 07:36:03,616 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-07-19 07:35:03,670 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm1_1   | 2023-07-19 07:35:03,696 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51734 remote=scm1/10.9.0.14:9861]
dn2_1    | 2023-07-19 07:38:23,552 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-07-19 07:35:59,592 [om2-impl-thread1] INFO server.RaftServer: om2: found a subdirectory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-07-19 07:35:59,661 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@6b0f266e[Not completed]
scm2_1   | 2023-07-19 07:35:25,018 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
dn1_1    | 2023-07-19 07:38:47,999 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-07-19 07:38:48,002 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 2023-07-19 07:36:03,652 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-07-19 07:35:24,339 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:25,339 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 2023-07-19 07:38:23,552 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om2_1    | 2023-07-19 07:35:59,661 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-07-19 07:35:59,721 [main] INFO om.OzoneManager: Creating RPC Server
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn1_1    | 2023-07-19 07:38:48,002 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-07-19 07:38:48,004 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om3_1    | 2023-07-19 07:36:03,653 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-07-19 07:35:25,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:03,721 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | 2023-07-19 07:35:49,920 [main] INFO node.SCMNodeManager: Entering startup safe mode.
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 2023-07-19 07:38:23,556 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
om2_1    | 2023-07-19 07:36:00,085 [om2-groupManagement] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn1_1    | 2023-07-19 07:38:48,007 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,430 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
scm2_1   | 2023-07-19 07:35:25,076 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
om3_1    | 2023-07-19 07:36:03,655 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn4_1    | 2023-07-19 07:35:26,340 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:03,834 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/in_use.lock acquired by nodename 8@95d08cfc10eb
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 2023-07-19 07:38:23,556 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-07-19 07:38:48,008 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | java.nio.channels.ClosedChannelException
om2_1    | 2023-07-19 07:36:00,136 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm2_1   | 2023-07-19 07:35:25,321 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
om1_1    | 2023-07-19 07:36:24,694 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om3_1    | 2023-07-19 07:36:03,656 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:35:26,341 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:03,905 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=36a933ad-cfeb-4d3b-aa37-2c29c320331e} from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/raft-meta
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 2023-07-19 07:38:48,010 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om3_1    | 2023-07-19 07:36:03,657 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-07-19 07:35:27,341 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:04,888 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
scm2_1   | 2023-07-19 07:35:25,351 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 2023-07-19 07:36:24,696 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 2023-07-19 07:36:00,162 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-07-19 07:36:00,168 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-07-19 07:38:48,019 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:23,556 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-07-19 07:36:03,785 [om3-groupManagement] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-07-19 07:36:03,797 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-07-19 07:36:03,881 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-07-19 07:35:49,972 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm2_1   | 2023-07-19 07:35:25,358 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-07-19 07:36:24,717 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-07-19 07:36:24,727 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-07-19 07:36:00,173 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn1_1    | 2023-07-19 07:38:48,049 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-07-19 07:38:48,051 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:38:23,557 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-07-19 07:35:04,954 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-07-19 07:35:05,029 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-07-19 07:35:05,033 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:35:05,041 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-07-19 07:35:49,976 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om2_1    | 2023-07-19 07:36:00,173 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-07-19 07:36:03,886 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-07-19 07:36:04,420 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn2_1    | 2023-07-19 07:38:23,557 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-07-19 07:35:27,342 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:28,342 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
scm3_1   | 2023-07-19 07:35:50,660 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-07-19 07:36:00,181 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-07-19 07:36:04,569 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om3_1    | 2023-07-19 07:36:04,722 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
dn2_1    | 2023-07-19 07:38:23,558 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-07-19 07:35:28,343 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:28,344 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
scm3_1   | 2023-07-19 07:35:50,660 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 2023-07-19 07:36:00,731 [om2-groupManagement] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-07-19 07:36:04,745 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-07-19 07:36:05,618 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om3_1    | 2023-07-19 07:36:06,503 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-07-19 07:36:06,560 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-07-19 07:36:06,582 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-07-19 07:35:05,060 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 2023-07-19 07:35:50,756 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3_1   | 2023-07-19 07:35:50,756 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-07-19 07:35:05,115 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:05,275 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-07-19 07:35:05,276 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
dn5_1    | 2023-07-19 07:35:21,871 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:21,897 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:22,872 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:22,898 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:23,873 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:23,899 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:24,874 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:24,900 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:25,875 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:25,904 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:26,876 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:26,905 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:27,877 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:27,908 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:28,878 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:28,909 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:29,882 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:30,883 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:30,884 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From aff5372d1efd/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
scm2_1   | WARNING: An illegal reflective access operation has occurred
scm2_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm2_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm2_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm2_1   | WARNING: All illegal access operations will be denied in a future release
scm2_1   | 2023-07-19 07:35:25,386 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-07-19 07:35:50,830 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-07-19 07:35:50,839 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-07-19 07:35:50,942 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3_1   | 2023-07-19 07:35:50,968 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3_1   | 2023-07-19 07:35:51,643 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om1_1    | 2023-07-19 07:36:24,828 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-07-19 07:38:48,051 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-07-19 07:38:48,054 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-19 07:38:48,055 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-07-19 07:38:48,056 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: start as a follower, conf=-1: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:38:48,057 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-07-19 07:38:48,057 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState
dn1_1    | 2023-07-19 07:38:48,061 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-08992B8D070F,id=a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm3_1   | 2023-07-19 07:35:51,663 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-07-19 07:35:52,100 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3_1   | 2023-07-19 07:35:52,878 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3_1   | 2023-07-19 07:35:53,318 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-07-19 07:35:53,318 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | WARNING: An illegal reflective access operation has occurred
scm3_1   | WARNING: Illegal reflective access by org.apache.hadoop.hdds.utils.MetricsUtil (file:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar) to method java.lang.Class.annotationData()
scm3_1   | WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.hdds.utils.MetricsUtil
scm3_1   | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
scm3_1   | WARNING: All illegal access operations will be denied in a future release
scm3_1   | 2023-07-19 07:35:53,470 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
om3_1    | 2023-07-19 07:36:06,586 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-07-19 07:35:25,415 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-07-19 07:35:25,420 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm2_1   | 2023-07-19 07:35:25,631 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm2_1   | 2023-07-19 07:35:27,964 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-07-19 07:35:28,053 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-07-19 07:35:28,154 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm2_1   | 2023-07-19 07:35:28,162 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1   | 2023-07-19 07:35:28,265 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-07-19 07:35:28,276 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-07-19 07:36:24,830 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-07-19 07:36:24,834 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-07-19 07:36:24,844 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-07-19 07:36:24,851 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-07-19 07:36:24,852 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-07-19 07:36:24,856 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om1_1    | 2023-07-19 07:36:24,857 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om1_1    | 2023-07-19 07:36:24,859 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-07-19 07:36:24,859 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-07-19 07:36:24,876 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-07-19 07:36:24,876 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | java.net.ConnectException: Call From 6cb42cec657e/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
om3_1    | 2023-07-19 07:36:06,601 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-07-19 07:36:06,613 [om3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-07-19 07:36:08,588 [main] INFO reflections.Reflections: Reflections took 4754 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
om3_1    | 2023-07-19 07:36:09,832 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-07-19 07:36:09,918 [main] INFO ipc.Server: Listener at om3:9862
om3_1    | 2023-07-19 07:36:09,946 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
scm1_1   | 2023-07-19 07:35:05,290 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:35:05,346 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
scm1_1   | 2023-07-19 07:35:05,353 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-07-19 07:35:05,369 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-07-19 07:35:05,390 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:05,393 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:05,398 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-07-19 07:35:05,402 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-07-19 07:35:05,407 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-07-19 07:35:05,423 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-07-19 07:35:05,523 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-07-19 07:35:05,545 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:35:05,718 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-07-19 07:35:05,726 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-07-19 07:35:05,729 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-07-19 07:35:06,261 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 0: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:06,274 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_0-0
scm1_1   | 2023-07-19 07:35:06,312 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 1: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:06,425 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 17: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-07-19 07:35:06,434 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 19: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:06,479 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 31: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-07-19 07:35:06,533 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 33: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:06,570 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_1-48
scm1_1   | 2023-07-19 07:35:06,604 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 49: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:06,683 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_49-66
scm1_1   | 2023-07-19 07:35:06,686 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:07,053 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67
scm1_1   | 2023-07-19 07:35:07,124 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 78
scm1_1   | 2023-07-19 07:35:07,124 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
scm1_1   | 2023-07-19 07:35:08,032 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: start as a follower, conf=67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:08,033 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from      null to FOLLOWER at term 7 for startAsFollower
scm1_1   | 2023-07-19 07:35:08,039 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:08,059 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-07-19 07:35:08,080 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-05548E1F8073,id=36a933ad-cfeb-4d3b-aa37-2c29c320331e
scm1_1   | 2023-07-19 07:35:08,085 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-07-19 07:35:08,105 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-07-19 07:35:08,112 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-07-19 07:35:08,118 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-07-19 07:35:08,122 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-07-19 07:35:08,218 [main] INFO server.RaftServer: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start RPC server
scm1_1   | 2023-07-19 07:35:09,023 [main] INFO server.GrpcService: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: GrpcService started, listening on 9894
scm1_1   | 2023-07-19 07:35:09,083 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-36a933ad-cfeb-4d3b-aa37-2c29c320331e: Started
scm1_1   | 2023-07-19 07:35:09,202 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-07-19 07:35:09,202 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-07-19 07:35:09,477 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm1_1   | 2023-07-19 07:35:09,482 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm1_1   | 2023-07-19 07:35:09,482 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm1_1   | 2023-07-19 07:35:10,795 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-07-19 07:35:11,298 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-07-19 07:35:11,301 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-07-19 07:35:13,119 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.FollowerState: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5080662352ns, electionTimeout:5026ms
scm1_1   | 2023-07-19 07:35:13,121 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:13,130 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
scm1_1   | 2023-07-19 07:35:13,155 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-07-19 07:35:13,182 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1
scm1_1   | 2023-07-19 07:35:13,254 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:13,759 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 57eda1b0-9276-42ed-8a05-f1155bfae1c0
scm1_1   | 2023-07-19 07:35:13,831 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-07-19 07:35:13,999 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-07-19 07:35:14,032 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for b28076e1-4ec3-4254-8902-2272d74360c6
scm1_1   | 2023-07-19 07:35:15,766 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:15,770 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:15,772 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm1_1   | 2023-07-19 07:35:15,775 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:15,776 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:15,782 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1 PRE_VOTE round 0: result REJECTED
scm1_1   | 2023-07-19 07:35:15,786 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
scm1_1   | 2023-07-19 07:35:15,786 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1
scm1_1   | 2023-07-19 07:35:15,787 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection1] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:17,016 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-07-19 07:35:17,030 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
dn3_1    | 	... 12 more
dn3_1    | 2023-07-19 07:35:29,991 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:30,993 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:38:23,558 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 does not exist. Creating ...
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,430 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 2023-07-19 07:36:13,618 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om3_1    | 2023-07-19 07:36:14,401 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-07-19 07:36:14,545 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-07-19 07:36:14,545 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-07-19 07:36:14,967 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-07-19 07:36:14,969 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-07-19 07:36:15,005 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@2e956d7203d7
om3_1    | 2023-07-19 07:36:15,022 [om3-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om3} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
om3_1    | 2023-07-19 07:36:15,204 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:36:15,226 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,430 [IPC Server handler 26 on default port 9891] INFO ipc.Server: IPC Server handler 26 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-07-19 07:38:23,560 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/in_use.lock acquired by nodename 7@e710b1935f2e
dn2_1    | 2023-07-19 07:38:23,563 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 has been successfully formatted.
dn2_1    | 2023-07-19 07:38:23,564 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO ratis.ContainerStateMachine: group-649965091133: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-07-19 07:38:23,583 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-07-19 07:38:23,584 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-19 07:38:23,587 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:23,587 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-19 07:38:23,588 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-07-19 07:38:23,588 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:23,590 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-07-19 07:35:17,121 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 2023-07-19 07:38:23,591 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-07-19 07:38:23,591 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:23,591 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133
dn2_1    | 2023-07-19 07:38:23,592 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-07-19 07:38:23,592 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-07-19 07:38:23,593 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:23,593 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-07-19 07:38:23,593 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-07-19 07:38:23,594 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-19 07:38:23,594 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-07-19 07:35:28,278 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm1_1   | 2023-07-19 07:35:18,020 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-07-19 07:35:18,048 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3_1   | 2023-07-19 07:35:53,519 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 2, healthy pipeline threshold count is 1
scm3_1   | 2023-07-19 07:35:53,550 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
dn1_1    | 2023-07-19 07:38:48,061 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om2_1    | 2023-07-19 07:36:00,732 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-07-19 07:36:00,829 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-07-19 07:36:00,829 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-07-19 07:35:28,278 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-07-19 07:35:28,337 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-07-19 07:35:28,363 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-07-19 07:35:28,367 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm2_1   | 2023-07-19 07:35:28,367 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn1_1    | 2023-07-19 07:38:48,062 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-07-19 07:38:48,062 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om3_1    | 2023-07-19 07:36:15,258 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-07-19 07:36:15,258 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 2023-07-19 07:35:54,066 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm3_1   | 2023-07-19 07:36:01,591 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-07-19 07:36:02,459 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-07-19 07:36:02,847 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm3_1   | 2023-07-19 07:36:02,865 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-07-19 07:36:03,295 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
dn1_1    | 2023-07-19 07:38:48,062 [a9b83a7e-59b8-4455-b30a-c01eee264fbd-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-07-19 07:38:48,063 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-07-19 07:36:15,260 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-19 07:35:31,994 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:32,995 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:33,977 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 69ed6f01c113/10.9.0.19 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:48524 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
om2_1    | 2023-07-19 07:36:01,454 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn1_1    | 2023-07-19 07:38:48,064 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:03,324 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-07-19 07:36:03,325 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
om3_1    | 2023-07-19 07:36:15,261 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 2023-07-19 07:38:48,068 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=301e375a-65de-4355-b5a7-08992b8d070f
dn1_1    | 2023-07-19 07:38:48,069 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=301e375a-65de-4355-b5a7-08992b8d070f.
scm3_1   | 2023-07-19 07:36:03,332 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3_1   | 2023-07-19 07:36:03,555 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-07-19 07:36:03,611 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-07-19 07:36:03,611 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm3_1   | 2023-07-19 07:36:03,633 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3_1   | 2023-07-19 07:36:04,482 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3_1   | 2023-07-19 07:36:04,483 [main] INFO server.StorageContainerManager: 
scm3_1   | Container Balancer status:
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 2023-07-19 07:38:53,212 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO impl.FollowerState: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5154788628ns, electionTimeout:5148ms
scm3_1   | Key                            Value
scm3_1   | Running                        false
om3_1    | 2023-07-19 07:36:15,278 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn2_1    | 2023-07-19 07:38:23,595 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-19 07:38:23,596 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-07-19 07:38:23,598 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:23,935 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: Detected pause in JVM or host machine approximately 0.167s with 0.330s GC time.
dn2_1    | GC pool 'ParNew' had collection(s): count=1 time=18ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn1_1    | 2023-07-19 07:38:53,213 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: shutdown a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState
scm3_1   | Container Balancer Configuration values:
scm3_1   | Key                                                Value
scm3_1   | Threshold                                          10
om3_1    | 2023-07-19 07:36:15,288 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-07-19 07:36:15,289 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-07-19 07:36:24,877 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-07-19 07:36:24,877 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-07-19 07:36:24,884 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn1_1    | 2023-07-19 07:38:53,213 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-07-19 07:38:53,214 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
om3_1    | 2023-07-19 07:36:15,290 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-07-19 07:36:15,339 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-07-19 07:36:15,343 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-07-19 07:36:15,347 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-07-19 07:36:15,361 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
dn1_1    | 2023-07-19 07:38:53,214 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-FollowerState] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2
dn1_1    | 2023-07-19 07:38:53,218 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:38:53,218 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
dn1_1    | 2023-07-19 07:38:53,222 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 
om3_1    | 2023-07-19 07:36:15,366 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
dn1_1    | 2023-07-19 07:38:53,223 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.LeaderElection: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2 ELECTION round 0: result PASSED (term=1)
scm2_1   | 2023-07-19 07:35:28,560 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-07-19 07:35:28,561 [main] INFO server.StorageContainerManager: 
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
dn1_1    | 2023-07-19 07:38:53,223 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: shutdown a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2
scm3_1   | 2023-07-19 07:36:04,503 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1   | 2023-07-19 07:36:04,550 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
scm2_1   | Running                        false
scm2_1   | Container Balancer Configuration values:
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | Max Size to Move per Iteration                     500GB
dn2_1    | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=312ms
dn4_1    | 	... 12 more
dn1_1    | 2023-07-19 07:38:53,223 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-07-19 07:38:53,223 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-08992B8D070F with new leaderId: a9b83a7e-59b8-4455-b30a-c01eee264fbd
dn1_1    | 2023-07-19 07:38:53,224 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: change Leader from null to a9b83a7e-59b8-4455-b30a-c01eee264fbd at term 1 for becomeLeader, leader elected after 5292ms
dn1_1    | 2023-07-19 07:38:53,224 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-07-19 07:38:53,224 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-19 07:38:53,224 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om3_1    | 2023-07-19 07:36:15,375 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-07-19 07:36:15,384 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-07-19 07:36:15,402 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-07-19 07:36:15,407 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-07-19 07:36:15,508 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn2_1    | 2023-07-19 07:38:23,937 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:29,346 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:38:53,225 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm2_1   | Max Size Entering Target per Iteration             26GB
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-07-19 07:36:15,511 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-07-19 07:36:15,599 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-07-19 07:36:15,600 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-07-19 07:36:15,603 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-07-19 07:36:15,684 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 112
dn2_1    | 2023-07-19 07:38:23,938 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:38:23,938 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-07-19 07:38:23,938 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-07-19 07:38:23,938 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2_1   | Max Size Leaving Source per Iteration              26GB
om1_1    | 2023-07-19 07:36:24,884 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-07-19 07:36:24,885 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
om1_1    | 2023-07-19 07:36:24,885 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
om1_1    | 2023-07-19 07:36:24,886 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-07-19 07:36:24,887 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-07-19 07:36:01,610 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:23,939 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:30,348 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-07-19 07:38:53,226 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-07-19 07:38:53,226 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 
om1_1    | 2023-07-19 07:36:24,897 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderStateImpl
scm3_1   | 2023-07-19 07:36:04,585 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-07-19 07:36:15,685 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-07-19 07:36:15,712 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:36:15,714 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om2_1    | 2023-07-19 07:36:01,707 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-07-19 07:36:01,712 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-07-19 07:36:02,474 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn2_1    | 2023-07-19 07:38:23,939 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-07-19 07:35:30,959 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | 2023-07-19 07:38:53,230 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-07-19 07:38:53,230 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm2_1   | 2023-07-19 07:35:28,561 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
om1_1    | 2023-07-19 07:36:24,916 [om1@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:113
scm3_1   | 2023-07-19 07:36:04,585 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
om3_1    | 2023-07-19 07:36:15,721 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-07-19 07:36:15,744 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-07-19 07:36:15,748 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-07-19 07:36:03,125 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-07-19 07:36:03,297 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-07-19 07:36:03,312 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-07-19 07:38:23,939 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
scm2_1   | 2023-07-19 07:35:28,567 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om1_1    | 2023-07-19 07:36:24,979 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 113: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:36:04,642 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | 2023-07-19 07:36:04,689 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/in_use.lock acquired by nodename 7@43575e796980
scm3_1   | 2023-07-19 07:36:04,754 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=} from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/raft-meta
scm3_1   | 2023-07-19 07:36:05,062 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:03,330 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn2_1    | 2023-07-19 07:38:23,940 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-649965091133,id=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:48524 remote=scm2/10.9.0.15:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
om2_1    | 2023-07-19 07:36:03,361 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-07-19 07:38:23,940 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm2_1   | 2023-07-19 07:35:28,569 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
scm2_1   | 2023-07-19 07:35:28,571 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
om1_1    | 2023-07-19 07:36:24,981 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-LEADER: reject ELECTION from om3: already has voted for om1 at current term 5
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om2_1    | 2023-07-19 07:36:03,379 [om2-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-07-19 07:36:06,178 [main] INFO reflections.Reflections: Reflections took 5446 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
dn2_1    | 2023-07-19 07:38:23,940 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm2_1   | 2023-07-19 07:35:28,577 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2_1   | 2023-07-19 07:35:28,593 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/in_use.lock acquired by nodename 7@de73b91882ca
om3_1    | 2023-07-19 07:36:15,749 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-07-19 07:36:15,751 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-07-19 07:36:15,752 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-07-19 07:36:24,981 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om3<-om1#0:FAIL-t5. Peer's state: om1@group-D66704EFC61C:t5, leader=om1, voted=om1, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=113: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:845)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,430 [IPC Server handler 20 on default port 9891] INFO ipc.Server: IPC Server handler 20 on default port 9891 caught an exception
dn2_1    | 2023-07-19 07:38:23,940 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
scm2_1   | 2023-07-19 07:35:28,663 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=36a933ad-cfeb-4d3b-aa37-2c29c320331e} from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/raft-meta
scm2_1   | 2023-07-19 07:35:28,796 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-07-19 07:38:53,231 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO impl.RoleInfo: a9b83a7e-59b8-4455-b30a-c01eee264fbd: start a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderStateImpl
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
om3_1    | 2023-07-19 07:36:15,752 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-07-19 07:36:15,754 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-07-19 07:36:25,553 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_113
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn2_1    | 2023-07-19 07:38:23,940 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-07-19 07:38:23,941 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:05,100 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-07-19 07:36:05,188 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-07-19 07:36:05,195 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-07-19 07:38:53,231 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
scm1_1   | 2023-07-19 07:35:18,050 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-07-19 07:36:26,724 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
dn2_1    | 2023-07-19 07:38:23,947 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-19 07:38:23,947 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9ca86faf-1894-4e8b-baec-649965091133
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
scm3_1   | 2023-07-19 07:36:05,217 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-07-19 07:35:28,801 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-07-19 07:38:53,236 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/301e375a-65de-4355-b5a7-08992b8d070f/current/log_inprogress_0
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
om3_1    | 2023-07-19 07:36:15,783 [main] INFO server.RaftServer: om3: start RPC server
scm1_1   | 2023-07-19 07:35:18,062 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
om1_1    | startupRole: FOLLOWER
dn2_1    | 2023-07-19 07:38:24,137 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-07-19 07:36:05,219 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-07-19 07:36:05,266 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn1_1    | 2023-07-19 07:38:53,253 [a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F-LeaderElection2] INFO server.RaftServer$Division: a9b83a7e-59b8-4455-b30a-c01eee264fbd@group-08992B8D070F: set configuration 0: peers:[a9b83a7e-59b8-4455-b30a-c01eee264fbd|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:33,996 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-07-19 07:36:16,126 [main] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-07-19 07:36:16,141 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om3_1    | 2023-07-19 07:36:16,154 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om3_1    | 2023-07-19 07:36:16,155 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om1_1    | , id: "om3"
dn2_1    | 2023-07-19 07:38:27,285 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5176515720ns, electionTimeout:5172ms
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om2_1    | 2023-07-19 07:36:07,305 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-07-19 07:36:05,349 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-07-19 07:36:05,351 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-07-19 07:36:05,357 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:35:34,997 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:35,998 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-07-19 07:35:19,049 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-07-19 07:35:19,050 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | address: "om3:9872"
dn2_1    | 2023-07-19 07:38:27,286 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState
scm2_1   | 2023-07-19 07:35:28,813 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-07-19 07:35:28,813 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:36:07,383 [main] INFO ipc.Server: Listener at om2:9862
dn1_1    | 2023-07-19 07:39:42,086 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om2_1    | 2023-07-19 07:36:07,399 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-07-19 07:36:10,549 [main] INFO om.OzoneManagerPrepareState: Deleted prepare marker file: /data/metadata/current/prepareMarker
om2_1    | 2023-07-19 07:36:11,210 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-07-19 07:40:42,087 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-19 07:41:42,088 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn1_1    | 2023-07-19 07:42:42,089 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
dn3_1    | 2023-07-19 07:35:36,999 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:19,523 [main] INFO util.log: Logging initialized @86704ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | startupRole: FOLLOWER
scm2_1   | 2023-07-19 07:35:28,816 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-07-19 07:35:28,817 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-07-19 07:35:28,822 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-07-19 07:36:11,279 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn2_1    | 2023-07-19 07:38:27,286 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-07-19 07:38:27,287 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-19 07:35:37,550 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
scm1_1   | 2023-07-19 07:35:19,911 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-36a933ad-cfeb-4d3b-aa37-2c29c320331e: Detected pause in JVM or host machine approximately 0.163s without any GCs.
scm1_1   | 2023-07-19 07:35:20,867 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.FollowerState: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5080361666ns, electionTimeout:5026ms
scm1_1   | 2023-07-19 07:35:20,868 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:20,868 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
om3_1    | 2023-07-19 07:36:16,155 [main] INFO upgrade.UpgradeFinalizer: Skipping action QuotaRepairUpgradeAction since it has already been run.
om3_1    | 2023-07-19 07:36:16,159 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-07-19 07:36:16,511 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-07-19 07:36:16,513 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-07-19 07:36:16,719 [main] INFO util.log: Logging initialized @141612ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm2_1   | 2023-07-19 07:35:28,861 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-07-19 07:35:28,861 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-07-19 07:35:28,865 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:35:28,902 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
dn2_1    | 2023-07-19 07:38:27,287 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
om3_1    | 2023-07-19 07:36:17,745 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-07-19 07:35:37,563 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn3_1    | 2023-07-19 07:35:38,000 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:38,180 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn3_1    | 2023-07-19 07:35:38,181 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:35:38,284 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/in_use.lock acquired by nodename 7@69ed6f01c113
dn3_1    | 2023-07-19 07:35:38,299 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=f02af6ab-c12d-469b-a775-f6b30900ff13} from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/raft-meta
scm2_1   | 2023-07-19 07:35:28,907 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-07-19 07:35:28,919 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2_1   | 2023-07-19 07:35:28,923 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-07-19 07:35:28,929 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-07-19 07:35:28,930 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-07-19 07:35:28,936 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-07-19 07:35:28,938 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-07-19 07:35:28,945 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-07-19 07:35:29,016 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-07-19 07:35:29,021 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-07-19 07:36:17,820 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-07-19 07:36:11,281 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
scm2_1   | 2023-07-19 07:35:29,044 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-07-19 07:36:17,932 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-07-19 07:36:11,756 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 2023-07-19 07:35:31,349 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:32,350 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:36:05,420 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073
scm3_1   | 2023-07-19 07:36:05,422 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-07-19 07:36:05,432 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-07-19 07:36:05,449 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-07-19 07:36:17,961 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-07-19 07:36:11,780 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
scm2_1   | 2023-07-19 07:35:29,045 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-07-19 07:35:29,047 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn2_1    | 2023-07-19 07:38:27,288 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,288 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
om3_1    | 2023-07-19 07:36:17,961 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-07-19 07:36:11,822 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@05bec1501e17
scm2_1   | 2023-07-19 07:35:29,091 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 0: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:29,092 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_0-0
scm2_1   | 2023-07-19 07:35:29,095 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 1: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,291 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,291 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-07-19 07:38:27,291 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2
dn2_1    | 2023-07-19 07:38:27,292 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om3_1    | 2023-07-19 07:36:17,966 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-07-19 07:36:11,858 [om2-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=4, votedFor=om2} from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/raft-meta
scm2_1   | 2023-07-19 07:35:29,107 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 17: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn4_1    | 2023-07-19 07:35:33,351 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 2023-07-19 07:35:38,297 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/in_use.lock acquired by nodename 7@69ed6f01c113
scm3_1   | 2023-07-19 07:36:05,457 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn2_1    | 2023-07-19 07:38:27,292 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-14D0CFE0E993 with new leaderId: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,430 [IPC Server handler 18 on default port 9891] INFO ipc.Server: IPC Server handler 18 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-07-19 07:35:29,108 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 19: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:33,385 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 6cb42cec657e/10.9.0.20 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:57566 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 2023-07-19 07:35:38,306 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/in_use.lock acquired by nodename 7@69ed6f01c113
dn3_1    | 2023-07-19 07:35:38,306 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=140718b2-320a-4020-b7ea-662533776c74} from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/raft-meta
dn2_1    | 2023-07-19 07:38:27,292 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: change Leader from null to 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef at term 1 for becomeLeader, leader elected after 5367ms
om3_1    | 2023-07-19 07:36:18,556 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om1_1    | ]
om1_1    | 2023-07-19 07:38:36,370 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
om1_1    | 2023-07-19 07:38:36,373 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
scm2_1   | 2023-07-19 07:35:29,110 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 31: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om2_1    | 2023-07-19 07:36:12,072 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:12,095 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-07-19 07:36:05,463 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-07-19 07:36:05,473 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-07-19 07:38:27,293 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om3_1    | 2023-07-19 07:36:18,594 [main] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-07-19 07:36:18,603 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om3_1    | 2023-07-19 07:36:18,983 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-07-19 07:36:18,983 [main] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-07-19 07:35:29,112 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 33: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:38:36,395 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
om2_1    | 2023-07-19 07:36:12,171 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-07-19 07:36:05,476 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-07-19 07:36:05,487 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-07-19 07:36:05,613 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn2_1    | 2023-07-19 07:38:27,293 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-07-19 07:35:20,869 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-07-19 07:35:20,869 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm3_1   | 2023-07-19 07:36:05,620 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-07-19 07:36:19,007 [main] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-07-19 07:36:19,081 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@38848217{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-07-19 07:38:27,293 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 2023-07-19 07:35:20,901 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 7 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:20,943 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
scm2_1   | 2023-07-19 07:35:29,117 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_1-48
scm2_1   | 2023-07-19 07:35:29,120 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 49: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:38:36,429 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om1_1    | 2023-07-19 07:38:36,430 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-07-19 07:36:05,738 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-07-19 07:36:19,097 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a78d380{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-07-19 07:35:38,322 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=140718b2-320a-4020-b7ea-662533776c74} from /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/raft-meta
dn2_1    | 2023-07-19 07:38:27,308 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-07-19 07:35:20,965 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:20,965 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om1_1    | 2023-07-19 07:38:36,431 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: QUOTA.
om1_1    | 2023-07-19 07:38:36,432 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature QUOTA has been finalized.
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 2023-07-19 07:36:05,752 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-07-19 07:36:05,757 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn3_1    | 2023-07-19 07:35:38,649 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,310 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-07-19 07:35:20,966 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:20,966 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:20,966 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2 PRE_VOTE round 0: result REJECTED
scm1_1   | 2023-07-19 07:35:20,966 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
scm1_1   | 2023-07-19 07:35:20,967 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2
om3_1    | 2023-07-19 07:36:20,695 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5f5a33ed{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-9981832549782906264/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
dn3_1    | 2023-07-19 07:35:38,652 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,310 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-07-19 07:35:20,967 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection2] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:21,095 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-07-19 07:35:29,130 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_49-66
om1_1    | 2023-07-19 07:38:36,432 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm3_1   | 2023-07-19 07:36:06,038 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 0: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 2023-07-19 07:35:38,646 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: set configuration 3: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,313 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om2_1    | 2023-07-19 07:36:12,174 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:36:12,192 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm2_1   | 2023-07-19 07:35:29,135 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:38:36,432 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm3_1   | 2023-07-19 07:36:06,044 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_0-0
om3_1    | 2023-07-19 07:36:20,782 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5061688980ns, electionTimeout:5026ms
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn3_1    | 2023-07-19 07:35:38,783 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Setting the last applied index to (t:7, i:46)
dn2_1    | 2023-07-19 07:38:27,314 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-07-19 07:36:12,201 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-07-19 07:35:21,174 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-07-19 07:35:29,372 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67
om1_1    | 2023-07-19 07:38:36,446 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 6
scm3_1   | 2023-07-19 07:36:06,067 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 1: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:36:06,146 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 17: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-07-19 07:36:06,160 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 19: peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 2023-07-19 07:35:38,801 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO ratis.ContainerStateMachine: group-B30FA8B5BAEF: Setting the last applied index to (t:3, i:4)
dn2_1    | 2023-07-19 07:38:27,315 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderStateImpl
om2_1    | 2023-07-19 07:36:12,224 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:21,278 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm2_1   | 2023-07-19 07:35:29,391 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 78
scm2_1   | 2023-07-19 07:35:29,392 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO segmented.SegmentedRaftLogWorker: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
scm2_1   | 2023-07-19 07:35:29,838 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: start as a follower, conf=67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:29,841 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: changes role from      null to FOLLOWER at term 7 for startAsFollower
scm2_1   | 2023-07-19 07:35:29,861 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO impl.RoleInfo: b28076e1-4ec3-4254-8902-2272d74360c6: start b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState
scm3_1   | 2023-07-19 07:36:06,205 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 31: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-07-19 07:36:06,216 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 33: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:38,834 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO ratis.ContainerStateMachine: group-8563B54DD732: Setting the last applied index to (t:6, i:20)
dn2_1    | 2023-07-19 07:38:27,316 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker: Starting segment from index:0
om2_1    | 2023-07-19 07:36:12,281 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-07-19 07:35:21,295 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-07-19 07:35:21,296 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-07-19 07:35:21,296 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-07-19 07:35:21,836 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm1_1   | 2023-07-19 07:35:21,842 [main] INFO http.HttpServer2: Jetty bound to port 9876
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
om1_1    | 2023-07-19 07:40:07,095 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
scm3_1   | 2023-07-19 07:36:06,231 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.LogSegment: Successfully read 48 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_1-48
scm3_1   | 2023-07-19 07:36:06,240 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 49: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,318 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-LeaderElection2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993: set configuration 0: peers:[8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:12,290 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-07-19 07:35:21,868 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om1_1    | 2023-07-19 07:40:12,539 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:600)
scm3_1   | 2023-07-19 07:36:06,249 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.LogSegment: Successfully read 18 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_49-66
dn3_1    | 2023-07-19 07:35:39,001 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-07-19 07:38:27,319 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-14D0CFE0E993-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3937008a-fbf0-499b-90a2-14d0cfe0e993/current/log_inprogress_0
om2_1    | 2023-07-19 07:36:12,291 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:36:12,357 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-07-19 07:36:20,789 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
om1_1    | 2023-07-19 07:40:27,739 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:652)
scm3_1   | 2023-07-19 07:36:06,252 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:36:06,580 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.LogSegment: Successfully read 12 entries from segment file /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67
dn3_1    | 2023-07-19 07:35:40,006 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:36:12,362 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-07-19 07:36:12,368 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-07-19 07:36:20,789 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om1_1    | 2023-07-19 07:40:40,532 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:773)
dn2_1    | 2023-07-19 07:38:27,381 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5147983617ns, electionTimeout:5135ms
scm3_1   | 2023-07-19 07:36:06,662 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 78
scm3_1   | 2023-07-19 07:36:06,662 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 66
om2_1    | 2023-07-19 07:36:12,385 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:22,350 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-07-19 07:36:20,833 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om3_1    | 2023-07-19 07:36:20,833 [om3@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-07-19 07:36:20,932 [main] INFO server.AbstractConnector: Started ServerConnector@1fc5c0b2{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
scm3_1   | 2023-07-19 07:36:07,206 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: start as a follower, conf=67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:36:07,207 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: changes role from      null to FOLLOWER at term 7 for startAsFollower
om2_1    | 2023-07-19 07:36:12,391 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-07-19 07:35:22,350 [main] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-07-19 07:35:29,864 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:35:29,865 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:35:29,871 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-05548E1F8073,id=b28076e1-4ec3-4254-8902-2272d74360c6
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:375)
dn3_1    | 2023-07-19 07:35:40,669 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-07-19 07:36:12,399 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-07-19 07:36:12,409 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-07-19 07:35:29,886 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-07-19 07:35:29,888 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om3_1    | 2023-07-19 07:36:20,933 [main] INFO server.Server: Started @145826ms
om3_1    | 2023-07-19 07:36:20,956 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-07-19 07:38:27,382 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 2023-07-19 07:35:40,693 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-19 07:35:40,770 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-07-19 07:35:22,372 [main] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | 2023-07-19 07:35:22,507 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@568f4faa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn2_1    | 2023-07-19 07:38:27,382 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 2023-07-19 07:35:40,808 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
scm1_1   | 2023-07-19 07:35:22,515 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2774dcf4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-07-19 07:35:23,340 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@153cfd86{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-2139076274528718824/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 2023-07-19 07:35:23,447 [main] INFO server.AbstractConnector: Started ServerConnector@5ed25612{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-07-19 07:35:23,450 [main] INFO server.Server: Started @90632ms
scm1_1   | 2023-07-19 07:35:23,470 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-07-19 07:35:23,470 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-07-19 07:35:23,473 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn2_1    | 2023-07-19 07:38:27,383 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 2023-07-19 07:35:40,816 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-19 07:35:40,818 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:347)
om3_1    | 2023-07-19 07:36:20,956 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-07-19 07:36:20,961 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-07-19 07:36:20,965 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-07-19 07:36:21,022 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:36:21,151 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-07-19 07:36:21,706 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn2_1    | 2023-07-19 07:38:27,383 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3
dn3_1    | 2023-07-19 07:35:40,819 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:35:40,831 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1632)
om3_1    | 2023-07-19 07:36:21,893 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-07-19 07:36:21,893 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:35:29,890 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2_1   | 2023-07-19 07:35:29,893 [b28076e1-4ec3-4254-8902-2272d74360c6-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-07-19 07:35:29,913 [main] INFO server.RaftServer: b28076e1-4ec3-4254-8902-2272d74360c6: start RPC server
scm2_1   | 2023-07-19 07:35:30,157 [main] INFO server.GrpcService: b28076e1-4ec3-4254-8902-2272d74360c6: GrpcService started, listening on 9894
dn2_1    | 2023-07-19 07:38:27,384 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:40,836 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-19 07:35:40,843 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om3_1    | 2023-07-19 07:36:21,900 [om3@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
scm2_1   | 2023-07-19 07:35:30,186 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b28076e1-4ec3-4254-8902-2272d74360c6: Started
scm2_1   | 2023-07-19 07:35:30,224 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-07-19 07:35:30,226 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2_1   | 2023-07-19 07:35:30,237 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
dn2_1    | 2023-07-19 07:38:27,393 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for f02af6ab-c12d-469b-a775-f6b30900ff13
dn2_1    | 2023-07-19 07:38:27,394 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-07-19 07:38:27,399 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:40,863 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-19 07:35:40,863 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	... 12 more
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 2023-07-19 07:36:21,907 [om3@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om3_1    | 2023-07-19 07:36:23,681 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm3_1   | 2023-07-19 07:36:07,220 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO impl.RoleInfo: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: start 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState
scm3_1   | 2023-07-19 07:36:07,230 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:07,237 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:40,863 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:35:31,886 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:32,888 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:33,889 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-07-19 07:35:30,248 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm2_1   | 2023-07-19 07:35:30,249 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm2_1   | 2023-07-19 07:35:30,604 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-07-19 07:36:12,430 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:40,864 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-19 07:35:40,866 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-07-19 07:35:40,877 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om3_1    | 2023-07-19 07:36:24,348 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:112))
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 2023-07-19 07:35:30,660 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-07-19 07:36:12,432 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-07-19 07:36:07,242 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-05548E1F8073,id=57eda1b0-9276-42ed-8a05-f1155bfae1c0
scm3_1   | 2023-07-19 07:36:07,276 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:40,881 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-19 07:35:40,884 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-07-19 07:35:30,660 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-07-19 07:36:07,277 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-07-19 07:36:07,281 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-07-19 07:36:07,286 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-07-19 07:36:07,354 [main] INFO server.RaftServer: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: start RPC server
dn5_1    | 2023-07-19 07:35:33,921 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 2023-07-19 07:36:24,417 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-07-19 07:36:24,498 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:32,058 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-07-19 07:35:32,064 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-07-19 07:36:12,528 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-07-19 07:36:12,533 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | java.net.SocketTimeoutException: Call From aff5372d1efd/10.9.0.21 to scm2:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51412 remote=scm2/10.9.0.15:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-07-19 07:36:24,498 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om2#0:OK-t4
dn2_1    | 2023-07-19 07:38:27,399 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 140718b2-320a-4020-b7ea-662533776c74
recon_1  | 2023-07-19 07:35:48,468 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 2023-07-19 07:36:24,513 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result PASSED
om3_1    | 2023-07-19 07:36:24,513 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om3#0:OK-t4. Peer's state: om3@group-D66704EFC61C:t4, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:27,448 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 2023-07-19 07:35:26,104 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.FollowerState: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137067393ns, electionTimeout:5109ms
scm1_1   | 2023-07-19 07:35:26,106 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
om1_1    | 2023-07-19 07:41:25,126 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-equdq for user:hadoop
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-07-19 07:36:24,519 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 5, (t:4, i:112))
dn3_1    | 2023-07-19 07:35:40,961 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-19 07:35:40,964 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-07-19 07:36:12,629 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-07-19 07:36:12,631 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 2023-07-19 07:35:26,107 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
om1_1    | 2023-07-19 07:41:29,781 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buc-akpcb of layout FILE_SYSTEM_OPTIMIZED in volume: vol-equdq
om1_1    | 2023-07-19 07:41:42,335 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-vanff' with snapshotId: '19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53' under path 'vol-equdq/buc-akpcb'
om1_1    | 2023-07-19 07:41:42,507 [Thread-347] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
om1_1    | 2023-07-19 07:41:42,510 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 in 166 milliseconds
dn2_1    | 2023-07-19 07:38:27,450 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.LeaderElection:   Response 0: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t0
dn3_1    | 2023-07-19 07:35:40,966 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:36:12,635 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-07-19 07:36:12,667 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 112
om2_1    | 2023-07-19 07:36:12,670 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-07-19 07:35:26,108 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 2023-07-19 07:35:26,109 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3
scm1_1   | 2023-07-19 07:35:26,129 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 7 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-07-19 07:41:42,572 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 58 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 availability.
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:57566 remote=scm2/10.9.0.15:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
scm2_1   | 2023-07-19 07:35:32,110 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 2023-07-19 07:35:26,161 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:26,165 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:26,166 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om1_1    | 2023-07-19 07:41:42,580 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 for snapshot snap-vanff
om2_1    | 2023-07-19 07:36:12,680 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:12,684 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 4 for startAsFollower
om2_1    | 2023-07-19 07:36:12,692 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
scm2_1   | 2023-07-19 07:35:32,361 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-07-19 07:35:32,362 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-07-19 07:35:26,166 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
om1_1    | 2023-07-19 07:42:08,081 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-hzygr' with snapshotId: 'ccb2cd94-8b8f-40ee-b105-bc05abc45411' under path 'vol-equdq/buc-akpcb'
om2_1    | 2023-07-19 07:36:12,709 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
dn3_1    | 2023-07-19 07:35:40,969 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-19 07:35:40,970 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm1_1   | 2023-07-19 07:35:26,166 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:32,368 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-07-19 07:35:32,371 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-07-19 07:35:32,767 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
om1_1    | 2023-07-19 07:42:08,161 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 in 78 milliseconds
om2_1    | 2023-07-19 07:36:12,713 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-07-19 07:36:24,531 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 5 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:40,974 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-07-19 07:35:26,167 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3 PRE_VOTE round 0: result REJECTED
scm3_1   | 2023-07-19 07:36:07,937 [main] INFO server.GrpcService: 57eda1b0-9276-42ed-8a05-f1155bfae1c0: GrpcService started, listening on 9894
scm3_1   | 2023-07-19 07:36:07,977 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-57eda1b0-9276-42ed-8a05-f1155bfae1c0: Started
scm2_1   | 2023-07-19 07:35:32,770 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-07-19 07:42:08,163 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 2 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 availability.
om2_1    | 2023-07-19 07:36:12,714 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-19 07:38:27,450 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.LeaderElection:   Response 1: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-07-19 07:35:26,167 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
scm1_1   | 2023-07-19 07:35:26,168 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3
scm1_1   | 2023-07-19 07:35:26,168 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection3] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:31,315 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.FollowerState: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5147013570ns, electionTimeout:5126ms
om1_1    | 2023-07-19 07:42:08,164 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 for snapshot snap-hzygr
om2_1    | 2023-07-19 07:36:12,718 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-07-19 07:36:12,722 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-07-19 07:36:12,722 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-07-19 07:36:24,531 [grpc-default-executor-1] INFO impl.VoteContext: om3@group-D66704EFC61C-CANDIDATE: reject ELECTION from om1: already has voted for om3 at current term 5
dn3_1    | 2023-07-19 07:35:40,989 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-19 07:35:41,007 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 2023-07-19 07:35:31,326 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:31,326 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
scm3_1   | 2023-07-19 07:36:08,382 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-07-19 07:36:08,386 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
om1_1    | 2023-07-19 07:42:13,182 [IPC Server handler 11 on default port 9862] INFO snapshot.SnapshotDiffManager: Submitting snap diff report generation request for volume: vol-equdq, bucket: buc-akpcb, fromSnapshot: snap-vanff and toSnapshot: snap-hzygr
om2_1    | 2023-07-19 07:36:12,726 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
dn2_1    | 2023-07-19 07:38:27,450 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3 PRE_VOTE round 0: result REJECTED
om3_1    | 2023-07-19 07:36:24,551 [grpc-default-executor-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:FAIL-t5. Peer's state: om3@group-D66704EFC61C:t5, leader=null, voted=om3, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:36:24,565 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-07-19 07:36:24,567 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
om1_1    | 2023-07-19 07:42:13,287 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotDiffManager: Started snap diff report generation for volume: 'vol-equdq', bucket: 'buc-akpcb', fromSnapshot: 'snap-vanff', toSnapshot: 'snap-hzygr'
om2_1    | 2023-07-19 07:36:13,047 [main] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-07-19 07:36:13,951 [main] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om3_1    | 2023-07-19 07:36:24,993 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:35:41,010 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-19 07:35:41,010 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:35:41,091 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn3_1    | 2023-07-19 07:35:41,092 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-19 07:35:41,092 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:35:41,099 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef
om2_1    | 2023-07-19 07:36:13,967 [main] INFO upgrade.UpgradeFinalizer: Running pre-finalized state validations for unfinalized layout features.
om2_1    | 2023-07-19 07:36:13,970 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om3_1    | 2023-07-19 07:36:24,993 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om3<-om1#0:FAIL-t5
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn3_1    | 2023-07-19 07:35:41,181 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn3_1    | 2023-07-19 07:35:41,181 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-19 07:35:41,181 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
om1_1    | 2023-07-19 07:42:13,296 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om1_1    | 2023-07-19 07:42:13,301 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 2023-07-19 07:38:27,452 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 2023-07-19 07:36:24,993 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 1: om3<-om2#0:FAIL-t5
om3_1    | 2023-07-19 07:36:24,994 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om3@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result REJECTED
om3_1    | 2023-07-19 07:36:24,996 [om3@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
om3_1    | 2023-07-19 07:36:24,996 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-LeaderElection1
om3_1    | 2023-07-19 07:36:24,997 [om3@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
om2_1    | 2023-07-19 07:36:13,976 [main] INFO upgrade.UpgradeFinalizer: Running first upgrade commands for unfinalized layout features.
om1_1    | 2023-07-19 07:42:13,317 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn2_1    | 2023-07-19 07:38:27,452 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 2023-07-19 07:36:25,166 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om1 at term 5 for appendEntries, leader elected after 20755ms
scm3_1   | 2023-07-19 07:36:08,403 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm3_1   | 2023-07-19 07:36:08,424 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm3_1   | 2023-07-19 07:36:08,424 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm3_1   | 2023-07-19 07:36:10,391 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | 2023-07-19 07:36:10,641 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
om1_1    | 2023-07-19 07:42:13,534 [snapshot-diff-job-thread-id-0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
dn2_1    | 2023-07-19 07:38:27,452 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-LeaderElection3] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState
om3_1    | 2023-07-19 07:36:25,255 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 113: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:36:10,641 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
scm2_1   | 2023-07-19 07:35:32,966 [main] INFO util.log: Logging initialized @21640ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-07-19 07:35:33,522 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
scm3_1   | 2023-07-19 07:36:11,316 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
dn4_1    | 2023-07-19 07:35:34,352 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:35,353 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:36,354 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:37,356 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-07-19 07:36:25,271 [om3-server-thread2] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:113
om3_1    | 2023-07-19 07:36:25,930 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_113
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 2023-07-19 07:35:31,336 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1    | 2023-07-19 07:36:13,977 [main] INFO upgrade.UpgradeFinalizer: Skipping action QuotaRepairUpgradeAction since it has already been run.
om1_1    | 2023-07-19 07:42:13,547 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
scm3_1   | 2023-07-19 07:36:11,329 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread1] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: change Leader from null to b28076e1-4ec3-4254-8902-2272d74360c6 at term 8 for appendEntries, leader elected after 25066ms
scm3_1   | 2023-07-19 07:36:11,349 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread2] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: Failed appendEntries as the first entry (index 0) already exists (snapshotIndex: 78, commitIndex: 78)
dn2_1    | 2023-07-19 07:38:28,507 [grpc-default-executor-2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-075378E90876, 0, (t:0, i:0))
dn3_1    | 2023-07-19 07:35:41,181 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:31,338 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4
om2_1    | 2023-07-19 07:36:13,983 [main] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-07-19 07:42:13,548 [snapshot-diff-job-thread-id-0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn2_1    | 2023-07-19 07:38:28,507 [grpc-default-executor-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: receive requestVote(ELECTION, 140718b2-320a-4020-b7ea-662533776c74, group-075378E90876, 1, (t:0, i:0))
scm3_1   | 2023-07-19 07:36:11,534 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread2] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: inconsistency entries. Reply:b28076e1-4ec3-4254-8902-2272d74360c6<-57eda1b0-9276-42ed-8a05-f1155bfae1c0#132:FAIL-t8,INCONSISTENCY,nextIndex=79,followerCommit=78,matchIndex=-1
scm3_1   | 2023-07-19 07:36:11,801 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread2] INFO server.RaftServer$Division: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073: set configuration 79: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-07-19 07:35:48,429 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:57392 / 10.9.0.20:57392: output error
scm1_1   | 2023-07-19 07:35:31,356 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 7 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:14,309 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-07-19 07:36:14,310 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-07-19 07:38:28,516 [grpc-default-executor-0] INFO impl.VoteContext: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FOLLOWER: accept ELECTION from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-19 07:35:41,181 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 2023-07-19 07:36:11,818 [57eda1b0-9276-42ed-8a05-f1155bfae1c0-server-thread2] INFO segmented.SegmentedRaftLogWorker: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker: Rolling segment log-67_78 to index:78
recon_1  | 2023-07-19 07:35:48,429 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
scm1_1   | 2023-07-19 07:35:31,374 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-07-19 07:42:13,549 [snapshot-diff-job-thread-id-0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-07-19 07:36:14,619 [main] INFO util.log: Logging initialized @137141ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-07-19 07:36:16,022 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | 2023-07-19 07:38:28,516 [grpc-default-executor-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:140718b2-320a-4020-b7ea-662533776c74
dn2_1    | 2023-07-19 07:38:28,516 [grpc-default-executor-0] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState
scm3_1   | 2023-07-19 07:36:11,849 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67 to /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_67-78
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-07-19 07:35:31,376 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-07-19 07:42:13,705 [snapshot-diff-job-thread-id-0] INFO snapshot.SnapshotDiffManager: Starting diff report generation for jobId: 7b0447f8-9328-4491-adad-d7f18d1250a0.
om2_1    | 2023-07-19 07:36:16,055 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51412 remote=scm2/10.9.0.15:9861]
scm2_1   | 2023-07-19 07:35:33,569 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-07-19 07:35:33,639 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | 2023-07-19 07:35:33,646 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
dn2_1    | 2023-07-19 07:38:28,517 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState was interrupted
dn3_1    | 2023-07-19 07:35:41,182 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-19 07:35:41,183 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-07-19 07:35:31,376 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
om1_1    | 2023-07-19 07:42:15,755 [KeyDeletingService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
om2_1    | 2023-07-19 07:36:16,148 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-07-19 07:36:16,168 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-07-19 07:36:16,168 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-07-19 07:36:16,185 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-07-19 07:36:16,646 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om2_1    | 2023-07-19 07:36:16,682 [main] INFO http.HttpServer2: Jetty bound to port 9874
dn2_1    | 2023-07-19 07:38:28,517 [grpc-default-executor-0] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FollowerState
dn3_1    | 2023-07-19 07:35:41,184 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-07-19 07:36:12,088 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_79
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-07-19 07:35:31,376 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-07-19 07:42:15,757 [KeyDeletingService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om1_1    | 2023-07-19 07:42:15,758 [KeyDeletingService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om3_1    | 2023-07-19 07:36:26,591 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm2_1   | 2023-07-19 07:35:33,646 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-07-19 07:35:33,647 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-07-19 07:35:33,742 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm2_1   | 2023-07-19 07:35:33,744 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-07-19 07:35:33,750 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm2_1   | 2023-07-19 07:35:33,913 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-07-19 07:35:33,913 [main] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-07-19 07:35:33,917 [main] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
om3_1    | [id: "om1"
dn2_1    | 2023-07-19 07:38:28,531 [grpc-default-executor-0] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876 replies to ELECTION vote request: 140718b2-320a-4020-b7ea-662533776c74<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t1. Peer's state: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876:t1, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:41,184 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:35:41,185 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 2023-07-19 07:36:12,127 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
dn2_1    | 2023-07-19 07:38:28,532 [grpc-default-executor-2] INFO impl.VoteContext: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-FOLLOWER: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
scm2_1   | 2023-07-19 07:35:33,949 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@589dfa6f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-07-19 07:35:41,189 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-19 07:35:41,194 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:35:41,196 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn4_1    | 2023-07-19 07:35:37,616 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-07-19 07:35:37,644 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
om3_1    | address: "om3:9872"
dn2_1    | 2023-07-19 07:38:28,559 [grpc-default-executor-2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t1. Peer's state: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876:t1, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:33,951 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7c5f29c6{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-07-19 07:35:34,184 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@595fed99{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-874756801451812259/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm2_1   | 2023-07-19 07:35:34,198 [main] INFO server.AbstractConnector: Started ServerConnector@56e5c8fb{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 2023-07-19 07:36:12,128 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-07-19 07:36:12,130 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm3_1   | 2023-07-19 07:36:12,131 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
dn4_1    | 2023-07-19 07:35:38,072 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
dn4_1    | 2023-07-19 07:35:38,079 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn4_1    | 2023-07-19 07:35:38,205 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/in_use.lock acquired by nodename 7@6cb42cec657e
dn2_1    | 2023-07-19 07:38:28,561 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-075378E90876 with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 2023-07-19 07:35:41,203 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1   | 2023-07-19 07:35:34,198 [main] INFO server.Server: Started @22873ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 2023-07-19 07:36:12,290 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-07-19 07:42:15,901 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-07-19 07:42:15,902 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
dn4_1    | 2023-07-19 07:35:38,229 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/in_use.lock acquired by nodename 7@6cb42cec657e
dn4_1    | 2023-07-19 07:35:38,264 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/in_use.lock acquired by nodename 7@6cb42cec657e
dn4_1    | 2023-07-19 07:35:38,331 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=f02af6ab-c12d-469b-a775-f6b30900ff13} from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/raft-meta
dn2_1    | 2023-07-19 07:38:28,562 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 1 for appendEntries, leader elected after 6416ms
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 2023-07-19 07:36:12,295 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:13,007 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-07-19 07:36:13,309 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
dn2_1    | 2023-07-19 07:38:28,566 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread2] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 2023-07-19 07:36:13,604 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-07-19 07:36:13,907 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-07-19 07:36:13,913 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-07-19 07:38:36,390 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
dn2_1    | 2023-07-19 07:38:28,568 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread2] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
scm2_1   | 2023-07-19 07:35:34,203 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-07-19 07:42:15,913 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:36:16,700 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
dn4_1    | 2023-07-19 07:35:38,335 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=adc6845d-6cb4-4b43-88ca-47ca3f1a71df} from /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/raft-meta
dn4_1    | 2023-07-19 07:35:38,335 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=140718b2-320a-4020-b7ea-662533776c74} from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/raft-meta
dn2_1    | 2023-07-19 07:38:28,569 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-075378E90876-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/current/log_inprogress_0
dn3_1    | 2023-07-19 07:35:41,205 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-07-19 07:35:34,890 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:35,891 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:36:15,386 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3_1   | 2023-07-19 07:36:15,418 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om1_1    | 2023-07-19 07:42:15,916 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om2_1    | 2023-07-19 07:36:17,083 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-07-19 07:36:17,083 [main] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-07-19 07:36:17,127 [main] INFO server.session: node0 Scavenging every 600000ms
om3_1    | 2023-07-19 07:38:36,391 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
scm1_1   | 2023-07-19 07:35:31,376 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn2_1    | 2023-07-19 07:38:29,047 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107690363ns, electionTimeout:5100ms
dn3_1    | 2023-07-19 07:35:41,207 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-19 07:35:41,207 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-07-19 07:36:15,439 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-07-19 07:42:29,271 [IPC Server handler 5 on default port 9862] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn4_1    | 2023-07-19 07:35:38,395 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:36,892 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-07-19 07:35:31,377 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.LeaderElection: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4 PRE_VOTE round 0: result REJECTED
dn2_1    | 2023-07-19 07:38:29,048 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState
dn3_1    | 2023-07-19 07:35:41,210 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-19 07:35:41,296 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm3_1   | 2023-07-19 07:36:15,450 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
om1_1    | 2023-07-19 07:42:37,460 [IPC Server handler 24 on default port 9862] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
om2_1    | 2023-07-19 07:36:17,258 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2676d96a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-07-19 07:35:48,351 [IPC Server handler 30 on default port 9891] WARN ipc.Server: IPC Server handler 30 on default port 9891, call Call#13 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:51760 / 10.9.0.19:51760: output error
recon_1  | 2023-07-19 07:35:48,348 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:40486 / 10.9.0.19:40486: output error
om3_1    | 2023-07-19 07:38:36,397 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
dn5_1    | 2023-07-19 07:35:37,608 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
scm1_1   | 2023-07-19 07:35:31,377 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn2_1    | 2023-07-19 07:38:29,048 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-07-19 07:35:41,296 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-19 07:35:41,298 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-07-19 07:35:34,203 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-07-19 07:42:45,640 [IPC Server handler 54 on default port 9862] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
om1_1    | 2023-07-19 07:42:54,005 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snap-vanff' under path 'vol-equdq/buc-akpcb'
dn2_1    | 2023-07-19 07:38:29,048 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn2_1    | 2023-07-19 07:38:29,048 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4
dn2_1    | 2023-07-19 07:38:29,059 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-07-19 07:38:29,060 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:16,163 [IPC Server handler 12 on default port 9861] WARN ipc.Server: IPC Server handler 12 on default port 9861, call Call#29 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:56406 / 10.9.0.21:56406: output error
scm3_1   | 2023-07-19 07:36:16,164 [IPC Server handler 67 on default port 9861] WARN ipc.Server: IPC Server handler 67 on default port 9861, call Call#27 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:48918 / 10.9.0.18:48918: output error
scm3_1   | 2023-07-19 07:36:16,167 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#28 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:43104 / 10.9.0.18:43104: output error
scm3_1   | 2023-07-19 07:36:16,189 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#23 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:47222 / 10.9.0.17:47222: output error
scm3_1   | 2023-07-19 07:36:16,189 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#16 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:42438 / 10.9.0.21:42438: output error
scm3_1   | 2023-07-19 07:36:16,189 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#25 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:57254 / 10.9.0.20:57254: output error
scm3_1   | 2023-07-19 07:36:16,182 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#26 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:59586 / 10.9.0.20:59586: output error
dn5_1    | 2023-07-19 07:35:37,666 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm2_1   | 2023-07-19 07:35:34,209 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-07-19 07:35:34,941 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO impl.FollowerState: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5086842946ns, electionTimeout:5074ms
om2_1    | 2023-07-19 07:36:17,264 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4407b042{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-07-19 07:36:17,844 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153493028ns, electionTimeout:5109ms
om2_1    | 2023-07-19 07:36:17,853 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-07-19 07:36:17,859 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
om2_1    | 2023-07-19 07:36:17,880 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om2_1    | 2023-07-19 07:36:17,880 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
dn2_1    | 2023-07-19 07:38:29,062 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-07-19 07:38:29,089 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn2_1    | 2023-07-19 07:38:29,090 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.LeaderElection:   Response 0: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t0
dn2_1    | 2023-07-19 07:38:29,090 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.LeaderElection:   Response 1: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t0
dn2_1    | 2023-07-19 07:38:29,090 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.LeaderElection: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4 PRE_VOTE round 0: result REJECTED
dn2_1    | 2023-07-19 07:38:29,090 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
dn2_1    | 2023-07-19 07:38:29,090 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4
dn2_1    | 2023-07-19 07:38:29,096 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-LeaderElection4] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState
dn2_1    | 2023-07-19 07:38:29,269 [grpc-default-executor-1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-649965091133, 0, (t:0, i:0))
dn2_1    | 2023-07-19 07:38:29,271 [grpc-default-executor-1] INFO impl.VoteContext: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FOLLOWER: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:38,641 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: set configuration 3: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:38,635 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:38,681 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:38,844 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO ratis.ContainerStateMachine: group-20996759F8FD: Setting the last applied index to (t:3, i:4)
scm2_1   | 2023-07-19 07:35:34,942 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: b28076e1-4ec3-4254-8902-2272d74360c6: shutdown b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState
scm2_1   | 2023-07-19 07:35:34,943 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om2_1    | 2023-07-19 07:36:17,945 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 4 for 69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:18,502 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-07-19 07:36:18,503 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-07-19 07:36:18,520 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-07-19 07:36:18,541 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
dn4_1    | 2023-07-19 07:35:38,870 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Setting the last applied index to (t:7, i:46)
scm2_1   | 2023-07-19 07:35:34,948 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm2_1   | 2023-07-19 07:35:34,948 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-FollowerState] INFO impl.RoleInfo: b28076e1-4ec3-4254-8902-2272d74360c6: start b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1
scm2_1   | 2023-07-19 07:35:34,953 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:34,995 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 57eda1b0-9276-42ed-8a05-f1155bfae1c0
scm2_1   | 2023-07-19 07:35:35,000 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-07-19 07:36:19,462 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3c89b864{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-1759449756118075683/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om2_1    | 2023-07-19 07:36:19,650 [main] INFO server.AbstractConnector: Started ServerConnector@6cc90398{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-07-19 07:36:19,651 [main] INFO server.Server: Started @142172ms
om2_1    | 2023-07-19 07:36:19,749 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-07-19 07:36:19,749 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-07-19 07:38:36,398 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om3_1    | 2023-07-19 07:38:36,402 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
om3_1    | 2023-07-19 07:38:36,403 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: QUOTA.
scm1_1   | 2023-07-19 07:35:31,377 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4
scm1_1   | 2023-07-19 07:35:31,378 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-LeaderElection4] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm3_1   | 2023-07-19 07:36:16,172 [IPC Server handler 10 on default port 9861] WARN ipc.Server: IPC Server handler 10 on default port 9861, call Call#29 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:46444 / 10.9.0.19:46444: output error
om3_1    | 2023-07-19 07:38:36,404 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature QUOTA has been finalized.
om3_1    | 2023-07-19 07:38:36,405 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om3_1    | 2023-07-19 07:38:36,406 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-07-19 07:35:38,873 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO ratis.ContainerStateMachine: group-8563B54DD732: Setting the last applied index to (t:6, i:20)
scm2_1   | 2023-07-19 07:35:35,002 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:35:35,007 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 36a933ad-cfeb-4d3b-aa37-2c29c320331e
scm2_1   | 2023-07-19 07:35:35,464 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-07-19 07:35:35,966 [grpc-default-executor-1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: receive requestVote(PRE_VOTE, b28076e1-4ec3-4254-8902-2272d74360c6, group-05548E1F8073, 7, (t:7, i:78))
scm1_1   | 2023-07-19 07:35:35,974 [grpc-default-executor-1] INFO impl.VoteContext: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FOLLOWER: accept PRE_VOTE from b28076e1-4ec3-4254-8902-2272d74360c6: our priority 0 <= candidate's priority 0
om3_1    | 2023-07-19 07:38:36,419 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 6
om3_1    | 2023-07-19 07:40:07,219 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om3_1    | 2023-07-19 07:40:12,570 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om3_1    | 2023-07-19 07:40:27,747 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn4_1    | 2023-07-19 07:35:39,396 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm1_1   | 2023-07-19 07:35:36,009 [grpc-default-executor-1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 replies to PRE_VOTE vote request: b28076e1-4ec3-4254-8902-2272d74360c6<-36a933ad-cfeb-4d3b-aa37-2c29c320331e#0:OK-t7. Peer's state: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073:t7, leader=null, voted=36a933ad-cfeb-4d3b-aa37-2c29c320331e, raftlog=Memoized:36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLog:OPENED:c78, conf=67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:36,285 [grpc-default-executor-1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: receive requestVote(ELECTION, b28076e1-4ec3-4254-8902-2272d74360c6, group-05548E1F8073, 8, (t:7, i:78))
om3_1    | 2023-07-19 07:40:40,524 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
dn4_1    | 2023-07-19 07:35:40,145 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
scm1_1   | 2023-07-19 07:35:36,286 [grpc-default-executor-1] INFO impl.VoteContext: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FOLLOWER: accept ELECTION from b28076e1-4ec3-4254-8902-2272d74360c6: our priority 0 <= candidate's priority 0
om2_1    | 2023-07-19 07:36:19,761 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-07-19 07:36:19,802 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-07-19 07:36:19,843 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm2_1   | 2023-07-19 07:35:36,150 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 1 exception(s):
scm3_1   | 2023-07-19 07:36:16,202 [IPC Server handler 10 on default port 9861] INFO ipc.Server: IPC Server handler 10 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 2023-07-19 07:35:48,348 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:58014 / 10.9.0.18:58014: output error
dn3_1    | 2023-07-19 07:35:41,299 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-07-19 07:38:29,271 [grpc-default-executor-1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t0. Peer's state: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133:t0, leader=null, voted=, raftlog=Memoized:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-07-19 07:35:48,434 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
scm1_1   | 2023-07-19 07:35:36,286 [grpc-default-executor-1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:b28076e1-4ec3-4254-8902-2272d74360c6
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om2_1    | 2023-07-19 07:36:20,379 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
dn3_1    | 2023-07-19 07:35:41,301 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-07-19 07:38:29,391 [grpc-default-executor-1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: receive requestVote(ELECTION, f02af6ab-c12d-469b-a775-f6b30900ff13, group-649965091133, 1, (t:0, i:0))
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-07-19 07:35:36,286 [grpc-default-executor-1] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: shutdown 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
scm1_1   | 2023-07-19 07:35:36,287 [grpc-default-executor-1] INFO impl.RoleInfo: 36a933ad-cfeb-4d3b-aa37-2c29c320331e: start 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:375)
scm2_1   | 2023-07-19 07:35:36,152 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Response 0: b28076e1-4ec3-4254-8902-2272d74360c6<-36a933ad-cfeb-4d3b-aa37-2c29c320331e#0:OK-t7
scm2_1   | 2023-07-19 07:35:36,155 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:35:40,186 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-07-19 07:35:40,186 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-19 07:35:41,401 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-19 07:35:41,409 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:35:41,404 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 2023-07-19 07:35:36,287 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState] INFO impl.FollowerState: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-FollowerState was interrupted
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
om2_1    | 2023-07-19 07:36:21,866 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
dn3_1    | 2023-07-19 07:35:41,404 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-19 07:35:41,430 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-07-19 07:35:36,318 [grpc-default-executor-1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 replies to ELECTION vote request: b28076e1-4ec3-4254-8902-2272d74360c6<-36a933ad-cfeb-4d3b-aa37-2c29c320331e#0:OK-t8. Peer's state: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073:t8, leader=null, voted=b28076e1-4ec3-4254-8902-2272d74360c6, raftlog=Memoized:36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLog:OPENED:c78, conf=67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn3_1    | 2023-07-19 07:35:41,434 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:35:44,065 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 2023-07-19 07:35:36,723 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-server-thread1] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-07-19 07:35:36,156 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 PRE_VOTE round 0: result PASSED
om2_1    | 2023-07-19 07:36:22,926 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.347469930s. [buffered_nanos=2368365957, waiting_for_connection]
dn3_1    | 2023-07-19 07:35:44,084 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-19 07:35:44,090 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 2023-07-19 07:35:36,727 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-server-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: change Leader from null to b28076e1-4ec3-4254-8902-2272d74360c6 at term 8 for appendEntries, leader elected after 54958ms
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:35:40,219 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-07-19 07:35:40,220 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:35:36,235 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 ELECTION round 0: submit vote requests at term 8 for 67: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:23,079 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.483092928s. [buffered_nanos=2529810596, waiting_for_connection]
om2_1    | 2023-07-19 07:36:23,080 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn3_1    | 2023-07-19 07:35:44,092 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:44,193 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 2023-07-19 07:35:36,733 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-server-thread1] INFO server.RaftServer$Division: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073: set configuration 79: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:36,741 [36a933ad-cfeb-4d3b-aa37-2c29c320331e-server-thread1] INFO segmented.SegmentedRaftLogWorker: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker: Rolling segment log-67_78 to index:78
scm1_1   | 2023-07-19 07:35:36,755 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67 to /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_67-78
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm2_1   | 2023-07-19 07:35:36,257 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:35:36,257 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-07-19 07:36:23,081 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.347469930s. [buffered_nanos=2368365957, waiting_for_connection]
dn3_1    | 2023-07-19 07:35:44,206 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-07-19 07:38:29,392 [grpc-default-executor-1] INFO impl.VoteContext: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FOLLOWER: accept ELECTION from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn2_1    | 2023-07-19 07:38:29,392 [grpc-default-executor-1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f02af6ab-c12d-469b-a775-f6b30900ff13
dn2_1    | 2023-07-19 07:38:29,392 [grpc-default-executor-1] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: shutdown 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState
dn4_1    | 2023-07-19 07:35:40,224 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-07-19 07:35:36,892 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_79
scm1_1   | 2023-07-19 07:35:36,972 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-07-19 07:35:36,265 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-07-19 07:36:23,081 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: DEADLINE_EXCEEDED: deadline exceeded after 2.483092928s. [buffered_nanos=2529810596, waiting_for_connection]
dn3_1    | 2023-07-19 07:35:44,208 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:44,490 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn4_1    | 2023-07-19 07:35:40,225 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-07-19 07:35:36,976 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om2_1    | 2023-07-19 07:36:23,081 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 PRE_VOTE round 0: result REJECTED
scm2_1   | 2023-07-19 07:35:36,324 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1: ELECTION PASSED received 1 response(s) and 1 exception(s):
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-07-19 07:41:25,132 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-equdq for user:hadoop
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn4_1    | 2023-07-19 07:35:40,227 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-07-19 07:35:36,978 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm1_1   | 2023-07-19 07:35:36,978 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
om2_1    | 2023-07-19 07:36:23,082 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
dn2_1    | 2023-07-19 07:38:29,392 [grpc-default-executor-1] INFO impl.RoleInfo: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: start 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState
scm2_1   | 2023-07-19 07:35:36,324 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Response 0: b28076e1-4ec3-4254-8902-2272d74360c6<-36a933ad-cfeb-4d3b-aa37-2c29c320331e#0:OK-t8
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn3_1    | 2023-07-19 07:35:44,525 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-07-19 07:41:29,793 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buc-akpcb of layout FILE_SYSTEM_OPTIMIZED in volume: vol-equdq
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn4_1    | 2023-07-19 07:35:40,227 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-07-19 07:35:36,983 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-07-19 07:36:23,082 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
dn2_1    | 2023-07-19 07:38:29,392 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState] INFO impl.FollowerState: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-FollowerState was interrupted
scm2_1   | 2023-07-19 07:35:36,324 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-07-19 07:35:44,577 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:44,796 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: set configuration 0: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:40,230 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-07-19 07:35:36,991 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om2_1    | 2023-07-19 07:36:23,082 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
dn2_1    | 2023-07-19 07:38:29,404 [grpc-default-executor-1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133 replies to ELECTION vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t1. Peer's state: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133:t1, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn4_1    | 2023-07-19 07:35:40,245 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-07-19 07:35:36,994 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:56388 / 10.9.0.18:56388
scm1_1   | 2023-07-19 07:35:37,011 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:42972 / 10.9.0.19:42972
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn2_1    | 2023-07-19 07:38:29,576 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-649965091133 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn2_1    | 2023-07-19 07:38:29,576 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 1 for appendEntries, leader elected after 6027ms
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
om2_1    | 2023-07-19 07:36:24,138 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om1, group-D66704EFC61C, 4, (t:4, i:112))
scm1_1   | 2023-07-19 07:35:37,019 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:59290 / 10.9.0.17:59290
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn4_1    | 2023-07-19 07:35:40,227 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-07-19 07:38:29,576 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO server.RaftServer$Division: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
om2_1    | 2023-07-19 07:36:24,165 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om1: our priority 0 <= candidate's priority 0
scm1_1   | 2023-07-19 07:35:37,026 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:51734 / 10.9.0.21:51734
dn5_1    | 2023-07-19 07:35:37,669 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
dn5_1    | 2023-07-19 07:35:37,899 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-07-19 07:41:42,330 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-vanff' with snapshotId: '19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53' under path 'vol-equdq/buc-akpcb'
om3_1    | 2023-07-19 07:41:42,447 [Thread-317] INFO rocksdiff.RocksDBCheckpointDiffer: Skipped the compaction entry. Compaction input files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] and output files: [/data/metadata/om.db/000096.sst, /data/metadata/om.db/000077.sst, /data/metadata/om.db/000068.sst, /data/metadata/om.db/000046.sst] are same.
scm2_1   | 2023-07-19 07:35:36,324 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.LeaderElection: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1 ELECTION round 0: result PASSED
scm3_1   | 2023-07-19 07:36:16,172 [IPC Server handler 8 on default port 9861] WARN ipc.Server: IPC Server handler 8 on default port 9861, call Call#22 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:46860 / 10.9.0.17:46860: output error
scm3_1   | 2023-07-19 07:36:16,283 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861 caught an exception
dn2_1    | 2023-07-19 07:38:29,580 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef-server-thread1] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker: Starting segment from index:0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om2_1    | 2023-07-19 07:36:24,206 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om1<-om2#0:OK-t4. Peer's state: om2@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:37,033 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:57698 / 10.9.0.18:57698
dn5_1    | 2023-07-19 07:35:38,387 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
om3_1    | 2023-07-19 07:41:42,466 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 in 126 milliseconds
dn3_1    | 2023-07-19 07:35:44,829 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:36,324 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.RoleInfo: b28076e1-4ec3-4254-8902-2272d74360c6: shutdown b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1
scm3_1   | java.nio.channels.ClosedChannelException
dn4_1    | 2023-07-19 07:35:40,254 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-07-19 07:38:29,582 [8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef@group-649965091133-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/current/log_inprogress_0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om2_1    | 2023-07-19 07:36:24,377 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(PRE_VOTE, om3, group-D66704EFC61C, 4, (t:4, i:112))
scm1_1   | 2023-07-19 07:35:37,043 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:42072 / 10.9.0.20:42072
dn5_1    | 2023-07-19 07:35:38,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:38,561 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/in_use.lock acquired by nodename 7@aff5372d1efd
dn3_1    | 2023-07-19 07:35:44,840 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:44,841 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_0-0
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-07-19 07:35:40,254 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-07-19 07:38:46,188 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-07-19 07:36:24,384 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept PRE_VOTE from om3: our priority 0 <= candidate's priority 0
scm1_1   | 2023-07-19 07:35:37,050 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:57090 / 10.9.0.19:57090
om3_1    | 2023-07-19 07:41:42,571 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 97 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 availability.
om3_1    | 2023-07-19 07:41:42,578 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 for snapshot snap-vanff
dn3_1    | 2023-07-19 07:35:44,962 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: set configuration 1: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:44,968 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_1-2
scm1_1   | 2023-07-19 07:35:37,054 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:58318 / 10.9.0.17:58318
dn4_1    | 2023-07-19 07:35:40,255 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-07-19 07:42:08,101 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-hzygr' with snapshotId: 'ccb2cd94-8b8f-40ee-b105-bc05abc45411' under path 'vol-equdq/buc-akpcb'
dn3_1    | 2023-07-19 07:35:44,969 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_0-4
scm2_1   | 2023-07-19 07:35:36,331 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om2_1    | 2023-07-19 07:36:24,385 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to PRE_VOTE vote request: om3<-om2#0:OK-t4. Peer's state: om2@group-D66704EFC61C:t4, leader=null, voted=om2, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-07-19 07:35:37,064 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:57710 / 10.9.0.18:57710
dn5_1    | 2023-07-19 07:35:38,606 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/in_use.lock acquired by nodename 7@aff5372d1efd
dn3_1    | 2023-07-19 07:35:45,047 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: set configuration 5: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:36,331 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 8.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
om2_1    | 2023-07-19 07:36:24,458 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 5, (t:4, i:112))
scm1_1   | 2023-07-19 07:35:37,067 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861: skipped Call#4 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:51750 / 10.9.0.21:51750
dn5_1    | 2023-07-19 07:35:38,611 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/in_use.lock acquired by nodename 7@aff5372d1efd
dn4_1    | 2023-07-19 07:35:40,289 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:35:40,291 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-19 07:35:45,066 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: set configuration 3: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:45,062 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_0-7
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
om2_1    | 2023-07-19 07:36:24,461 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om2_1    | 2023-07-19 07:36:24,462 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:om1
om2_1    | 2023-07-19 07:36:24,463 [grpc-default-executor-1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
dn4_1    | 2023-07-19 07:35:40,308 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 2023-07-19 07:35:36,332 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,8>
dn3_1    | 2023-07-19 07:35:45,108 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_5-10
dn3_1    | 2023-07-19 07:35:45,111 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_inprogress_3
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn5_1    | 2023-07-19 07:35:38,615 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO storage.RaftStorage: Read RaftStorageMetadata{term=3, votedFor=f02af6ab-c12d-469b-a775-f6b30900ff13} from /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/raft-meta
dn5_1    | 2023-07-19 07:35:38,619 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO storage.RaftStorage: Read RaftStorageMetadata{term=7, votedFor=140718b2-320a-4020-b7ea-662533776c74} from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/raft-meta
om2_1    | 2023-07-19 07:36:24,464 [grpc-default-executor-1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-07-19 07:36:24,464 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState was interrupted
scm2_1   | 2023-07-19 07:35:36,339 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: change Leader from null to b28076e1-4ec3-4254-8902-2272d74360c6 at term 8 for becomeLeader, leader elected after 12734ms
dn3_1    | 2023-07-19 07:35:45,130 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:45,112 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: set configuration 8: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
dn2_1    | 2023-07-19 07:39:46,188 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-07-19 07:35:38,656 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=6, votedFor=f02af6ab-c12d-469b-a775-f6b30900ff13} from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/raft-meta
dn5_1    | 2023-07-19 07:35:38,900 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-07-19 07:36:24,480 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:OK-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:40,357 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-07-19 07:35:36,367 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-19 07:35:45,179 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:42:08,155 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 in 52 milliseconds
dn2_1    | 2023-07-19 07:40:46,189 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 2023-07-19 07:35:37,070 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:42080 / 10.9.0.20:42080
om2_1    | 2023-07-19 07:36:24,627 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om3, group-D66704EFC61C, 5, (t:4, i:112))
dn4_1    | 2023-07-19 07:35:40,358 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-07-19 07:35:36,392 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
dn3_1    | 2023-07-19 07:35:45,185 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
om3_1    | 2023-07-19 07:42:08,160 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 4 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 availability.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn2_1    | 2023-07-19 07:41:46,190 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn2_1    | 2023-07-19 07:42:46,191 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:35:38,964 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:38,962 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:45,292 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-07-19 07:35:40,361 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:38,968 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: set configuration 3: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:36,394 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-07-19 07:35:36,421 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm2_1   | 2023-07-19 07:35:36,421 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-19 07:35:45,201 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11
om3_1    | 2023-07-19 07:42:08,162 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 for snapshot snap-hzygr
scm1_1   | 2023-07-19 07:35:37,076 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861: skipped Call#6 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:57092 / 10.9.0.19:57092
scm1_1   | 2023-07-19 07:35:37,082 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861: skipped Call#7 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:58330 / 10.9.0.17:58330
dn4_1    | 2023-07-19 07:35:40,361 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:35:39,117 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO ratis.ContainerStateMachine: group-49D8FC4B32B3: Setting the last applied index to (t:3, i:4)
dn5_1    | 2023-07-19 07:35:39,143 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Setting the last applied index to (t:7, i:46)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
dn3_1    | 2023-07-19 07:35:45,329 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn3_1    | 2023-07-19 07:35:45,330 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn3_1    | 2023-07-19 07:35:45,371 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.LogSegment: Successfully read 22 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_8-29
dn3_1    | 2023-07-19 07:35:45,384 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-07-19 07:42:16,452 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-07-19 07:35:40,375 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:35:39,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO ratis.ContainerStateMachine: group-8563B54DD732: Setting the last applied index to (t:6, i:20)
scm2_1   | 2023-07-19 07:35:36,424 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 2023-07-19 07:35:36,449 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm2_1   | 2023-07-19 07:35:36,454 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-07-19 07:35:45,430 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30
scm1_1   | 2023-07-19 07:35:37,088 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#10 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:49976 / 10.9.0.18:49976
scm1_1   | 2023-07-19 07:35:37,098 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861: skipped Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:60498 / 10.9.0.20:60498
dn4_1    | 2023-07-19 07:35:40,382 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:35:40,369 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-07-19 07:35:40,383 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-07-19 07:35:36,484 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
om3_1    | 2023-07-19 07:42:16,460 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
dn3_1    | 2023-07-19 07:35:45,439 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
om2_1    | 2023-07-19 07:36:24,630 [grpc-default-executor-1] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: reject ELECTION from om3: already has voted for om1 at current term 5
om2_1    | 2023-07-19 07:36:24,633 [grpc-default-executor-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om3<-om2#0:FAIL-t5. Peer's state: om2@group-D66704EFC61C:t5, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c112, conf=69: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:40,383 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:35:40,398 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:40,477 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn5_1    | 2023-07-19 07:35:39,968 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn4_1    | 2023-07-19 07:35:40,490 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-07-19 07:35:36,486 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:35:36,486 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm2_1   | 2023-07-19 07:35:36,489 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-07-19 07:35:40,969 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:45,440 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 29
dn3_1    | 2023-07-19 07:35:46,133 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:46,375 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: start as a follower, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:46,375 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: changes role from      null to FOLLOWER at term 7 for startAsFollower
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 2023-07-19 07:42:16,462 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 2023-07-19 07:35:36,491 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-07-19 07:35:40,501 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:35:41,155 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,228 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1   | 2023-07-19 07:36:16,202 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
om3_1    | 2023-07-19 07:42:16,469 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-07-19 07:42:16,745 [SstFilteringService#0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-07-19 07:42:16,768 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:42:16,770 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om3_1    | 2023-07-19 07:42:16,771 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-07-19 07:42:16,772 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om3_1    | 2023-07-19 07:42:54,008 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snap-vanff' under path 'vol-equdq/buc-akpcb'
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 2023-07-19 07:35:37,679 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:42424 / 10.9.0.20:42424: output error
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om2_1    | 2023-07-19 07:36:25,302 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om1 at term 5 for appendEntries, leader elected after 23828ms
om2_1    | 2023-07-19 07:36:25,330 [om2-server-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 113: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-07-19 07:36:25,387 [om2-server-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:113
om2_1    | 2023-07-19 07:36:26,171 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_113
dn4_1    | 2023-07-19 07:35:40,484 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
scm1_1   | 2023-07-19 07:35:37,689 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:36660 / 10.9.0.21:36660: output error
scm1_1   | 2023-07-19 07:35:37,689 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#7 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:36650 / 10.9.0.21:36650: output error
scm1_1   | 2023-07-19 07:35:37,689 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:49990 / 10.9.0.18:49990: output error
scm1_1   | 2023-07-19 07:35:37,689 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:34274 / 10.9.0.19:34274: output error
dn3_1    | 2023-07-19 07:35:46,379 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: start as a follower, conf=3: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:46,392 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: start as a follower, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:46,407 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn3_1    | 2023-07-19 07:35:46,407 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
dn3_1    | 2023-07-19 07:35:46,382 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState
scm1_1   | 2023-07-19 07:35:37,689 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#10 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:57236 / 10.9.0.17:57236: output error
scm1_1   | 2023-07-19 07:35:37,713 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-07-19 07:35:46,423 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:35:46,407 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn3_1    | 2023-07-19 07:35:46,441 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState
dn3_1    | 2023-07-19 07:35:46,502 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:46,503 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
dn3_1    | 2023-07-19 07:35:46,512 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-19 07:35:46,517 [140718b2-320a-4020-b7ea-662533776c74-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-19 07:35:46,579 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:46,580 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:46,580 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn3_1    | 2023-07-19 07:35:46,584 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:46,584 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B30FA8B5BAEF,id=140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:35:46,590 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-07-19 07:35:46,591 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-19 07:35:46,591 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-19 07:35:47,683 [140718b2-320a-4020-b7ea-662533776c74-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-19 07:35:47,683 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:35:47,684 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-07-19 07:35:36,492 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-07-19 07:35:36,492 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm2_1   | 2023-07-19 07:35:36,492 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-19 07:35:47,686 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-07-19 07:35:40,510 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-07-19 07:35:40,510 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-07-19 07:35:40,511 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:35:40,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-07-19 07:35:40,516 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-07-19 07:35:40,525 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-07-19 07:35:40,525 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn5_1    | 2023-07-19 07:35:41,210 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-07-19 07:35:41,263 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-07-19 07:35:36,492 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:35:40,527 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-07-19 07:35:40,527 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-07-19 07:35:40,527 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-07-19 07:35:40,529 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-07-19 07:35:40,529 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
om2_1    | 2023-07-19 07:36:26,843 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-07-19 07:38:36,386 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 2023-07-19 07:35:36,492 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-07-19 07:38:36,387 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HSYNC.
dn5_1    | 2023-07-19 07:35:41,329 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:35:41,334 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,334 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:35:41,340 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,348 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:35:41,338 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:35:41,359 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,359 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:35:41,359 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-07-19 07:35:41,357 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:35:41,368 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-07-19 07:35:41,374 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,381 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-07-19 07:35:41,381 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,393 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,425 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:35:41,430 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:35:41,436 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:35:41,436 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,456 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
om2_1    | 2023-07-19 07:38:36,394 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature HSYNC has been finalized.
om2_1    | 2023-07-19 07:38:36,395 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: FILESYSTEM_SNAPSHOT.
om2_1    | 2023-07-19 07:38:36,396 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature FILESYSTEM_SNAPSHOT has been finalized.
om2_1    | 2023-07-19 07:38:36,396 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: QUOTA.
om2_1    | 2023-07-19 07:38:36,398 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Layout feature QUOTA has been finalized.
om2_1    | 2023-07-19 07:38:36,398 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
om2_1    | 2023-07-19 07:38:36,398 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.UpgradeFinalizer: Finalization is done.
om2_1    | 2023-07-19 07:38:36,416 [OMDoubleBufferFlushThread] INFO upgrade.OMFinalizeUpgradeResponse: Layout version to persist to DB : 6
om2_1    | 2023-07-19 07:40:07,169 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:new2-volume for user:hadoop
om2_1    | 2023-07-19 07:40:12,577 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: new2-volume
om2_1    | 2023-07-19 07:40:27,753 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: new2-bucket of layout FILE_SYSTEM_OPTIMIZED in volume: s3v
dn4_1    | 2023-07-19 07:35:40,532 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-07-19 07:35:40,536 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-07-19 07:35:40,572 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd
dn4_1    | 2023-07-19 07:35:40,572 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-07-19 07:35:40,577 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-07-19 07:35:40,585 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:35:40,590 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-07-19 07:35:40,591 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-07-19 07:35:40,591 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-07-19 07:35:40,592 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-07-19 07:35:40,593 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-07-19 07:35:40,723 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-07-19 07:35:41,457 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:35:41,468 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
dn5_1    | 2023-07-19 07:35:41,462 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:35:41,479 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-07-19 07:40:40,527 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:new2-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:214)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:375)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:567)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-07-19 07:35:41,515 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:358)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-07-19 07:41:25,140 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol-equdq for user:hadoop
om2_1    | 2023-07-19 07:41:29,790 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: buc-akpcb of layout FILE_SYSTEM_OPTIMIZED in volume: vol-equdq
om2_1    | 2023-07-19 07:41:42,321 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-vanff' with snapshotId: '19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53' under path 'vol-equdq/buc-akpcb'
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn3_1    | 2023-07-19 07:35:47,686 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-07-19 07:35:47,687 [140718b2-320a-4020-b7ea-662533776c74-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-19 07:35:47,710 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:47,727 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:47,727 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:47,805 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: start RPC server
dn3_1    | 2023-07-19 07:35:47,928 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 140718b2-320a-4020-b7ea-662533776c74: GrpcService started, listening on 9858
dn3_1    | 2023-07-19 07:35:47,947 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 140718b2-320a-4020-b7ea-662533776c74: GrpcService started, listening on 9856
dn3_1    | 2023-07-19 07:35:47,956 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: 140718b2-320a-4020-b7ea-662533776c74: GrpcService started, listening on 9857
dn4_1    | 2023-07-19 07:35:40,742 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,516 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn4_1    | 2023-07-19 07:35:40,743 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,525 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,202 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:37,715 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-07-19 07:35:40,736 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-07-19 07:35:40,748 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:35:42,246 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-07-19 07:35:48,048 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 140718b2-320a-4020-b7ea-662533776c74 is started using port 9858 for RATIS
dn3_1    | 2023-07-19 07:35:48,049 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 140718b2-320a-4020-b7ea-662533776c74 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-07-19 07:35:48,049 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 140718b2-320a-4020-b7ea-662533776c74 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-07-19 07:35:48,058 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-140718b2-320a-4020-b7ea-662533776c74: Started
dn3_1    | 2023-07-19 07:35:48,262 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-07-19 07:35:48,355 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn3_1    | 2023-07-19 07:35:48,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:49,736 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:50,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:51,678 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5271535254ns, electionTimeout:5097ms
dn3_1    | 2023-07-19 07:35:51,680 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
dn3_1    | 2023-07-19 07:35:51,681 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn3_1    | 2023-07-19 07:35:51,685 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-19 07:35:51,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:51,756 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-140718b2-320a-4020-b7ea-662533776c74: Detected pause in JVM or host machine approximately 0.102s without any GCs.
dn3_1    | 2023-07-19 07:35:51,771 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1
dn3_1    | 2023-07-19 07:35:51,781 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5399245564ns, electionTimeout:5188ms
dn3_1    | 2023-07-19 07:35:51,794 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState
dn3_1    | 2023-07-19 07:35:51,797 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn3_1    | 2023-07-19 07:35:51,801 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-19 07:35:51,801 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2
dn3_1    | 2023-07-19 07:35:51,918 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:51,935 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:52,444 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:52,461 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:52,546 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:52,552 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:52,549 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn3_1    | 2023-07-19 07:35:52,549 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 2023-07-19 07:35:52,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:52,921 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:6480616960ns, electionTimeout:5183ms
dn3_1    | 2023-07-19 07:35:52,927 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState
dn3_1    | 2023-07-19 07:35:52,928 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn3_1    | 2023-07-19 07:35:52,928 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-19 07:35:52,928 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3
dn3_1    | 2023-07-19 07:35:53,070 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:53,127 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn3_1    | 2023-07-19 07:35:53,471 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:53,473 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn3_1    | 2023-07-19 07:35:53,474 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3
dn3_1    | 2023-07-19 07:35:53,500 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn3_1    | 2023-07-19 07:35:53,511 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B30FA8B5BAEF with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:35:53,528 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 4 for becomeLeader, leader elected after 50759ms
dn3_1    | 2023-07-19 07:35:53,758 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:53,883 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-19 07:35:54,317 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:35:54,391 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-19 07:35:54,536 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-19 07:35:54,578 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-07-19 07:35:42,294 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:35:42,382 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,389 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,390 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-07-19 07:35:42,486 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,486 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,486 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-07-19 07:35:42,577 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,652 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:35:42,658 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-07-19 07:35:42,801 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:42,822 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:42,819 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: set configuration 0: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:42,862 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_0-0
dn4_1    | 2023-07-19 07:35:42,890 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: set configuration 1: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:42,938 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_1-2
dn4_1    | 2023-07-19 07:35:42,959 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: set configuration 3: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,032 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_inprogress_3
dn4_1    | 2023-07-19 07:35:43,005 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_0-7
dn4_1    | 2023-07-19 07:35:42,976 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_0-4
dn4_1    | 2023-07-19 07:35:43,040 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: set configuration 5: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,049 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_5-10
dn4_1    | 2023-07-19 07:35:43,054 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,076 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11
dn4_1    | 2023-07-19 07:35:43,087 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn4_1    | 2023-07-19 07:35:43,102 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: set configuration 8: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,102 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn4_1    | 2023-07-19 07:35:43,132 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn4_1    | 2023-07-19 07:35:43,147 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn4_1    | 2023-07-19 07:35:43,206 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.LogSegment: Successfully read 22 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_8-29
dn4_1    | 2023-07-19 07:35:43,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,270 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:43,414 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30
dn4_1    | 2023-07-19 07:35:43,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn4_1    | 2023-07-19 07:35:43,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 29
dn4_1    | 2023-07-19 07:35:43,802 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: start as a follower, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:43,804 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn4_1    | 2023-07-19 07:35:43,813 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn4_1    | 2023-07-19 07:35:43,824 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn4_1    | 2023-07-19 07:35:43,884 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:41,533 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:35:41,535 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
om2_1    | 2023-07-19 07:41:42,475 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 in 151 milliseconds
om2_1    | 2023-07-19 07:41:42,525 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 45 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 availability.
om2_1    | 2023-07-19 07:41:42,531 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-19a0dc62-3f66-44e9-8c8b-c3a4cb6efb53 for snapshot snap-vanff
om2_1    | 2023-07-19 07:42:08,087 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotCreateRequest: Created snapshot: 'snap-hzygr' with snapshotId: 'ccb2cd94-8b8f-40ee-b105-bc05abc45411' under path 'vol-equdq/buc-akpcb'
om2_1    | 2023-07-19 07:42:08,191 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 in 101 milliseconds
om2_1    | 2023-07-19 07:42:08,192 [OMDoubleBufferFlushThread] INFO db.RDBCheckpointUtils: Waited for 0 milliseconds for checkpoint directory /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 availability.
om2_1    | 2023-07-19 07:42:08,194 [OMDoubleBufferFlushThread] INFO om.OmSnapshotManager: Created checkpoint : /data/metadata/db.snapshots/checkpointState/om.db-ccb2cd94-8b8f-40ee-b105-bc05abc45411 for snapshot snap-hzygr
om2_1    | 2023-07-19 07:42:14,202 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:42:14,207 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-hzygr
om2_1    | 2023-07-19 07:42:14,209 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:42:14,219 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-07-19 07:42:14,446 [SstFilteringService#0] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-07-19 07:42:14,467 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:42:14,468 [SstFilteringService#0] INFO snapshot.SnapshotCache: Loading snapshot. Table key: /vol-equdq/buc-akpcb/snap-vanff
om2_1    | 2023-07-19 07:42:14,470 [SstFilteringService#0] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-07-19 07:42:14,471 [SstFilteringService#0] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om2_1    | 2023-07-19 07:42:53,998 [OM StateMachine ApplyTransaction Thread - 0] INFO snapshot.OMSnapshotDeleteRequest: Deleted snapshot 'snap-vanff' under path 'vol-equdq/buc-akpcb'
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,291 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-07-19 07:36:16,202 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 21 on default port 9891] INFO ipc.Server: IPC Server handler 21 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:37,715 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm2_1   | 2023-07-19 07:35:36,497 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm2_1   | 2023-07-19 07:35:36,498 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-07-19 07:35:36,498 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-07-19 07:35:36,501 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO impl.RoleInfo: b28076e1-4ec3-4254-8902-2272d74360c6: start b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderStateImpl
scm2_1   | 2023-07-19 07:35:36,510 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker: Rolling segment log-67_78 to index:78
scm2_1   | 2023-07-19 07:35:36,520 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_67 to /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_67-78
scm2_1   | 2023-07-19 07:35:36,552 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/305aa27f-3511-4772-a91d-05548e1f8073/current/log_inprogress_79
scm2_1   | 2023-07-19 07:35:36,563 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-LeaderElection1] INFO server.RaftServer$Division: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073: set configuration 79: peers:[57eda1b0-9276-42ed-8a05-f1155bfae1c0|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 36a933ad-cfeb-4d3b-aa37-2c29c320331e|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:36,632 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:36,682 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 80 -> 0
scm2_1   | 2023-07-19 07:35:36,764 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:36,798 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-07-19 07:35:36,808 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:36,815 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 80 -> 0
scm2_1   | 2023-07-19 07:35:36,916 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:36,925 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 80 -> 0
scm2_1   | 2023-07-19 07:35:36,941 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:36,948 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm2_1   | 2023-07-19 07:35:36,965 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm2_1   | 2023-07-19 07:35:37,007 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
scm2_1   | 2023-07-19 07:35:37,012 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:37,713 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 28 on default port 9891] INFO ipc.Server: IPC Server handler 28 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,202 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm2_1   | 2023-07-19 07:35:37,016 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 2, pipeline's with at least one datanode reported threshold count is 2
scm2_1   | 2023-07-19 07:35:37,022 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 2023-07-19 07:35:37,038 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,041 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,043 [grpc-default-executor-5] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:37,056 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2_1   | 2023-07-19 07:35:37,059 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn4_1.ha_net:57566 / 10.9.0.20:57566
scm2_1   | 2023-07-19 07:35:37,066 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861: skipped Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn2_1.ha_net:44402 / 10.9.0.18:44402
scm2_1   | 2023-07-19 07:35:37,076 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-07-19 07:35:37,253 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,255 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,262 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | 2023-07-19 07:35:43,896 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-07-19 07:35:43,898 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-07-19 07:35:45,816 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:45,809 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 2023-07-19 07:35:41,537 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:35:41,523 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn5_1    | 2023-07-19 07:35:41,551 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-07-19 07:35:41,551 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:35:41,561 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,562 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,562 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-07-19 07:35:41,563 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-07-19 07:35:41,567 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-07-19 07:35:41,575 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-07-19 07:35:41,575 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-07-19 07:35:41,607 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 2023-07-19 07:35:37,421 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:48524 / 10.9.0.19:48524: output error
scm2_1   | 2023-07-19 07:35:37,439 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn5_1.ha_net:51412 / 10.9.0.21:51412: output error
scm2_1   | 2023-07-19 07:35:37,448 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 27 on default port 9891] INFO ipc.Server: IPC Server handler 27 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:37,713 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn3_1    | 2023-07-19 07:35:54,597 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-19 07:35:54,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:54,832 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:35:54,865 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-19 07:35:55,036 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderStateImpl
dn3_1    | 2023-07-19 07:35:55,160 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn3_1    | 2023-07-19 07:35:55,242 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_inprogress_3 to /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_3-4
dn3_1    | 2023-07-19 07:35:55,300 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderElection3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: set configuration 5: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:55,491 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/current/log_inprogress_5
dn3_1    | 2023-07-19 07:35:55,770 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:56,772 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:57,480 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-07-19 07:35:45,805 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: start as a follower, conf=3: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:45,885 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn4_1    | 2023-07-19 07:35:45,808 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:45,894 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:43,902 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: start as a follower, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,201 [IPC Server handler 11 on default port 9861] WARN ipc.Server: IPC Server handler 11 on default port 9861, call Call#30 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn3_1.ha_net:47416 / 10.9.0.19:47416: output error
scm3_1   | 2023-07-19 07:36:16,303 [IPC Server handler 11 on default port 9861] INFO ipc.Server: IPC Server handler 11 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-07-19 07:35:41,616 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-07-19 07:35:41,621 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn5_1    | 2023-07-19 07:35:41,623 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-07-19 07:35:41,624 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-07-19 07:35:41,627 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-07-19 07:35:41,615 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-07-19 07:35:41,646 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-07-19 07:35:41,646 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-07-19 07:35:41,646 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-07-19 07:35:41,647 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-07-19 07:35:41,750 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,752 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:41,755 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,761 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-07-19 07:35:41,766 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:37,450 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:37,713 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn3_1    | 2023-07-19 07:35:57,480 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
dn3_1    | 2023-07-19 07:35:57,481 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:57,592 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:57,593 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:57,741 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:35:57,750 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
dn3_1    | 2023-07-19 07:35:57,750 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:57,773 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:57,852 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:35:57,941 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:58,774 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:59,119 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:35:59,132 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:35:59,135 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection:   Response 0: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7
dn4_1    | 2023-07-19 07:35:45,895 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from      null to FOLLOWER at term 7 for startAsFollower
dn4_1    | 2023-07-19 07:35:45,872 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-07-19 07:35:45,895 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:35:45,895 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState
dn4_1    | 2023-07-19 07:35:46,058 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:46,087 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:46,088 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:46,088 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:46,103 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-20996759F8FD,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn4_1    | 2023-07-19 07:35:46,103 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-07-19 07:35:46,103 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-07-19 07:35:43,216 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:43,227 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:35:43,237 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,239 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,244 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:43,439 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,442 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,443 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:43,628 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,629 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:35:43,631 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:43,928 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:43,940 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:43,943 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:43,999 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_0-0
dn5_1    | 2023-07-19 07:35:44,091 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: set configuration 1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,109 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.LogSegment: Successfully read 5 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_0-4
dn5_1    | 2023-07-19 07:35:44,065 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.LogSegment: Successfully read 8 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_0-7
dn5_1    | 2023-07-19 07:35:44,109 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_1-2
dn5_1    | 2023-07-19 07:35:44,134 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: set configuration 8: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,143 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: set configuration 5: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,144 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: set configuration 3: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.LogSegment: Successfully read 6 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_5-10
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:37,463 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from ha_dn1_1.ha_net:52090 / 10.9.0.17:52090: output error
scm2_1   | 2023-07-19 07:35:37,465 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
dn3_1    | 2023-07-19 07:35:59,143 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-CANDIDATE: reject PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 1 > candidate's priority 0
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,201 [IPC Server handler 67 on default port 9861] INFO ipc.Server: IPC Server handler 67 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 24 on default port 9891] INFO ipc.Server: IPC Server handler 24 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-07-19 07:35:46,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-07-19 07:35:46,105 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start RPC server
dn3_1    | 2023-07-19 07:35:59,153 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t7. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,157 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 1: result PASSED
dn3_1    | 2023-07-19 07:35:59,162 [grpc-default-executor-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-8563B54DD732, 6, (t:6, i:20))
dn3_1    | 2023-07-19 07:35:59,157 [grpc-default-executor-3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-8563B54DD732, 6, (t:6, i:20))
dn3_1    | 2023-07-19 07:35:59,179 [grpc-default-executor-0] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 0 <= candidate's priority 0
dn3_1    | 2023-07-19 07:35:59,199 [grpc-default-executor-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732 replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t6. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,203 [grpc-default-executor-3] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 0 <= candidate's priority 0
dn3_1    | 2023-07-19 07:35:59,234 [grpc-default-executor-3] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732 replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t6. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,234 [grpc-default-executor-1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:35:59,330 [grpc-default-executor-1] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-CANDIDATE: reject PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 1 > candidate's priority 0
dn3_1    | 2023-07-19 07:35:59,333 [grpc-default-executor-1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t8. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,237 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 ELECTION round 1: submit vote requests at term 8 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,358 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:46,133 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: GrpcService started, listening on 9858
dn4_1    | 2023-07-19 07:35:46,141 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: GrpcService started, listening on 9856
dn4_1    | 2023-07-19 07:35:46,152 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: GrpcService started, listening on 9857
dn4_1    | 2023-07-19 07:35:46,202 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adc6845d-6cb4-4b43-88ca-47ca3f1a71df is started using port 9858 for RATIS
dn4_1    | 2023-07-19 07:35:46,202 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adc6845d-6cb4-4b43-88ca-47ca3f1a71df is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-07-19 07:35:46,202 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adc6845d-6cb4-4b43-88ca-47ca3f1a71df is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-07-19 07:35:46,203 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Started
dn4_1    | 2023-07-19 07:35:46,419 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-07-19 07:35:46,588 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-07-19 07:35:46,905 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:47,910 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:48,915 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:49,918 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:50,916 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7105445973ns, electionTimeout:5020ms
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:43,344 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm1_1   | 2023-07-19 07:35:43,614 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9b83a7e-59b8-4455-b30a-c01eee264fbd{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
dn3_1    | 2023-07-19 07:35:59,358 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:35:59,637 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:35:59,646 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-CANDIDATE: reject PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 1 > candidate's priority 0
dn3_1    | 2023-07-19 07:35:59,648 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t8. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,651 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 6, (t:6, i:20))
dn3_1    | 2023-07-19 07:35:59,652 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn5_1    | 2023-07-19 07:35:44,239 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:44,244 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.LogSegment: Successfully read 2 entries from segment file /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_inprogress_3
dn5_1    | 2023-07-19 07:35:44,262 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 4
dn5_1    | 2023-07-19 07:35:44,288 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 2
dn5_1    | 2023-07-19 07:35:44,280 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: set configuration 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,307 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.LogSegment: Successfully read 10 entries from segment file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11
dn5_1    | 2023-07-19 07:35:44,319 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 20
dn5_1    | 2023-07-19 07:35:44,319 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 10
dn5_1    | 2023-07-19 07:35:44,288 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.LogSegment: Successfully read 22 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_8-29
dn5_1    | 2023-07-19 07:35:44,335 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: set configuration 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,347 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.LogSegment: Successfully read 17 entries from segment file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30
dn4_1    | 2023-07-19 07:35:50,917 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn4_1    | 2023-07-19 07:35:50,922 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:50,923 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn4_1    | 2023-07-19 07:35:50,939 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-07-19 07:35:50,948 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1
dn4_1    | 2023-07-19 07:35:50,985 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:51,256 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5361011269ns, electionTimeout:5169ms
dn4_1    | 2023-07-19 07:35:51,256 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:35:51,257 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn4_1    | 2023-07-19 07:35:51,257 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-07-19 07:35:51,259 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2
dn4_1    | 2023-07-19 07:35:51,194 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5299453379ns, electionTimeout:5100ms
dn4_1    | 2023-07-19 07:35:51,331 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState
dn4_1    | 2023-07-19 07:35:51,336 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn4_1    | 2023-07-19 07:35:51,344 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 23 on default port 9891] INFO ipc.Server: IPC Server handler 23 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm1_1   | 2023-07-19 07:35:43,778 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-07-19 07:35:43,893 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:35:44,082 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-07-19 07:35:43,893 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | 2023-07-19 07:35:44,140 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:45,258 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-07-19 07:35:45,267 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:46,880 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-07-19 07:35:46,880 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:47,247 [IPC Server handler 55 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm1_1   | 2023-07-19 07:35:47,248 [IPC Server handler 55 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-07-19 07:35:47,248 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:35:47,250 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,201 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-07-19 07:35:59,653 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t6. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,663 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 6, (t:6, i:20))
dn3_1    | 2023-07-19 07:35:59,664 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-19 07:35:59,665 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t6. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,688 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:35:59,692 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-CANDIDATE: reject PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 1 > candidate's priority 0
dn3_1    | 2023-07-19 07:35:59,698 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t8. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:35:59,787 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:35:59,937 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:35:59,942 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection:   Response 0: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6
dn3_1    | 2023-07-19 07:35:59,942 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection:   Response 1: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6
dn3_1    | 2023-07-19 07:35:59,946 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1 PRE_VOTE round 1: result REJECTED
dn3_1    | 2023-07-19 07:35:59,954 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn3_1    | 2023-07-19 07:35:59,954 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:37,866 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,866 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,942 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,942 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:37,954 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:38,042 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,048 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,062 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:35:44,381 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 46
dn5_1    | 2023-07-19 07:35:44,413 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> 29
dn5_1    | 2023-07-19 07:35:44,722 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: start as a follower, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,723 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: changes role from      null to FOLLOWER at term 7 for startAsFollower
dn5_1    | 2023-07-19 07:35:44,727 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState
dn5_1    | 2023-07-19 07:35:44,737 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:44,793 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:44,811 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 2023-07-19 07:35:47,275 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:47,275 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-07-19 07:35:48,088 [IPC Server handler 37 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm1_1   | 2023-07-19 07:35:48,089 [IPC Server handler 37 on default port 9861] INFO node.SCMNodeManager: Registered Data node : adc6845d-6cb4-4b43-88ca-47ca3f1a71df{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-07-19 07:35:48,099 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:35:48,110 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1   | 2023-07-19 07:35:48,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 2023-07-19 07:35:48,113 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-07-19 07:35:48,117 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1   | 2023-07-19 07:35:48,129 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn4_1    | 2023-07-19 07:35:51,347 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3
dn4_1    | 2023-07-19 07:35:51,439 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:51,517 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:51,573 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn4_1    | 2023-07-19 07:35:51,808 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:35:51,808 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:51,898 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:51,931 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:52,010 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:52,010 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:52,010 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 140718b2-320a-4020-b7ea-662533776c74
dn4_1    | 2023-07-19 07:35:52,283 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:52,283 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn3_1    | 2023-07-19 07:35:59,956 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-LeaderElection1] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
dn3_1    | 2023-07-19 07:36:00,045 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm2_1   | 2023-07-19 07:35:38,247 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,258 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,304 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:38,454 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,462 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:38,551 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:39,119 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,123 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,188 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:16,201 [IPC Server handler 12 on default port 9861] INFO ipc.Server: IPC Server handler 12 on default port 9861 caught an exception
scm3_1   | java.nio.channels.ClosedChannelException
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn5_1    | 2023-07-19 07:35:44,814 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: start as a follower, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,814 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: start as a follower, conf=3: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:44,844 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: changes role from      null to FOLLOWER at term 3 for startAsFollower
dn5_1    | 2023-07-19 07:35:44,811 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-19 07:36:00,049 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection:   Response 0: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t8
dn3_1    | 2023-07-19 07:36:00,050 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2 ELECTION round 1: result PASSED
dn3_1    | 2023-07-19 07:36:00,052 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2
dn3_1    | 2023-07-19 07:36:00,054 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
dn3_1    | 2023-07-19 07:36:00,055 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-03BEFB15C8DE with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:36:00,057 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 8 for becomeLeader, leader elected after 55073ms
dn3_1    | 2023-07-19 07:36:00,070 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-07-19 07:36:00,074 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:36:00,077 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-19 07:36:00,082 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-19 07:36:00,084 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 22 on default port 9891] INFO ipc.Server: IPC Server handler 22 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn5_1    | 2023-07-19 07:35:44,845 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState
dn5_1    | 2023-07-19 07:35:44,845 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:44,844 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: changes role from      null to FOLLOWER at term 6 for startAsFollower
dn5_1    | 2023-07-19 07:35:44,875 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState
dn5_1    | 2023-07-19 07:35:44,876 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-07-19 07:35:44,877 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread3] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-07-19 07:35:44,907 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-49D8FC4B32B3,id=f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:44,910 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-07-19 07:35:44,911 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-07-19 07:35:44,911 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-07-19 07:35:44,912 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread2] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-07-19 07:35:44,925 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:47,057 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-07-19 07:35:48,134 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:48,135 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm1_1   | 2023-07-19 07:35:48,148 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1_1   | 2023-07-19 07:35:49,220 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:49,575 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02af6ab-c12d-469b-a775-f6b30900ff13
scm1_1   | 2023-07-19 07:35:49,576 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f02af6ab-c12d-469b-a775-f6b30900ff13{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-07-19 07:35:39,188 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,215 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:39,493 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,505 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:35:52,283 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3
dn4_1    | 2023-07-19 07:35:52,284 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn4_1    | 2023-07-19 07:35:52,284 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-20996759F8FD with new leaderId: adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn4_1    | 2023-07-19 07:35:52,321 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: change Leader from null to adc6845d-6cb4-4b43-88ca-47ca3f1a71df at term 4 for becomeLeader, leader elected after 48182ms
dn4_1    | 2023-07-19 07:35:52,791 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-07-19 07:35:52,873 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-07-19 07:35:39,544 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:39,680 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,686 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,699 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:39,779 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,782 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:39,803 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:40,371 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:40,371 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:40,833 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:40,901 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:40,975 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:41,659 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:36:00,085 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-07-19 07:36:00,087 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:36:00,088 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-19 07:36:00,136 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: receive requestVote(ELECTION, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 7, (t:6, i:20))
dn3_1    | 2023-07-19 07:36:00,147 [grpc-default-executor-2] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FOLLOWER: accept ELECTION from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-19 07:36:00,148 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 2023-07-19 07:36:00,150 [grpc-default-executor-2] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
dn3_1    | 2023-07-19 07:36:00,151 [grpc-default-executor-2] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
dn3_1    | 2023-07-19 07:36:00,152 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState was interrupted
dn3_1    | 2023-07-19 07:36:00,240 [grpc-default-executor-2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732 replies to ELECTION vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t7. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t7, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:36:00,355 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-19 07:36:00,357 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:36:00,358 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-19 07:36:00,376 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm3_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:49,576 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:35:49,577 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-07-19 07:35:49,961 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:50,443 [IPC Server handler 62 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/140718b2-320a-4020-b7ea-662533776c74
scm1_1   | 2023-07-19 07:35:50,444 [IPC Server handler 62 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 140718b2-320a-4020-b7ea-662533776c74{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-07-19 07:35:50,445 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:35:50,446 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-07-19 07:35:50,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
dn5_1    | 2023-07-19 07:35:47,103 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:47,027 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:46,976 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 2023-07-19 07:35:52,875 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-07-19 07:35:52,936 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:53,044 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-07-19 07:35:53,074 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-07-19 07:35:53,107 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-07-19 07:35:53,312 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-07-19 07:35:53,370 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-07-19 07:35:53,452 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderStateImpl
dn4_1    | 2023-07-19 07:35:53,661 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn4_1    | 2023-07-19 07:35:53,704 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_inprogress_3 to /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_3-4
dn4_1    | 2023-07-19 07:35:53,841 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/current/log_inprogress_5
dn4_1    | 2023-07-19 07:35:53,913 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderElection3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: set configuration 5: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:53,951 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:35:50,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-07-19 07:35:50,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-07-19 07:35:50,447 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-07-19 07:36:48,348 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm1_1   | 2023-07-19 07:36:48,349 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm3_1   | 2023-07-19 07:36:16,677 [main] INFO util.log: Logging initialized @45011ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-07-19 07:36:17,023 [IPC Server handler 78 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm2_1   | 2023-07-19 07:35:41,660 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:41,875 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:41,884 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:41,899 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:42,051 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:42,061 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:42,072 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:42,214 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:42,219 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:42,221 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:42,914 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:42,914 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:43,199 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:43,226 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:36:00,383 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-19 07:36:00,384 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:36:00,384 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-19 07:36:00,395 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn3_1    | 2023-07-19 07:36:00,396 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-19 07:36:00,397 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-19 07:36:00,427 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-07-19 07:36:00,431 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:36:00,431 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-07-19 07:36:00,433 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-19 07:36:00,433 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-19 07:36:00,434 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:36:00,435 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-19 07:36:00,436 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm3_1   | 2023-07-19 07:36:17,100 [IPC Server handler 78 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9b83a7e-59b8-4455-b30a-c01eee264fbd{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-07-19 07:36:17,162 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3_1   | 2023-07-19 07:36:17,223 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3_1   | 2023-07-19 07:36:17,268 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-07-19 07:36:17,274 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-07-19 07:36:17,269 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:17,496 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:17,496 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:17,723 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-07-19 07:36:17,784 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3_1   | 2023-07-19 07:36:17,899 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-07-19 07:36:17,936 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm3_1   | 2023-07-19 07:36:17,938 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-07-19 07:36:17,942 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm1_1   | 2023-07-19 07:36:49,069 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn3_1    | 2023-07-19 07:36:00,437 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:35:54,954 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:55,958 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:56,965 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:57,008 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn4_1    | 2023-07-19 07:35:57,008 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
dn4_1    | 2023-07-19 07:35:57,009 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:57,029 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:36:00,437 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-07-19 07:35:57,031 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:57,097 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn4_1    | 2023-07-19 07:35:57,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
dn4_1    | 2023-07-19 07:35:57,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:43,231 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 2023-07-19 07:36:18,262 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm3_1   | 2023-07-19 07:36:18,267 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm3_1   | 2023-07-19 07:36:18,282 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm3_1   | 2023-07-19 07:36:18,681 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-07-19 07:36:18,689 [main] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-07-19 07:36:18,731 [main] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | 2023-07-19 07:36:49,091 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-07-19 07:35:43,598 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 2023-07-19 07:36:00,473 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderStateImpl
dn4_1    | 2023-07-19 07:35:57,132 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:35:57,143 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:35:57,968 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:58,389 [grpc-default-executor-2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-03BEFB15C8DE, 7, (t:7, i:46))
scm1_1   | 2023-07-19 07:36:49,093 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
scm1_1   | 2023-07-19 07:36:49,105 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
scm1_1   | 2023-07-19 07:36:49,117 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn3_1    | 2023-07-19 07:36:00,490 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolling segment log-30_46 to index:46
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 2023-07-19 07:35:43,604 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:36:19,016 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@577bfadb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-07-19 07:36:19,026 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@729cd862{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm3_1   | 2023-07-19 07:36:20,143 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7893c715{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-8773157457046542631/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm1_1   | 2023-07-19 07:36:49,129 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm1_1   | 2023-07-19 07:36:49,130 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-07-19 07:36:00,499 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30 to /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_30-46
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-07-19 07:35:46,768 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:35:47,237 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:47,103 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-07-19 07:35:43,619 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | 2023-07-19 07:35:58,402 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:36:00,580 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_47
scm1_1   | 2023-07-19 07:36:49,145 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-07-19 07:36:49,156 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
scm1_1   | 2023-07-19 07:36:49,157 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:49,159 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:49,165 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
dn3_1    | 2023-07-19 07:36:00,703 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: set configuration 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-07-19 07:35:47,243 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-07-19 07:35:47,243 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm2_1   | 2023-07-19 07:35:43,820 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:43,863 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:43,881 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:44,020 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:36:00,885 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:44,041 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-07-19 07:36:49,165 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn3_1    | 2023-07-19 07:36:01,123 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8563B54DD732 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 2023-07-19 07:36:01,170 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 7 for appendEntries, leader elected after 56505ms
dn4_1    | 2023-07-19 07:35:58,412 [grpc-default-executor-6] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-8563B54DD732, 6, (t:6, i:20))
dn4_1    | 2023-07-19 07:35:58,428 [grpc-default-executor-5] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:58,433 [grpc-default-executor-6] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 0
dn4_1    | 2023-07-19 07:35:58,399 [grpc-default-executor-7] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-8563B54DD732, 6, (t:6, i:20))
dn4_1    | 2023-07-19 07:35:58,419 [grpc-default-executor-4] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 7, (t:7, i:46))
dn4_1    | 2023-07-19 07:35:58,418 [grpc-default-executor-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-03BEFB15C8DE, 7, (t:7, i:46))
dn3_1    | 2023-07-19 07:36:01,320 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: set configuration 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:47,244 [f02af6ab-c12d-469b-a775-f6b30900ff13-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-07-19 07:35:47,244 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: start RPC server
dn5_1    | 2023-07-19 07:35:47,320 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: f02af6ab-c12d-469b-a775-f6b30900ff13: GrpcService started, listening on 9858
dn5_1    | 2023-07-19 07:35:47,335 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: f02af6ab-c12d-469b-a775-f6b30900ff13: GrpcService started, listening on 9856
scm1_1   | 2023-07-19 07:36:49,169 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm3_1   | 2023-07-19 07:36:20,221 [main] INFO server.AbstractConnector: Started ServerConnector@2262f0d8{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3_1   | 2023-07-19 07:36:20,221 [main] INFO server.Server: Started @48559ms
scm3_1   | 2023-07-19 07:36:20,249 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn3_1    | 2023-07-19 07:36:01,396 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker: Rolling segment log-11_20 to index:20
dn5_1    | 2023-07-19 07:35:47,358 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO server.GrpcService: f02af6ab-c12d-469b-a775-f6b30900ff13: GrpcService started, listening on 9857
scm3_1   | 2023-07-19 07:36:20,249 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3_1   | 2023-07-19 07:36:20,256 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
dn4_1    | 2023-07-19 07:35:58,414 [grpc-default-executor-1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 6, (t:6, i:20))
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn3_1    | 2023-07-19 07:36:01,460 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11 to /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_11-20
dn5_1    | 2023-07-19 07:35:47,422 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02af6ab-c12d-469b-a775-f6b30900ff13 is started using port 9858 for RATIS
scm1_1   | 2023-07-19 07:36:49,170 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
dn4_1    | 2023-07-19 07:35:58,478 [grpc-default-executor-3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 6, (t:6, i:20))
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn3_1    | 2023-07-19 07:36:01,480 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_21
dn3_1    | 2023-07-19 07:36:01,888 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:47,436 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02af6ab-c12d-469b-a775-f6b30900ff13 is started using port 9857 for RATIS_ADMIN
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm2_1   | 2023-07-19 07:35:44,074 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | 2023-07-19 07:35:58,522 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:36:02,902 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:36:03,998 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-140718b2-320a-4020-b7ea-662533776c74: Detected pause in JVM or host machine approximately 0.102s without any GCs.
dn5_1    | 2023-07-19 07:35:47,436 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis f02af6ab-c12d-469b-a775-f6b30900ff13 is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-07-19 07:35:47,439 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-f02af6ab-c12d-469b-a775-f6b30900ff13: Started
dn4_1    | 2023-07-19 07:35:58,529 [grpc-default-executor-4] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-19 07:36:16,196 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm1_1   | 2023-07-19 07:36:49,172 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
dn5_1    | 2023-07-19 07:35:47,604 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm3_1   | 2023-07-19 07:36:20,939 [IPC Server handler 70 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm2_1   | 2023-07-19 07:35:44,208 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm2_1   | 2023-07-19 07:35:44,217 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:36:48,356 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 2023-07-19 07:36:49,172 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
dn5_1    | 2023-07-19 07:35:47,763 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | 2023-07-19 07:36:20,972 [IPC Server handler 70 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-07-19 07:36:20,974 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:20,997 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
dn4_1    | 2023-07-19 07:35:58,530 [grpc-default-executor-4] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:37:01,061 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 2023-07-19 07:36:49,173 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 in state CLOSED which uses HEALTHY_READONLY datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd. This will send close commands for its containers.
dn5_1    | 2023-07-19 07:35:48,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:36:20,997 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-07-19 07:36:21,006 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-07-19 07:36:22,363 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm3_1   | 2023-07-19 07:36:22,364 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : adc6845d-6cb4-4b43-88ca-47ca3f1a71df{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn3_1    | 2023-07-19 07:37:01,063 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn5_1    | 2023-07-19 07:35:49,117 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm3_1   | 2023-07-19 07:36:22,366 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:22,400 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-07-19 07:36:22,417 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm3_1   | 2023-07-19 07:36:22,418 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn3_1    | 2023-07-19 07:37:01,064 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm1_1   | 2023-07-19 07:36:49,175 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm1_1   | 2023-07-19 07:36:49,175 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a in state CLOSED which uses HEALTHY_READONLY datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:49,175 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm1_1   | 2023-07-19 07:36:49,176 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:49,177 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:49,179 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:36:50,879 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:37:01,065 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
scm1_1   | 2023-07-19 07:36:52,385 [IPC Server handler 58 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:36:59,576 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:00,091 [IPC Server handler 56 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:16,874 [IPC Server handler 7 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:17,895 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
dn3_1    | 2023-07-19 07:37:01,065 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-07-19 07:35:49,984 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5257145307ns, electionTimeout:5107ms
dn5_1    | 2023-07-19 07:35:49,987 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState
dn5_1    | 2023-07-19 07:35:49,992 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn5_1    | 2023-07-19 07:35:50,013 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-07-19 07:35:50,017 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1
dn5_1    | 2023-07-19 07:35:50,028 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:50,127 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:37:01,065 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn5_1    | 2023-07-19 07:35:50,136 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 2023-07-19 07:35:44,218 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:44,329 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:44,343 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-07-19 07:35:58,536 [grpc-default-executor-0] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 0
dn4_1    | 2023-07-19 07:35:58,536 [grpc-default-executor-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:58,539 [grpc-default-executor-2] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 0
dn4_1    | 2023-07-19 07:35:58,540 [grpc-default-executor-2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
dn4_1    | 2023-07-19 07:35:58,673 [grpc-default-executor-6] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:58,700 [grpc-default-executor-3] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:58,717 [grpc-default-executor-3] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm3_1   | 2023-07-19 07:36:22,418 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-07-19 07:36:22,421 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3_1   | 2023-07-19 07:36:22,424 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-07-19 07:35:44,384 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:44,400 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a9b83a7e-59b8-4455-b30a-c01eee264fbd{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-07-19 07:35:44,743 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:44,755 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
scm1_1   | 2023-07-19 07:37:20,899 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:22,327 [IPC Server handler 58 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:29,549 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:30,108 [IPC Server handler 58 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,276 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn4_1    | 2023-07-19 07:35:58,719 [grpc-default-executor-7] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 0
dn4_1    | 2023-07-19 07:35:58,737 [grpc-default-executor-7] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:58,741 [grpc-default-executor-1] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-CANDIDATE: accept PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:58,780 [grpc-default-executor-1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:59,007 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:35:59,164 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:37:01,066 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm1_1   | 2023-07-19 07:37:31,344 [IPC Server handler 65 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,346 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
scm1_1   | 2023-07-19 07:37:31,397 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,444 [IPC Server handler 61 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,582 [FixedThreadPoolWithAffinityExecutor-0-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn3_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm3_1   | 2023-07-19 07:36:22,425 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3_1   | 2023-07-19 07:36:22,434 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:22,676 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:22,677 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:27,853 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:27,853 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:29,595 [IPC Server handler 52 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm2_1   | 2023-07-19 07:35:44,753 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:44,876 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-07-19 07:35:44,880 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-07-19 07:35:45,086 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-07-19 07:35:45,014 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1   | 2023-07-19 07:35:45,094 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-07-19 07:35:45,470 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-07-19 07:35:45,471 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,543 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,545 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,559 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:45,673 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,711 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,768 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm2_1   | 2023-07-19 07:35:45,849 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,855 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,882 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:45,982 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:45,987 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:35:50,138 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:50,159 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-07-19 07:35:50,159 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 2023-07-19 07:35:51,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:52,133 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:52,170 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7295307051ns, electionTimeout:5067ms
dn5_1    | 2023-07-19 07:35:52,234 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState
dn5_1    | 2023-07-19 07:35:52,234 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:37:30,544 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Completed APPEND_ENTRIES, lastRequest: f02af6ab-c12d-469b-a775-f6b30900ff13->140718b2-320a-4020-b7ea-662533776c74#4-t7,previous=(t:7, i:21),leaderCommit=22,initializing? true,entries: size=1, first=(t:7, i:22), METADATAENTRY(c:21)
scm2_1   | 2023-07-19 07:35:46,004 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,100 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,110 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:35:59,165 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection:   Response 0: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6
dn4_1    | 2023-07-19 07:35:59,165 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1 PRE_VOTE round 1: result REJECTED
dn4_1    | 2023-07-19 07:35:59,167 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn4_1    | 2023-07-19 07:35:59,169 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1
dn3_1    | 2023-07-19 07:37:30,545 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Completed APPEND_ENTRIES, lastReply: null
scm3_1   | 2023-07-19 07:36:29,598 [IPC Server handler 52 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f02af6ab-c12d-469b-a775-f6b30900ff13{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-07-19 07:36:29,600 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:29,602 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-07-19 07:36:30,102 [IPC Server handler 82 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/140718b2-320a-4020-b7ea-662533776c74
scm3_1   | 2023-07-19 07:36:30,102 [IPC Server handler 82 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 140718b2-320a-4020-b7ea-662533776c74{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-07-19 07:36:30,103 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-07-19 07:36:30,105 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
dn3_1    | 2023-07-19 07:37:30,555 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Completed APPEND_ENTRIES, lastRequest: null
scm3_1   | 2023-07-19 07:36:30,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3_1   | 2023-07-19 07:36:30,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3_1   | 2023-07-19 07:36:30,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3_1   | 2023-07-19 07:36:30,106 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1   | 2023-07-19 07:36:32,959 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:32,962 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:38,118 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:37:30,614 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Completed APPEND_ENTRIES, lastReply: serverReply {
scm3_1   | 2023-07-19 07:36:38,118 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:43,133 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:43,134 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:48,274 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:48,275 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:48,308 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm3_1   | 2023-07-19 07:36:48,309 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
dn3_1    |   requestorId: "f02af6ab-c12d-469b-a775-f6b30900ff13"
scm3_1   | 2023-07-19 07:36:49,047 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn3_1    |   replyId: "140718b2-320a-4020-b7ea-662533776c74"
dn3_1    |   raftGroupId {
scm2_1   | 2023-07-19 07:35:46,118 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,228 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,230 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,241 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,284 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,291 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:35:52,235 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    |     id: "mV\227\372\022#@\377\272l\205c\265M\3272"
dn4_1    | 2023-07-19 07:35:59,169 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection1] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn4_1    | 2023-07-19 07:35:59,393 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: receive requestVote(ELECTION, f02af6ab-c12d-469b-a775-f6b30900ff13, group-8563B54DD732, 7, (t:6, i:20))
dn4_1    | 2023-07-19 07:35:59,393 [grpc-default-executor-5] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FOLLOWER: accept ELECTION from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:59,396 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:35:59,398 [grpc-default-executor-5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn5_1    | 2023-07-19 07:35:52,235 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2
dn5_1    | 2023-07-19 07:35:52,435 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7589247940ns, electionTimeout:5188ms
dn3_1    |   }
dn3_1    |   callId: 41
scm3_1   | 2023-07-19 07:36:49,066 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
dn3_1    |   success: true
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | }
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:35:52,435 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState
dn5_1    | 2023-07-19 07:35:52,436 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-07-19 07:35:52,438 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-07-19 07:35:52,438 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3
dn5_1    | 2023-07-19 07:35:52,382 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:52,486 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | term: 7
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn5_1    | 2023-07-19 07:35:52,488 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:52,535 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 3 for 3: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:52,602 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3 PRE_VOTE round 0: result PASSED (term=3)
dn5_1    | 2023-07-19 07:35:52,939 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for 3: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:52,954 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3 ELECTION round 0: result PASSED (term=4)
dn5_1    | 2023-07-19 07:35:53,017 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3
dn3_1    | nextIndex: 23
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn3_1    | followerCommit: 22
dn4_1    | 2023-07-19 07:35:59,398 [grpc-default-executor-5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn4_1    | 2023-07-19 07:35:59,406 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState was interrupted
dn4_1    | 2023-07-19 07:35:59,449 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732 replies to ELECTION vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t7, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:35:59,500 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-07-19 07:35:59,500 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection:   Response 0: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t7
dn4_1    | 2023-07-19 07:35:59,502 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection:   Response 1: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t7
dn4_1    | 2023-07-19 07:35:59,502 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2 PRE_VOTE round 1: result REJECTED
dn3_1    | matchIndex: 18446744073709551615
dn4_1    | 2023-07-19 07:35:59,503 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn4_1    | 2023-07-19 07:35:59,503 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2
dn4_1    | 2023-07-19 07:35:59,505 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection2] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:35:59,707 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: receive requestVote(ELECTION, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 8, (t:7, i:46))
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn3_1    | isHearbeat: true
dn3_1    | 
dn3_1    | 2023-07-19 07:37:31,097 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: remove  FOLLOWER 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732:t7, leader=f02af6ab-c12d-469b-a775-f6b30900ff13, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLog:OPENED:c22, conf=21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-07-19 07:37:31,100 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: shutdown
dn3_1    | 2023-07-19 07:37:31,100 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:37:31,100 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState
scm3_1   | 2023-07-19 07:36:49,087 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-07-19 07:35:53,021 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
dn5_1    | 2023-07-19 07:35:53,039 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-49D8FC4B32B3 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:53,119 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 4 for becomeLeader, leader elected after 48189ms
dn5_1    | 2023-07-19 07:35:53,142 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-07-19 07:37:31,101 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-FollowerState was interrupted
dn3_1    | 2023-07-19 07:37:31,102 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Taking a snapshot at:(t:7, i:22) file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm3_1   | 2023-07-19 07:36:49,089 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn5_1    | 2023-07-19 07:35:53,662 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-07-19 07:35:53,845 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:35:53,871 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-07-19 07:35:54,016 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-07-19 07:35:54,016 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-07-19 07:35:54,042 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1  | 2023-07-19 07:35:48,435 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
scm3_1   | 2023-07-19 07:36:49,117 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
scm3_1   | 2023-07-19 07:36:49,125 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
scm3_1   | 2023-07-19 07:36:49,125 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm3_1   | 2023-07-19 07:36:49,142 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
scm3_1   | 2023-07-19 07:36:49,143 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 2023-07-19 07:36:49,146 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,147 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
recon_1  | java.nio.channels.ClosedChannelException
scm3_1   | 2023-07-19 07:36:49,147 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn3_1    | 2023-07-19 07:37:31,103 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater: set stopIndex = 22
dn3_1    | 2023-07-19 07:37:31,107 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Finished taking a snapshot at:(t:7, i:22) file:/data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22 took: 6 ms
dn3_1    | 2023-07-19 07:37:31,110 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater: Took a snapshot at index 22
dn3_1    | 2023-07-19 07:37:31,111 [140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 22
dn3_1    | 2023-07-19 07:37:31,118 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: closes. applyIndex: 22
dn3_1    | 2023-07-19 07:37:31,734 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732-SegmentedRaftLogWorker close()
dn3_1    | 2023-07-19 07:37:31,789 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-07-19 07:35:59,709 [grpc-default-executor-5] INFO impl.VoteContext: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FOLLOWER: accept ELECTION from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn4_1    | 2023-07-19 07:35:59,710 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:140718b2-320a-4020-b7ea-662533776c74
dn4_1    | 2023-07-19 07:35:59,713 [grpc-default-executor-5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:35:59,714 [grpc-default-executor-5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:35:59,714 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState was interrupted
dn4_1    | 2023-07-19 07:35:59,753 [grpc-default-executor-5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE replies to ELECTION vote request: 140718b2-320a-4020-b7ea-662533776c74<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t8. Peer's state: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn4_1    | 2023-07-19 07:36:00,009 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:46,297 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,372 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,378 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,385 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,435 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
dn4_1    | 2023-07-19 07:36:00,790 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8563B54DD732 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:36:00,796 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 7 for appendEntries, leader elected after 58465ms
dn4_1    | 2023-07-19 07:36:00,889 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: set configuration 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:36:00,891 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker: Rolling segment log-11_20 to index:20
dn4_1    | 2023-07-19 07:36:00,918 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11 to /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_11-20
dn4_1    | 2023-07-19 07:36:00,936 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_21
dn4_1    | 2023-07-19 07:36:01,010 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:36:01,270 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-03BEFB15C8DE with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
dn4_1    | 2023-07-19 07:36:01,274 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread1] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 8 for appendEntries, leader elected after 57039ms
dn4_1    | 2023-07-19 07:36:01,357 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread2] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: set configuration 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:36:01,358 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-server-thread2] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolling segment log-30_46 to index:46
dn4_1    | 2023-07-19 07:36:01,360 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30 to /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_30-46
dn4_1    | 2023-07-19 07:36:01,366 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_47
dn4_1    | 2023-07-19 07:36:02,055 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:36:03,058 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:36:16,173 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-07-19 07:36:46,588 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:35:54,162 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:54,186 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	... 3 more
scm1_1   | 2023-07-19 07:37:31,800 [IPC Server handler 9 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,845 [IPC Server handler 7 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,876 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:31,901 [IPC Server handler 4 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:32,636 [IPC Server handler 1 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm1_1   | 2023-07-19 07:37:32,663 [IPC Server handler 6 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:32,669 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
scm1_1   | 2023-07-19 07:37:32,670 [FixedThreadPoolWithAffinityExecutor-1-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1001
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm2_1   | 2023-07-19 07:35:46,436 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,458 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,526 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,560 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,573 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,662 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,670 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn5_1    | 2023-07-19 07:35:54,215 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-07-19 07:35:54,257 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderStateImpl
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-07-19 07:35:54,462 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker: Rolling segment log-3_4 to index:4
dn5_1    | 2023-07-19 07:35:54,543 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_inprogress_3 to /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_3-4
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-07-19 07:37:31,789 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn3_1    | 2023-07-19 07:37:31,870 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn3_1    | 2023-07-19 07:37:31,870 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn3_1    | 2023-07-19 07:37:31,917 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-8563B54DD732: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn3_1    | 2023-07-19 07:37:31,922 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 command on datanode 140718b2-320a-4020-b7ea-662533776c74.
dn3_1    | 2023-07-19 07:37:31,922 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: remove    LEADER 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE:t8, leader=140718b2-320a-4020-b7ea-662533776c74, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c48, conf=47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-07-19 07:37:31,923 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: shutdown
dn5_1    | 2023-07-19 07:35:54,615 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/current/log_inprogress_5
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm3_1   | 2023-07-19 07:36:49,149 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
dn5_1    | 2023-07-19 07:35:54,667 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderElection3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: set configuration 5: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:46,675 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,734 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,744 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,794 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn5_1    | 2023-07-19 07:35:55,167 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-07-19 07:35:46,798 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-07-19 07:35:55,230 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn5_1    | 2023-07-19 07:35:55,230 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1 PRE_VOTE round 0: result TIMEOUT
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-07-19 07:37:31,923 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=140718b2-320a-4020-b7ea-662533776c74
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm2_1   | 2023-07-19 07:35:46,818 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,934 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,976 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:46,980 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:46,994 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    | 2023-07-19 07:37:23,285 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:37:23,287 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-07-19 07:37:23,288 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-07-19 07:37:23,288 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn4_1    | 2023-07-19 07:37:23,288 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm3_1   | 2023-07-19 07:36:49,152 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,153 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,154 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,154 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
scm3_1   | 2023-07-19 07:36:49,154 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 in state CLOSED which uses HEALTHY_READONLY datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,155 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm2_1   | 2023-07-19 07:35:46,994 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
dn4_1    | 2023-07-19 07:37:23,289 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm3_1   | 2023-07-19 07:36:49,156 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a in state CLOSED which uses HEALTHY_READONLY datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,157 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
scm3_1   | 2023-07-19 07:36:49,157 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,158 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm3_1   | 2023-07-19 07:36:49,159 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:35:47,315 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:47,318 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:35:55,230 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1 PRE_VOTE round 1: submit vote requests at term 7 for 30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:55,306 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:35:47,327 [grpc-default-executor-5] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:35:47,452 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn3_1    | 2023-07-19 07:37:31,923 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-LeaderStateImpl
dn3_1    | 2023-07-19 07:37:31,927 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
dn3_1    | 2023-07-19 07:37:31,933 [grpc-default-executor-5] INFO server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-07-19 07:37:31,936 [grpc-default-executor-5] INFO leader.FollowerInfo: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 48
dn5_1    | 2023-07-19 07:35:55,307 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:56,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:57,195 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:57,575 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2: PRE_VOTE TIMEOUT received 0 response(s) and 0 exception(s):
dn5_1    | 2023-07-19 07:35:57,576 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 PRE_VOTE round 0: result TIMEOUT
recon_1  | 2023-07-19 07:35:48,434 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
scm3_1   | 2023-07-19 07:36:50,869 [IPC Server handler 73 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:36:52,364 [IPC Server handler 25 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:36:53,304 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:53,304 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn3_1    | 2023-07-19 07:37:31,938 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-07-19 07:37:31,938 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm2_1   | 2023-07-19 07:35:47,457 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:47,545 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:47,985 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:47,985 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:48,001 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:48,005 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:48,016 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:48,133 [IPC Server handler 41 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm2_1   | 2023-07-19 07:35:48,185 [IPC Server handler 41 on default port 9861] INFO node.SCMNodeManager: Registered Data node : adc6845d-6cb4-4b43-88ca-47ca3f1a71df{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-07-19 07:35:48,185 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 2023-07-19 07:35:57,579 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 PRE_VOTE round 1: submit vote requests at term 6 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:57,605 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:35:57,605 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:35:58,198 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:35:58,586 [grpc-default-executor-1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-8563B54DD732, 6, (t:6, i:20))
dn5_1    | 2023-07-19 07:35:58,588 [grpc-default-executor-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-03BEFB15C8DE, 7, (t:7, i:46))
dn5_1    | 2023-07-19 07:35:58,602 [grpc-default-executor-2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 7, (t:7, i:46))
dn5_1    | 2023-07-19 07:35:58,608 [grpc-default-executor-0] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 0 <= candidate's priority 0
dn5_1    | 2023-07-19 07:35:58,674 [grpc-default-executor-4] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-03BEFB15C8DE, 7, (t:7, i:46))
dn5_1    | 2023-07-19 07:35:58,755 [grpc-default-executor-3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: receive requestVote(PRE_VOTE, adc6845d-6cb4-4b43-88ca-47ca3f1a71df, group-8563B54DD732, 6, (t:6, i:20))
dn5_1    | 2023-07-19 07:35:58,609 [grpc-default-executor-1] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-CANDIDATE: reject PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 1 > candidate's priority 0
dn5_1    | 2023-07-19 07:35:58,778 [grpc-default-executor-5] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-8563B54DD732, 6, (t:6, i:20))
dn5_1    | 2023-07-19 07:35:58,790 [grpc-default-executor-1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732 replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:37:31,942 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Taking a snapshot at:(t:8, i:48) file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48
dn3_1    | 2023-07-19 07:37:31,958 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Finished taking a snapshot at:(t:8, i:48) file:/data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48 took: 16 ms
dn3_1    | 2023-07-19 07:37:31,960 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater: Took a snapshot at index 48
dn3_1    | 2023-07-19 07:37:31,960 [140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 48
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:37:23,290 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm1_1   | 	... 3 more
scm1_1   | 2023-07-19 07:37:32,692 [IPC Server handler 8 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-07-19 07:37:31,968 [grpc-default-executor-5] INFO server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-07-19 07:37:31,968 [grpc-default-executor-5] INFO leader.FollowerInfo: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df: decreaseNextIndex nextIndex: updateUnconditionally 49 -> 48
dn3_1    | 2023-07-19 07:37:31,974 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-StateMachineUpdater: set stopIndex = 48
scm1_1   | 2023-07-19 07:37:32,734 [IPC Server handler 9 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:32,735 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
scm1_1   | 2023-07-19 07:37:32,737 [FixedThreadPoolWithAffinityExecutor-1-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
dn3_1    | 2023-07-19 07:37:31,974 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: closes. applyIndex: 48
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:37:30,540 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastRequest: f02af6ab-c12d-469b-a775-f6b30900ff13->adc6845d-6cb4-4b43-88ca-47ca3f1a71df#2-t7,previous=(t:7, i:21),leaderCommit=21,initializing? true,entries: size=1, first=(t:7, i:22), METADATAENTRY(c:21)
dn4_1    | 2023-07-19 07:37:30,541 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastReply: null
dn4_1    | 2023-07-19 07:37:30,549 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-07-19 07:37:30,623 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastReply: serverReply {
dn4_1    |   requestorId: "f02af6ab-c12d-469b-a775-f6b30900ff13"
dn3_1    | 2023-07-19 07:37:31,981 [grpc-default-executor-5] INFO server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-07-19 07:37:31,982 [grpc-default-executor-5] INFO leader.FollowerInfo: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->adc6845d-6cb4-4b43-88ca-47ca3f1a71df: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 47
dn3_1    | 2023-07-19 07:37:31,997 [grpc-default-executor-5] INFO server.GrpcLogAppender: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 2023-07-19 07:37:31,998 [grpc-default-executor-5] INFO leader.FollowerInfo: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE->f02af6ab-c12d-469b-a775-f6b30900ff13: decreaseNextIndex nextIndex: updateUnconditionally 48 -> 47
dn3_1    | 2023-07-19 07:37:32,606 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE-SegmentedRaftLogWorker close()
dn3_1    | 2023-07-19 07:37:32,618 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn3_1    | 2023-07-19 07:37:32,618 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn3_1    | 2023-07-19 07:37:32,701 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn3_1    | 2023-07-19 07:37:32,701 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn3_1    | 2023-07-19 07:37:32,744 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-03BEFB15C8DE: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm2_1   | 2023-07-19 07:35:48,183 [IPC Server handler 32 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm2_1   | 2023-07-19 07:35:48,208 [IPC Server handler 32 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | 2023-07-19 07:36:58,361 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:36:58,361 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:36:59,569 [IPC Server handler 50 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:00,100 [IPC Server handler 82 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-07-19 07:35:58,817 [grpc-default-executor-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t7. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:48,216 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-07-19 07:35:48,265 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:35:58,819 [grpc-default-executor-4] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 0 <= candidate's priority 0
dn5_1    | 2023-07-19 07:35:58,919 [grpc-default-executor-4] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t7. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:58,926 [grpc-default-executor-2] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn5_1    | 2023-07-19 07:35:58,973 [grpc-default-executor-2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t7. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:58,791 [grpc-default-executor-5] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-CANDIDATE: reject PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 1 > candidate's priority 0
dn5_1    | 2023-07-19 07:35:59,033 [grpc-default-executor-5] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:59,035 [grpc-default-executor-3] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-CANDIDATE: reject PRE_VOTE from adc6845d-6cb4-4b43-88ca-47ca3f1a71df: our priority 1 > candidate's priority 0
dn5_1    | 2023-07-19 07:35:59,085 [grpc-default-executor-3] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732 replies to PRE_VOTE vote request: adc6845d-6cb4-4b43-88ca-47ca3f1a71df<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:58,907 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-8563B54DD732, 6, (t:6, i:20))
dn5_1    | 2023-07-19 07:35:58,995 [grpc-default-executor-7] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 7, (t:7, i:46))
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,434 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
scm2_1   | 2023-07-19 07:35:48,266 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 0, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-07-19 07:35:48,266 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-07-19 07:35:48,297 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-07-19 07:35:48,344 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-07-19 07:35:48,344 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Pipelines with at least one datanode reported count is 2, required at least one datanode reported per pipeline count is 2
scm2_1   | 2023-07-19 07:35:48,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-07-19 07:35:48,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-07-19 07:35:48,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-07-19 07:35:48,381 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-07-19 07:35:48,382 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm2_1   | 2023-07-19 07:35:48,485 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 1.
scm3_1   | 2023-07-19 07:37:03,546 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:03,546 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:37:08,650 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:08,650 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:37:13,784 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:13,784 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:37:16,896 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:17,890 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm3_1   | 2023-07-19 07:37:18,958 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:18,958 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:37:20,902 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:22,319 [IPC Server handler 23 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn3_1    | 2023-07-19 07:37:32,746 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de command on datanode 140718b2-320a-4020-b7ea-662533776c74.
dn4_1    |   replyId: "adc6845d-6cb4-4b43-88ca-47ca3f1a71df"
dn4_1    |   raftGroupId {
dn4_1    |     id: "mV\227\372\022#@\377\272l\205c\265M\3272"
dn4_1    |   }
dn4_1    |   callId: 41
dn4_1    |   success: true
dn4_1    | }
dn4_1    | term: 7
dn4_1    | nextIndex: 23
dn3_1    | 2023-07-19 07:37:32,747 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: remove    LEADER 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF:t4, leader=140718b2-320a-4020-b7ea-662533776c74, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLog:OPENED:c6, conf=5: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn3_1    | 2023-07-19 07:37:32,747 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: shutdown
dn3_1    | 2023-07-19 07:37:32,747 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-B30FA8B5BAEF,id=140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:37:32,747 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-LeaderStateImpl
dn3_1    | 2023-07-19 07:37:32,747 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-PendingRequests: sendNotLeaderResponses
dn3_1    | 2023-07-19 07:37:32,751 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater: set stopIndex = 6
dn3_1    | 2023-07-19 07:37:32,752 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-B30FA8B5BAEF: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/sm/snapshot.4_6
dn3_1    | 2023-07-19 07:37:32,759 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-B30FA8B5BAEF: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef/sm/snapshot.4_6 took: 7 ms
dn3_1    | 2023-07-19 07:37:32,759 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater: Took a snapshot at index 6
dn3_1    | 2023-07-19 07:37:32,759 [140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater] INFO impl.StateMachineUpdater: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
dn3_1    | 2023-07-19 07:37:32,764 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: closes. applyIndex: 6
dn3_1    | 2023-07-19 07:37:32,922 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF-SegmentedRaftLogWorker close()
dn5_1    | 2023-07-19 07:35:59,141 [grpc-default-executor-7] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-CANDIDATE: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn5_1    | 2023-07-19 07:35:59,141 [grpc-default-executor-7] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t7. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t7, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn4_1    | followerCommit: 22
dn4_1    | matchIndex: 18446744073709551615
scm3_1   | 2023-07-19 07:37:23,969 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:23,969 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:35:49,193 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-07-19 07:35:49,236 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,236 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,273 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,279 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,293 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:49,348 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,356 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,398 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:49,594 [IPC Server handler 78 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:59,101 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-CANDIDATE: reject PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 1 > candidate's priority 0
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-07-19 07:35:59,215 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | isHearbeat: true
dn5_1    | 2023-07-19 07:35:59,221 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t6. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732:t6, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLog:OPENED:c20, conf=11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:37:29,167 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:29,167 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 
dn4_1    | 2023-07-19 07:37:31,944 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastRequest: 140718b2-320a-4020-b7ea-662533776c74->adc6845d-6cb4-4b43-88ca-47ca3f1a71df#2-t8,previous=(t:8, i:47),leaderCommit=47,initializing? true,entries: size=1, first=(t:8, i:48), METADATAENTRY(c:47)
dn4_1    | 2023-07-19 07:37:31,946 [grpc-default-executor-5] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastReply: null
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	... 3 more
scm1_1   | 2023-07-19 07:37:32,936 [IPC Server handler 3 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:32,976 [IPC Server handler 11 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:32,998 [IPC Server handler 34 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:33,018 [IPC Server handler 56 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:46,878 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:52,346 [IPC Server handler 65 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:53,102 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm1_1   | 2023-07-19 07:37:53,102 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:37:53,413 [IPC Server handler 61 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-07-19 07:35:59,260 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:37:32,925 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-B30FA8B5BAEF: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/63a4529d-d8b2-46dd-93ed-b30fa8b5baef
dn3_1    | 2023-07-19 07:37:32,925 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef command on datanode 140718b2-320a-4020-b7ea-662533776c74.
dn3_1    | 2023-07-19 07:37:36,228 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
scm3_1   | 2023-07-19 07:37:29,546 [IPC Server handler 45 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:30,091 [IPC Server handler 82 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:35:49,594 [IPC Server handler 78 on default port 9861] INFO node.SCMNodeManager: Registered Data node : f02af6ab-c12d-469b-a775-f6b30900ff13{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-07-19 07:35:59,260 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection:   Response 0: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t6
dn4_1    | 2023-07-19 07:37:31,962 [grpc-default-executor-9] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastRequest: null
dn4_1    | 2023-07-19 07:37:31,975 [grpc-default-executor-9] INFO server.GrpcServerProtocolService: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: Completed APPEND_ENTRIES, lastReply: serverReply {
dn4_1    |   requestorId: "140718b2-320a-4020-b7ea-662533776c74"
dn4_1    |   replyId: "adc6845d-6cb4-4b43-88ca-47ca3f1a71df"
dn4_1    |   raftGroupId {
dn4_1    |     id: "\322U/\035\332tCO\273=\003\276\373\025\310\336"
dn4_1    |   }
dn4_1    |   callId: 41
scm2_1   | 2023-07-19 07:35:49,595 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm2_1   | 2023-07-19 07:35:49,596 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 2.
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm3_1   | 2023-07-19 07:37:31,265 [IPC Server handler 16 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,317 [IPC Server handler 2 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,318 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
scm3_1   | 2023-07-19 07:37:31,395 [IPC Server handler 32 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,455 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:35:49,596 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-07-19 07:35:49,656 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1  | 2023-07-19 07:35:48,471 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm2_1   | 2023-07-19 07:35:49,658 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:49,686 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn5_1    | 2023-07-19 07:35:59,260 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 PRE_VOTE round 1: result PASSED
dn5_1    | 2023-07-19 07:35:59,285 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 ELECTION round 1: submit vote requests at term 7 for 11: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:35:49,981 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
dn4_1    |   success: true
dn4_1    | }
dn4_1    | term: 8
dn4_1    | nextIndex: 49
scm1_1   | 2023-07-19 07:37:53,472 [IPC Server handler 44 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,512 [FixedThreadPoolWithAffinityExecutor-0-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn5_1    | 2023-07-19 07:35:59,327 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:35:59,330 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:35:50,420 [IPC Server handler 60 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/140718b2-320a-4020-b7ea-662533776c74
dn4_1    | followerCommit: 48
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:35:50,421 [IPC Server handler 60 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 140718b2-320a-4020-b7ea-662533776c74{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn5_1    | 2023-07-19 07:35:59,464 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-07-19 07:35:59,464 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection:   Response 0: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn5_1    | 2023-07-19 07:35:59,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2 ELECTION round 1: result PASSED
dn5_1    | 2023-07-19 07:35:59,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm1_1   | 2023-07-19 07:37:53,503 [IPC Server handler 26 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:53,534 [IPC Server handler 72 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn5_1    | 2023-07-19 07:35:59,498 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: changes role from CANDIDATE to LEADER at term 7 for changeToLeader
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm1_1   | 2023-07-19 07:37:54,929 [IPC Server handler 5 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 2023-07-19 07:37:54,949 [IPC Server handler 11 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
dn4_1    | matchIndex: 18446744073709551615
dn4_1    | isHearbeat: true
dn4_1    | 
dn4_1    | 2023-07-19 07:37:36,206 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7180356543ns, electionTimeout:5116ms
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 2023-07-19 07:37:54,980 [IPC Server handler 34 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
dn5_1    | 2023-07-19 07:35:59,498 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8563B54DD732 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:35:59,502 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 7 for becomeLeader, leader elected after 56983ms
dn5_1    | 2023-07-19 07:35:59,508 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-07-19 07:35:59,514 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 2023-07-19 07:37:54,992 [IPC Server handler 56 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm2_1   | 2023-07-19 07:35:50,427 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-07-19 07:35:59,514 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-07-19 07:37:36,206 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn4_1    | 2023-07-19 07:37:36,207 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn4_1    | 2023-07-19 07:37:36,207 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 2023-07-19 07:38:05,103 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
scm1_1   | 2023-07-19 07:38:05,103 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-07-19 07:35:50,428 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-07-19 07:35:59,519 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-07-19 07:35:59,522 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-07-19 07:35:59,522 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-07-19 07:35:59,534 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 2023-07-19 07:38:05,103 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm2_1   | 2023-07-19 07:35:50,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
dn5_1    | 2023-07-19 07:35:59,534 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-07-19 07:35:59,723 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: receive requestVote(ELECTION, 140718b2-320a-4020-b7ea-662533776c74, group-03BEFB15C8DE, 8, (t:7, i:46))
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm1_1   | 2023-07-19 07:38:05,104 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-07-19 07:38:17,104 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 2023-07-19 07:37:36,207 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-07-19 07:38:17,104 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-07-19 07:35:59,751 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-CANDIDATE: accept ELECTION from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 2023-07-19 07:37:36,212 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: change Leader from f02af6ab-c12d-469b-a775-f6b30900ff13 to null at term 7 for PRE_VOTE
recon_1  | 2023-07-19 07:35:48,471 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 2023-07-19 07:37:36,212 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 7 for 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | 2023-07-19 07:38:21,997 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn3_1    | 2023-07-19 07:37:37,120 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   |   id: "3937008a-fbf0-499b-90a2-14d0cfe0e993"
scm1_1   |   uuid128 {
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 2023-07-19 07:37:36,272 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
dn4_1    | 2023-07-19 07:37:36,289 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn4_1    | 2023-07-19 07:37:36,289 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 2023-07-19 07:35:50,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm2_1   | 2023-07-19 07:35:50,429 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn5_1    | 2023-07-19 07:35:59,751 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 8 for candidate:140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 2023-07-19 07:35:59,751 [grpc-default-executor-6] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1
dn5_1    | 2023-07-19 07:35:59,752 [grpc-default-executor-6] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 2023-07-19 07:35:50,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm2_1   | 2023-07-19 07:35:50,473 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn5_1    | 2023-07-19 07:35:59,816 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE replies to ELECTION vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t8. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c46, conf=30: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:35:59,943 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1: PRE_VOTE DISCOVERED_A_NEW_TERM (term=8) received 2 response(s) and 0 exception(s):
dn5_1    | 2023-07-19 07:35:59,946 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection:   Response 0: f02af6ab-c12d-469b-a775-f6b30900ff13<-adc6845d-6cb4-4b43-88ca-47ca3f1a71df#0:OK-t7
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm2_1   | 2023-07-19 07:35:50,525 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:50,525 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   |     mostSigBits: 4122764580813293979
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	... 3 more
scm3_1   | 2023-07-19 07:37:31,809 [IPC Server handler 68 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   |     leastSigBits: -8024828698947425901
scm3_1   | 2023-07-19 07:37:31,847 [IPC Server handler 69 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,870 [IPC Server handler 73 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:31,898 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:32,637 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm2_1   | 2023-07-19 07:35:50,542 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 2023-07-19 07:37:32,656 [IPC Server handler 53 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:32,691 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:32,659 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
scm3_1   | 2023-07-19 07:37:32,707 [FixedThreadPoolWithAffinityExecutor-1-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1001
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm2_1   | 2023-07-19 07:35:50,548 [grpc-default-executor-5] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:50,551 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:51,505 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-b28076e1-4ec3-4254-8902-2272d74360c6: Detected pause in JVM or host machine approximately 0.114s with 0.545s GC time.
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm1_1   |   }
scm1_1   | }
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm2_1   | GC pool 'ParNew' had collection(s): count=1 time=545ms
scm2_1   | 2023-07-19 07:35:51,782 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:51,784 [grpc-default-executor-5] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:51,942 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
dn4_1    | 2023-07-19 07:37:36,290 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
dn4_1    | 2023-07-19 07:37:36,290 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn4_1    | 2023-07-19 07:37:36,291 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4 PRE_VOTE round 0: result REJECTED
scm2_1   | 2023-07-19 07:35:51,984 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 2023-07-19 07:37:36,293 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
scm1_1   | isLeader: false
dn5_1    | 2023-07-19 07:35:59,947 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection:   Response 1: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t8
dn5_1    | 2023-07-19 07:35:59,947 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-LeaderElection1 PRE_VOTE round 1: result DISCOVERED_A_NEW_TERM (term=8)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn4_1    | 2023-07-19 07:37:36,293 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4
dn4_1    | 2023-07-19 07:37:36,293 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection4] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:35:52,090 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | bytesWritten: 0
scm1_1   |  from dn=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18).
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:36:00,032 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-07-19 07:36:00,042 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:35:53,034 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn4_1    | 2023-07-19 07:37:37,102 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:7421757745ns, electionTimeout:5009ms
dn4_1    | 2023-07-19 07:37:37,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
scm2_1   | 2023-07-19 07:35:53,034 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-07-19 07:36:00,046 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-07-19 07:36:00,081 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-07-19 07:36:00,090 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-07-19 07:36:00,093 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:36:00,096 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:35:53,046 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:53,047 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn5_1    | 2023-07-19 07:36:00,097 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-07-19 07:36:00,099 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:37:37,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn4_1    | 2023-07-19 07:37:37,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn4_1    | 2023-07-19 07:37:37,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5
dn4_1    | 2023-07-19 07:37:37,109 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: change Leader from 140718b2-320a-4020-b7ea-662533776c74 to null at term 8 for PRE_VOTE
dn4_1    | 2023-07-19 07:37:37,109 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 8 for 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:37:37,145 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:37,149 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn5_1    | 2023-07-19 07:36:00,099 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-07-19 07:37:37,150 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
dn4_1    | 2023-07-19 07:37:37,151 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:37,151 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn3_1    | 2023-07-19 07:37:41,472 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
scm2_1   | 2023-07-19 07:35:53,051 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:54,285 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:54,285 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:37,151 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-07-19 07:37:37,152 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-07-19 07:35:54,304 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn5_1    | 2023-07-19 07:36:00,133 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-07-19 07:36:00,138 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm2_1   | 2023-07-19 07:35:54,306 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:36:00,141 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-07-19 07:36:00,146 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn5_1    | 2023-07-19 07:36:00,146 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-07-19 07:36:00,149 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn5_1    | 2023-07-19 07:36:00,149 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-07-19 07:36:00,150 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2_1   | 2023-07-19 07:35:54,332 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn5_1    | 2023-07-19 07:36:00,150 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:36:00,150 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-07-19 07:36:00,218 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderStateImpl
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm2_1   | 2023-07-19 07:35:55,537 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
dn5_1    | 2023-07-19 07:36:00,236 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker: Rolling segment log-11_20 to index:20
dn5_1    | 2023-07-19 07:36:00,260 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:36:00,320 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderElection2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: set configuration 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:35:55,537 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:55,550 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:37,152 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5
dn5_1    | 2023-07-19 07:36:00,344 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_11 to /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_11-20
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn4_1    | 2023-07-19 07:37:37,154 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection5] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:37:41,450 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5156295657ns, electionTimeout:5150ms
dn4_1    | 2023-07-19 07:37:41,450 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
recon_1  | 2023-07-19 07:35:48,533 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
dn5_1    | 2023-07-19 07:36:00,347 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/current/log_inprogress_21
dn5_1    | 2023-07-19 07:36:01,262 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:36:01,493 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-03BEFB15C8DE with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 2023-07-19 07:36:01,496 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 8 for appendEntries, leader elected after 56827ms
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
recon_1  | java.nio.channels.ClosedChannelException
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 2023-07-19 07:37:41,450 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn5_1    | 2023-07-19 07:36:01,540 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: set configuration 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
dn5_1    | 2023-07-19 07:36:01,542 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolling segment log-30_46 to index:46
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
dn5_1    | 2023-07-19 07:36:01,652 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_30 to /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_30-46
scm2_1   | 2023-07-19 07:35:55,551 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:55,577 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	... 3 more
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
dn4_1    | 2023-07-19 07:37:41,451 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn5_1    | 2023-07-19 07:36:01,670 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/current/log_inprogress_47
scm2_1   | 2023-07-19 07:35:55,611 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 2023-07-19 07:37:32,733 [IPC Server handler 60 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn5_1    | 2023-07-19 07:36:02,267 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-07-19 07:37:41,451 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 2023-07-19 07:36:03,268 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-07-19 07:36:16,176 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm2_1   | 2023-07-19 07:35:55,614 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:37:32,734 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn4_1    | 2023-07-19 07:37:41,456 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6 PRE_VOTE round 0: submit vote requests at term 7 for 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 2023-07-19 07:36:47,777 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm2_1   | 2023-07-19 07:35:55,615 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 2023-07-19 07:37:32,741 [FixedThreadPoolWithAffinityExecutor-1-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-07-19 07:37:41,469 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm1_1   | 	... 3 more
dn5_1    | 2023-07-19 07:37:30,509 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-07-19 07:35:56,788 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
dn4_1    | 2023-07-19 07:37:41,481 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
dn4_1    | 2023-07-19 07:37:41,481 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-07-19 07:38:26,107 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
scm2_1   | 2023-07-19 07:35:56,792 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,511 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
dn4_1    | 2023-07-19 07:37:41,481 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
dn4_1    | 2023-07-19 07:37:41,481 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
scm1_1   | 2023-07-19 07:38:26,107 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-07-19 07:35:56,801 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,512 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
dn4_1    | 2023-07-19 07:37:41,481 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-07-19 07:37:41,482 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:35:56,803 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:37:30,512 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
dn4_1    | 2023-07-19 07:37:41,483 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6
dn4_1    | 2023-07-19 07:37:41,483 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection6] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:35:56,805 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,513 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:288)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
dn4_1    | 2023-07-19 07:37:42,163 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5009182830ns, electionTimeout:5002ms
dn4_1    | 2023-07-19 07:37:42,164 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn3_1    | 2023-07-19 07:37:42,174 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:35:58,039 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,513 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | 2023-07-19 07:38:28,348 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm1_1   |   id: "502632ea-08d6-4150-a7b0-075378e90876"
scm1_1   |   uuid128 {
dn4_1    | 2023-07-19 07:37:42,164 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn4_1    | 2023-07-19 07:37:42,165 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
recon_1  | 2023-07-19 07:35:48,535 [IPC Server handler 30 on default port 9891] INFO ipc.Server: IPC Server handler 30 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   |     mostSigBits: 5775359552901235024
dn4_1    | 2023-07-19 07:37:42,165 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7
dn4_1    | 2023-07-19 07:37:42,170 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7 PRE_VOTE round 0: submit vote requests at term 8 for 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:37:42,182 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm2_1   | 2023-07-19 07:35:58,040 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:35:58,078 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   |     leastSigBits: -6363578218382292874
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
scm2_1   | 2023-07-19 07:35:58,078 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   |   }
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm2_1   | 2023-07-19 07:35:58,111 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | }
scm1_1   | isLeader: true
scm2_1   | 2023-07-19 07:35:58,208 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm1_1   | bytesWritten: 0
scm2_1   | 2023-07-19 07:35:58,217 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm2_1   | 2023-07-19 07:35:58,251 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   |  from dn=140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19).
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7
dn4_1    | 2023-07-19 07:37:42,198 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection7] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:37:46,589 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-07-19 07:37:46,646 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5162562301ns, electionTimeout:5155ms
dn4_1    | 2023-07-19 07:37:46,647 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
scm2_1   | 2023-07-19 07:35:58,301 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:58,325 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn4_1    | 2023-07-19 07:37:46,647 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:35:58,361 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn4_1    | 2023-07-19 07:37:46,647 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm2_1   | 2023-07-19 07:35:58,485 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-07-19 07:37:46,647 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:35:58,492 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
dn5_1    | 2023-07-19 07:37:30,514 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm3_1   | 	... 3 more
dn4_1    | 2023-07-19 07:37:46,651 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8 PRE_VOTE round 0: submit vote requests at term 7 for 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2_1   | 2023-07-19 07:35:58,496 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm3_1   | 2023-07-19 07:37:32,946 [IPC Server handler 76 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-07-19 07:37:46,661 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm2_1   | 2023-07-19 07:35:58,557 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm3_1   | 2023-07-19 07:37:32,981 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-07-19 07:37:46,670 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2_1   | 2023-07-19 07:35:58,577 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm3_1   | 2023-07-19 07:37:33,005 [IPC Server handler 82 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
dn4_1    | 2023-07-19 07:37:46,670 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn4_1    | 2023-07-19 07:37:46,670 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm3_1   | 2023-07-19 07:37:33,025 [IPC Server handler 79 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 2023-07-19 07:37:34,297 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:37:34,298 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
dn4_1    | 2023-07-19 07:37:46,670 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:35:58,595 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	... 3 more
scm1_1   | 2023-07-19 07:38:29,486 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm1_1   |   id: "9ca86faf-1894-4e8b-baec-649965091133"
scm3_1   | 2023-07-19 07:37:39,360 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   |   uuid128 {
dn4_1    | 2023-07-19 07:37:46,670 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8 PRE_VOTE round 0: result REJECTED
dn4_1    | 2023-07-19 07:37:46,671 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn4_1    | 2023-07-19 07:37:46,671 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm1_1   |     mostSigBits: -7158348809883070837
dn4_1    | 2023-07-19 07:37:46,672 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection8] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
scm2_1   | 2023-07-19 07:35:58,674 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:58,676 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   |     leastSigBits: -4977492878163373773
dn4_1    | 2023-07-19 07:37:47,213 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5014396147ns, electionTimeout:5007ms
dn4_1    | 2023-07-19 07:37:47,213 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn4_1    | 2023-07-19 07:37:47,213 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn4_1    | 2023-07-19 07:37:47,214 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm1_1   |   }
scm3_1   | 2023-07-19 07:37:39,361 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-07-19 07:37:47,214 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9
dn4_1    | 2023-07-19 07:37:47,218 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9 PRE_VOTE round 0: submit vote requests at term 8 for 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:37:47,228 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 2023-07-19 07:35:58,685 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:58,744 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:58,746 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:37:44,562 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:37:47,242 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:47,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
dn4_1    | 2023-07-19 07:37:47,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:47,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm3_1   | 2023-07-19 07:37:44,562 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:37:46,888 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:49,596 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:37:47,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9 PRE_VOTE round 0: result REJECTED
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm1_1   | }
dn3_1    | 2023-07-19 07:37:46,665 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn4_1    | 2023-07-19 07:37:47,248 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm2_1   | 2023-07-19 07:35:58,756 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:58,825 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:37:49,596 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | isLeader: true
scm1_1   | bytesWritten: 0
dn4_1    | 2023-07-19 07:37:47,248 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9
dn4_1    | 2023-07-19 07:37:47,248 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection9] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm2_1   | 2023-07-19 07:35:58,837 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:58,860 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 2023-07-19 07:37:52,345 [IPC Server handler 99 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   |  from dn=f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21).
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-07-19 07:37:51,730 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5058448188ns, electionTimeout:5057ms
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 2023-07-19 07:37:53,389 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:37:30,516 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
dn4_1    | 2023-07-19 07:37:51,731 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
recon_1  | 2023-07-19 07:35:49,112 [IPC Server handler 29 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
scm2_1   | 2023-07-19 07:35:59,291 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 2023-07-19 07:37:53,390 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 2023-07-19 07:37:51,731 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
recon_1  | 2023-07-19 07:35:49,556 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for aff5372d1efd
scm2_1   | 2023-07-19 07:35:59,291 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:59,337 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:37:53,417 [IPC Server handler 30 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
dn4_1    | 2023-07-19 07:37:51,731 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 2023-07-19 07:35:49,884 [IPC Server handler 29 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
recon_1  | 2023-07-19 07:35:50,321 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for 69ed6f01c113
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3_1   | 2023-07-19 07:37:53,468 [IPC Server handler 33 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:53,504 [IPC Server handler 39 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
recon_1  | 2023-07-19 07:36:14,660 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-07-19 07:36:14,741 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn4_1    | 2023-07-19 07:37:51,731 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10
scm3_1   | 2023-07-19 07:37:53,541 [IPC Server handler 43 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
dn4_1    | 2023-07-19 07:37:51,735 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10 PRE_VOTE round 0: submit vote requests at term 7 for 21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:37:54,603 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
scm2_1   | 2023-07-19 07:35:59,347 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:36:14,782 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn4_1    | 2023-07-19 07:37:51,761 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm3_1   | 2023-07-19 07:37:54,604 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:35:59,366 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:59,417 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:59,422 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
dn4_1    | 2023-07-19 07:37:51,762 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
scm3_1   | 2023-07-19 07:37:54,926 [IPC Server handler 72 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:54,955 [IPC Server handler 77 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:37:47,231 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
recon_1  | 2023-07-19 07:36:14,956 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn4_1    | 2023-07-19 07:37:51,762 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm3_1   | 2023-07-19 07:37:54,970 [IPC Server handler 70 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:37:54,998 [IPC Server handler 82 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:35:59,449 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:35:59,533 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
dn4_1    | 2023-07-19 07:37:51,762 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
recon_1  | 2023-07-19 07:36:15,064 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-07-19 07:35:59,547 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:35:59,602 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 2023-07-19 07:37:59,781 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
dn4_1    | 2023-07-19 07:37:51,762 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
recon_1  | 2023-07-19 07:36:15,206 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 333 milliseconds.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm2_1   | 2023-07-19 07:35:59,715 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm3_1   | 2023-07-19 07:37:59,786 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 2023-07-19 07:37:51,762 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10 PRE_VOTE round 0: result REJECTED
recon_1  | 2023-07-19 07:36:15,494 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 752 milliseconds to process 0 existing database records.
recon_1  | 2023-07-19 07:36:16,147 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 627 milliseconds for processing 4 containers.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm2_1   | 2023-07-19 07:35:59,716 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-07-19 07:38:04,975 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-07-19 07:37:51,763 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
recon_1  | 2023-07-19 07:36:16,966 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm2_1   | 2023-07-19 07:35:59,797 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:37:30,517 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
dn5_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:37:51,763 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10
recon_1  | 2023-07-19 07:36:27,093 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm2_1   | 2023-07-19 07:36:00,542 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:00,542 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:00,547 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 2023-07-19 07:37:51,763 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-LeaderElection10] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
recon_1  | 2023-07-19 07:36:27,098 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm2_1   | 2023-07-19 07:36:00,547 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:36:27,098 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-19 07:36:27,098 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1  | 2023-07-19 07:36:27,098 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
recon_1  | 2023-07-19 07:36:27,098 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
scm3_1   | 2023-07-19 07:38:04,976 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn4_1    | 2023-07-19 07:37:52,284 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5035653735ns, electionTimeout:5035ms
dn4_1    | 2023-07-19 07:37:52,285 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
recon_1  | 2023-07-19 07:36:27,100 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm3_1   | 2023-07-19 07:38:05,390 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn4_1    | 2023-07-19 07:37:52,285 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from  FOLLOWER to CANDIDATE at term 8 for changeToCandidate
dn4_1    | 2023-07-19 07:37:52,287 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
recon_1  | 2023-07-19 07:36:27,100 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 128 
scm2_1   | 2023-07-19 07:36:00,553 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:38:05,391 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
recon_1  | 2023-07-19 07:36:28,075 [pool-27-thread-1] WARN tasks.OmUpdateEventValidator: Validation failed for keyType: /old1-volume/old1-bucket/old1-key, tableName: deletedTable, action: PUT, Expected value type: org.apache.hadoop.ozone.om.helpers.RepeatedOmKeyInfo, Actual value type: org.apache.hadoop.ozone.om.helpers.OmKeyInfo
recon_1  | 2023-07-19 07:36:28,085 [pool-27-thread-1] WARN tasks.OmUpdateEventValidator: Validation failed for keyType: /s3v/old1-bucket/key1-shell, tableName: deletedTable, action: PUT, Expected value type: org.apache.hadoop.ozone.om.helpers.RepeatedOmKeyInfo, Actual value type: org.apache.hadoop.ozone.om.helpers.OmKeyInfo
dn4_1    | 2023-07-19 07:37:52,289 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11
scm2_1   | 2023-07-19 07:36:00,553 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 2023-07-19 07:38:05,391 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
recon_1  | 2023-07-19 07:36:28,088 [pool-27-thread-1] WARN tasks.OmUpdateEventValidator: Validation failed for keyType: /old1-volume/old1-bucket/old1-key, tableName: deletedTable, action: DELETE, Expected value type: org.apache.hadoop.ozone.om.helpers.RepeatedOmKeyInfo, Actual value type: org.apache.hadoop.ozone.om.helpers.OmKeyInfo
recon_1  | 2023-07-19 07:36:28,092 [pool-27-thread-1] WARN tasks.OmUpdateEventValidator: Validation failed for keyType: /s3v/old1-bucket/key1-shell, tableName: deletedTable, action: DELETE, Expected value type: org.apache.hadoop.ozone.om.helpers.RepeatedOmKeyInfo, Actual value type: org.apache.hadoop.ozone.om.helpers.OmKeyInfo
dn4_1    | 2023-07-19 07:37:52,298 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11 PRE_VOTE round 0: submit vote requests at term 8 for 47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:36:00,626 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 2023-07-19 07:38:05,391 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-07-19 07:36:28,101 [pool-27-thread-1] WARN tasks.OmUpdateEventValidator: Validation failed for keyType: /s3v/old1-bucket/key2-s3api, tableName: deletedTable, action: PUT, Expected value type: org.apache.hadoop.ozone.om.helpers.RepeatedOmKeyInfo, Actual value type: org.apache.hadoop.ozone.om.helpers.OmKeyInfo
recon_1  | 2023-07-19 07:36:28,104 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 12, SequenceNumber diff: 33, SequenceNumber Lag from OM 0.
dn4_1    | 2023-07-19 07:37:52,332 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:36:00,713 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 2023-07-19 07:38:10,110 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-07-19 07:36:28,104 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 33 records
recon_1  | 2023-07-19 07:36:28,430 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/old2-bucket.
dn4_1    | 2023-07-19 07:37:52,338 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11 got exception when requesting votes: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:36:00,713 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 2023-07-19 07:38:10,111 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-07-19 07:36:28,431 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:28,434 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:37:52,339 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11: PRE_VOTE REJECTED received 0 response(s) and 2 exception(s):
scm2_1   | 2023-07-19 07:36:00,740 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 2023-07-19 07:38:15,164 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /old1-volume/old1-bucket/old1-key.
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /old1-volume/old1-bucket.
scm2_1   | 2023-07-19 07:36:00,904 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 2023-07-19 07:38:15,164 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm3_1   | 2023-07-19 07:38:17,392 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn5_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm2_1   | 2023-07-19 07:36:00,906 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 2023-07-19 07:38:17,392 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm2_1   | 2023-07-19 07:36:00,926 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:00,988 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm3_1   | 2023-07-19 07:38:20,225 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:38:20,225 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:38:22,029 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2_1   | 2023-07-19 07:36:01,007 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	... 3 more
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   |   id: "3937008a-fbf0-499b-90a2-14d0cfe0e993"
scm3_1   |   uuid128 {
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/old1-bucket/key1-shell.
scm2_1   | 2023-07-19 07:36:01,029 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   | 2023-07-19 07:38:33,831 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-07-19 07:37:52,339 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection:   Exception 0: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:52,339 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection:   Exception 1: java.util.concurrent.ExecutionException: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: INTERNAL: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
dn4_1    | 2023-07-19 07:37:52,339 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11 PRE_VOTE round 0: result REJECTED
scm2_1   | 2023-07-19 07:36:01,094 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   |   id: "9a97a9af-3060-4a66-97a0-7bbd49954ef8"
scm1_1   |   uuid128 {
scm1_1   |     mostSigBits: -7307185300489352602
scm1_1   |     leastSigBits: -7520875324795171080
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/old1-bucket.
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
scm2_1   | 2023-07-19 07:36:01,112 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:52,339 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn4_1    | 2023-07-19 07:37:52,340 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11
dn4_1    | 2023-07-19 07:37:52,340 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-LeaderElection11] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
scm1_1   |   }
scm1_1   | }
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:37:48,357 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm1_1   | isLeader: false
dn5_1    | 2023-07-19 07:37:30,530 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: remove    LEADER f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732:t7, leader=f02af6ab-c12d-469b-a775-f6b30900ff13, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLog:OPENED:c22, conf=21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm2_1   | 2023-07-19 07:36:01,147 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | 2023-07-19 07:37:53,285 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:37:51,752 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-8563B54DD732 not found.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | bytesWritten: 0
scm2_1   | 2023-07-19 07:36:01,184 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,286 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn4_1    | 2023-07-19 07:37:53,286 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-07-19 07:37:53,286 [Command processor thread] WARN upgrade.UpgradeFinalizer: FinalizeUpgrade : Waiting for container 1 to close, current state is: OPEN
dn4_1    | 2023-07-19 07:37:53,286 [Command processor thread] INFO upgrade.UpgradeFinalizer: Pre Finalization checks failed on the DataNode.
dn5_1    | 2023-07-19 07:37:30,533 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: shutdown
scm1_1   |  from dn=140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19).
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn4_1    | 2023-07-19 07:37:53,286 [Command processor thread] WARN upgrade.DefaultUpgradeFinalizationExecutor: Upgrade Finalization failed with following Exception. 
scm2_1   | 2023-07-19 07:36:01,185 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   |     mostSigBits: 4122764580813293979
scm3_1   |     leastSigBits: -8024828698947425901
dn5_1    | 2023-07-19 07:37:30,533 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=f02af6ab-c12d-469b-a775-f6b30900ff13
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm2_1   | 2023-07-19 07:36:01,244 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   |   }
scm3_1   | }
dn5_1    | 2023-07-19 07:37:30,533 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-LeaderStateImpl
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-07-19 07:36:01,993 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:01,994 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:02,005 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,536 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
scm2_1   | 2023-07-19 07:36:02,011 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:02,022 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:03,245 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,536 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-GrpcLogAppender-LogAppenderDaemon] WARN server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-GrpcLogAppender: Wait interrupted by java.lang.InterruptedException
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
scm2_1   | 2023-07-19 07:36:03,245 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
recon_1  | 2023-07-19 07:36:28,438 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm2_1   | 2023-07-19 07:36:03,257 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
recon_1  | 2023-07-19 07:36:28,440 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:28,440 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm2_1   | 2023-07-19 07:36:03,257 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
recon_1  | 2023-07-19 07:36:28,441 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:28,441 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/old1-bucket/key2-s3api.
recon_1  | 2023-07-19 07:36:28,448 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/old1-bucket.
recon_1  | 2023-07-19 07:36:28,448 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:28,448 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #PREPAREDINFO.
scm2_1   | 2023-07-19 07:36:03,260 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:36:28,448 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm3_1   | isLeader: false
scm3_1   | bytesWritten: 0
scm2_1   | 2023-07-19 07:36:04,496 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
recon_1  | 2023-07-19 07:36:28,448 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:36:29,331 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
dn5_1    | 2023-07-19 07:37:30,537 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-PendingRequests: sendNotLeaderResponses
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:37:53,287 [Command processor thread] ERROR commandhandler.FinalizeNewLayoutVersionCommandHandler: Exception during finalization.
scm2_1   | 2023-07-19 07:36:04,498 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
recon_1  | 2023-07-19 07:36:29,335 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn5_1    | 2023-07-19 07:37:30,549 [grpc-default-executor-6] INFO server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm3_1   |  from dn=8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18).
dn4_1    | PREFINALIZE_VALIDATION_FAILED org.apache.hadoop.ozone.upgrade.UpgradeException: Pre Finalization checks failed on the DataNode.
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:56)
scm2_1   | 2023-07-19 07:36:04,516 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,520 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,521 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:37:30,549 [grpc-default-executor-3] INFO server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 	at org.apache.hadoop.ozone.container.upgrade.DataNodeUpgradeFinalizer.preFinalizeUpgrade(DataNodeUpgradeFinalizer.java:40)
scm2_1   | 2023-07-19 07:36:04,541 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:36:29,345 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-07-19 07:36:29,408 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 4 OM DB update event(s).
recon_1  | 2023-07-19 07:36:29,632 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:36:04,548 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 2023-07-19 07:37:14,840 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1  | 2023-07-19 07:37:14,904 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-07-19 07:37:14,905 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 64
dn5_1    | 2023-07-19 07:37:30,564 [grpc-default-executor-3] INFO leader.FollowerInfo: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df: decreaseNextIndex nextIndex: updateUnconditionally 23 -> 22
dn5_1    | 2023-07-19 07:37:30,580 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Taking a snapshot at:(t:7, i:22) file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22
dn5_1    | 2023-07-19 07:37:30,580 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater: set stopIndex = 22
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.DefaultUpgradeFinalizationExecutor.execute(DefaultUpgradeFinalizationExecutor.java:46)
scm2_1   | 2023-07-19 07:36:04,570 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:37:29,651 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-07-19 07:37:29,652 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-07-19 07:37:29,652 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 161 
dn5_1    | 2023-07-19 07:37:30,566 [grpc-default-executor-6] INFO leader.FollowerInfo: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74: decreaseNextIndex nextIndex: updateUnconditionally 23 -> 22
dn5_1    | 2023-07-19 07:37:30,590 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Finished taking a snapshot at:(t:7, i:22) file:/data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22 took: 10 ms
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
recon_1  | 2023-07-19 07:37:29,663 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 2, SequenceNumber Lag from OM 0.
recon_1  | 2023-07-19 07:37:29,664 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 2 records
recon_1  | 2023-07-19 07:37:29,770 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:37:29,887 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
recon_1  | 2023-07-19 07:37:29,887 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-07-19 07:37:29,888 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-07-19 07:37:29,888 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
recon_1  | 2023-07-19 07:37:29,888 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-07-19 07:37:31,271 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Container #1 has state OPEN, but given state is CLOSING.
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:37:52,324 [grpc-default-executor-5] WARN server.GrpcServerProtocolService: 140718b2-320a-4020-b7ea-662533776c74: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->140718b2-320a-4020-b7ea-662533776c74#0
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-07-19 07:37:31,341 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
recon_1  | 2023-07-19 07:37:31,398 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Container #1002 has state OPEN, but given state is CLOSING.
recon_1  | 2023-07-19 07:37:31,439 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
recon_1  | 2023-07-19 07:37:32,625 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Container #1001 has state OPEN, but given state is CLOSING.
recon_1  | 2023-07-19 07:37:32,651 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
recon_1  | 2023-07-19 07:37:32,695 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Container #2 has state OPEN, but given state is CLOSING.
recon_1  | 2023-07-19 07:37:32,726 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
recon_1  | 2023-07-19 07:38:14,906 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-07-19 07:38:14,906 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1  | 2023-07-19 07:38:21,961 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3937008a-fbf0-499b-90a2-14d0cfe0e993. Trying to get from SCM.
recon_1  | 2023-07-19 07:38:22,009 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3937008a-fbf0-499b-90a2-14d0cfe0e993, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, CreationTimestamp2023-07-19T07:37:51.590Z[UTC]] to Recon pipeline metadata.
dn4_1    | 	at org.apache.hadoop.ozone.upgrade.BasicUpgradeFinalizer.finalize(BasicUpgradeFinalizer.java:99)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2_1   | 2023-07-19 07:36:04,584 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:30,595 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater: Took a snapshot at index 22
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	... 3 more
scm3_1   | 2023-07-19 07:38:25,401 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:38:25,401 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:38:26,392 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.finalizeUpgrade(DatanodeStateMachine.java:723)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.FinalizeNewLayoutVersionCommandHandler.handle(FinalizeNewLayoutVersionCommandHandler.java:78)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.commandhandler.CommandDispatcher.handle(CommandDispatcher.java:103)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: 140718b2-320a-4020-b7ea-662533776c74: group-03BEFB15C8DE not found.
scm3_1   | 2023-07-19 07:38:26,393 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
dn5_1    | 2023-07-19 07:37:30,597 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 22
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$initCommandHandlerThread$3(DatanodeStateMachine.java:652)
scm2_1   | 2023-07-19 07:36:04,590 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,595 [grpc-default-executor-3] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 2023-07-19 07:38:28,331 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn5_1    | 2023-07-19 07:37:30,605 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: closes. applyIndex: 22
dn5_1    | 2023-07-19 07:37:30,615 [grpc-default-executor-6] INFO server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74-AppendLogResponseHandler: follower responses appendEntries COMPLETED
scm2_1   | 2023-07-19 07:36:04,633 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,640 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,644 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   |   id: "502632ea-08d6-4150-a7b0-075378e90876"
dn5_1    | 2023-07-19 07:37:30,616 [grpc-default-executor-6] INFO leader.FollowerInfo: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->140718b2-320a-4020-b7ea-662533776c74: decreaseNextIndex nextIndex: updateUnconditionally 22 -> 21
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-07-19 07:37:53,301 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: remove  FOLLOWER adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732:t7, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLog:OPENED:c22, conf=21: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	... 3 more
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   |   uuid128 {
dn5_1    | 2023-07-19 07:37:30,627 [grpc-default-executor-6] INFO server.GrpcLogAppender: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df-AppendLogResponseHandler: follower responses appendEntries COMPLETED
dn4_1    | 2023-07-19 07:37:53,304 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: shutdown
scm1_1   | 2023-07-19 07:38:34,114 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm1_1   |   id: "b29414e4-15a3-4023-afdc-f99861290493"
recon_1  | 2023-07-19 07:38:22,160 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=502632ea-08d6-4150-a7b0-075378e90876. Trying to get from SCM.
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   |     mostSigBits: 5775359552901235024
dn5_1    | 2023-07-19 07:37:30,627 [grpc-default-executor-6] INFO leader.FollowerInfo: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732->adc6845d-6cb4-4b43-88ca-47ca3f1a71df: decreaseNextIndex nextIndex: updateUnconditionally 22 -> 21
dn5_1    | 2023-07-19 07:37:31,216 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732-SegmentedRaftLogWorker close()
recon_1  | 2023-07-19 07:38:22,168 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 502632ea-08d6-4150-a7b0-075378e90876, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.603Z[UTC]] to Recon pipeline metadata.
scm2_1   | 2023-07-19 07:36:04,679 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,680 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,692 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   |     leastSigBits: -6363578218382292874
dn5_1    | 2023-07-19 07:37:31,279 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-07-19 07:37:53,304 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-8563B54DD732,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
recon_1  | 2023-07-19 07:38:22,176 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-07-19 07:38:23,063 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn3_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm3_1   |   }
dn4_1    | 2023-07-19 07:37:53,304 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState
scm2_1   | 2023-07-19 07:36:04,731 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,747 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,758 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:04,796 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,304 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-FollowerState was interrupted
dn3_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 2023-07-19 07:37:53,305 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater: set stopIndex = 22
recon_1  | 2023-07-19 07:38:23,406 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)
recon_1  | 2023-07-19 07:38:23,577 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)
scm1_1   |   uuid128 {
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
dn4_1    | 2023-07-19 07:37:53,306 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Taking a snapshot at:(t:7, i:22) file /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22
recon_1  | 2023-07-19 07:38:23,578 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9ca86faf-1894-4e8b-baec-649965091133. Trying to get from SCM.
scm2_1   | 2023-07-19 07:36:04,797 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 2023-07-19 07:37:31,282 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn3_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | }
scm2_1   | 2023-07-19 07:36:04,820 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   |     mostSigBits: -5578811068557082589
dn5_1    | 2023-07-19 07:37:31,398 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn5_1    | 2023-07-19 07:37:31,399 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
recon_1  | 2023-07-19 07:38:23,582 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9ca86faf-1894-4e8b-baec-649965091133, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.648Z[UTC]] to Recon pipeline metadata.
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:36:04,863 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,869 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,881 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
recon_1  | 2023-07-19 07:38:23,585 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-07-19 07:38:23,993 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)
scm1_1   |     leastSigBits: -5774466189335395181
recon_1  | 2023-07-19 07:38:23,993 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)
dn5_1    | 2023-07-19 07:37:31,483 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-8563B54DD732: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
scm2_1   | 2023-07-19 07:36:04,931 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,311 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-8563B54DD732: Finished taking a snapshot at:(t:7, i:22) file:/data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732/sm/snapshot.7_22 took: 5 ms
dn4_1    | 2023-07-19 07:37:53,313 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater: Took a snapshot at index 22
dn4_1    | 2023-07-19 07:37:53,314 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-StateMachineUpdater: snapshotIndex: updateIncreasingly 20 -> 22
dn4_1    | 2023-07-19 07:37:53,325 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: closes. applyIndex: 22
dn4_1    | 2023-07-19 07:37:53,330 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732-SegmentedRaftLogWorker close()
recon_1  | 2023-07-19 07:38:24,093 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)
dn5_1    | 2023-07-19 07:37:31,487 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 command on datanode f02af6ab-c12d-469b-a775-f6b30900ff13.
scm1_1   |   }
scm2_1   | 2023-07-19 07:36:04,940 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,943 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    | 2023-07-19 07:37:31,498 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: remove    LEADER f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3:t4, leader=f02af6ab-c12d-469b-a775-f6b30900ff13, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLog:OPENED:c6, conf=5: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm1_1   | }
recon_1  | 2023-07-19 07:38:24,093 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)
recon_1  | 2023-07-19 07:38:27,301 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)
recon_1  | 2023-07-19 07:38:27,301 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)
dn5_1    | 2023-07-19 07:37:31,500 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: shutdown
dn5_1    | 2023-07-19 07:37:31,500 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-49D8FC4B32B3,id=f02af6ab-c12d-469b-a775-f6b30900ff13
scm1_1   | isLeader: false
scm1_1   | bytesWritten: 0
scm1_1   |  from dn=f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21).
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:36:04,977 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:04,983 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:38:28,349 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=502632ea-08d6-4150-a7b0-075378e90876 reported by 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)
dn5_1    | 2023-07-19 07:37:31,501 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-LeaderStateImpl
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | isLeader: true
scm3_1   | bytesWritten: 0
dn3_1    | 2023-07-19 07:38:01,063 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:01,063 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
dn3_1    | 2023-07-19 07:38:01,063 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 2023-07-19 07:36:04,995 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:05,025 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,038 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   |  from dn=140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19).
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-07-19 07:38:28,351 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)
dn5_1    | 2023-07-19 07:37:31,502 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-07-19 07:37:53,421 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-07-19 07:37:53,421 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-07-19 07:37:53,507 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn3_1    | 2023-07-19 07:38:01,063 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
recon_1  | 2023-07-19 07:38:29,449 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ca86faf-1894-4e8b-baec-649965091133 reported by f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)
dn5_1    | 2023-07-19 07:37:31,510 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater: set stopIndex = 6
dn4_1    | 2023-07-19 07:37:53,507 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn4_1    | 2023-07-19 07:37:53,545 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-8563B54DD732: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/6d5697fa-1223-40ff-ba6c-8563b54dd732
dn4_1    | 2023-07-19 07:37:53,549 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 command on datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df.
recon_1  | 2023-07-19 07:38:29,896 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn5_1    | 2023-07-19 07:37:31,512 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-49D8FC4B32B3: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/sm/snapshot.4_6
dn4_1    | 2023-07-19 07:37:53,550 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: remove    LEADER adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD:t4, leader=adc6845d-6cb4-4b43-88ca-47ca3f1a71df, voted=adc6845d-6cb4-4b43-88ca-47ca3f1a71df, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLog:OPENED:c6, conf=5: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn3_1    | 2023-07-19 07:38:01,065 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-07-19 07:38:01,065 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
recon_1  | 2023-07-19 07:38:29,897 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn4_1    | 2023-07-19 07:37:53,551 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: shutdown
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn3_1    | 2023-07-19 07:38:01,066 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-07-19 07:38:01,066 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-07-19 07:38:29,897 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 163 
dn5_1    | 2023-07-19 07:37:31,515 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-49D8FC4B32B3: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3/sm/snapshot.4_6 took: 3 ms
dn4_1    | 2023-07-19 07:37:53,551 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-20996759F8FD,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn4_1    | 2023-07-19 07:37:53,551 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-LeaderStateImpl
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:36:05,046 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:05,087 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,552 [PipelineCommandHandlerThread-0] INFO impl.PendingRequests: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-PendingRequests: sendNotLeaderResponses
dn4_1    | 2023-07-19 07:37:53,561 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-20996759F8FD: Taking a snapshot at:(t:4, i:6) file /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/sm/snapshot.4_6
dn4_1    | 2023-07-19 07:37:53,562 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater: set stopIndex = 6
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:36:05,106 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,115 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-07-19 07:38:29,919 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
scm2_1   | 2023-07-19 07:36:05,165 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,565 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-20996759F8FD: Finished taking a snapshot at:(t:4, i:6) file:/data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd/sm/snapshot.4_6 took: 4 ms
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
recon_1  | 2023-07-19 07:38:29,919 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
scm2_1   | 2023-07-19 07:36:05,167 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:37:53,565 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater: Took a snapshot at index 6
dn4_1    | 2023-07-19 07:37:53,565 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
recon_1  | 2023-07-19 07:38:33,815 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9a97a9af-3060-4a66-97a0-7bbd49954ef8. Trying to get from SCM.
scm2_1   | 2023-07-19 07:36:05,180 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn4_1    | 2023-07-19 07:37:53,566 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: closes. applyIndex: 6
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
recon_1  | 2023-07-19 07:38:33,840 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9a97a9af-3060-4a66-97a0-7bbd49954ef8, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:38:03.625Z[UTC]] to Recon pipeline metadata.
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn4_1    | 2023-07-19 07:37:54,082 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD-SegmentedRaftLogWorker close()
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
recon_1  | 2023-07-19 07:38:34,111 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b29414e4-15a3-4023-afdc-f99861290493. Trying to get from SCM.
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn4_1    | 2023-07-19 07:37:54,087 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-20996759F8FD: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/e6765bce-cd48-45ed-b1c7-20996759f8fd
recon_1  | 2023-07-19 07:38:34,129 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b29414e4-15a3-4023-afdc-f99861290493, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:38:03.589Z[UTC]] to Recon pipeline metadata.
scm2_1   | 2023-07-19 07:36:05,222 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,228 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:38:34,130 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b29414e4-15a3-4023-afdc-f99861290493 reported by f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd command on datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df.
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: remove  FOLLOWER adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE:t8, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c48, conf=47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
recon_1  | 2023-07-19 07:38:47,973 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=301e375a-65de-4355-b5a7-08992b8d070f. Trying to get from SCM.
recon_1  | 2023-07-19 07:38:47,993 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 301e375a-65de-4355-b5a7-08992b8d070f, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a9b83a7e-59b8-4455-b30a-c01eee264fbd, CreationTimestamp2023-07-19T07:38:18.591Z[UTC]] to Recon pipeline metadata.
scm2_1   | 2023-07-19 07:36:05,233 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:36:05,276 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: shutdown
dn5_1    | 2023-07-19 07:37:31,516 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater: Took a snapshot at index 6
dn5_1    | 2023-07-19 07:37:31,516 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-StateMachineUpdater: snapshotIndex: updateIncreasingly 4 -> 6
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:36:05,283 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,295 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:36:05,350 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,353 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:36:05,379 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:05,403 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,413 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
recon_1  | 2023-07-19 07:38:48,000 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=301e375a-65de-4355-b5a7-08992b8d070f reported by a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17)
recon_1  | 2023-07-19 07:38:56,182 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=3b24ef8c-1b15-473e-825d-aaddeaeb0eb6. Trying to get from SCM.
recon_1  | 2023-07-19 07:38:56,196 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 3b24ef8c-1b15-473e-825d-aaddeaeb0eb6, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:adc6845d-6cb4-4b43-88ca-47ca3f1a71df, CreationTimestamp2023-07-19T07:38:27.593Z[UTC]] to Recon pipeline metadata.
recon_1  | 2023-07-19 07:39:14,907 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
dn4_1    | 2023-07-19 07:37:54,088 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-FollowerState was interrupted
dn4_1    | 2023-07-19 07:37:54,088 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater: set stopIndex = 48
scm2_1   | 2023-07-19 07:36:05,418 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
recon_1  | 2023-07-19 07:39:14,907 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
dn4_1    | 2023-07-19 07:37:54,089 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Taking a snapshot at:(t:8, i:48) file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48
dn4_1    | 2023-07-19 07:37:54,098 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Finished taking a snapshot at:(t:8, i:48) file:/data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48 took: 10 ms
dn5_1    | 2023-07-19 07:37:31,518 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: closes. applyIndex: 6
dn5_1    | 2023-07-19 07:37:31,930 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Completed APPEND_ENTRIES, lastRequest: 140718b2-320a-4020-b7ea-662533776c74->f02af6ab-c12d-469b-a775-f6b30900ff13#5-t8,previous=(t:8, i:47),leaderCommit=48,initializing? true,entries: size=1, first=(t:8, i:48), METADATAENTRY(c:47)
dn5_1    | 2023-07-19 07:37:31,931 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Completed APPEND_ENTRIES, lastReply: null
dn5_1    | 2023-07-19 07:37:31,939 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Completed APPEND_ENTRIES, lastRequest: null
recon_1  | 2023-07-19 07:39:29,932 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn4_1    | 2023-07-19 07:37:54,099 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater: Took a snapshot at index 48
dn4_1    | 2023-07-19 07:37:54,099 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 48
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-07-19 07:36:05,463 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn5_1    | 2023-07-19 07:37:31,995 [grpc-default-executor-6] INFO server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Completed APPEND_ENTRIES, lastReply: serverReply {
recon_1  | 2023-07-19 07:39:29,933 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn4_1    | 2023-07-19 07:37:54,100 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: closes. applyIndex: 48
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn3_1    | 2023-07-19 07:38:01,067 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1   | 2023-07-19 07:36:05,470 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    |   requestorId: "140718b2-320a-4020-b7ea-662533776c74"
recon_1  | 2023-07-19 07:39:29,933 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 163 
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn4_1    | 2023-07-19 07:37:54,913 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE-SegmentedRaftLogWorker close()
dn4_1    | 2023-07-19 07:37:54,920 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
scm2_1   | 2023-07-19 07:36:05,481 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
dn5_1    |   replyId: "f02af6ab-c12d-469b-a775-f6b30900ff13"
recon_1  | 2023-07-19 07:39:29,949 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 1, SequenceNumber diff: 2, SequenceNumber Lag from OM 0.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn4_1    | 2023-07-19 07:37:54,920 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
scm1_1   | 	... 3 more
scm1_1   | 2023-07-19 07:38:47,968 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
scm2_1   | 2023-07-19 07:36:05,558 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,561 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn4_1    | 2023-07-19 07:37:54,967 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn4_1    | 2023-07-19 07:37:54,967 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn4_1    | 2023-07-19 07:37:55,001 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-03BEFB15C8DE: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
scm2_1   | 2023-07-19 07:36:05,560 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,748 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn4_1    | 2023-07-19 07:37:55,001 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de command on datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df.
dn4_1    | 2023-07-19 07:38:23,285 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:38:23,286 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
recon_1  | 2023-07-19 07:39:29,949 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 2 records
recon_1  | 2023-07-19 07:39:30,128 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-07-19 07:38:23,286 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn4_1    | 2023-07-19 07:38:23,286 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn4_1    | 2023-07-19 07:38:23,290 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
scm2_1   | 2023-07-19 07:36:05,748 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,753 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,917 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm3_1   | 	... 3 more
dn4_1    | 2023-07-19 07:38:23,290 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn4_1    | 2023-07-19 07:38:23,295 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-07-19 07:38:23,295 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
recon_1  | 2023-07-19 07:39:30,488 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
recon_1  | 2023-07-19 07:39:30,489 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
scm2_1   | 2023-07-19 07:36:05,754 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm3_1   | 2023-07-19 07:38:29,427 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn4_1    | 2023-07-19 07:38:23,298 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn4_1    | 2023-07-19 07:38:23,299 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
dn4_1    | 2023-07-19 07:38:23,299 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
dn4_1    | 2023-07-19 07:38:23,299 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
recon_1  | 2023-07-19 07:39:30,489 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
scm2_1   | 2023-07-19 07:36:05,765 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,778 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-07-19 07:38:23,299 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    |   raftGroupId {
dn5_1    |     id: "\322U/\035\332tCO\273=\003\276\373\025\310\336"
dn5_1    |   }
recon_1  | 2023-07-19 07:39:30,491 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 0 OM DB update event(s).
scm2_1   | 2023-07-19 07:36:05,798 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:05,806 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-07-19 07:38:23,300 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn5_1    |   callId: 45
dn5_1    |   success: true
dn5_1    | }
recon_1  | 2023-07-19 07:39:30,491 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
recon_1  | 2023-07-19 07:40:14,908 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-07-19 07:40:14,908 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
scm3_1   |   id: "9ca86faf-1894-4e8b-baec-649965091133"
dn4_1    | 2023-07-19 07:38:23,300 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:38:23,300 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:38:23,300 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:38:23,300 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn4_1    | 2023-07-19 07:38:25,990 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-07-19 07:36:05,827 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:40:21,015 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2001 got from ha_dn3_1.ha_net.
recon_1  | 2023-07-19 07:40:21,097 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2001 to Recon.
dn4_1    | 2023-07-19 07:38:46,590 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn4_1    | 2023-07-19 07:38:56,077 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@52f6900a] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),1001(0)], numOfContainers=3, numOfBlocks=5
scm1_1   |   id: "301e375a-65de-4355-b5a7-08992b8d070f"
scm1_1   |   uuid128 {
scm1_1   |     mostSigBits: 3467269624517509973
scm2_1   | 2023-07-19 07:36:05,831 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:40:30,503 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm3_1   |   uuid128 {
scm2_1   | 2023-07-19 07:36:05,987 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   |     mostSigBits: -7158348809883070837
scm1_1   |     leastSigBits: -5357303777780234481
scm1_1   |   }
scm2_1   | 2023-07-19 07:36:05,990 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:40:30,504 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm3_1   |     leastSigBits: -4977492878163373773
scm1_1   | }
scm1_1   | isLeader: false
dn4_1    | 2023-07-19 07:38:56,090 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: addNew group-AADDEAEB0EB6:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-AADDEAEB0EB6:java.util.concurrent.CompletableFuture@3c5fbcd0[Not completed]
scm2_1   | 2023-07-19 07:36:06,000 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | term: 8
scm2_1   | 2023-07-19 07:36:06,000 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   |   }
scm1_1   | bytesWritten: 0
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-07-19 07:38:56,097 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: new RaftServerImpl for group-AADDEAEB0EB6:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | nextIndex: 49
dn5_1    | followerCommit: 48
dn5_1    | matchIndex: 18446744073709551615
scm1_1   |  from dn=a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17).
scm3_1   | }
dn5_1    | isHearbeat: true
recon_1  | 2023-07-19 07:40:30,504 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 165 
scm2_1   | 2023-07-19 07:36:06,020 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:38:56,098 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:38:22,918 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | isLeader: true
dn5_1    | 
dn5_1    | 2023-07-19 07:37:32,036 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3-SegmentedRaftLogWorker close()
recon_1  | 2023-07-19 07:40:30,535 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 5, SequenceNumber diff: 16, SequenceNumber Lag from OM 0.
dn4_1    | 2023-07-19 07:38:56,098 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-07-19 07:38:56,098 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-07-19 07:38:22,919 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm3_1   | bytesWritten: 0
scm3_1   |  from dn=f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21).
dn5_1    | 2023-07-19 07:37:32,041 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-49D8FC4B32B3: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/18094581-8ef4-4cd5-8263-49d8fc4b32b3
dn5_1    | 2023-07-19 07:37:32,041 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 command on datanode f02af6ab-c12d-469b-a775-f6b30900ff13.
recon_1  | 2023-07-19 07:40:30,535 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 16 records
dn4_1    | 2023-07-19 07:38:56,098 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-07-19 07:38:56,099 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-07-19 07:38:56,099 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn5_1    | 2023-07-19 07:37:32,042 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: remove  FOLLOWER f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE:t8, leader=140718b2-320a-4020-b7ea-662533776c74, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLog:OPENED:c48, conf=47: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null RUNNING
dn5_1    | 2023-07-19 07:37:32,042 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: shutdown
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn3_1    | 2023-07-19 07:38:22,923 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-07-19 07:37:32,042 [PipelineCommandHandlerThread-0] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-03BEFB15C8DE,id=f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:38:56,100 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: ConfigurationManager, init=-1: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-07-19 07:36:06,020 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn5_1    | 2023-07-19 07:37:32,042 [PipelineCommandHandlerThread-0] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState
dn5_1    | 2023-07-19 07:37:32,042 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-FollowerState was interrupted
dn5_1    | 2023-07-19 07:37:32,043 [PipelineCommandHandlerThread-0] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater: set stopIndex = 48
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2_1   | 2023-07-19 07:36:06,057 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:06,064 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:06,075 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
dn5_1    | 2023-07-19 07:37:32,045 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Taking a snapshot at:(t:8, i:48) file /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48
scm2_1   | 2023-07-19 07:36:06,099 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn5_1    | 2023-07-19 07:37:32,047 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater] INFO ratis.ContainerStateMachine: group-03BEFB15C8DE: Finished taking a snapshot at:(t:8, i:48) file:/data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de/sm/snapshot.8_48 took: 2 ms
dn3_1    | 2023-07-19 07:38:22,923 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1  | 2023-07-19 07:40:30,617 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm2_1   | 2023-07-19 07:36:06,103 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:06,112 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:06,127 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn3_1    | 2023-07-19 07:38:22,926 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
recon_1  | 2023-07-19 07:40:30,617 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for hadoop.
recon_1  | 2023-07-19 07:40:30,617 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:40:30,618 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /new2-volume.
scm2_1   | 2023-07-19 07:36:06,130 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
dn5_1    | 2023-07-19 07:37:32,047 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater: Took a snapshot at index 48
dn3_1    | 2023-07-19 07:38:22,926 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1  | 2023-07-19 07:40:30,618 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:40:30,620 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /new2-volume/new2-bucket.
recon_1  | 2023-07-19 07:40:30,620 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm2_1   | 2023-07-19 07:36:06,164 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
dn5_1    | 2023-07-19 07:37:32,048 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater] INFO impl.StateMachineUpdater: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-StateMachineUpdater: snapshotIndex: updateIncreasingly 46 -> 48
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:38:22,926 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:36:06,193 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-07-19 07:38:22,926 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:38:22,929 [grpc-default-executor-5] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-075378E90876:java.util.concurrent.CompletableFuture@20ae7e61[Not completed]
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 2023-07-19 07:38:22,936 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:37:32,050 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: closes. applyIndex: 48
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn4_1    | 2023-07-19 07:38:56,100 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
recon_1  | 2023-07-19 07:40:30,620 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /new2-volume/new2-bucket.
recon_1  | 2023-07-19 07:40:30,621 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:40:30,621 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v.
recon_1  | 2023-07-19 07:40:30,831 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
dn5_1    | 2023-07-19 07:37:32,921 [PipelineCommandHandlerThread-0] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE-SegmentedRaftLogWorker close()
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-07-19 07:38:56,103 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:36:06,196 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm2_1   | 2023-07-19 07:36:06,208 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:40:30,837 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn5_1    | 2023-07-19 07:37:32,949 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn4_1    | 2023-07-19 07:38:56,104 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn3_1    | 2023-07-19 07:38:22,949 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:37:32,950 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn5_1    | 2023-07-19 07:37:33,004 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn4_1    | 2023-07-19 07:38:56,105 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:36:07,277 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
recon_1  | 2023-07-19 07:40:30,837 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn3_1    | 2023-07-19 07:38:22,949 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:37:33,007 [PipelineCommandHandlerThread-0] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
scm1_1   | 	... 3 more
dn4_1    | 2023-07-19 07:38:56,105 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-07-19 07:36:07,278 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn3_1    | 2023-07-19 07:38:22,949 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-07-19 07:37:33,034 [PipelineCommandHandlerThread-0] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-03BEFB15C8DE: Succeed to remove RaftStorageDirectory Storage Directory /data/metadata/ratis/d2552f1d-da74-434f-bb3d-03befb15c8de
dn5_1    | 2023-07-19 07:37:33,035 [PipelineCommandHandlerThread-0] INFO commandhandler.ClosePipelineCommandHandler: Close Pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de command on datanode f02af6ab-c12d-469b-a775-f6b30900ff13.
dn4_1    | 2023-07-19 07:38:56,106 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-07-19 07:36:07,283 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn5_1    | 2023-07-19 07:37:36,227 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
scm1_1   | 2023-07-19 07:38:56,198 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn4_1    | 2023-07-19 07:38:56,106 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-07-19 07:40:30,859 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 1 OM DB update event(s).
scm2_1   | 2023-07-19 07:36:07,285 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:07,303 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm1_1   |   id: "3b24ef8c-1b15-473e-825d-aaddeaeb0eb6"
dn4_1    | 2023-07-19 07:38:56,108 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1  | 2023-07-19 07:40:30,907 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm2_1   | 2023-07-19 07:36:07,313 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:07,332 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   |   uuid128 {
dn4_1    | 2023-07-19 07:38:56,113 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-07-19 07:38:56,116 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-07-19 07:38:56,118 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-19 07:38:22,952 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   |     mostSigBits: 4261794532428171070
dn4_1    | 2023-07-19 07:38:56,118 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 2023-07-19 07:40:32,539 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-07-19 07:36:07,339 [grpc-default-executor-4] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm2_1   | 2023-07-19 07:36:07,340 [grpc-default-executor-4] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: decreaseNextIndex nextIndex: updateUnconditionally 81 -> 0
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn4_1    | 2023-07-19 07:38:56,118 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-07-19 07:40:32,587 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn3_1    | 2023-07-19 07:38:22,952 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-07-19 07:36:07,366 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn4_1    | 2023-07-19 07:38:56,119 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-07-19 07:38:56,120 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3b24ef8c-1b15-473e-825d-aaddeaeb0eb6 does not exist. Creating ...
dn3_1    | 2023-07-19 07:38:22,953 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:38:22,953 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 does not exist. Creating ...
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 2023-07-19 07:38:56,125 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3b24ef8c-1b15-473e-825d-aaddeaeb0eb6/in_use.lock acquired by nodename 7@6cb42cec657e
dn4_1    | 2023-07-19 07:38:56,129 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3b24ef8c-1b15-473e-825d-aaddeaeb0eb6 has been successfully formatted.
dn4_1    | 2023-07-19 07:38:56,132 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO ratis.ContainerStateMachine: group-AADDEAEB0EB6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm3_1   | 	... 3 more
scm3_1   | 2023-07-19 07:38:30,466 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn4_1    | 2023-07-19 07:38:56,132 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-19 07:38:22,959 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/in_use.lock acquired by nodename 7@69ed6f01c113
dn3_1    | 2023-07-19 07:38:22,961 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 has been successfully formatted.
scm3_1   | 2023-07-19 07:38:30,466 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:38:33,830 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 2023-07-19 07:38:56,132 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-07-19 07:38:22,962 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO ratis.ContainerStateMachine: group-075378E90876: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-19 07:38:22,962 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   |   id: "9a97a9af-3060-4a66-97a0-7bbd49954ef8"
scm3_1   |   uuid128 {
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 2023-07-19 07:38:22,963 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm2_1   | 2023-07-19 07:36:07,367 [grpc-default-executor-3] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
dn4_1    | 2023-07-19 07:38:56,133 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   |     mostSigBits: -7307185300489352602
dn3_1    | 2023-07-19 07:38:22,963 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm2_1   | 2023-07-19 07:36:11,696 [grpc-default-executor-1] WARN server.GrpcLogAppender: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0-AppendLogResponseHandler: received INCONSISTENCY reply with nextIndex 79
scm1_1   |     leastSigBits: -9052891805839061322
scm1_1   |   }
dn4_1    | 2023-07-19 07:38:56,133 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   |     leastSigBits: -7520875324795171080
dn3_1    | 2023-07-19 07:38:22,963 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm2_1   | 2023-07-19 07:36:11,698 [grpc-default-executor-1] INFO leader.FollowerInfo: b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073->57eda1b0-9276-42ed-8a05-f1155bfae1c0: setNextIndex nextIndex: updateUnconditionally 81 -> 79
scm1_1   | }
scm1_1   | isLeader: false
dn3_1    | 2023-07-19 07:38:22,963 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm2_1   | 2023-07-19 07:36:20,430 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn4_1    | 2023-07-19 07:38:56,134 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | bytesWritten: 0
scm1_1   |  from dn=adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20).
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:38:22,973 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:38:22,974 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
recon_1  | 2023-07-19 07:40:32,624 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-07-19 07:36:48,165 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization started.
scm2_1   | 2023-07-19 07:36:48,323 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Stopping RatisPipelineUtilsThread.
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:36:48,331 [RatisPipelineUtilsThread - 0] WARN pipeline.BackgroundPipelineCreator: RatisPipelineUtilsThread is interrupted.
scm2_1   | 2023-07-19 07:36:48,355 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint FINALIZATION_STARTED
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm1_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-07-19 07:38:56,243 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:38:56,244 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-07-19 07:36:48,513 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1 closed for pipeline=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732
scm3_1   |   }
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:38:56,246 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
dn5_1    | 2023-07-19 07:37:37,114 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
recon_1  | 2023-07-19 07:40:32,665 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn4_1    | 2023-07-19 07:38:56,246 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:38:56,247 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO segmented.SegmentedRaftLogWorker: new adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3b24ef8c-1b15-473e-825d-aaddeaeb0eb6
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1  | 2023-07-19 07:40:35,659 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn2_1.ha_net.
scm2_1   | 2023-07-19 07:36:48,515 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1, current state: CLOSING
scm2_1   | 2023-07-19 07:36:48,553 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1002 closed for pipeline=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-19 07:38:22,975 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
recon_1  | 2023-07-19 07:40:35,703 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2002 got from ha_dn3_1.ha_net.
recon_1  | 2023-07-19 07:40:35,747 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn3_1    | 2023-07-19 07:38:22,976 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:38:22,976 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-07-19 07:36:48,571 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1002, current state: CLOSING
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
recon_1  | 2023-07-19 07:40:35,756 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2002 to Recon.
scm1_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
dn3_1    | 2023-07-19 07:38:22,976 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-07-19 07:36:48,598 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d5697fa-1223-40ff-ba6c-8563b54dd732, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.745427Z[UTC]] moved to CLOSED state
dn4_1    | 2023-07-19 07:38:56,247 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | }
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-07-19 07:38:22,984 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-07-19 07:38:56,247 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm2_1   | 2023-07-19 07:36:48,625 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e6765bce-cd48-45ed-b1c7-20996759f8fd, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:adc6845d-6cb4-4b43-88ca-47ca3f1a71df, CreationTimestamp2023-07-19T07:35:24.746659Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm3_1   | isLeader: false
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-07-19 07:38:23,031 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:38:56,276 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 2023-07-19 07:36:48,670 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 18094581-8ef4-4cd5-8263-49d8fc4b32b3, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.734168Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm3_1   | bytesWritten: 0
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 2023-07-19 07:38:56,276 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1   | 2023-07-19 07:36:48,722 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b7ee2aa-396d-4f71-b4bf-944115277f4a, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, CreationTimestamp2023-07-19T07:35:24.745731Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm3_1   |  from dn=140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19).
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:38:23,032 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-07-19 07:38:56,276 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-07-19 07:36:48,763 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #2 closed for pipeline=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn4_1    | 2023-07-19 07:38:56,277 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-07-19 07:36:48,766 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #2, current state: CLOSING
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn3_1    | 2023-07-19 07:38:23,033 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-07-19 07:38:56,277 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-07-19 07:36:48,794 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Container #1001 closed for pipeline=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn3_1    | 2023-07-19 07:38:23,034 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-07-19 07:36:48,795 [EventQueue-CloseContainerForCloseContainerEventHandler] INFO container.CloseContainerEventHandler: Close container Event triggered for container : #1001, current state: CLOSING
scm2_1   | 2023-07-19 07:36:48,849 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d2552f1d-da74-434f-bb3d-03befb15c8de, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.746371Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
dn3_1    | 2023-07-19 07:38:23,034 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-07-19 07:38:56,277 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-07-19 07:36:48,914 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 64e66587-f45d-4b2d-8462-aa6ee9a1af61, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a9b83a7e-59b8-4455-b30a-c01eee264fbd, CreationTimestamp2023-07-19T07:35:24.739718Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
recon_1  | 2023-07-19 07:41:14,930 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-07-19 07:41:14,931 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 21
recon_1  | 2023-07-19 07:41:16,150 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn3_1    | 2023-07-19 07:38:23,064 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:38:56,278 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-07-19 07:36:48,986 [IPC Server handler 81 on default port 9860] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63a4529d-d8b2-46dd-93ed-b30fa8b5baef, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.735675Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
recon_1  | 2023-07-19 07:41:16,159 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 7 milliseconds for processing 6 containers.
recon_1  | 2023-07-19 07:41:16,166 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 14 pipelines in house.
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
dn3_1    | 2023-07-19 07:38:23,064 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-07-19 07:38:56,280 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-07-19 07:36:48,992 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer:   Existing pipelines and containers will be closed during Upgrade.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
dn3_1    | 2023-07-19 07:38:23,065 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState
dn3_1    | 2023-07-19 07:38:23,087 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-075378E90876,id=140718b2-320a-4020-b7ea-662533776c74
scm2_1   |   New pipelines creation will remain frozen until Upgrade is finalized.
scm2_1   | 2023-07-19 07:36:49,031 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
scm2_1   | 2023-07-19 07:36:49,043 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-07-19 07:41:16,172 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 from Recon.
recon_1  | 2023-07-19 07:41:16,176 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d5697fa-1223-40ff-ba6c-8563b54dd732, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.745Z[UTC]] moved to CLOSED state
dn4_1    | 2023-07-19 07:38:56,384 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-19 07:38:23,087 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
recon_1  | 2023-07-19 07:41:16,180 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d5697fa-1223-40ff-ba6c-8563b54dd732, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.745Z[UTC]] removed.
dn4_1    | 2023-07-19 07:38:56,386 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-07-19 07:37:41,461 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
recon_1  | 2023-07-19 07:41:16,181 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd from Recon.
recon_1  | 2023-07-19 07:41:16,183 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e6765bce-cd48-45ed-b1c7-20996759f8fd, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:adc6845d-6cb4-4b43-88ca-47ca3f1a71df, CreationTimestamp2023-07-19T07:35:24.746Z[UTC]] moved to CLOSED state
recon_1  | 2023-07-19 07:41:16,192 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e6765bce-cd48-45ed-b1c7-20996759f8fd, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:adc6845d-6cb4-4b43-88ca-47ca3f1a71df, CreationTimestamp2023-07-19T07:35:24.746Z[UTC]] removed.
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
recon_1  | 2023-07-19 07:41:16,195 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 from Recon.
recon_1  | 2023-07-19 07:41:16,196 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 18094581-8ef4-4cd5-8263-49d8fc4b32b3, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.734Z[UTC]] moved to CLOSED state
recon_1  | 2023-07-19 07:41:16,198 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 18094581-8ef4-4cd5-8263-49d8fc4b32b3, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.734Z[UTC]] removed.
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:36:49,071 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:36:49,074 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-07-19 07:41:16,203 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef from Recon.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
dn3_1    | 2023-07-19 07:38:23,087 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-07-19 07:38:56,387 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-07-19 07:41:16,205 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63a4529d-d8b2-46dd-93ed-b30fa8b5baef, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.735Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn3_1    | 2023-07-19 07:38:23,087 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 	... 3 more
dn4_1    | 2023-07-19 07:38:56,388 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2_1   | 2023-07-19 07:36:49,100 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
recon_1  | 2023-07-19 07:41:16,205 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63a4529d-d8b2-46dd-93ed-b30fa8b5baef, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.735Z[UTC]] removed.
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 2023-07-19 07:38:23,087 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-19 07:38:23,102 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:38:56,391 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2_1   | 2023-07-19 07:36:49,107 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
recon_1  | 2023-07-19 07:41:16,206 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a from Recon.
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn4_1    | 2023-07-19 07:38:56,397 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: start as a follower, conf=-1: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-07-19 07:38:56,399 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
recon_1  | 2023-07-19 07:41:16,207 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b7ee2aa-396d-4f71-b4bf-944115277f4a, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, CreationTimestamp2023-07-19T07:35:24.745Z[UTC]] moved to CLOSED state
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm2_1   | 2023-07-19 07:36:49,107 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm2_1   | 2023-07-19 07:36:49,126 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
recon_1  | 2023-07-19 07:41:16,207 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b7ee2aa-396d-4f71-b4bf-944115277f4a, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, CreationTimestamp2023-07-19T07:35:24.745Z[UTC]] removed.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn4_1    | 2023-07-19 07:38:56,399 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
recon_1  | 2023-07-19 07:41:16,208 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de from Recon.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm2_1   | 2023-07-19 07:36:49,127 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY READONLY state.
dn4_1    | 2023-07-19 07:38:56,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-AADDEAEB0EB6,id=adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm1_1   | 2023-07-19 07:39:31,659 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-07-19 07:39:39,124 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn4_1    | 2023-07-19 07:38:56,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm2_1   | 2023-07-19 07:36:49,127 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:39:39,368 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-07-19 07:39:49,698 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn4_1    | 2023-07-19 07:38:56,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
recon_1  | 2023-07-19 07:41:16,209 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d2552f1d-da74-434f-bb3d-03befb15c8de, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.746Z[UTC]] moved to CLOSED state
recon_1  | 2023-07-19 07:41:16,209 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d2552f1d-da74-434f-bb3d-03befb15c8de, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:OPEN, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.746Z[UTC]] removed.
recon_1  | 2023-07-19 07:41:16,210 [PipelineSyncTask] INFO scm.ReconPipelineManager: Removing invalid pipeline PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 from Recon.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm2_1   | 2023-07-19 07:36:49,129 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:40:09,129 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm1_1   | 2023-07-19 07:40:09,422 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn4_1    | 2023-07-19 07:38:56,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-07-19 07:41:16,211 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 64e66587-f45d-4b2d-8462-aa6ee9a1af61, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a9b83a7e-59b8-4455-b30a-c01eee264fbd, CreationTimestamp2023-07-19T07:35:24.739Z[UTC]] moved to CLOSED state
recon_1  | 2023-07-19 07:41:16,212 [PipelineSyncTask] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 64e66587-f45d-4b2d-8462-aa6ee9a1af61, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:a9b83a7e-59b8-4455-b30a-c01eee264fbd, CreationTimestamp2023-07-19T07:35:24.739Z[UTC]] removed.
recon_1  | 2023-07-19 07:41:16,216 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 65 milliseconds.
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:36:49,129 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode f02af6ab-c12d-469b-a775-f6b30900ff13. This will send close commands for its containers.
scm1_1   | 2023-07-19 07:40:17,264 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
scm1_1   | 2023-07-19 07:40:17,328 [36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
dn3_1    | 2023-07-19 07:38:23,103 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-07-19 07:41:30,918 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-07-19 07:41:30,918 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 2023-07-19 07:40:32,535 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm1_1   | 2023-07-19 07:40:32,582 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
dn4_1    | 2023-07-19 07:38:56,426 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-07-19 07:41:30,918 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 181 
recon_1  | 2023-07-19 07:41:30,939 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 7, SequenceNumber diff: 21, SequenceNumber Lag from OM 0.
dn3_1    | 2023-07-19 07:38:24,067 [grpc-default-executor-5] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-649965091133:java.util.concurrent.CompletableFuture@6bb30af[Not completed]
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn4_1    | 2023-07-19 07:38:56,429 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=3b24ef8c-1b15-473e-825d-aaddeaeb0eb6
scm3_1   | 	... 3 more
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:36:49,131 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY READONLY state.
recon_1  | 2023-07-19 07:41:30,939 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 21 records
dn4_1    | 2023-07-19 07:38:56,430 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3b24ef8c-1b15-473e-825d-aaddeaeb0eb6.
scm3_1   | 2023-07-19 07:38:34,114 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn5_1    | 2023-07-19 07:37:42,180 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:38:56,430 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   |   id: "b29414e4-15a3-4023-afdc-f99861290493"
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
dn3_1    | 2023-07-19 07:38:24,069 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/new2-bucket.
dn4_1    | 2023-07-19 07:38:56,430 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   |   uuid128 {
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:39:01,506 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO impl.FollowerState: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5107202655ns, electionTimeout:5076ms
scm3_1   |     mostSigBits: -5578811068557082589
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/new2-bucket.
dn4_1    | 2023-07-19 07:39:01,507 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState
scm3_1   |     leastSigBits: -5774466189335395181
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
dn3_1    | 2023-07-19 07:38:24,070 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:39:01,507 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm3_1   |   }
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 2023-07-19 07:38:24,070 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1  | 2023-07-19 07:41:30,996 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:41:30,999 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/new2-bucket.
recon_1  | 2023-07-19 07:41:30,999 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:41:30,999 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /s3v/new2-bucket.
recon_1  | 2023-07-19 07:41:30,999 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn3_1    | 2023-07-19 07:38:24,070 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:38:24,070 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:38:24,070 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-07-19 07:38:24,071 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:38:24,071 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn3_1    | 2023-07-19 07:38:24,072 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-07-19 07:38:24,072 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:38:24,072 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn3_1    | 2023-07-19 07:38:24,073 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
recon_1  | 2023-07-19 07:41:31,000 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for hadoop.
scm3_1   | }
scm3_1   | isLeader: false
scm2_1   | 2023-07-19 07:36:49,131 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:36:49,131 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
recon_1  | 2023-07-19 07:41:31,000 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:41:31,000 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq.
recon_1  | 2023-07-19 07:41:31,214 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
scm2_1   | 2023-07-19 07:36:49,132 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef in state CLOSED which uses HEALTHY_READONLY datanode 140718b2-320a-4020-b7ea-662533776c74. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:36:49,132 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY READONLY state.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | bytesWritten: 0
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm3_1   |  from dn=f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21).
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
recon_1  | 2023-07-19 07:41:31,217 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
scm2_1   | 2023-07-19 07:36:49,132 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 in state CLOSED which uses HEALTHY_READONLY datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:36:49,132 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY READONLY state.
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:38:24,074 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 does not exist. Creating ...
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
recon_1  | 2023-07-19 07:41:31,217 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
scm2_1   | 2023-07-19 07:36:49,134 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:36:49,134 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
dn3_1    | 2023-07-19 07:38:24,076 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/in_use.lock acquired by nodename 7@69ed6f01c113
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm1_1   | 	... 3 more
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
recon_1  | 2023-07-19 07:41:31,243 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 2 OM DB update event(s).
scm2_1   | 2023-07-19 07:36:49,148 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: SCM Finalization has crossed checkpoint MLV_EQUALS_SLV
scm2_1   | 2023-07-19 07:36:49,148 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-07-19 07:38:24,077 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 has been successfully formatted.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-07-19 07:40:32,598 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm1_1   | 2023-07-19 07:40:32,599 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:38:24,078 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO ratis.ContainerStateMachine: group-649965091133: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-19 07:38:24,078 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 2023-07-19 07:41:31,256 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
dn3_1    | 2023-07-19 07:38:24,082 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-07-19 07:39:01,507 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
recon_1  | 2023-07-19 07:42:14,931 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1  | 2023-07-19 07:42:14,932 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1  | 2023-07-19 07:42:31,266 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
dn3_1    | 2023-07-19 07:38:24,082 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:24,082 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-07-19 07:39:01,507 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-FollowerState] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:36:49,162 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de in state CLOSED which uses HEALTHY_READONLY datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df. This will send close commands for its containers.
scm2_1   | 2023-07-19 07:36:49,162 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY READONLY state.
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn5_1    | 2023-07-19 07:37:46,655 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
recon_1  | 2023-07-19 07:42:31,266 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-07-19 07:42:31,266 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 202 
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2_1   | 2023-07-19 07:36:49,163 [EventQueue-HealthyReadonlyNodeForHealthyReadOnlyNodeHandler] INFO node.HealthyReadOnlyNodeHandler: Sending close command for pipeline PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a in state CLOSED which uses HEALTHY_READONLY datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef. This will send close commands for its containers.
dn3_1    | 2023-07-19 07:38:24,082 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
recon_1  | 2023-07-19 07:42:31,334 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 9, SequenceNumber diff: 28, SequenceNumber Lag from OM 0.
recon_1  | 2023-07-19 07:42:31,334 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 28 records
dn3_1    | 2023-07-19 07:38:24,083 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-07-19 07:39:01,512 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm2_1   | 2023-07-19 07:36:49,168 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
dn3_1    | 2023-07-19 07:38:24,083 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-07-19 07:39:01,512 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12 PRE_VOTE round 0: result PASSED (term=0)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12 ELECTION round 0: submit vote requests at term 1 for -1: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm2_1   | 2023-07-19 07:36:50,888 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:36:52,380 [IPC Server handler 52 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2_1   | 2023-07-19 07:36:54,148 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-07-19 07:38:24,083 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:36:59,150 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.LeaderElection: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12 ELECTION round 0: result PASSED (term=1)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm2_1   | 2023-07-19 07:36:59,564 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: shutdown adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2_1   | 2023-07-19 07:37:00,092 [IPC Server handler 60 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm2_1   | 2023-07-19 07:37:04,150 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm2_1   | 2023-07-19 07:37:09,150 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
scm3_1   | 	... 3 more
scm3_1   | 2023-07-19 07:38:35,558 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-AADDEAEB0EB6 with new leaderId: adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: change Leader from null to adc6845d-6cb4-4b43-88ca-47ca3f1a71df at term 1 for becomeLeader, leader elected after 5409ms
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:38:24,084 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:37:14,151 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm2_1   | 2023-07-19 07:37:16,882 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-07-19 07:39:01,515 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:37:17,870 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for delTxnId, expected lastId is 0, actual lastId is 2000.
scm2_1   | 2023-07-19 07:37:17,931 [IPC Server handler 43 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 2000 to 3000.
scm3_1   | 2023-07-19 07:38:35,559 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-07-19 07:42:31,361 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb.
recon_1  | 2023-07-19 07:42:31,362 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
dn4_1    | 2023-07-19 07:39:01,516 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
recon_1  | 2023-07-19 07:42:31,362 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for #TRANSACTIONINFO.
recon_1  | 2023-07-19 07:42:31,362 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb/snap-hzygr.
dn4_1    | 2023-07-19 07:39:01,529 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-07-19 07:37:47,221 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
dn4_1    | 2023-07-19 07:39:01,533 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-07-19 07:39:01,535 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-07-19 07:39:01,536 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-07-19 07:38:40,749 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:38:24,091 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-19 07:38:24,099 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn4_1    | 2023-07-19 07:39:01,538 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO impl.RoleInfo: adc6845d-6cb4-4b43-88ca-47ca3f1a71df: start adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderStateImpl
recon_1  | 2023-07-19 07:42:31,362 [pool-49-thread-1] WARN tasks.OmTableInsightTask: Update event does not have the old Key Info for /vol-equdq/buc-akpcb/snap-vanff.
recon_1  | 2023-07-19 07:42:31,645 [pool-49-thread-1] INFO tasks.OmTableInsightTask: Completed a 'process' run of OmTableInsightTask.
dn4_1    | 2023-07-19 07:39:01,539 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-07-19 07:38:24,104 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
scm2_1   | 2023-07-19 07:37:19,151 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-07-19 07:38:40,750 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:38:45,835 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-07-19 07:39:01,541 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3b24ef8c-1b15-473e-825d-aaddeaeb0eb6/current/log_inprogress_0
dn4_1    | 2023-07-19 07:39:01,551 [adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6-LeaderElection12] INFO server.RaftServer$Division: adc6845d-6cb4-4b43-88ca-47ca3f1a71df@group-AADDEAEB0EB6: set configuration 0: peers:[adc6845d-6cb4-4b43-88ca-47ca3f1a71df|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-07-19 07:37:19,173 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm2_1   | 2023-07-19 07:37:20,886 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn2_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:22,330 [IPC Server handler 52 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn4_1    | 2023-07-19 07:39:46,590 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1  | 2023-07-19 07:42:31,646 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm3_1   | 2023-07-19 07:38:45,836 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:38:47,985 [EventQueue-PipelineReportForPipelineReportHandler] ERROR pipeline.PipelineReportHandler: Could not process pipeline report=pipelineID {
dn4_1    | 2023-07-19 07:40:32,516 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn4_1    | 2023-07-19 07:40:32,516 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   |   id: "301e375a-65de-4355-b5a7-08992b8d070f"
scm3_1   |   uuid128 {
recon_1  | 2023-07-19 07:42:31,646 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-07-19 07:42:31,651 [pool-49-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   |     mostSigBits: 3467269624517509973
scm3_1   |     leastSigBits: -5357303777780234481
recon_1  | 2023-07-19 07:42:31,664 [pool-49-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
dn3_1    | 2023-07-19 07:38:24,105 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:37:24,152 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   |   }
dn4_1    | 2023-07-19 07:40:32,523 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 2023-07-19 07:38:24,105 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2_1   | 2023-07-19 07:37:24,872 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 since it stays at CLOSED stage.
scm2_1   | 2023-07-19 07:37:24,874 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 close command to datanode 140718b2-320a-4020-b7ea-662533776c74
dn4_1    | 2023-07-19 07:40:32,524 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
dn3_1    | 2023-07-19 07:38:24,106 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2_1   | 2023-07-19 07:37:24,874 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 close command to datanode f02af6ab-c12d-469b-a775-f6b30900ff13
scm3_1   | }
dn4_1    | 2023-07-19 07:40:32,537 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
scm2_1   | 2023-07-19 07:37:24,874 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 close command to datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm3_1   | isLeader: false
scm3_1   | bytesWritten: 0
dn3_1    | 2023-07-19 07:38:24,106 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm2_1   | 2023-07-19 07:37:24,894 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6d5697fa-1223-40ff-ba6c-8563b54dd732, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.745427Z[UTC]] removed.
scm3_1   |  from dn=a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17).
dn4_1    | 2023-07-19 07:40:32,557 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 45.
dn3_1    | 2023-07-19 07:38:24,106 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	... 3 more
scm2_1   | 2023-07-19 07:37:24,894 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd since it stays at CLOSED stage.
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm1_1   | 2023-07-19 07:40:32,631 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-07-19 07:37:24,894 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd close command to datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df
scm2_1   | 2023-07-19 07:37:24,909 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e6765bce-cd48-45ed-b1c7-20996759f8fd, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:adc6845d-6cb4-4b43-88ca-47ca3f1a71df, CreationTimestamp2023-07-19T07:35:24.746659Z[UTC]] removed.
dn4_1    | 2023-07-19 07:40:32,560 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn3_1    | 2023-07-19 07:38:24,107 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm1_1   | 2023-07-19 07:40:32,632 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1001
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:37:24,909 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 since it stays at CLOSED stage.
dn4_1    | 2023-07-19 07:40:32,581 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn4_1    | 2023-07-19 07:40:32,613 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 28.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:37:24,909 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=18094581-8ef4-4cd5-8263-49d8fc4b32b3 close command to datanode f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:40:32,616 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn3_1    | 2023-07-19 07:38:24,107 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2_1   | 2023-07-19 07:37:24,928 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 18094581-8ef4-4cd5-8263-49d8fc4b32b3, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:f02af6ab-c12d-469b-a775-f6b30900ff13, CreationTimestamp2023-07-19T07:35:24.734168Z[UTC]] removed.
dn4_1    | 2023-07-19 07:40:32,642 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn3_1    | 2023-07-19 07:38:24,107 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-649965091133,id=140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm3_1   | 	at com.sun.proxy.$Proxy17.updatePipelineState(Unknown Source)
scm2_1   | 2023-07-19 07:37:24,928 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a since it stays at CLOSED stage.
dn4_1    | 2023-07-19 07:40:32,661 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 9.
dn3_1    | 2023-07-19 07:38:24,108 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2_1   | 2023-07-19 07:37:24,928 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=7b7ee2aa-396d-4f71-b4bf-944115277f4a close command to datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn4_1    | 2023-07-19 07:40:46,592 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 5/4995 blocks from 3 candidate containers.
dn3_1    | 2023-07-19 07:38:24,108 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-07-19 07:37:47,778 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineManagerImpl.openPipeline(PipelineManagerImpl.java:430)
scm2_1   | 2023-07-19 07:37:24,948 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 7b7ee2aa-396d-4f71-b4bf-944115277f4a, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, CreationTimestamp2023-07-19T07:35:24.745731Z[UTC]] removed.
dn4_1    | 2023-07-19 07:40:46,612 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200002.block
dn5_1    | 2023-07-19 07:37:51,740 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.processPipelineReport(PipelineReportHandler.java:135)
scm2_1   | 2023-07-19 07:37:24,948 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de since it stays at CLOSED stage.
dn4_1    | 2023-07-19 07:40:46,613 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200003.block
dn3_1    | 2023-07-19 07:38:24,108 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:93)
scm2_1   | 2023-07-19 07:37:24,950 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de close command to datanode 140718b2-320a-4020-b7ea-662533776c74
scm2_1   | 2023-07-19 07:37:24,950 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de close command to datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn3_1    | 2023-07-19 07:38:24,108 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm3_1   | 	at org.apache.hadoop.hdds.scm.pipeline.PipelineReportHandler.onMessage(PipelineReportHandler.java:52)
scm2_1   | 2023-07-19 07:37:24,950 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de close command to datanode f02af6ab-c12d-469b-a775-f6b30900ff13
dn4_1    | 2023-07-19 07:40:46,612 [BlockDeletingService#4] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201001.block
dn3_1    | 2023-07-19 07:38:24,118 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-8563B54DD732 not found.
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.SingleThreadExecutor.lambda$onMessage$1(SingleThreadExecutor.java:85)
scm2_1   | 2023-07-19 07:37:24,971 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d2552f1d-da74-434f-bb3d-03befb15c8de, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:CLOSED, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.746371Z[UTC]] removed.
dn4_1    | 2023-07-19 07:40:46,633 [BlockDeletingService#4] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201002.block
dn4_1    | 2023-07-19 07:40:46,612 [BlockDeletingService#6] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/1/chunks/111677748019200001.block
dn3_1    | 2023-07-19 07:38:24,120 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:37:24,971 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 since it stays at CLOSED stage.
dn4_1    | 2023-07-19 07:41:46,626 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
dn3_1    | 2023-07-19 07:38:27,440 [grpc-default-executor-5] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: receive requestVote(PRE_VOTE, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, group-075378E90876, 0, (t:0, i:0))
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-07-19 07:42:46,626 [BlockDeletingService#5] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm2_1   | 2023-07-19 07:37:24,971 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=64e66587-f45d-4b2d-8462-aa6ee9a1af61 close command to datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 2023-07-19 07:38:27,441 [grpc-default-executor-5] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FOLLOWER: reject PRE_VOTE from 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: our priority 1 > candidate's priority 0
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-07-19 07:37:25,001 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 64e66587-f45d-4b2d-8462-aa6ee9a1af61, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:a9b83a7e-59b8-4455-b30a-c01eee264fbd, CreationTimestamp2023-07-19T07:35:24.739718Z[UTC]] removed.
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:37:25,001 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Scrubbing pipeline: id: PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef since it stays at CLOSED stage.
dn3_1    | 2023-07-19 07:38:27,441 [grpc-default-executor-5] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876 replies to PRE_VOTE vote request: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t0. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876:t0, leader=null, voted=, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm2_1   | 2023-07-19 07:37:25,002 [BackgroundPipelineScrubberThread] INFO pipeline.RatisPipelineProvider: Send pipeline:PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef close command to datanode 140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:38:28,234 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5169880500ns, electionTimeout:5131ms
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm2_1   | 2023-07-19 07:37:25,016 [BackgroundPipelineScrubberThread] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 63a4529d-d8b2-46dd-93ed-b30fa8b5baef, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:CLOSED, leaderId:140718b2-320a-4020-b7ea-662533776c74, CreationTimestamp2023-07-19T07:35:24.735675Z[UTC]] removed.
dn3_1    | 2023-07-19 07:38:28,235 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:37:29,152 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-07-19 07:38:28,235 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm2_1   | 2023-07-19 07:37:29,554 [IPC Server handler 57 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,236 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn3_1    | 2023-07-19 07:38:28,236 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4
scm2_1   | 2023-07-19 07:37:30,092 [IPC Server handler 60 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn3_1    | 2023-07-19 07:38:28,243 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:38:28,254 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:37:31,256 [IPC Server handler 52 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:31,361 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:28,254 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:37:31,367 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 2023-07-19 07:38:28,255 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4-2] INFO server.GrpcServerProtocolClient: Build channel for 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn3_1    | 2023-07-19 07:38:28,257 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm2_1   | 2023-07-19 07:37:31,399 [IPC Server handler 49 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:31,447 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm2_1   | 2023-07-19 07:37:31,448 [FixedThreadPoolWithAffinityExecutor-0-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to QUASI_CLOSED state, datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) reported QUASI_CLOSED replica.
dn3_1    | 2023-07-19 07:38:28,257 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection:   Response 0: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t0
scm3_1   | 	... 3 more
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm2_1   | 2023-07-19 07:37:31,809 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:31,843 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:38:51,027 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm2_1   | 2023-07-19 07:37:31,871 [IPC Server handler 62 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:31,889 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:38:51,028 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:28,257 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4 PRE_VOTE round 0: result PASSED
dn3_1    | 2023-07-19 07:38:28,261 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:38:56,061 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn3_1    | 2023-07-19 07:38:28,271 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:38:28,271 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | 	... 3 more
dn3_1    | 2023-07-19 07:38:28,322 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-07-19 07:38:28,322 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection:   Response 0: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t1
scm3_1   | 2023-07-19 07:38:56,061 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:37:52,306 [grpc-default-executor-6] WARN server.GrpcServerProtocolService: f02af6ab-c12d-469b-a775-f6b30900ff13: Failed requestVote adc6845d-6cb4-4b43-88ca-47ca3f1a71df->f02af6ab-c12d-469b-a775-f6b30900ff13#0
scm2_1   | 2023-07-19 07:37:32,631 [IPC Server handler 78 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,322 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4 ELECTION round 0: result PASSED
scm3_1   | 2023-07-19 07:39:01,222 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | org.apache.ratis.protocol.exceptions.GroupMismatchException: f02af6ab-c12d-469b-a775-f6b30900ff13: group-03BEFB15C8DE not found.
dn3_1    | 2023-07-19 07:38:28,322 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4
scm1_1   | 2023-07-19 07:40:32,671 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy$ImplMap.get(RaftServerProxy.java:152)
scm2_1   | 2023-07-19 07:37:32,633 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=63a4529d-d8b2-46dd-93ed-b30fa8b5baef is not found
dn3_1    | 2023-07-19 07:38:28,323 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm3_1   | 2023-07-19 07:39:01,222 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:39:06,289 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:06,289 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImplFuture(RaftServerProxy.java:358)
scm2_1   | 2023-07-19 07:37:32,655 [IPC Server handler 94 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,323 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-075378E90876 with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
scm3_1   | 2023-07-19 07:39:11,413 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:11,413 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:39:16,545 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:367)
scm2_1   | 2023-07-19 07:37:32,656 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
dn3_1    | 2023-07-19 07:38:28,323 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 1 for becomeLeader, leader elected after 5398ms
scm3_1   | 2023-07-19 07:39:16,545 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:39:21,681 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:21,681 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.getImpl(RaftServerProxy.java:362)
scm2_1   | 2023-07-19 07:37:32,700 [IPC Server handler 93 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,344 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 	at org.apache.ratis.server.impl.RaftServerProxy.requestVote(RaftServerProxy.java:625)
scm2_1   | 2023-07-19 07:37:32,730 [IPC Server handler 74 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn3_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,347 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-07-19 07:40:32,677 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1002
scm1_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 	at org.apache.ratis.grpc.server.GrpcServerProtocolService.requestVote(GrpcServerProtocolService.java:190)
dn3_1    | 2023-07-19 07:38:28,347 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
scm2_1   | 2023-07-19 07:37:32,744 [FixedThreadPoolWithAffinityExecutor-1-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to QUASI_CLOSED state, datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) reported QUASI_CLOSED replica.
dn5_1    | 	at org.apache.ratis.proto.grpc.RaftServerProtocolServiceGrpc$MethodHandlers.invoke(RaftServerProtocolServiceGrpc.java:451)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn3_1    | 2023-07-19 07:38:28,355 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm3_1   | 2023-07-19 07:39:26,682 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:26,683 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn3_1    | 2023-07-19 07:38:28,355 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-07-19 07:38:28,355 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-07-19 07:39:31,648 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm2_1   | 2023-07-19 07:37:32,943 [IPC Server handler 85 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm1_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn3_1    | 2023-07-19 07:38:28,355 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-07-19 07:39:31,874 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:37:32,973 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)
dn3_1    | 2023-07-19 07:38:28,361 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-07-19 07:38:28,363 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm3_1   | 2023-07-19 07:39:31,875 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:355)
scm2_1   | 2023-07-19 07:37:32,992 [IPC Server handler 84 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | 2023-07-19 07:38:28,363 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:28,363 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm1_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:867)
scm2_1   | 2023-07-19 07:37:33,028 [IPC Server handler 60 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn5_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:39:36,925 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:36,925 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
scm2_1   | 2023-07-19 07:37:34,152 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-07-19 07:39:39,111 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
scm3_1   | 2023-07-19 07:39:39,378 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-07-19 07:38:28,369 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-07-19 07:38:28,369 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-07-19 07:38:28,388 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:37:39,152 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-07-19 07:39:42,078 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:39:42,079 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:39:47,275 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm1_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm1_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
scm1_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm1_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm1_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm1_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm1_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 36a933ad-cfeb-4d3b-aa37-2c29c320331e@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:37:44,153 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:38:28,388 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn3_1    | 2023-07-19 07:38:28,389 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm1_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm1_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:28,389 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:38:00,509 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
scm2_1   | 2023-07-19 07:37:46,888 [IPC Server handler 66 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn1_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:49,154 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn3_1    | 2023-07-19 07:38:28,389 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-07-19 07:38:28,390 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-07-19 07:38:00,509 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Finalize Upgrade called!
scm2_1   | 2023-07-19 07:37:49,175 [RatisPipelineUtilsThread - 0] ERROR pipeline.PipelinePlacementPolicy: No healthy node found to allocate container.
scm2_1   | 2023-07-19 07:37:51,586 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18) moved to HEALTHY state.
dn3_1    | 2023-07-19 07:38:28,390 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:28,390 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
scm2_1   | 2023-07-19 07:37:51,586 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn3_1    | 2023-07-19 07:38:28,391 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 2023-07-19 07:38:00,509 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization started.
dn5_1    | 2023-07-19 07:38:00,510 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS.
dn3_1    | 2023-07-19 07:38:28,391 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 	... 3 more
dn5_1    | 2023-07-19 07:38:00,512 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature RATIS_DATASTREAM_PORT_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-07-19 07:38:28,391 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm2_1   | 2023-07-19 07:37:51,591 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3937008a-fbf0-499b-90a2-14d0cfe0e993 to datanode:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm3_1   | 2023-07-19 07:39:47,275 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:00,512 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: WEBUI_PORTS_IN_DATANODEDETAILS.
dn5_1    | 2023-07-19 07:38:00,513 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature WEBUI_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-07-19 07:38:28,392 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
scm2_1   | 2023-07-19 07:37:51,632 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 3937008a-fbf0-499b-90a2-14d0cfe0e993, Nodes: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:37:51.590987Z[UTC]]
scm3_1   | 2023-07-19 07:39:52,284 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:00,513 [Command processor thread] INFO upgrade.UpgradeFinalizer: No onFinalize work defined for feature: HADOOP_PRC_PORTS_IN_DATANODEDETAILS.
dn3_1    | 2023-07-19 07:38:28,393 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
scm2_1   | 2023-07-19 07:37:51,632 [RatisPipelineUtilsThread - 0] ERROR scm.SCMCommonPlacementPolicy: Unable to find enough nodes that meet the space requirement of 1073741824 bytes for metadata and 1073741824 bytes for data in healthy node set. Required 3. Found 1.
scm3_1   | 2023-07-19 07:39:52,284 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:00,514 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Layout feature HADOOP_PRC_PORTS_IN_DATANODEDETAILS has been finalized.
dn3_1    | 2023-07-19 07:38:28,393 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-07-19 07:37:52,345 [IPC Server handler 55 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-07-19 07:38:00,514 [Command processor thread] INFO upgrade.AbstractLayoutVersionManager: Finalization is complete.
scm3_1   | 2023-07-19 07:39:57,287 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-07-19 07:38:28,393 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-07-19 07:38:00,514 [Command processor thread] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm3_1   | 2023-07-19 07:39:57,288 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:38:28,393 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderStateImpl
scm2_1   | 2023-07-19 07:37:52,347 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732 is not found
dn5_1    | 2023-07-19 07:38:00,514 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:28,395 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-07-19 07:40:02,405 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:40:02,406 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-07-19 07:37:52,347 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=e6765bce-cd48-45ed-b1c7-20996759f8fd is not found
dn5_1    | 2023-07-19 07:38:00,514 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:28,400 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/current/log_inprogress_0
scm2_1   | 2023-07-19 07:37:52,348 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Reported pipeline PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de is not found
scm3_1   | 2023-07-19 07:40:07,570 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:00,515 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:28,426 [140718b2-320a-4020-b7ea-662533776c74@group-075378E90876-LeaderElection4] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-075378E90876: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:37:53,418 [IPC Server handler 54 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm2_1   | 2023-07-19 07:37:53,476 [IPC Server handler 0 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-07-19 07:38:00,515 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:29,075 [grpc-default-executor-6] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: receive requestVote(PRE_VOTE, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, group-649965091133, 0, (t:0, i:0))
scm2_1   | 2023-07-19 07:37:53,510 [IPC Server handler 56 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:40:07,570 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:00,515 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:29,076 [grpc-default-executor-6] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FOLLOWER: accept PRE_VOTE from 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: our priority 0 <= candidate's priority 0
scm2_1   | 2023-07-19 07:37:53,532 [IPC Server handler 53 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:40:09,105 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-07-19 07:38:00,515 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:29,076 [grpc-default-executor-6] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133 replies to PRE_VOTE vote request: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t0. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133:t0, leader=null, voted=, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-07-19 07:37:54,154 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-07-19 07:40:09,391 [EventQueue-DeleteBlockStatusForDeletedBlockLogImpl] WARN block.DeletedBlockLogImpl: Skip commit transactions since current SCM is not leader.
dn5_1    | 2023-07-19 07:38:04,015 [Command processor thread] INFO commandhandler.FinalizeNewLayoutVersionCommandHandler: Processing FinalizeNewLayoutVersionCommandHandler command.
dn3_1    | 2023-07-19 07:38:29,848 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] WARN impl.FollowerState: Unexpected long sleep: sleep 5184ms but took extra 543848367ns (> threshold = 300ms)
scm2_1   | 2023-07-19 07:37:54,928 [IPC Server handler 85 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:40:12,685 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:23,355 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:38:29,860 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:37:54,954 [IPC Server handler 83 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
scm3_1   | 2023-07-19 07:40:12,686 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:40:17,259 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
scm3_1   | 2023-07-19 07:40:17,337 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm2_1   | 2023-07-19 07:37:54,973 [IPC Server handler 84 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:38:29,849 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-140718b2-320a-4020-b7ea-662533776c74: Detected pause in JVM or host machine approximately 0.636s with 0.694s GC time.
scm3_1   | 2023-07-19 07:40:17,795 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-07-19 07:37:54,992 [IPC Server handler 81 on default port 9861] WARN node.SCMNodeManager: Data node ha_dn4_1.ha_net can not be used in any pipeline in the cluster. DataNode MetadataLayoutVersion = 4, SCM MetadataLayoutVersion = 7
dn3_1    | GC pool 'ParNew' had collection(s): count=1 time=694ms
scm3_1   | 2023-07-19 07:40:17,795 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:38:29,891 [grpc-default-executor-6] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: receive requestVote(ELECTION, f02af6ab-c12d-469b-a775-f6b30900ff13, group-649965091133, 1, (t:0, i:0))
scm3_1   | 2023-07-19 07:40:22,983 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-07-19 07:37:59,154 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm3_1   | 2023-07-19 07:40:22,983 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:38:03,588 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) moved to HEALTHY state.
scm2_1   | 2023-07-19 07:38:03,588 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-07-19 07:40:27,994 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-07-19 07:38:29,892 [grpc-default-executor-6] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FOLLOWER: accept ELECTION from f02af6ab-c12d-469b-a775-f6b30900ff13: our priority 0 <= candidate's priority 1
dn3_1    | 2023-07-19 07:38:29,893 [grpc-default-executor-6] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:f02af6ab-c12d-469b-a775-f6b30900ff13
scm3_1   | 2023-07-19 07:40:27,994 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-07-19 07:38:03,588 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) moved to HEALTHY state.
scm2_1   | 2023-07-19 07:38:03,588 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm3_1   | 2023-07-19 07:40:32,529 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-07-19 07:38:03,589 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b29414e4-15a3-4023-afdc-f99861290493 to datanode:f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 2023-07-19 07:38:29,893 [grpc-default-executor-6] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState
scm3_1   | 2023-07-19 07:40:32,532 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:38:03,601 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: b29414e4-15a3-4023-afdc-f99861290493, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.589540Z[UTC]]
dn3_1    | 2023-07-19 07:38:29,861 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:23,356 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm2_1   | 2023-07-19 07:38:03,603 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=502632ea-08d6-4150-a7b0-075378e90876 to datanode:140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:38:29,894 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState was interrupted
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:38:03,603 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=502632ea-08d6-4150-a7b0-075378e90876 to datanode:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
dn3_1    | 2023-07-19 07:38:29,894 [grpc-default-executor-6] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FollowerState
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-07-19 07:38:03,604 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=502632ea-08d6-4150-a7b0-075378e90876 to datanode:f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-07-19 07:38:29,884 [grpc-default-executor-7] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: receive requestVote(PRE_VOTE, f02af6ab-c12d-469b-a775-f6b30900ff13, group-649965091133, 0, (t:0, i:0))
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-07-19 07:38:03,624 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 502632ea-08d6-4150-a7b0-075378e90876, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18)f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.603749Z[UTC]]
dn3_1    | 2023-07-19 07:38:29,962 [grpc-default-executor-6] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133 replies to ELECTION vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:OK-t1. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133:t1, leader=null, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-07-19 07:38:03,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9a97a9af-3060-4a66-97a0-7bbd49954ef8 to datanode:140718b2-320a-4020-b7ea-662533776c74
dn3_1    | 2023-07-19 07:38:29,965 [140718b2-320a-4020-b7ea-662533776c74-server-thread2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-649965091133 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:38:23,358 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
dn3_1    | 2023-07-19 07:38:29,965 [140718b2-320a-4020-b7ea-662533776c74-server-thread2] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 1 for appendEntries, leader elected after 5893ms
dn5_1    | 2023-07-19 07:38:23,359 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:38:23,360 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
dn5_1    | 2023-07-19 07:38:23,360 [grpc-default-executor-6] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-075378E90876:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-075378E90876:java.util.concurrent.CompletableFuture@1fbc5725[Not completed]
scm2_1   | 2023-07-19 07:38:03,645 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9a97a9af-3060-4a66-97a0-7bbd49954ef8, Nodes: 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.625484Z[UTC]]
dn3_1    | 2023-07-19 07:38:29,972 [grpc-default-executor-7] INFO impl.VoteContext: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-FOLLOWER: reject PRE_VOTE from f02af6ab-c12d-469b-a775-f6b30900ff13: this server is a follower and still has a valid leader f02af6ab-c12d-469b-a775-f6b30900ff13
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
dn5_1    | 2023-07-19 07:38:23,365 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-07-19 07:38:03,648 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ca86faf-1894-4e8b-baec-649965091133 to datanode:f02af6ab-c12d-469b-a775-f6b30900ff13
dn3_1    | 2023-07-19 07:38:29,981 [grpc-default-executor-7] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133 replies to PRE_VOTE vote request: f02af6ab-c12d-469b-a775-f6b30900ff13<-140718b2-320a-4020-b7ea-662533776c74#0:FAIL-t1. Peer's state: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133:t1, leader=f02af6ab-c12d-469b-a775-f6b30900ff13, voted=f02af6ab-c12d-469b-a775-f6b30900ff13, raftlog=Memoized:140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:38:29,993 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:23,365 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-07-19 07:38:03,648 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ca86faf-1894-4e8b-baec-649965091133 to datanode:140718b2-320a-4020-b7ea-662533776c74
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn3_1    | 2023-07-19 07:38:29,993 [140718b2-320a-4020-b7ea-662533776c74-server-thread1] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-07-19 07:38:23,365 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-07-19 07:38:23,366 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
dn3_1    | 2023-07-19 07:38:30,004 [140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-649965091133-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/current/log_inprogress_0
dn5_1    | 2023-07-19 07:38:23,366 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-07-19 07:38:03,648 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ca86faf-1894-4e8b-baec-649965091133 to datanode:8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 2023-07-19 07:38:33,782 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 140718b2-320a-4020-b7ea-662533776c74: addNew group-7BBD49954EF8:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-7BBD49954EF8:java.util.concurrent.CompletableFuture@76ad26a0[Not completed]
dn3_1    | 2023-07-19 07:38:33,785 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74: new RaftServerImpl for group-7BBD49954EF8:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm2_1   | 2023-07-19 07:38:03,668 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=9ca86faf-1894-4e8b-baec-649965091133 contains same datanodes as previous pipelines: PipelineID=502632ea-08d6-4150-a7b0-075378e90876 nodeIds: f02af6ab-c12d-469b-a775-f6b30900ff13, 140718b2-320a-4020-b7ea-662533776c74, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 2023-07-19 07:38:33,785 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-07-19 07:38:23,366 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 does not exist. Creating ...
dn5_1    | 2023-07-19 07:38:23,368 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/in_use.lock acquired by nodename 7@aff5372d1efd
dn3_1    | 2023-07-19 07:38:33,785 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-07-19 07:38:03,668 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9ca86faf-1894-4e8b-baec-649965091133, Nodes: f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21)140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19)8518cd71-5a5f-48df-96c1-a0a7caa9b1ef(ha_dn2_1.ha_net/10.9.0.18), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:03.648163Z[UTC]]
scm2_1   | 2023-07-19 07:38:03,686 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm2_1   | 2023-07-19 07:38:04,155 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm2_1   | 2023-07-19 07:38:09,155 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm2_1   | 2023-07-19 07:38:14,155 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
dn5_1    | 2023-07-19 07:38:23,371 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876 has been successfully formatted.
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 2023-07-19 07:38:33,785 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-07-19 07:38:18,589 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17) moved to HEALTHY state.
dn5_1    | 2023-07-19 07:38:23,378 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO ratis.ContainerStateMachine: group-075378E90876: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:38:33,791 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:38:18,589 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-07-19 07:38:23,379 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-07-19 07:38:33,791 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:38:23,386 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-07-19 07:38:18,591 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=301e375a-65de-4355-b5a7-08992b8d070f to datanode:a9b83a7e-59b8-4455-b30a-c01eee264fbd
scm2_1   | 2023-07-19 07:38:18,603 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 301e375a-65de-4355-b5a7-08992b8d070f, Nodes: a9b83a7e-59b8-4455-b30a-c01eee264fbd(ha_dn1_1.ha_net/10.9.0.17), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:18.591616Z[UTC]]
dn5_1    | 2023-07-19 07:38:23,392 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn3_1    | 2023-07-19 07:38:33,791 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-19 07:38:33,791 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: ConfigurationManager, init=-1: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-07-19 07:38:18,606 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
dn3_1    | 2023-07-19 07:38:33,793 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-07-19 07:38:23,405 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn5_1    | 2023-07-19 07:38:23,405 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-07-19 07:38:19,156 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm2_1   | 2023-07-19 07:38:21,975 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=3937008a-fbf0-499b-90a2-14d0cfe0e993
dn5_1    | 2023-07-19 07:38:23,405 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:23,408 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:38:23,408 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:38:24,156 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Waiting for at least one open Ratis 3 pipeline after SCM finalization.
scm2_1   | 2023-07-19 07:38:27,590 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO node.ReadOnlyHealthyToHealthyNodeHandler: Datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) moved to HEALTHY state.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-07-19 07:38:23,409 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:23,410 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-07-19 07:38:27,590 [EventQueue-HealthyReadonlyToHealthyNodeForReadOnlyHealthyToHealthyNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
dn5_1    | 2023-07-19 07:38:23,410 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-07-19 07:38:33,795 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2_1   | 2023-07-19 07:38:27,593 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3b24ef8c-1b15-473e-825d-aaddeaeb0eb6 to datanode:adc6845d-6cb4-4b43-88ca-47ca3f1a71df
dn5_1    | 2023-07-19 07:38:23,410 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:38:33,800 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm2_1   | 2023-07-19 07:38:27,606 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 3b24ef8c-1b15-473e-825d-aaddeaeb0eb6, Nodes: adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-07-19T07:38:27.593370Z[UTC]]
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:38:23,411 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | 2023-07-19 07:38:27,608 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm2_1   | 2023-07-19 07:38:28,365 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=502632ea-08d6-4150-a7b0-075378e90876
scm2_1   | 2023-07-19 07:38:29,157 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Open pipeline found after SCM finalization
scm2_1   | 2023-07-19 07:38:29,192 [IPC Server handler 81 on default port 9860] INFO upgrade.UpgradeFinalizer: Finalization is done.
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	... 3 more
scm3_1   | 2023-07-19 07:40:32,567 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-07-19 07:38:29,476 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9ca86faf-1894-4e8b-baec-649965091133
scm3_1   | 2023-07-19 07:40:32,632 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 2
scm2_1   | 2023-07-19 07:38:32,765 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 15 blocks to be deleted for 3 datanodes, task elapsed time: 5ms
dn5_1    | 2023-07-19 07:38:23,411 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-07-19 07:38:23,412 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-07-19 07:38:33,838 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9a97a9af-3060-4a66-97a0-7bbd49954ef8
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm2_1   | 2023-07-19 07:38:34,141 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=b29414e4-15a3-4023-afdc-f99861290493
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn5_1    | 2023-07-19 07:38:23,413 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-07-19 07:38:23,415 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-07-19 07:38:48,001 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=301e375a-65de-4355-b5a7-08992b8d070f
dn5_1    | 2023-07-19 07:38:23,415 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
scm2_1   | 2023-07-19 07:38:56,185 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=3b24ef8c-1b15-473e-825d-aaddeaeb0eb6
dn5_1    | 2023-07-19 07:38:23,416 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
scm2_1   | 2023-07-19 07:38:57,615 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-07-19 07:38:23,423 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:38:33,801 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9a97a9af-3060-4a66-97a0-7bbd49954ef8 does not exist. Creating ...
scm2_1   | 2023-07-19 07:39:06,967 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn3_1    | 2023-07-19 07:38:33,808 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9a97a9af-3060-4a66-97a0-7bbd49954ef8/in_use.lock acquired by nodename 7@69ed6f01c113
dn3_1    | 2023-07-19 07:38:33,811 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9a97a9af-3060-4a66-97a0-7bbd49954ef8 has been successfully formatted.
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO ratis.ContainerStateMachine: group-7BBD49954EF8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-07-19 07:39:27,618 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn5_1    | 2023-07-19 07:38:23,487 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:38:23,487 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:38:23,487 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
scm2_1   | 2023-07-19 07:39:32,761 [SCMBlockDeletingService#0] INFO block.SCMBlockDeletingService: Totally added 10 blocks to be deleted for 2 datanodes, task elapsed time: 1ms
dn5_1    | 2023-07-19 07:38:23,488 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:38:23,488 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-07-19 07:38:23,489 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
scm2_1   | 2023-07-19 07:39:32,909 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:23,489 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:38:23,489 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm2_1   | 2023-07-19 07:39:50,096 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn3_1    | 2023-07-19 07:38:33,833 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
scm2_1   | 2023-07-19 07:39:57,620 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn3_1    | 2023-07-19 07:38:33,859 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:23,495 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:23,496 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:38:23,502 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-075378E90876,id=f02af6ab-c12d-469b-a775-f6b30900ff13
scm2_1   | 2023-07-19 07:40:17,252 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for containerId, expected lastId is 0, actual lastId is 2000.
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:23,502 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-07-19 07:40:17,273 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 2000 to 3000.
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9a97a9af-3060-4a66-97a0-7bbd49954ef8
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm2_1   | 2023-07-19 07:40:17,323 [b28076e1-4ec3-4254-8902-2272d74360c6@group-05548E1F8073-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019203000.
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 2023-07-19 07:38:23,502 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:38:23,502 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-07-19 07:38:23,502 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 2023-07-19 07:38:23,970 [grpc-default-executor-6] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] returns group-649965091133:java.util.concurrent.CompletableFuture@6e89cd27[Not completed]
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm2_1   | 2023-07-19 07:40:17,340 [IPC Server handler 0 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019203000 to 111677748019204000.
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-649965091133:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-07-19 07:40:25,379 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1 with BCSID 18, which is in QUASI_CLOSED state.
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.395171Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om2} to adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1689752995382 and scm deadline 1689753025382
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.395171Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om2} to f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1689752995394 and scm deadline 1689753025394
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn3_1    | 2023-07-19 07:38:33,860 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-19 07:38:33,862 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.395171Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om2} to 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1689752995394 and scm deadline 1689753025394
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
dn3_1    | 2023-07-19 07:38:33,862 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:33,958 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn3_1    | 2023-07-19 07:38:33,958 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #2 with BCSID 45, which is in QUASI_CLOSED state.
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 2023-07-19 07:38:33,958 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-19 07:38:33,958 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.769385Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om2} to 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1689752995394 and scm deadline 1689753025394
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn5_1    | 2023-07-19 07:38:23,973 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-07-19 07:38:33,958 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 	... 3 more
dn5_1    | 2023-07-19 07:38:23,974 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-07-19 07:40:25,394 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.769385Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om2} to f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1689752995394 and scm deadline 1689753025394
dn3_1    | 2023-07-19 07:38:33,986 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: start as a follower, conf=-1: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:40:32,641 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn5_1    | 2023-07-19 07:38:23,974 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm2_1   | 2023-07-19 07:40:25,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 2, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#2, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.769385Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om2} to adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1689752995395 and scm deadline 1689753025395
dn3_1    | 2023-07-19 07:38:33,986 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm3_1   | 2023-07-19 07:40:32,668 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1001
dn5_1    | 2023-07-19 07:38:23,974 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-07-19 07:38:33,987 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn5_1    | 2023-07-19 07:38:23,974 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-07-19 07:40:25,395 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1001 with BCSID 28, which is in QUASI_CLOSED state.
dn3_1    | 2023-07-19 07:38:34,001 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7BBD49954EF8,id=140718b2-320a-4020-b7ea-662533776c74
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
dn5_1    | 2023-07-19 07:38:23,974 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm2_1   | 2023-07-19 07:40:25,395 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.670595Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om3} to f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1689752995395 and scm deadline 1689753025395
dn3_1    | 2023-07-19 07:38:34,001 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn5_1    | 2023-07-19 07:38:23,975 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm2_1   | 2023-07-19 07:40:25,396 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.670595Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om3} to 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1689752995395 and scm deadline 1689753025395
dn3_1    | 2023-07-19 07:38:34,001 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn5_1    | 2023-07-19 07:38:23,975 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-07-19 07:40:25,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1001, pipelineID: PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, force: true] for container ContainerInfo{id=#1001, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:32.670595Z, pipelineID=PipelineID=d2552f1d-da74-434f-bb3d-03befb15c8de, owner=om3} to adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1689752995396 and scm deadline 1689753025396
dn3_1    | 2023-07-19 07:38:34,001 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
dn5_1    | 2023-07-19 07:38:23,975 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-07-19 07:40:25,398 [ReplicationMonitor] INFO health.QuasiClosedContainerHandler: Force closing container #1002 with BCSID 9, which is in QUASI_CLOSED state.
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
dn5_1    | 2023-07-19 07:38:23,975 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm2_1   | 2023-07-19 07:40:25,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.456078Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om3} to adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) with datanode deadline 1689752995398 and scm deadline 1689753025398
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
scm2_1   | 2023-07-19 07:40:25,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.456078Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om3} to 140718b2-320a-4020-b7ea-662533776c74(ha_dn3_1.ha_net/10.9.0.19) with datanode deadline 1689752995398 and scm deadline 1689753025398
dn3_1    | 2023-07-19 07:38:34,001 [140718b2-320a-4020-b7ea-662533776c74-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-07-19 07:40:25,398 [ReplicationMonitor] INFO replication.ReplicationManager: Sending command [closeContainerCommand: containerID: 1002, pipelineID: PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, force: true] for container ContainerInfo{id=#1002, state=QUASI_CLOSED, stateEnterTime=2023-07-19T07:37:31.456078Z, pipelineID=PipelineID=6d5697fa-1223-40ff-ba6c-8563b54dd732, owner=om3} to f02af6ab-c12d-469b-a775-f6b30900ff13(ha_dn5_1.ha_net/10.9.0.21) with datanode deadline 1689752995398 and scm deadline 1689753025398
dn3_1    | 2023-07-19 07:38:34,003 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-07-19 07:40:25,402 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 44 milliseconds for processing 5 containers.
dn3_1    | 2023-07-19 07:38:34,003 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-07-19 07:40:27,622 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-07-19 07:38:34,010 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9a97a9af-3060-4a66-97a0-7bbd49954ef8
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 2023-07-19 07:40:32,539 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn3_1    | 2023-07-19 07:38:34,011 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9a97a9af-3060-4a66-97a0-7bbd49954ef8.
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm2_1   | 2023-07-19 07:40:32,569 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #2 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn3_1    | 2023-07-19 07:38:39,061 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO impl.FollowerState: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073807759ns, electionTimeout:5058ms
dn5_1    | 2023-07-19 07:38:23,976 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 does not exist. Creating ...
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-07-19 07:40:32,619 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1001 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
dn3_1    | 2023-07-19 07:38:39,062 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState
dn5_1    | 2023-07-19 07:38:23,978 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/in_use.lock acquired by nodename 7@aff5372d1efd
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | 2023-07-19 07:38:39,062 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-07-19 07:38:23,982 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133 has been successfully formatted.
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn3_1    | 2023-07-19 07:38:39,062 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm2_1   | 2023-07-19 07:40:32,666 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm2_1   | 2023-07-19 07:40:51,466 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn3_1    | 2023-07-19 07:38:39,062 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-FollowerState] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5
dn3_1    | 2023-07-19 07:38:39,063 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
scm2_1   | 2023-07-19 07:40:57,625 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm2_1   | 2023-07-19 07:41:27,627 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn3_1    | 2023-07-19 07:38:39,063 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn3_1    | 2023-07-19 07:38:39,066 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:38:39,066 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.LeaderElection: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5 ELECTION round 0: result PASSED (term=1)
scm2_1   | 2023-07-19 07:41:57,628 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm2_1   | 2023-07-19 07:42:27,629 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn3_1    | 2023-07-19 07:38:39,067 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: shutdown 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5
dn3_1    | 2023-07-19 07:38:39,067 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn5_1    | 2023-07-19 07:38:24,003 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO ratis.ContainerStateMachine: group-649965091133: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-07-19 07:38:39,067 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7BBD49954EF8 with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
scm2_1   | 2023-07-19 07:42:29,296 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn3_1    | 2023-07-19 07:38:39,070 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 1 for becomeLeader, leader elected after 5271ms
dn5_1    | 2023-07-19 07:38:24,004 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 	... 3 more
scm2_1   | 2023-07-19 07:42:37,464 [IPC Server handler 71 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn3_1    | 2023-07-19 07:38:39,071 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 2023-07-19 07:40:32,679 [FixedThreadPoolWithAffinityExecutor-9-0] INFO container.IncrementalContainerReportHandler: Moving container #1002 to CLOSED state, datanode adc6845d-6cb4-4b43-88ca-47ca3f1a71df(ha_dn4_1.ha_net/10.9.0.20) reported CLOSED replica.
scm3_1   | 2023-07-19 07:40:32,680 [FixedThreadPoolWithAffinityExecutor-9-0] ERROR container.IncrementalContainerReportHandler: Exception while processing ICR for container 1002
dn5_1    | 2023-07-19 07:38:24,005 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:38:24,005 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | org.apache.hadoop.hdds.scm.exceptions.SCMException: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:38:39,072 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.translateException(SCMHAInvocationHandler.java:170)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invokeRatis(SCMHAInvocationHandler.java:108)
dn3_1    | 2023-07-19 07:38:39,072 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-07-19 07:38:39,073 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-07-19 07:38:39,075 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm2_1   | 2023-07-19 07:42:45,644 [IPC Server handler 89 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 10.9.0.14
dn5_1    | 2023-07-19 07:38:24,005 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-07-19 07:38:39,076 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.ha.SCMHAInvocationHandler.invoke(SCMHAInvocationHandler.java:76)
dn3_1    | 2023-07-19 07:38:39,077 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 	at com.sun.proxy.$Proxy18.updateContainerState(Unknown Source)
dn5_1    | 2023-07-19 07:38:24,005 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-07-19 07:42:57,631 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
dn3_1    | 2023-07-19 07:38:39,078 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.ContainerManagerImpl.updateContainerState(ContainerManagerImpl.java:276)
dn5_1    | 2023-07-19 07:38:24,006 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:24,006 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-07-19 07:38:39,078 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO impl.RoleInfo: 140718b2-320a-4020-b7ea-662533776c74: start 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderStateImpl
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.updateContainerState(AbstractContainerReportHandler.java:340)
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:38:39,078 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-07-19 07:38:39,080 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9a97a9af-3060-4a66-97a0-7bbd49954ef8/current/log_inprogress_0
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.AbstractContainerReportHandler.processContainerReplica(AbstractContainerReportHandler.java:120)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:100)
scm3_1   | 	at org.apache.hadoop.hdds.scm.container.IncrementalContainerReportHandler.onMessage(IncrementalContainerReportHandler.java:43)
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133
scm3_1   | 	at org.apache.hadoop.hdds.server.events.FixedThreadPoolWithAffinityExecutor$ContainerReportProcessTask.run(FixedThreadPoolWithAffinityExecutor.java:276)
dn3_1    | 2023-07-19 07:38:39,098 [140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8-LeaderElection5] INFO server.RaftServer$Division: 140718b2-320a-4020-b7ea-662533776c74@group-7BBD49954EF8: set configuration 0: peers:[140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 2023-07-19 07:38:48,358 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm3_1   | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 2023-07-19 07:39:00,111 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2a50b32d] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),1001(0)], numOfContainers=3, numOfBlocks=5
dn3_1    | 2023-07-19 07:39:40,156 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2a50b32d] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),1001(1)], numOfContainers=3, numOfBlocks=5
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-07-19 07:39:40,158 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 2023-07-19 07:39:40,165 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1001 is either received out of order or retried, 1001 <= 1001
scm3_1   | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-07-19 07:39:40,165 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 2
scm3_1   | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | 2023-07-19 07:38:24,007 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-07-19 07:39:48,358 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
scm3_1   | Caused by: org.apache.ratis.protocol.exceptions.NotLeaderException: Server 57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073 is not the leader b28076e1-4ec3-4254-8902-2272d74360c6|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER
dn3_1    | 2023-07-19 07:40:40,071 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.generateNotLeaderException(RaftServerImpl.java:744)
dn5_1    | 2023-07-19 07:38:24,009 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-07-19 07:40:40,072 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.checkLeaderState(RaftServerImpl.java:709)
dn5_1    | 2023-07-19 07:38:24,010 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-07-19 07:40:40,076 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.submitClientRequestAsync(RaftServerImpl.java:850)
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$null$12(RaftServerImpl.java:831)
dn5_1    | 2023-07-19 07:38:24,010 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-07-19 07:40:40,077 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
scm3_1   | 	at org.apache.ratis.util.JavaUtils.callAsUnchecked(JavaUtils.java:117)
dn5_1    | 2023-07-19 07:38:24,010 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-07-19 07:40:40,077 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
scm3_1   | 	at org.apache.ratis.server.impl.RaftServerImpl.lambda$executeSubmitClientRequestAsync$13(RaftServerImpl.java:831)
dn5_1    | 2023-07-19 07:38:24,011 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-07-19 07:40:40,091 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 45.
scm3_1   | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
dn5_1    | 2023-07-19 07:38:24,013 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-07-19 07:40:40,103 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
scm3_1   | 	... 3 more
dn5_1    | 2023-07-19 07:38:24,019 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-07-19 07:40:40,104 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn3_1    | 2023-07-19 07:40:40,111 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 28.
dn5_1    | 2023-07-19 07:38:24,019 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-07-19 07:40:40,124 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
scm3_1   | 2023-07-19 07:40:33,028 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:24,019 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-07-19 07:40:40,125 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
scm3_1   | 2023-07-19 07:40:33,028 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:24,021 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-19 07:40:40,134 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 9.
dn3_1    | 2023-07-19 07:40:48,360 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 10/4990 blocks from 3 candidate containers.
scm3_1   | 2023-07-19 07:40:38,044 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:24,021 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-07-19 07:40:48,383 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200002.block
scm3_1   | 2023-07-19 07:40:38,044 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:40:48,384 [BlockDeletingService#7] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/1/chunks/111677748019200001.block
scm3_1   | 2023-07-19 07:40:43,102 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:24,022 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-07-19 07:40:48,388 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200003.block
dn5_1    | 2023-07-19 07:38:24,022 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-07-19 07:40:48,392 [BlockDeletingService#6] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201001.block
dn3_1    | 2023-07-19 07:40:48,393 [BlockDeletingService#6] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201002.block
scm3_1   | 2023-07-19 07:40:43,102 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:41:48,369 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 5/4995 blocks from 3 candidate containers.
dn3_1    | 2023-07-19 07:41:48,369 [BlockDeletingService#5] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 2
dn5_1    | 2023-07-19 07:38:24,022 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState
dn3_1    | 2023-07-19 07:41:48,370 [BlockDeletingService#4] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 1
dn5_1    | 2023-07-19 07:38:24,023 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:24,024 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:40:48,254 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:24,024 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-649965091133,id=f02af6ab-c12d-469b-a775-f6b30900ff13
scm3_1   | 2023-07-19 07:40:48,254 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-07-19 07:41:48,370 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 1001 with pending delete block count 2
dn5_1    | 2023-07-19 07:38:24,024 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-07-19 07:40:53,337 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
dn3_1    | 2023-07-19 07:42:48,369 [BlockDeletingService#8] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:38:24,024 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-07-19 07:38:24,025 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-07-19 07:38:24,025 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-07-19 07:38:27,430 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: receive requestVote(PRE_VOTE, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, group-075378E90876, 0, (t:0, i:0))
scm3_1   | 2023-07-19 07:40:53,402 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:40:53,403 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:40:58,570 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:40:58,570 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:41:03,573 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:03,573 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:41:08,578 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:08,578 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:27,431 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FOLLOWER: accept PRE_VOTE from 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: our priority 0 <= candidate's priority 0
dn5_1    | 2023-07-19 07:38:27,431 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876 replies to PRE_VOTE vote request: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t0. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876:t0, leader=null, voted=, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:28,248 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: receive requestVote(PRE_VOTE, 140718b2-320a-4020-b7ea-662533776c74, group-075378E90876, 0, (t:0, i:0))
dn5_1    | 2023-07-19 07:38:28,248 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FOLLOWER: accept PRE_VOTE from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
scm3_1   | 2023-07-19 07:41:13,697 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:13,697 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:41:18,794 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:18,794 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:41:23,862 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:23,862 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:41:28,872 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:28,872 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:28,251 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876 replies to PRE_VOTE vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t0. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876:t0, leader=null, voted=, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:28,294 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: receive requestVote(ELECTION, 140718b2-320a-4020-b7ea-662533776c74, group-075378E90876, 1, (t:0, i:0))
dn5_1    | 2023-07-19 07:38:28,295 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FOLLOWER: accept ELECTION from 140718b2-320a-4020-b7ea-662533776c74: our priority 0 <= candidate's priority 1
dn5_1    | 2023-07-19 07:38:28,295 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:140718b2-320a-4020-b7ea-662533776c74
dn5_1    | 2023-07-19 07:38:28,295 [grpc-default-executor-6] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState
dn5_1    | 2023-07-19 07:38:28,295 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState was interrupted
dn5_1    | 2023-07-19 07:38:28,302 [grpc-default-executor-6] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-FollowerState
scm3_1   | 2023-07-19 07:41:33,880 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:28,314 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876 replies to ELECTION vote request: 140718b2-320a-4020-b7ea-662533776c74<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:OK-t1. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876:t1, leader=null, voted=140718b2-320a-4020-b7ea-662533776c74, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:41:33,880 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:28,416 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-075378E90876 with new leaderId: 140718b2-320a-4020-b7ea-662533776c74
scm3_1   | 2023-07-19 07:41:38,969 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:28,416 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread1] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: change Leader from null to 140718b2-320a-4020-b7ea-662533776c74 at term 1 for appendEntries, leader elected after 5057ms
dn5_1    | 2023-07-19 07:38:28,432 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread2] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:41:38,969 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:28,433 [f02af6ab-c12d-469b-a775-f6b30900ff13-server-thread2] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-07-19 07:41:44,034 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:28,435 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-075378E90876-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/502632ea-08d6-4150-a7b0-075378e90876/current/log_inprogress_0
scm3_1   | 2023-07-19 07:41:44,035 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,041 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-f02af6ab-c12d-469b-a775-f6b30900ff13: Detected pause in JVM or host machine approximately 0.165s with 0.180s GC time.
scm3_1   | 2023-07-19 07:41:49,129 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | GC pool 'ParNew' had collection(s): count=1 time=180ms
scm3_1   | 2023-07-19 07:41:49,129 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,083 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: receive requestVote(PRE_VOTE, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef, group-649965091133, 0, (t:0, i:0))
scm3_1   | 2023-07-19 07:41:54,290 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:41:54,290 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,084 [grpc-default-executor-6] INFO impl.VoteContext: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FOLLOWER: reject PRE_VOTE from 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef: our priority 1 > candidate's priority 0
scm3_1   | 2023-07-19 07:41:59,355 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,084 [grpc-default-executor-6] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133 replies to PRE_VOTE vote request: 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef<-f02af6ab-c12d-469b-a775-f6b30900ff13#0:FAIL-t0. Peer's state: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133:t0, leader=null, voted=, raftlog=Memoized:f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:41:59,356 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,163 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5140315138ns, electionTimeout:5128ms
scm3_1   | 2023-07-19 07:42:04,415 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,163 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState
scm3_1   | 2023-07-19 07:42:04,415 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,167 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm3_1   | 2023-07-19 07:42:09,607 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,168 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm3_1   | 2023-07-19 07:42:09,608 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,170 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4
dn5_1    | 2023-07-19 07:38:29,178 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:42:14,670 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,217 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:42:14,671 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,217 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:42:19,862 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,219 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4-2] INFO server.GrpcServerProtocolClient: Build channel for 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef
scm3_1   | 2023-07-19 07:42:19,862 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,279 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-07-19 07:42:24,897 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:42:24,898 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,279 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection:   Response 0: f02af6ab-c12d-469b-a775-f6b30900ff13<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t0
scm3_1   | 2023-07-19 07:42:30,057 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,280 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4 PRE_VOTE round 0: result PASSED
dn5_1    | 2023-07-19 07:38:29,283 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-07-19 07:42:30,057 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:42:35,193 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,290 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-07-19 07:42:35,193 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:42:40,316 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,290 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:42:40,316 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,421 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
scm3_1   | 2023-07-19 07:42:45,427 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,421 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection:   Response 0: f02af6ab-c12d-469b-a775-f6b30900ff13<-8518cd71-5a5f-48df-96c1-a0a7caa9b1ef#0:OK-t1
scm3_1   | 2023-07-19 07:42:45,427 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,421 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4 ELECTION round 0: result PASSED
scm3_1   | 2023-07-19 07:42:50,542 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,421 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4
scm3_1   | 2023-07-19 07:42:50,542 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-07-19 07:42:55,582 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,422 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm3_1   | 2023-07-19 07:42:55,582 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,422 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-649965091133 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
scm3_1   | 2023-07-19 07:43:00,617 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:29,422 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 1 for becomeLeader, leader elected after 5447ms
dn5_1    | 2023-07-19 07:38:29,422 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-07-19 07:38:29,425 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm3_1   | 2023-07-19 07:43:00,617 [57eda1b0-9276-42ed-8a05-f1155bfae1c0@group-05548E1F8073-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:29,427 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-07-19 07:38:29,429 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-07-19 07:38:29,429 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-07-19 07:38:29,429 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-07-19 07:38:29,432 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:38:29,432 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-07-19 07:38:29,432 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-07-19 07:38:29,432 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:29,432 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-07-19 07:38:29,433 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-07-19 07:38:29,433 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-07-19 07:38:29,442 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:38:29,442 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-07-19 07:38:29,442 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-07-19 07:38:29,442 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:38:29,442 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-07-19 07:38:29,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn5_1    | 2023-07-19 07:38:29,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:29,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn5_1    | 2023-07-19 07:38:29,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn5_1    | 2023-07-19 07:38:29,472 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderStateImpl
dn5_1    | 2023-07-19 07:38:29,473 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-07-19 07:38:29,490 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ca86faf-1894-4e8b-baec-649965091133/current/log_inprogress_0
dn5_1    | 2023-07-19 07:38:29,551 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133-LeaderElection4] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-649965091133: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER, 140718b2-320a-4020-b7ea-662533776c74|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 8518cd71-5a5f-48df-96c1-a0a7caa9b1ef|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:10.9.0.18:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:34,087 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13: new RaftServerImpl for group-F99861290493:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: ConfigurationManager, init=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-07-19 07:38:34,088 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-07-19 07:38:34,089 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-07-19 07:38:34,089 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-07-19 07:38:34,089 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-07-19 07:38:34,090 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
dn5_1    | 2023-07-19 07:38:34,090 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-07-19 07:38:34,090 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-07-19 07:38:34,090 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-07-19 07:38:34,093 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-07-19 07:38:34,096 [PipelineCommandHandlerThread-0] INFO server.RaftServer: f02af6ab-c12d-469b-a775-f6b30900ff13: addNew group-F99861290493:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns      null f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn5_1    | 2023-07-19 07:38:34,097 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b29414e4-15a3-4023-afdc-f99861290493 does not exist. Creating ...
dn5_1    | 2023-07-19 07:38:34,099 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b29414e4-15a3-4023-afdc-f99861290493/in_use.lock acquired by nodename 7@aff5372d1efd
dn5_1    | 2023-07-19 07:38:34,103 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b29414e4-15a3-4023-afdc-f99861290493 has been successfully formatted.
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO ratis.ContainerStateMachine: group-F99861290493: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-07-19 07:38:34,104 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: new f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b29414e4-15a3-4023-afdc-f99861290493
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-07-19 07:38:34,105 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-07-19 07:38:34,124 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-07-19 07:38:34,161 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:38:34,161 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-07-19 07:38:34,161 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-07-19 07:38:34,161 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-07-19 07:38:34,161 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-07-19 07:38:34,173 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: start as a follower, conf=-1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:34,173 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-07-19 07:38:34,173 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState
dn5_1    | 2023-07-19 07:38:34,178 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F99861290493,id=f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:38:34,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-07-19 07:38:34,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-07-19 07:38:34,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-07-19 07:38:34,179 [f02af6ab-c12d-469b-a775-f6b30900ff13-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-07-19 07:38:34,180 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=b29414e4-15a3-4023-afdc-f99861290493
dn5_1    | 2023-07-19 07:38:34,182 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b29414e4-15a3-4023-afdc-f99861290493.
dn5_1    | 2023-07-19 07:38:34,183 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-07-19 07:38:34,183 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-07-19 07:38:39,326 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO impl.FollowerState: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5152759933ns, electionTimeout:5142ms
dn5_1    | 2023-07-19 07:38:39,326 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState
dn5_1    | 2023-07-19 07:38:39,326 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-07-19 07:38:39,326 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
dn5_1    | 2023-07-19 07:38:39,326 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-FollowerState] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5
dn5_1    | 2023-07-19 07:38:39,333 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:39,333 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5 PRE_VOTE round 0: result PASSED (term=0)
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5 ELECTION round 0: submit vote requests at term 1 for -1: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.LeaderElection: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: shutdown f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-F99861290493 with new leaderId: f02af6ab-c12d-469b-a775-f6b30900ff13
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: change Leader from null to f02af6ab-c12d-469b-a775-f6b30900ff13 at term 1 for becomeLeader, leader elected after 5246ms
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-07-19 07:38:39,336 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:38:39,340 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-07-19 07:38:39,345 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-07-19 07:38:39,345 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-07-19 07:38:39,345 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-07-19 07:38:39,345 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-07-19 07:38:39,349 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-07-19 07:38:39,349 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO impl.RoleInfo: f02af6ab-c12d-469b-a775-f6b30900ff13: start f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderStateImpl
dn5_1    | 2023-07-19 07:38:39,351 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-07-19 07:38:39,353 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b29414e4-15a3-4023-afdc-f99861290493/current/log_inprogress_0
dn5_1    | 2023-07-19 07:38:39,366 [f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493-LeaderElection5] INFO server.RaftServer$Division: f02af6ab-c12d-469b-a775-f6b30900ff13@group-F99861290493: set configuration 0: peers:[f02af6ab-c12d-469b-a775-f6b30900ff13|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-07-19 07:38:47,779 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:39:01,069 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2b80e5a9] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(0),2(0),1001(0)], numOfContainers=3, numOfBlocks=5
dn5_1    | 2023-07-19 07:39:41,120 [org.apache.hadoop.ozone.container.common.statemachine.commandhandler.DeleteBlocksCommandHandler$DeleteCmdWorker@2b80e5a9] INFO commandhandler.DeleteBlocksCommandHandler: Start to delete container blocks, TXIDs=[1(1),2(1),1001(1)], numOfContainers=3, numOfBlocks=5
dn5_1    | 2023-07-19 07:39:41,128 [DeleteBlocksCommandHandlerThread-2] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1001 is either received out of order or retried, 1001 <= 1001
dn5_1    | 2023-07-19 07:39:41,121 [DeleteBlocksCommandHandlerThread-3] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 1 is either received out of order or retried, 1 <= 1
dn5_1    | 2023-07-19 07:39:41,134 [DeleteBlocksCommandHandlerThread-4] INFO commandhandler.DeleteBlocksCommandHandler: Delete blocks for containerId: 2 is either received out of order or retried, 2 <= 2
dn5_1    | 2023-07-19 07:39:47,780 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
dn5_1    | 2023-07-19 07:40:40,339 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn5_1    | 2023-07-19 07:40:40,340 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is synced with bcsId 18.
dn5_1    | 2023-07-19 07:40:40,348 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1 is closed with bcsId 18.
dn5_1    | 2023-07-19 07:40:40,349 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn5_1    | 2023-07-19 07:40:40,349 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is synced with bcsId 45.
dn5_1    | 2023-07-19 07:40:40,375 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 2 is closed with bcsId 45.
dn5_1    | 2023-07-19 07:40:40,377 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn5_1    | 2023-07-19 07:40:40,378 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is synced with bcsId 28.
dn5_1    | 2023-07-19 07:40:40,410 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1001 is closed with bcsId 28.
dn5_1    | 2023-07-19 07:40:40,418 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn5_1    | 2023-07-19 07:40:40,430 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is synced with bcsId 9.
dn5_1    | 2023-07-19 07:40:40,444 [Command processor thread] INFO keyvalue.KeyValueContainer: Container 1002 is closed with bcsId 9.
dn5_1    | 2023-07-19 07:40:47,782 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 10/4990 blocks from 3 candidate containers.
dn5_1    | 2023-07-19 07:40:47,809 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200002.block
dn5_1    | 2023-07-19 07:40:47,810 [BlockDeletingService#3] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/2/chunks/111677748019200003.block
dn5_1    | 2023-07-19 07:40:47,811 [BlockDeletingService#4] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201001.block
dn5_1    | 2023-07-19 07:40:47,822 [BlockDeletingService#6] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir0/1/chunks/111677748019200001.block
dn5_1    | 2023-07-19 07:40:47,822 [BlockDeletingService#4] INFO impl.FilePerBlockStrategy: Deleted block file: /data/hdds/hdds/CID-305aa27f-3511-4772-a91d-05548e1f8073/current/containerDir1/1001/chunks/111677748019201002.block
dn5_1    | 2023-07-19 07:41:47,809 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 5/4995 blocks from 3 candidate containers.
dn5_1    | 2023-07-19 07:41:47,810 [BlockDeletingService#5] INFO background.BlockDeletingService: No transaction found in container 2 with pending delete block count 2
dn5_1    | 2023-07-19 07:41:47,811 [BlockDeletingService#2] INFO background.BlockDeletingService: No transaction found in container 1 with pending delete block count 1
dn5_1    | 2023-07-19 07:41:47,811 [BlockDeletingService#0] INFO background.BlockDeletingService: No transaction found in container 1001 with pending delete block count 2
dn5_1    | 2023-07-19 07:42:47,810 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
