Attaching to xcompat_old_client_1_0_0_1, xcompat_datanode_3, xcompat_datanode_2, xcompat_datanode_1, xcompat_old_client_1_1_0_1, xcompat_new_client_1, xcompat_recon_1, xcompat_old_client_1_2_1_1, xcompat_scm_1, xcompat_s3g_1, xcompat_om_1, xcompat_old_client_1_3_0_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:25:34,610 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = e4c3c8f1a551/172.19.0.8
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:25:34,697 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:25:34,955 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:25:35,513 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:25:36,335 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:25:36,335 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:25:37,059 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e4c3c8f1a551 ip:172.19.0.8
datanode_1          | 2023-06-29 21:25:37,951 [main] INFO reflections.Reflections: Reflections took 627 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_1          | 2023-06-29 21:25:40,702 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-29 21:25:40,984 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-29 21:25:42,067 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:25:42,186 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-29 21:25:42,206 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:25:42,207 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:25:42,460 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:25:42,517 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:25:42,532 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-29 21:25:42,543 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-29 21:25:42,551 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-29 21:25:42,552 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:25:42,704 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:25:42,720 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:25:50,442 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-29 21:25:50,779 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:25:50,997 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:25:51,942 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:25:51,944 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-29 21:25:51,961 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:25:51,961 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-29 21:25:51,999 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-29 21:25:52,000 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-29 21:25:52,000 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:25:52,001 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:25:52,038 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:25:52,039 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:25:52,104 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-29 21:25:52,121 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-29 21:25:52,121 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-29 21:25:53,892 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-29 21:25:53,905 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-29 21:25:53,906 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-29 21:25:53,911 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:25:53,911 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:25:53,940 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:25:54,306 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-29 21:25:54,674 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-29 21:25:55,301 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:25:55,395 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:25:55,617 [main] INFO util.log: Logging initialized @29633ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:25:56,109 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-29 21:25:56,173 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:25:56,233 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:25:56,239 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:25:56,243 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:25:56,243 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:25:56,468 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-29 21:25:56,471 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-29 21:25:56,491 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-29 21:25:56,644 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:25:56,644 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:25:56,649 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-29 21:25:56,721 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f9b5f01{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:25:56,730 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3180131e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:25:57,193 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c20841d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-2382031973498982409/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:25:57,257 [main] INFO server.AbstractConnector: Started ServerConnector@3f919230{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:25:57,257 [main] INFO server.Server: Started @31273ms
datanode_1          | 2023-06-29 21:25:57,263 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-29 21:25:57,263 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-29 21:25:57,272 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:25:57,441 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-29 21:25:57,587 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-06-29 21:25:57,621 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-29 21:25:58,599 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-29 21:25:58,599 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-29 21:25:58,600 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-29 21:25:58,635 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-29 21:25:58,651 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-29 21:25:59,112 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.7:9891
datanode_1          | 2023-06-29 21:25:59,352 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:26:01,914 [EndpointStateMachine task thread for recon/172.19.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.7:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:01,930 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:02,937 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:03,938 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:04,938 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:05,940 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:06,941 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:06,955 [EndpointStateMachine task thread for recon/172.19.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From e4c3c8f1a551/172.19.0.8 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:36912 remote=recon/172.19.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:36912 remote=recon/172.19.0.7:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-29 21:26:07,942 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:08,943 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:26:13,954 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From e4c3c8f1a551/172.19.0.8 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:36278 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.8:36278 remote=scm/172.19.0.5:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-29 21:26:15,296 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-c72a8840-c889-40f2-b9ef-3114e4382634/container.db to cache
datanode_1          | 2023-06-29 21:26:15,300 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-c72a8840-c889-40f2-b9ef-3114e4382634/container.db for volume DS-c72a8840-c889-40f2-b9ef-3114e4382634
datanode_1          | 2023-06-29 21:26:15,318 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-29 21:26:15,324 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-29 21:26:15,426 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-29 21:26:15,426 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:15,457 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start RPC server
datanode_1          | 2023-06-29 21:26:15,463 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: adfef5aa-bdc6-4ac3-9280-f2125c45b895: GrpcService started, listening on 9858
datanode_1          | 2023-06-29 21:26:15,465 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: adfef5aa-bdc6-4ac3-9280-f2125c45b895: GrpcService started, listening on 9856
datanode_1          | 2023-06-29 21:26:15,467 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: adfef5aa-bdc6-4ac3-9280-f2125c45b895: GrpcService started, listening on 9857
datanode_1          | 2023-06-29 21:26:15,471 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adfef5aa-bdc6-4ac3-9280-f2125c45b895 is started using port 9858 for RATIS
datanode_1          | 2023-06-29 21:26:15,471 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adfef5aa-bdc6-4ac3-9280-f2125c45b895 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-29 21:26:15,471 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis adfef5aa-bdc6-4ac3-9280-f2125c45b895 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-29 21:26:15,472 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-adfef5aa-bdc6-4ac3-9280-f2125c45b895: Started
datanode_1          | 2023-06-29 21:26:15,498 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:26:19,917 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adfef5aa-bdc6-4ac3-9280-f2125c45b895: addNew group-7EED12F328BF:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-7EED12F328BF:java.util.concurrent.CompletableFuture@1a980846[Not completed]
datanode_1          | 2023-06-29 21:26:19,993 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895: new RaftServerImpl for group-7EED12F328BF:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:26:20,001 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:26:20,005 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:26:20,005 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:26:20,008 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:20,008 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:26:20,008 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:26:20,033 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: ConfigurationManager, init=-1: peers:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:26:20,042 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:26:20,053 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:26:20,062 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:26:20,104 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:20,114 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:20,150 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:26:20,152 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:26:20,203 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-29 21:26:20,312 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:20,323 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:26:20,326 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:26:20,327 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:26:20,329 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:26:20,334 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:26:20,336 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a5ac173a-04cc-4476-b4a8-7eed12f328bf does not exist. Creating ...
datanode_1          | 2023-06-29 21:26:20,349 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a5ac173a-04cc-4476-b4a8-7eed12f328bf/in_use.lock acquired by nodename 6@e4c3c8f1a551
datanode_1          | 2023-06-29 21:26:20,382 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a5ac173a-04cc-4476-b4a8-7eed12f328bf has been successfully formatted.
datanode_1          | 2023-06-29 21:26:20,463 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO ratis.ContainerStateMachine: group-7EED12F328BF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:26:20,470 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:26:20,562 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:26:20,567 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:20,581 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:26:20,602 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:26:20,612 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:20,647 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:26:20,648 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:26:20,648 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:20,680 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: new adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a5ac173a-04cc-4476-b4a8-7eed12f328bf
datanode_1          | 2023-06-29 21:26:20,681 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:26:20,682 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:20,685 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:20,688 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:26:20,691 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:26:20,694 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:26:20,694 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:20,694 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:26:20,771 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:20,771 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:20,854 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:20,855 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:20,855 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:20,946 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:25:34,514 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = ddbb7c644033/172.19.0.9
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:25:34,575 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:25:34,786 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:25:35,329 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:25:36,115 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:25:36,134 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:25:36,759 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ddbb7c644033 ip:172.19.0.9
datanode_2          | 2023-06-29 21:25:37,717 [main] INFO reflections.Reflections: Reflections took 687 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_2          | 2023-06-29 21:25:40,476 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-29 21:25:40,788 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-29 21:25:41,946 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:25:42,066 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-29 21:25:42,104 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:25:42,105 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:25:42,315 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:25:42,371 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:25:42,385 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-29 21:25:42,407 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-29 21:25:42,416 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-29 21:25:42,417 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-29 21:25:42,578 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:25:42,596 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:25:50,526 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-29 21:25:51,129 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:25:51,404 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:25:52,304 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:25:52,337 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-29 21:25:52,337 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:25:52,338 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-29 21:25:52,338 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-29 21:25:52,338 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-29 21:25:52,339 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:25:52,344 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:25:52,348 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:25:52,352 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:25:52,422 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:25:52,459 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-29 21:25:52,495 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-29 21:25:54,508 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-29 21:25:54,532 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-29 21:25:54,532 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-29 21:25:54,532 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:25:54,533 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:25:54,569 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:25:54,787 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-29 21:25:54,979 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-29 21:25:55,584 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:25:55,670 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:25:55,885 [main] INFO util.log: Logging initialized @29668ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:25:56,568 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-29 21:25:56,599 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:25:56,646 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:25:56,655 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:25:56,664 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-29 21:25:56,664 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:25:56,846 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-29 21:25:56,864 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-29 21:25:56,869 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-29 21:25:57,021 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:25:57,023 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:25:57,031 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-29 21:25:57,087 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6f9b5f01{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:25:57,096 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3180131e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:25:57,761 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c20841d{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4487981695503090983/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-29 21:25:57,806 [main] INFO server.AbstractConnector: Started ServerConnector@3f919230{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:25:57,810 [main] INFO server.Server: Started @31593ms
datanode_2          | 2023-06-29 21:25:57,830 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-29 21:25:57,830 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-29 21:25:57,831 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:25:58,007 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-29 21:25:58,118 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-06-29 21:25:58,136 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-29 21:25:59,142 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-29 21:25:59,147 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-29 21:25:59,148 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-29 21:25:59,159 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-29 21:25:59,204 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-29 21:25:59,529 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.7:9891
datanode_2          | 2023-06-29 21:25:59,653 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:26:02,495 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:03,496 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:04,497 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:05,498 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:06,499 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:06,532 [EndpointStateMachine task thread for recon/172.19.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From ddbb7c644033/172.19.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:51188 remote=recon/172.19.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 2023-06-29 21:26:20,946 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:26:20,965 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: start as a follower, conf=-1: peers:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:20,965 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:26:20,977 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState
datanode_1          | 2023-06-29 21:26:20,995 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:26:20,995 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:26:21,028 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-7EED12F328BF,id=adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:21,041 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:21,043 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:26:21,045 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:26:21,047 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:26:21,204 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf
datanode_1          | 2023-06-29 21:26:21,209 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf.
datanode_1          | 2023-06-29 21:26:21,210 [PipelineCommandHandlerThread-0] INFO server.RaftServer: adfef5aa-bdc6-4ac3-9280-f2125c45b895: addNew group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-3B5B76324720:java.util.concurrent.CompletableFuture@6309e851[Not completed]
datanode_1          | 2023-06-29 21:26:21,259 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895: new RaftServerImpl for group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:26:21,267 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:26:21,268 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:26:21,268 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:26:21,269 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:21,270 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:26:21,271 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:26:21,272 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: ConfigurationManager, init=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:26:21,281 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:26:21,304 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:26:21,307 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:26:21,308 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:21,313 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:21,316 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:26:21,316 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:26:21,316 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-29 21:26:21,327 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:21,350 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:26:21,356 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:26:21,356 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:26:21,356 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:26:21,356 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:26:21,357 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 does not exist. Creating ...
datanode_1          | 2023-06-29 21:26:21,360 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/in_use.lock acquired by nodename 6@e4c3c8f1a551
datanode_1          | 2023-06-29 21:26:21,364 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 has been successfully formatted.
datanode_1          | 2023-06-29 21:26:21,374 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO ratis.ContainerStateMachine: group-3B5B76324720: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:26:21,402 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:26:21,402 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:26:21,403 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:21,403 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:26:21,403 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:26:21,404 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:21,414 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:26:21,434 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:26:21,434 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:21,440 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: new adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_1          | 2023-06-29 21:26:21,441 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:26:21,442 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:21,442 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:21,442 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:26:21,442 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:26:21,442 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:26:21,448 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:21,448 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:26:21,452 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:21,463 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:22,077 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-adfef5aa-bdc6-4ac3-9280-f2125c45b895: Detected pause in JVM or host machine approximately 0.579s with 0.593s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=63ms
datanode_1          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=530ms
datanode_1          | 2023-06-29 21:26:22,109 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:22,112 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:22,112 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:22,113 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:26:22,114 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:26:22,121 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: start as a follower, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:22,122 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:26:22,122 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:25:34,901 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 6f589bb59d7b/172.19.0.13
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:51188 remote=recon/172.19.0.7:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-29 21:26:07,500 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:08,500 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:09,501 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:26:14,519 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From ddbb7c644033/172.19.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:57676 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 2023-06-29 21:26:22,123 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B5B76324720,id=adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:22,125 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:22,126 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:26:22,127 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:26:22,130 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:26:22,133 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_1          | 2023-06-29 21:26:22,167 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:26:22,168 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:26:24,575 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-adfef5aa-bdc6-4ac3-9280-f2125c45b895: Detected pause in JVM or host machine approximately 0.479s with 0.480s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=480ms
datanode_1          | 2023-06-29 21:26:26,072 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO impl.FollowerState: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099833610ns, electionTimeout:5068ms
datanode_1          | 2023-06-29 21:26:26,074 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: shutdown adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState
datanode_1          | 2023-06-29 21:26:26,085 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:26:26,115 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-29 21:26:26,121 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-FollowerState] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1
datanode_1          | 2023-06-29 21:26:26,161 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:26,162 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-29 21:26:26,179 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:26,179 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-29 21:26:26,179 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: shutdown adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1
datanode_1          | 2023-06-29 21:26:26,182 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:26:26,182 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-7EED12F328BF with new leaderId: adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:26,197 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: change Leader from null to adfef5aa-bdc6-4ac3-9280-f2125c45b895 at term 1 for becomeLeader, leader elected after 6077ms
datanode_1          | 2023-06-29 21:26:26,236 [grpc-default-executor-0] INFO server.RaftServer: adfef5aa-bdc6-4ac3-9280-f2125c45b895: addNew group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-D4362CAE379A:java.util.concurrent.CompletableFuture@e40f6ef[Not completed]
datanode_1          | 2023-06-29 21:26:26,246 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895: new RaftServerImpl for group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:26:26,252 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:26:26,252 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:26:26,258 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:26:26,258 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:26,258 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:26:26,252 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:26:26,263 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.9:57676 remote=scm/172.19.0.5:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-29 21:26:15,298 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-56b5bc01-877b-418e-9444-cf962849e55f/container.db to cache
datanode_2          | 2023-06-29 21:26:15,298 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-56b5bc01-877b-418e-9444-cf962849e55f/container.db for volume DS-56b5bc01-877b-418e-9444-cf962849e55f
datanode_2          | 2023-06-29 21:26:15,309 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-29 21:26:15,314 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-29 21:26:15,425 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-29 21:26:15,425 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_2          | 2023-06-29 21:26:15,467 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start RPC server
datanode_2          | 2023-06-29 21:26:15,480 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: GrpcService started, listening on 9858
datanode_2          | 2023-06-29 21:26:15,485 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: GrpcService started, listening on 9856
datanode_2          | 2023-06-29 21:26:15,491 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: GrpcService started, listening on 9857
datanode_2          | 2023-06-29 21:26:15,510 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 46a01eeb-eda4-4982-b56b-78fc1e331ee5 is started using port 9858 for RATIS
datanode_2          | 2023-06-29 21:26:15,510 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 46a01eeb-eda4-4982-b56b-78fc1e331ee5 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-29 21:26:15,510 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 46a01eeb-eda4-4982-b56b-78fc1e331ee5 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-29 21:26:15,511 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-46a01eeb-eda4-4982-b56b-78fc1e331ee5: Started
datanode_2          | 2023-06-29 21:26:15,539 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:26:20,537 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: addNew group-12C321B465F8:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-12C321B465F8:java.util.concurrent.CompletableFuture@4d8f90d9[Not completed]
datanode_2          | 2023-06-29 21:26:20,676 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: new RaftServerImpl for group-12C321B465F8:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:26:20,682 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:26:20,684 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:26:20,684 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:26:20,705 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:20,705 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:26:20,706 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:26:20,776 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: ConfigurationManager, init=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:26:20,778 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:26:20,887 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:26:20,896 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:26:21,024 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:21,061 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:21,119 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:26:21,124 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:26:21,323 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:26:21,534 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:26:21,543 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:26:21,543 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:26:21,548 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:26:21,548 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:26:21,548 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:26:21,549 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/fc9e5a89-5206-4c35-b48b-12c321b465f8 does not exist. Creating ...
datanode_2          | 2023-06-29 21:26:21,564 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/fc9e5a89-5206-4c35-b48b-12c321b465f8/in_use.lock acquired by nodename 7@ddbb7c644033
datanode_2          | 2023-06-29 21:26:21,600 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/fc9e5a89-5206-4c35-b48b-12c321b465f8 has been successfully formatted.
datanode_2          | 2023-06-29 21:26:21,632 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO ratis.ContainerStateMachine: group-12C321B465F8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:26:21,633 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:26:21,743 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:26:21,743 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:21,756 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:26:21,780 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:26:21,782 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:21,799 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:26:21,799 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:26:21,799 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:21,847 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/fc9e5a89-5206-4c35-b48b-12c321b465f8
datanode_2          | 2023-06-29 21:26:21,848 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:26:21,848 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:26:21,849 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:21,849 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:26:21,850 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:26:21,851 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:26:21,851 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:21,851 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:26:21,889 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:25:34,968 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:25:35,185 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:25:35,677 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:25:36,462 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:25:36,462 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:25:37,128 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6f589bb59d7b ip:172.19.0.13
datanode_3          | 2023-06-29 21:25:38,118 [main] INFO reflections.Reflections: Reflections took 742 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_3          | 2023-06-29 21:25:40,598 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-29 21:25:40,884 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-29 21:25:41,890 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:25:41,958 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-29 21:25:41,998 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:25:42,012 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:25:42,240 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:25:42,243 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:25:42,295 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-29 21:25:42,297 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-29 21:25:42,297 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-29 21:25:42,307 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:26:26,269 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: ConfigurationManager, init=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:26:26,270 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:26:26,270 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:26:26,270 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:26:26,271 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:26:26,272 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:26,272 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:26:26,273 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:26:26,273 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-29 21:26:26,279 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:26,281 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:26:26,282 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:26:26,282 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:26:26,282 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:26:26,282 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:26:26,283 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a does not exist. Creating ...
datanode_1          | 2023-06-29 21:26:26,291 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/in_use.lock acquired by nodename 6@e4c3c8f1a551
datanode_1          | 2023-06-29 21:26:26,297 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a has been successfully formatted.
datanode_1          | 2023-06-29 21:26:26,311 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO ratis.ContainerStateMachine: group-D4362CAE379A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:26:26,313 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:26:26,313 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:26:26,313 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:26,314 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:26:26,319 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:26:26,321 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:26,322 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:26:26,322 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:26:26,322 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:26,325 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: new adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a
datanode_1          | 2023-06-29 21:26:26,325 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:26:26,326 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:26,326 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:26,329 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:26:26,330 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:26:26,330 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:26:26,331 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:26:26,331 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:26:26,343 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:26:26,345 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:26,344 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:25:42,488 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:25:42,489 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:25:49,901 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-29 21:25:50,258 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:25:50,509 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:25:50,898 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:25:50,916 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-29 21:25:50,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:25:50,917 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-29 21:25:50,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-29 21:25:50,918 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-29 21:25:50,919 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:25:50,926 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:25:50,930 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:25:50,937 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:25:51,294 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:25:51,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-29 21:25:51,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-29 21:25:53,296 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-29 21:25:53,302 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-29 21:25:53,322 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-29 21:25:53,325 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:25:53,327 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:25:53,363 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:25:53,923 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-29 21:25:54,293 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-29 21:25:55,032 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:25:55,099 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:25:55,252 [main] INFO util.log: Logging initialized @28902ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:25:55,855 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-29 21:25:55,904 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:25:55,934 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:25:55,947 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:25:55,955 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:25:55,956 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:25:56,162 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-29 21:25:56,166 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-29 21:25:56,193 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-29 21:25:56,380 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:25:56,380 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:25:56,382 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-06-29 21:25:56,431 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@39007725{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:25:56,434 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a225014{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:25:56,971 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1ebb36df{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1745941016702282298/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:25:57,025 [main] INFO server.AbstractConnector: Started ServerConnector@63e40188{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:25:57,029 [main] INFO server.Server: Started @30679ms
datanode_3          | 2023-06-29 21:25:57,043 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-29 21:25:57,044 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-29 21:25:57,045 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:25:57,189 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-29 21:25:57,373 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-06-29 21:25:57,390 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-29 21:25:58,431 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-29 21:25:58,436 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-29 21:25:58,456 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-29 21:25:58,457 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-29 21:25:58,475 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-29 21:25:58,936 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.19.0.7:9891
datanode_3          | 2023-06-29 21:25:59,163 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-29 21:26:01,724 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:01,726 [EndpointStateMachine task thread for recon/172.19.0.7:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.19.0.7:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:02,726 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:03,727 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:04,728 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:05,729 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:06,730 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:06,774 [EndpointStateMachine task thread for recon/172.19.0.7:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 6f589bb59d7b/172.19.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:56514 remote=recon/172.19.0.7:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:56514 remote=recon/172.19.0.7:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-29 21:26:07,730 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:08,731 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:09,732 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.19.0.5:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:26:14,746 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 6f589bb59d7b/172.19.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:47566 remote=scm/172.19.0.5:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 2023-06-29 21:26:26,385 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:26:26,400 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:26,400 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:26:26,401 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:26,401 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:26:26,401 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:26:26,433 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: start as a follower, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:26,433 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:26:26,587 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:26:26,631 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:26:26,633 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:26:26,598 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState
datanode_1          | 2023-06-29 21:26:26,639 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720.
datanode_1          | 2023-06-29 21:26:26,646 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4362CAE379A,id=adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:26,646 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:26:26,646 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:26:26,646 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:26:26,646 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:26:26,649 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:26:26,649 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:26:26,704 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:26,705 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-29 21:26:26,707 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderStateImpl
datanode_1          | 2023-06-29 21:26:26,837 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:26:26,962 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-LeaderElection1] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF: set configuration 0: peers:[adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:27,199 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-7EED12F328BF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a5ac173a-04cc-4476-b4a8-7eed12f328bf/current/log_inprogress_0
datanode_1          | 2023-06-29 21:26:27,274 [grpc-default-executor-0] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: receive requestVote(PRE_VOTE, 7502f754-f707-4011-93cc-35463198dd88, group-3B5B76324720, 0, (t:0, i:0))
datanode_1          | 2023-06-29 21:26:27,276 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO impl.FollowerState: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5153809053ns, electionTimeout:5108ms
datanode_1          | 2023-06-29 21:26:27,284 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: shutdown adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState
datanode_1          | 2023-06-29 21:26:27,285 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:26:27,285 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-29 21:26:27,287 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-FollowerState] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2
datanode_1          | 2023-06-29 21:26:27,311 [grpc-default-executor-0] INFO impl.VoteContext: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-CANDIDATE: reject PRE_VOTE from 7502f754-f707-4011-93cc-35463198dd88: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-29 21:26:27,320 [grpc-default-executor-0] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720 replies to PRE_VOTE vote request: 7502f754-f707-4011-93cc-35463198dd88<-adfef5aa-bdc6-4ac3-9280-f2125c45b895#0:FAIL-t0. Peer's state: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720:t0, leader=null, voted=, raftlog=Memoized:adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:27,322 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:27,366 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_1          | 2023-06-29 21:26:27,393 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:26:27,396 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:26:27,393 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 7502f754-f707-4011-93cc-35463198dd88
datanode_1          | 2023-06-29 21:26:27,466 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:26:27,470 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection:   Response 0: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t0
datanode_1          | 2023-06-29 21:26:27,470 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2 PRE_VOTE round 0: result PASSED
datanode_1          | 2023-06-29 21:26:27,478 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:27,494 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:26:27,495 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection:   Response 0: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t1
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2 ELECTION round 0: result PASSED
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: shutdown adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3B5B76324720 with new leaderId: adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: change Leader from null to adfef5aa-bdc6-4ac3-9280-f2125c45b895 at term 1 for becomeLeader, leader elected after 6219ms
datanode_1          | 2023-06-29 21:26:27,527 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:26:27,530 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:26:21,890 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:21,928 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:21,928 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:21,933 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:26:21,970 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:26:21,970 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.19.0.13:47566 remote=scm/172.19.0.5:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-29 21:26:15,300 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-8f204510-5795-4358-86dc-b56b3f234cc4/container.db to cache
datanode_3          | 2023-06-29 21:26:15,301 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/DS-8f204510-5795-4358-86dc-b56b3f234cc4/container.db for volume DS-8f204510-5795-4358-86dc-b56b3f234cc4
datanode_3          | 2023-06-29 21:26:15,322 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-29 21:26:15,325 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-29 21:26:15,437 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-29 21:26:15,437 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:15,479 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.RaftServer: 7502f754-f707-4011-93cc-35463198dd88: start RPC server
datanode_3          | 2023-06-29 21:26:15,487 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 7502f754-f707-4011-93cc-35463198dd88: GrpcService started, listening on 9858
datanode_3          | 2023-06-29 21:26:15,489 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 7502f754-f707-4011-93cc-35463198dd88: GrpcService started, listening on 9856
datanode_3          | 2023-06-29 21:26:15,491 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO server.GrpcService: 7502f754-f707-4011-93cc-35463198dd88: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:26:15,502 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7502f754-f707-4011-93cc-35463198dd88 is started using port 9858 for RATIS
datanode_3          | 2023-06-29 21:26:15,502 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7502f754-f707-4011-93cc-35463198dd88 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-29 21:26:15,502 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7502f754-f707-4011-93cc-35463198dd88: Started
datanode_3          | 2023-06-29 21:26:15,503 [EndpointStateMachine task thread for scm/172.19.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7502f754-f707-4011-93cc-35463198dd88 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-29 21:26:15,531 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:26:19,754 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7502f754-f707-4011-93cc-35463198dd88: addNew group-9154B422D1FF:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-9154B422D1FF:java.util.concurrent.CompletableFuture@36d27e16[Not completed]
datanode_3          | 2023-06-29 21:26:19,894 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88: new RaftServerImpl for group-9154B422D1FF:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:26:19,929 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:26:19,937 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:26:19,937 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:26:19,938 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:19,938 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:26:19,942 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:26:22,001 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: start as a follower, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:22,003 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:26:22,008 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState
datanode_2          | 2023-06-29 21:26:22,017 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:26:22,017 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:26:22,050 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12C321B465F8,id=46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_2          | 2023-06-29 21:26:22,052 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:26:22,052 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:26:22,053 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:26:22,053 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:26:22,174 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8
datanode_2          | 2023-06-29 21:26:22,174 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8.
datanode_2          | 2023-06-29 21:26:22,175 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: addNew group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-3B5B76324720:java.util.concurrent.CompletableFuture@1e13edf0[Not completed]
datanode_2          | 2023-06-29 21:26:22,178 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: new RaftServerImpl for group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:26:22,179 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:26:22,179 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:26:22,179 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:26:22,179 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:22,196 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:26:22,196 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:26:22,196 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: ConfigurationManager, init=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:26:22,197 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:26:22,203 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:26:22,205 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:26:19,988 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: ConfigurationManager, init=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:26:19,988 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:26:20,004 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:26:20,004 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:26:20,052 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:20,063 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:20,078 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:26:20,084 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:26:20,141 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-29 21:26:20,216 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:26:20,222 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:26:20,223 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:26:20,223 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:26:20,225 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:26:20,230 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:26:20,231 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9308ada9-0866-4271-a96c-9154b422d1ff does not exist. Creating ...
datanode_3          | 2023-06-29 21:26:20,249 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9308ada9-0866-4271-a96c-9154b422d1ff/in_use.lock acquired by nodename 7@6f589bb59d7b
datanode_3          | 2023-06-29 21:26:20,263 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9308ada9-0866-4271-a96c-9154b422d1ff has been successfully formatted.
datanode_3          | 2023-06-29 21:26:20,335 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO ratis.ContainerStateMachine: group-9154B422D1FF: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:26:20,350 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:26:20,432 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:26:20,432 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:20,442 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:26:20,443 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:26:20,450 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:20,478 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:26:20,481 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:26:20,481 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:20,512 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9308ada9-0866-4271-a96c-9154b422d1ff
datanode_3          | 2023-06-29 21:26:20,515 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:26:20,521 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:20,524 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:20,528 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:26:20,532 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:26:20,539 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:26:20,541 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:20,544 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:26:20,595 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:20,599 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:20,658 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:20,660 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:20,663 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:20,694 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:20,694 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:20,707 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: start as a follower, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:20,711 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:26:20,717 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState
datanode_3          | 2023-06-29 21:26:20,737 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:20,737 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:20,787 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9154B422D1FF,id=7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:20,800 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:20,800 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:26:20,800 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:26:20,816 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:26:20,954 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff
datanode_3          | 2023-06-29 21:26:20,966 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff.
datanode_3          | 2023-06-29 21:26:20,984 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7502f754-f707-4011-93cc-35463198dd88: addNew group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] returns group-3B5B76324720:java.util.concurrent.CompletableFuture@69ac610e[Not completed]
datanode_3          | 2023-06-29 21:26:21,012 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88: new RaftServerImpl for group-3B5B76324720:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:26:21,015 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:26:21,015 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:26:21,016 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:26:21,017 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:21,017 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:26:21,017 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:26:21,018 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: ConfigurationManager, init=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:26:21,020 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:26:21,035 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:26:21,035 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:26:21,048 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:21,048 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:21,051 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:26:21,051 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:26:21,053 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-29 21:26:21,075 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:27,589 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-29 21:26:27,589 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:27,590 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-29 21:26:27,598 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-29 21:26:27,602 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-29 21:26:27,605 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:27,606 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-29 21:26:27,606 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-29 21:26:27,608 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:26:27,608 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-29 21:26:27,616 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-29 21:26:27,616 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:26:27,616 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-29 21:26:27,616 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_1          | 2023-06-29 21:26:27,627 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-29 21:26:27,627 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:26:27,627 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_1          | 2023-06-29 21:26:27,627 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_1          | 2023-06-29 21:26:27,627 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:26:27,633 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-29 21:26:27,634 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderStateImpl
datanode_1          | 2023-06-29 21:26:27,642 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:26:27,647 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/current/log_inprogress_0
datanode_1          | 2023-06-29 21:26:27,670 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720-LeaderElection2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-3B5B76324720: set configuration 0: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:31,216 [grpc-default-executor-2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: receive requestVote(PRE_VOTE, 7502f754-f707-4011-93cc-35463198dd88, group-D4362CAE379A, 0, (t:0, i:0))
datanode_1          | 2023-06-29 21:26:31,216 [grpc-default-executor-2] INFO impl.VoteContext: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FOLLOWER: accept PRE_VOTE from 7502f754-f707-4011-93cc-35463198dd88: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:26:31,218 [grpc-default-executor-2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A replies to PRE_VOTE vote request: 7502f754-f707-4011-93cc-35463198dd88<-adfef5aa-bdc6-4ac3-9280-f2125c45b895#0:OK-t0. Peer's state: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A:t0, leader=null, voted=, raftlog=Memoized:adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:31,253 [grpc-default-executor-2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: receive requestVote(ELECTION, 7502f754-f707-4011-93cc-35463198dd88, group-D4362CAE379A, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:26:31,271 [grpc-default-executor-2] INFO impl.VoteContext: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FOLLOWER: accept ELECTION from 7502f754-f707-4011-93cc-35463198dd88: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:26:31,272 [grpc-default-executor-2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:7502f754-f707-4011-93cc-35463198dd88
datanode_1          | 2023-06-29 21:26:31,273 [grpc-default-executor-2] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: shutdown adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState
datanode_1          | 2023-06-29 21:26:31,274 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState] INFO impl.FollowerState: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState was interrupted
datanode_1          | 2023-06-29 21:26:31,274 [grpc-default-executor-2] INFO impl.RoleInfo: adfef5aa-bdc6-4ac3-9280-f2125c45b895: start adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-FollowerState
datanode_1          | 2023-06-29 21:26:31,284 [grpc-default-executor-2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A replies to ELECTION vote request: 7502f754-f707-4011-93cc-35463198dd88<-adfef5aa-bdc6-4ac3-9280-f2125c45b895#0:OK-t1. Peer's state: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A:t1, leader=null, voted=7502f754-f707-4011-93cc-35463198dd88, raftlog=Memoized:adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:31,481 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4362CAE379A with new leaderId: 7502f754-f707-4011-93cc-35463198dd88
datanode_1          | 2023-06-29 21:26:31,481 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-server-thread1] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: change Leader from null to 7502f754-f707-4011-93cc-35463198dd88 at term 1 for appendEntries, leader elected after 5210ms
datanode_1          | 2023-06-29 21:26:31,501 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-server-thread2] INFO server.RaftServer$Division: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A: set configuration 0: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:26:31,503 [adfef5aa-bdc6-4ac3-9280-f2125c45b895-server-thread2] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:26:31,510 [adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: adfef5aa-bdc6-4ac3-9280-f2125c45b895@group-D4362CAE379A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/current/log_inprogress_0
datanode_1          | 2023-06-29 21:26:46,770 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-adfef5aa-bdc6-4ac3-9280-f2125c45b895: Detected pause in JVM or host machine approximately 0.145s with 0.218s GC time.
datanode_1          | GC pool 'ParNew' had collection(s): count=1 time=218ms
datanode_1          | 2023-06-29 21:27:15,500 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:28:15,500 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:29:15,501 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:30:15,502 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:31:15,502 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:26:21,075 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:26:21,078 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:26:21,080 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:26:21,081 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:26:21,081 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:26:21,084 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 does not exist. Creating ...
datanode_3          | 2023-06-29 21:26:21,088 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/in_use.lock acquired by nodename 7@6f589bb59d7b
datanode_3          | 2023-06-29 21:26:21,096 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 has been successfully formatted.
datanode_3          | 2023-06-29 21:26:21,116 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO ratis.ContainerStateMachine: group-3B5B76324720: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:26:21,124 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:26:21,127 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:26:21,131 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:21,132 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:26:21,134 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:26:21,134 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:21,139 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:26:21,146 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:26:21,153 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:21,153 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_3          | 2023-06-29 21:26:21,153 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:26:21,153 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:21,153 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:21,155 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:26:21,155 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:26:21,161 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:26:21,161 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:21,164 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:26:21,168 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:21,176 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:21,878 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-7502f754-f707-4011-93cc-35463198dd88: Detected pause in JVM or host machine approximately 0.340s with 0.695s GC time.
datanode_3          | GC pool 'ParNew' had collection(s): count=1 time=48ms
datanode_3          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=647ms
datanode_3          | 2023-06-29 21:26:21,886 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:21,891 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:21,891 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:21,892 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:21,892 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:21,895 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: start as a follower, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:21,895 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:26:22,206 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:26:22,207 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 does not exist. Creating ...
datanode_2          | 2023-06-29 21:26:22,229 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/in_use.lock acquired by nodename 7@ddbb7c644033
datanode_2          | 2023-06-29 21:26:22,232 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720 has been successfully formatted.
datanode_2          | 2023-06-29 21:26:22,232 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO ratis.ContainerStateMachine: group-3B5B76324720: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:26:22,232 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:26:22,233 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:26:22,233 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:22,233 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:26:22,234 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:26:22,234 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:22,235 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:26:22,235 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:26:22,235 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:22,235 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:26:22,244 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:26:22,255 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:22,255 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:26:22,255 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:22,256 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:22,873 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-46a01eeb-eda4-4982-b56b-78fc1e331ee5: Detected pause in JVM or host machine approximately 0.293s with 0.611s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=75ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=536ms
datanode_2          | 2023-06-29 21:26:22,911 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:22,917 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:22,923 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:26:22,934 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:26:22,934 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:26:22,934 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: start as a follower, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:22,934 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:26:22,935 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState
datanode_2          | 2023-06-29 21:26:22,938 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B5B76324720,id=46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_2          | 2023-06-29 21:26:22,941 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:21,896 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState
datanode_3          | 2023-06-29 21:26:21,924 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3B5B76324720,id=7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:21,928 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:21,929 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:26:21,930 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:26:21,932 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:26:21,938 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:21,941 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_3          | 2023-06-29 21:26:21,962 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:25,899 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720.
datanode_3          | 2023-06-29 21:26:25,900 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 7502f754-f707-4011-93cc-35463198dd88: addNew group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-D4362CAE379A:java.util.concurrent.CompletableFuture@506701ab[Not completed]
datanode_3          | 2023-06-29 21:26:25,901 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO impl.FollowerState: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5189503695ns, electionTimeout:5163ms
datanode_3          | 2023-06-29 21:26:25,906 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88: new RaftServerImpl for group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:26:25,909 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:26:25,909 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:26:25,910 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:26:25,906 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState
datanode_3          | 2023-06-29 21:26:25,915 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:25,915 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:26:25,915 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:26:25,917 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: ConfigurationManager, init=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:26:25,918 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:26:25,918 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:26:25,919 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:26:25,919 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:26:25,921 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:25,915 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:26:25,925 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:26:25,927 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:26:25,936 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-29 21:26:25,939 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-29 21:26:25,939 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1
datanode_3          | 2023-06-29 21:26:25,939 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:26:25,940 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:26:25,943 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:26:25,946 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:26:25,947 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:26:25,947 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:26:25,948 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a does not exist. Creating ...
datanode_3          | 2023-06-29 21:26:25,970 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/in_use.lock acquired by nodename 7@6f589bb59d7b
datanode_3          | 2023-06-29 21:26:25,972 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a has been successfully formatted.
datanode_3          | 2023-06-29 21:26:25,972 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO ratis.ContainerStateMachine: group-D4362CAE379A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:26:25,977 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:26:25,977 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:26:25,978 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:25,978 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:26:25,978 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:26:25,979 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:25,979 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:26:25,980 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:26:25,980 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:25,980 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a
datanode_3          | 2023-06-29 21:26:25,980 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:26:25,980 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:26:25,981 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:26:25,982 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:26:25,983 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:26,005 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:26,009 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:26,009 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:26:26,010 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:26,010 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-29 21:26:26,013 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:26,013 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:26:26,016 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:26,016 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-29 21:26:26,016 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1
datanode_3          | 2023-06-29 21:26:26,017 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:26:26,018 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9154B422D1FF with new leaderId: 7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:26,021 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: change Leader from null to 7502f754-f707-4011-93cc-35463198dd88 at term 1 for becomeLeader, leader elected after 5966ms
datanode_3          | 2023-06-29 21:26:26,027 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: start as a follower, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:26,027 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:26:26,028 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState
datanode_3          | 2023-06-29 21:26:26,033 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4362CAE379A,id=7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:26,033 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:26:26,033 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:26:26,035 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:26:26,035 [7502f754-f707-4011-93cc-35463198dd88-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:26:26,036 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:26,039 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a
datanode_3          | 2023-06-29 21:26:26,059 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:26,097 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:26:26,201 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:26,205 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:26:26,267 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:26:26,269 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:26:26,270 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:26:26,392 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:26,406 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:26:26,420 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderStateImpl
datanode_3          | 2023-06-29 21:26:26,459 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:26:26,565 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-LeaderElection1] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF: set configuration 0: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:26,996 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO impl.FollowerState: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5100290483ns, electionTimeout:5032ms
datanode_3          | 2023-06-29 21:26:27,008 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState
datanode_3          | 2023-06-29 21:26:27,008 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:26:27,009 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-29 21:26:27,009 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2
datanode_2          | 2023-06-29 21:26:22,941 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:26:22,941 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:26:22,941 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:26:22,943 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:26:22,959 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720
datanode_2          | 2023-06-29 21:26:22,960 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:26:25,235 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-46a01eeb-eda4-4982-b56b-78fc1e331ee5: Detected pause in JVM or host machine approximately 0.356s with 0.522s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=522ms
datanode_2          | 2023-06-29 21:26:26,410 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720.
datanode_2          | 2023-06-29 21:26:26,410 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: addNew group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] returns group-D4362CAE379A:java.util.concurrent.CompletableFuture@5bbf579e[Not completed]
datanode_2          | 2023-06-29 21:26:26,416 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: new RaftServerImpl for group-D4362CAE379A:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:26:26,417 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:26:26,418 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: ConfigurationManager, init=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:26:26,419 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:26:26,421 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:26:26,421 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:26:26,422 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:26:26,422 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:26,423 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:26:26,424 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:26:26,426 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:26:26,429 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:26:26,431 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:26:26,432 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:26:26,434 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:26:26,434 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:26:26,436 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:26:26,440 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a does not exist. Creating ...
datanode_2          | 2023-06-29 21:26:26,450 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/in_use.lock acquired by nodename 7@ddbb7c644033
datanode_2          | 2023-06-29 21:26:26,452 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a has been successfully formatted.
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO ratis.ContainerStateMachine: group-D4362CAE379A: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:26:26,461 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:26:26,462 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:26,462 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:26,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:26:26,472 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:26:26,472 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:26:26,472 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:26:26,472 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:26:26,492 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:26:26,499 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:26:26,523 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:26,523 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:26:26,525 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:26:26,526 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:26:26,526 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:26:26,533 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: start as a follower, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:26,535 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:26:26,535 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState
datanode_2          | 2023-06-29 21:26:26,536 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D4362CAE379A,id=46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_2          | 2023-06-29 21:26:26,536 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:26:26,536 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:26:26,536 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:26:26,536 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:26:26,538 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:26:26,557 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a
datanode_2          | 2023-06-29 21:26:26,575 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:26:27,022 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a.
datanode_3          | 2023-06-29 21:26:27,054 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:27,109 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:27,109 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:27,110 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_3          | 2023-06-29 21:26:27,121 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_3          | 2023-06-29 21:26:27,220 [7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-9154B422D1FF-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9308ada9-0866-4271-a96c-9154b422d1ff/current/log_inprogress_0
datanode_3          | 2023-06-29 21:26:27,259 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a.
datanode_3          | 2023-06-29 21:26:27,331 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:26:27,334 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7502f754-f707-4011-93cc-35463198dd88<-adfef5aa-bdc6-4ac3-9280-f2125c45b895#0:FAIL-t0
datanode_3          | 2023-06-29 21:26:27,337 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode_3          | 2023-06-29 21:26:27,339 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_3          | 2023-06-29 21:26:27,339 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2
datanode_3          | 2023-06-29 21:26:27,339 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-LeaderElection2] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState
datanode_3          | 2023-06-29 21:26:27,450 [grpc-default-executor-0] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: receive requestVote(PRE_VOTE, adfef5aa-bdc6-4ac3-9280-f2125c45b895, group-3B5B76324720, 0, (t:0, i:0))
datanode_3          | 2023-06-29 21:26:27,459 [grpc-default-executor-0] INFO impl.VoteContext: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FOLLOWER: accept PRE_VOTE from adfef5aa-bdc6-4ac3-9280-f2125c45b895: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:26:27,470 [grpc-default-executor-0] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720 replies to PRE_VOTE vote request: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-7502f754-f707-4011-93cc-35463198dd88#0:OK-t0. Peer's state: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720:t0, leader=null, voted=, raftlog=Memoized:7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:27,521 [grpc-default-executor-0] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: receive requestVote(ELECTION, adfef5aa-bdc6-4ac3-9280-f2125c45b895, group-3B5B76324720, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:26:27,522 [grpc-default-executor-0] INFO impl.VoteContext: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FOLLOWER: accept ELECTION from adfef5aa-bdc6-4ac3-9280-f2125c45b895: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:26:27,522 [grpc-default-executor-0] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_3          | 2023-06-29 21:26:27,523 [grpc-default-executor-0] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState
datanode_3          | 2023-06-29 21:26:27,523 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState] INFO impl.FollowerState: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState was interrupted
datanode_3          | 2023-06-29 21:26:27,537 [grpc-default-executor-0] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-FollowerState
datanode_3          | 2023-06-29 21:26:27,540 [grpc-default-executor-0] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720 replies to ELECTION vote request: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-7502f754-f707-4011-93cc-35463198dd88#0:OK-t1. Peer's state: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720:t1, leader=null, voted=adfef5aa-bdc6-4ac3-9280-f2125c45b895, raftlog=Memoized:7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:27,777 [7502f754-f707-4011-93cc-35463198dd88-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3B5B76324720 with new leaderId: adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_3          | 2023-06-29 21:26:27,777 [7502f754-f707-4011-93cc-35463198dd88-server-thread1] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: change Leader from null to adfef5aa-bdc6-4ac3-9280-f2125c45b895 at term 1 for appendEntries, leader elected after 6729ms
datanode_2          | 2023-06-29 21:26:27,107 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO impl.FollowerState: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099611035ns, electionTimeout:5089ms
datanode_2          | 2023-06-29 21:26:27,108 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: shutdown 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState
datanode_2          | 2023-06-29 21:26:27,125 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:26:27,134 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-29 21:26:27,135 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-FollowerState] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1
datanode_2          | 2023-06-29 21:26:27,154 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.LeaderElection: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,158 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.LeaderElection: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-29 21:26:27,168 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.LeaderElection: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,169 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.LeaderElection: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-29 21:26:27,169 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: shutdown 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1
datanode_2          | 2023-06-29 21:26:27,171 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:26:27,171 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12C321B465F8 with new leaderId: 46a01eeb-eda4-4982-b56b-78fc1e331ee5
datanode_2          | 2023-06-29 21:26:27,186 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: change Leader from null to 46a01eeb-eda4-4982-b56b-78fc1e331ee5 at term 1 for becomeLeader, leader elected after 6153ms
datanode_2          | 2023-06-29 21:26:27,226 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: receive requestVote(PRE_VOTE, 7502f754-f707-4011-93cc-35463198dd88, group-3B5B76324720, 0, (t:0, i:0))
datanode_2          | 2023-06-29 21:26:27,238 [grpc-default-executor-1] INFO impl.VoteContext: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FOLLOWER: accept PRE_VOTE from 7502f754-f707-4011-93cc-35463198dd88: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:26:27,295 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:26:27,338 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720 replies to PRE_VOTE vote request: 7502f754-f707-4011-93cc-35463198dd88<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t0. Peer's state: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720:t0, leader=null, voted=, raftlog=Memoized:46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,347 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:26:27,352 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:26:27,375 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:26:27,377 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:26:27,378 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:26:27,453 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: receive requestVote(PRE_VOTE, adfef5aa-bdc6-4ac3-9280-f2125c45b895, group-3B5B76324720, 0, (t:0, i:0))
datanode_2          | 2023-06-29 21:26:27,459 [grpc-default-executor-1] INFO impl.VoteContext: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FOLLOWER: accept PRE_VOTE from adfef5aa-bdc6-4ac3-9280-f2125c45b895: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:26:27,460 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720 replies to PRE_VOTE vote request: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t0. Peer's state: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720:t0, leader=null, voted=, raftlog=Memoized:46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,468 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:27,865 [7502f754-f707-4011-93cc-35463198dd88-server-thread3] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720: set configuration 0: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:27,868 [7502f754-f707-4011-93cc-35463198dd88-server-thread3] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:26:27,876 [7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-3B5B76324720-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/current/log_inprogress_0
datanode_3          | 2023-06-29 21:26:31,210 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO impl.FollowerState: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5181856511ns, electionTimeout:5149ms
datanode_3          | 2023-06-29 21:26:31,210 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState
datanode_3          | 2023-06-29 21:26:31,210 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:26:31,210 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-29 21:26:31,211 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-FollowerState] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3
datanode_3          | 2023-06-29 21:26:31,211 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:31,212 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:31,213 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:31,224 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:26:31,238 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection:   Response 0: 7502f754-f707-4011-93cc-35463198dd88<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t0
datanode_3          | 2023-06-29 21:26:31,239 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3 PRE_VOTE round 0: result PASSED
datanode_3          | 2023-06-29 21:26:31,241 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:26:31,243 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:26:31,249 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection:   Response 0: 7502f754-f707-4011-93cc-35463198dd88<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t1
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.LeaderElection: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: shutdown 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4362CAE379A with new leaderId: 7502f754-f707-4011-93cc-35463198dd88
datanode_3          | 2023-06-29 21:26:31,269 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: change Leader from null to 7502f754-f707-4011-93cc-35463198dd88 at term 1 for becomeLeader, leader elected after 5350ms
datanode_3          | 2023-06-29 21:26:31,271 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:26:31,273 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:31,273 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:26:31,274 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:26:31,276 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:26:31,276 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:26:31,276 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:26:31,276 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:26:31,328 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:26:31,328 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:31,328 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:26:31,330 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-29 21:26:31,330 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:26:31,331 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:26:31,331 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-29 21:26:31,331 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-29 21:26:31,331 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:26:31,331 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:26:31,333 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:26:31,337 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:26:31,337 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:26:31,337 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-29 21:26:31,337 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:26:31,338 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:26:31,338 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_3          | 2023-06-29 21:26:31,338 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_3          | 2023-06-29 21:26:31,338 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:26:31,338 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:26:31,339 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO impl.RoleInfo: 7502f754-f707-4011-93cc-35463198dd88: start 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderStateImpl
datanode_3          | 2023-06-29 21:26:31,341 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:26:31,344 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/current/log_inprogress_0
datanode_3          | 2023-06-29 21:26:31,357 [7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A-LeaderElection3] INFO server.RaftServer$Division: 7502f754-f707-4011-93cc-35463198dd88@group-D4362CAE379A: set configuration 0: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:27:15,536 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:28:15,536 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:29:15,537 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:30:15,538 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:31:15,538 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:26:27,483 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:26:27,497 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: receive requestVote(ELECTION, adfef5aa-bdc6-4ac3-9280-f2125c45b895, group-3B5B76324720, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:26:27,498 [grpc-default-executor-1] INFO impl.VoteContext: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FOLLOWER: accept ELECTION from adfef5aa-bdc6-4ac3-9280-f2125c45b895: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:26:27,500 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_2          | 2023-06-29 21:26:27,500 [grpc-default-executor-1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: shutdown 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState
datanode_2          | 2023-06-29 21:26:27,500 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState] INFO impl.FollowerState: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState was interrupted
datanode_2          | 2023-06-29 21:26:27,502 [grpc-default-executor-1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-FollowerState
datanode_2          | 2023-06-29 21:26:27,517 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderStateImpl
datanode_2          | 2023-06-29 21:26:27,519 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720 replies to ELECTION vote request: adfef5aa-bdc6-4ac3-9280-f2125c45b895<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t1. Peer's state: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720:t1, leader=null, voted=adfef5aa-bdc6-4ac3-9280-f2125c45b895, raftlog=Memoized:46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,586 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:26:27,649 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-LeaderElection1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8: set configuration 0: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,886 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3B5B76324720 with new leaderId: adfef5aa-bdc6-4ac3-9280-f2125c45b895
datanode_2          | 2023-06-29 21:26:27,887 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: change Leader from null to adfef5aa-bdc6-4ac3-9280-f2125c45b895 at term 1 for appendEntries, leader elected after 5681ms
datanode_2          | 2023-06-29 21:26:27,908 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread2] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720: set configuration 0: peers:[46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, 7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:27,912 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread2] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:26:28,056 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-3B5B76324720-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/621848c6-0b0a-4f59-bb77-3b5b76324720/current/log_inprogress_0
datanode_2          | 2023-06-29 21:26:28,088 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-12C321B465F8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/fc9e5a89-5206-4c35-b48b-12c321b465f8/current/log_inprogress_0
datanode_2          | 2023-06-29 21:26:31,216 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: receive requestVote(PRE_VOTE, 7502f754-f707-4011-93cc-35463198dd88, group-D4362CAE379A, 0, (t:0, i:0))
datanode_2          | 2023-06-29 21:26:31,217 [grpc-default-executor-1] INFO impl.VoteContext: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FOLLOWER: accept PRE_VOTE from 7502f754-f707-4011-93cc-35463198dd88: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:26:31,217 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A replies to PRE_VOTE vote request: 7502f754-f707-4011-93cc-35463198dd88<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t0. Peer's state: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A:t0, leader=null, voted=, raftlog=Memoized:46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:31,245 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: receive requestVote(ELECTION, 7502f754-f707-4011-93cc-35463198dd88, group-D4362CAE379A, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:26:31,245 [grpc-default-executor-1] INFO impl.VoteContext: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FOLLOWER: accept ELECTION from 7502f754-f707-4011-93cc-35463198dd88: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:26:31,245 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:7502f754-f707-4011-93cc-35463198dd88
datanode_2          | 2023-06-29 21:26:31,246 [grpc-default-executor-1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: shutdown 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState
datanode_2          | 2023-06-29 21:26:31,246 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState] INFO impl.FollowerState: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState was interrupted
datanode_2          | 2023-06-29 21:26:31,250 [grpc-default-executor-1] INFO impl.RoleInfo: 46a01eeb-eda4-4982-b56b-78fc1e331ee5: start 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-FollowerState
datanode_2          | 2023-06-29 21:26:31,264 [grpc-default-executor-1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A replies to ELECTION vote request: 7502f754-f707-4011-93cc-35463198dd88<-46a01eeb-eda4-4982-b56b-78fc1e331ee5#0:OK-t1. Peer's state: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A:t1, leader=null, voted=7502f754-f707-4011-93cc-35463198dd88, raftlog=Memoized:46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:31,407 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D4362CAE379A with new leaderId: 7502f754-f707-4011-93cc-35463198dd88
datanode_2          | 2023-06-29 21:26:31,407 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread1] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: change Leader from null to 7502f754-f707-4011-93cc-35463198dd88 at term 1 for appendEntries, leader elected after 4985ms
datanode_2          | 2023-06-29 21:26:31,469 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread2] INFO server.RaftServer$Division: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A: set configuration 0: peers:[7502f754-f707-4011-93cc-35463198dd88|rpc:172.19.0.13:9856|admin:172.19.0.13:9857|client:172.19.0.13:9858|dataStream:172.19.0.13:9858|priority:1|startupRole:FOLLOWER, 46a01eeb-eda4-4982-b56b-78fc1e331ee5|rpc:172.19.0.9:9856|admin:172.19.0.9:9857|client:172.19.0.9:9858|dataStream:172.19.0.9:9858|priority:0|startupRole:FOLLOWER, adfef5aa-bdc6-4ac3-9280-f2125c45b895|rpc:172.19.0.8:9856|admin:172.19.0.8:9857|client:172.19.0.8:9858|dataStream:172.19.0.8:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:26:31,471 [46a01eeb-eda4-4982-b56b-78fc1e331ee5-server-thread2] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:26:31,473 [46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 46a01eeb-eda4-4982-b56b-78fc1e331ee5@group-D4362CAE379A-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a80bcd66-eb5a-4504-8ea9-d4362cae379a/current/log_inprogress_0
datanode_2          | 2023-06-29 21:26:47,478 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-46a01eeb-eda4-4982-b56b-78fc1e331ee5: Detected pause in JVM or host machine approximately 0.223s with 0.429s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=429ms
datanode_2          | 2023-06-29 21:27:15,539 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:28:15,540 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:29:15,540 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:30:15,541 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:31:15,541 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:25:33,673 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = e26288d696f2/172.19.0.7
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
recon_1             | STARTUP_MSG:   java = 11.0.19
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:25:33,737 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-29 21:25:37,232 [main] INFO reflections.Reflections: Reflections took 398 ms to scan 1 urls, producing 20 keys and 75 values 
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:25:34,691 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8726e8df25d1/172.19.0.3
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-29 21:25:34,736 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:25:40,254 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:25:42,157 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:25:42,629 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.3:9862
om_1                | 2023-06-29 21:25:42,629 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:25:42,629 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:25:42,949 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:25:44,028 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-06-29 21:25:47,440 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-29 21:25:49,442 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-29 21:25:51,444 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-29 21:25:53,445 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-29 21:25:55,446 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-29 21:25:57,448 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1                | 2023-06-29 21:25:59,449 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-29 21:26:01,451 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-29 21:26:03,453 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-29 21:26:05,454 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-29 21:26:07,456 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 8726e8df25d1/172.19.0.3 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-29 21:26:10,414 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-29 21:26:12,418 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-29 21:26:14,423 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 2023-06-29 21:25:40,980 [main] INFO reflections.Reflections: Reflections took 357 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 2023-06-29 21:25:41,419 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-29 21:25:43,597 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:25:48,325 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:25:49,962 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-29 21:25:50,028 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.065 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-29 21:25:50,354 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:25:50,500 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:25:50,504 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-29 21:25:53,370 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:25:53,420 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:25:53,507 [main] INFO util.log: Logging initialized @28428ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:25:54,051 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-29 21:25:54,100 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-29 21:25:54,152 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:25:54,154 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:25:54,158 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:25:54,158 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:25:54,406 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-29 21:25:54,430 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:25:55,392 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:25:55,424 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-29 21:25:55,460 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-29 21:25:55,601 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:25:55,601 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-29 21:25:58,953 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:25:59,799 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:26:00,001 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-29 21:26:00,003 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-29 21:26:00,234 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:26:00,463 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-29 21:26:00,509 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-29 21:26:00,608 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-29 21:26:00,628 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-29 21:26:00,668 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-29 21:26:01,286 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-29 21:26:01,322 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:26:01,356 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-06-29 21:26:01,391 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-29 21:26:01,465 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-29 21:26:01,646 [main] INFO recon.ReconServer: Initializing support of Recon Features...
recon_1             | 2023-06-29 21:26:01,649 [main] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-29 21:26:01,649 [main] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-29 21:26:01,720 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-29 21:26:01,763 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-29 21:26:01,763 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-29 21:26:02,069 [main] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-29 21:26:02,072 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 2023-06-29 21:26:02,128 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-29 21:26:02,129 [main] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-29 21:26:02,131 [main] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-29 21:26:02,143 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@a319a2e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:25:40,827 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 7601999233fe/172.19.0.5
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
scm_1               | STARTUP_MSG:   java = 11.0.19
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:25:40,987 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:25:41,968 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:25:43,746 [main] INFO reflections.Reflections: Reflections took 1308 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-29 21:25:44,925 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:25:45,012 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:25:46,971 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-29 21:25:47,766 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:25:47,767 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:25:47,767 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:25:47,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:25:47,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-29 21:25:47,780 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-29 21:25:47,780 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-29 21:25:47,825 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:25:47,825 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-29 21:25:47,826 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:25:47,986 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-29 21:25:48,014 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:25:48,020 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-29 21:25:50,737 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-29 21:25:50,812 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-29 21:25:50,822 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-29 21:25:50,822 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:25:50,822 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:25:50,869 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:25:50,958 [main] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: addNew group-3CC3B5E8F8C9:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER] returns group-3CC3B5E8F8C9:java.util.concurrent.CompletableFuture@2f2bff16[Not completed]
scm_1               | 2023-06-29 21:25:51,138 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c: new RaftServerImpl for group-3CC3B5E8F8C9:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-29 21:25:51,165 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-29 21:25:51,165 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-29 21:25:51,180 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-29 21:25:51,184 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:25:51,187 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:25:51,188 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-29 21:25:51,266 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: ConfigurationManager, init=-1: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-29 21:25:51,266 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:25:51,335 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-29 21:25:51,348 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-29 21:25:51,596 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-29 21:25:51,642 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-29 21:25:51,681 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-29 21:25:51,692 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-29 21:25:51,917 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-29 21:25:52,021 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-29 21:25:53,316 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:25:53,338 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-29 21:25:53,385 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-29 21:25:53,391 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-29 21:25:53,392 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-29 21:25:53,404 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:25:53,406 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9 does not exist. Creating ...
scm_1               | 2023-06-29 21:25:53,433 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/in_use.lock acquired by nodename 13@7601999233fe
scm_1               | 2023-06-29 21:25:53,492 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9 has been successfully formatted.
scm_1               | 2023-06-29 21:25:53,574 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-29 21:25:53,636 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:25:53,645 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-06-29 21:26:02,144 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2082e0e4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-29 21:26:05,172 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6387fb09{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-16376945906503464307/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-29 21:26:05,195 [main] INFO server.AbstractConnector: Started ServerConnector@531292ed{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-29 21:26:05,197 [main] INFO server.Server: Started @40118ms
recon_1             | 2023-06-29 21:26:05,200 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-29 21:26:05,200 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-29 21:26:05,202 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-29 21:26:05,202 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-29 21:26:05,216 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-29 21:26:05,225 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-29 21:26:05,226 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-29 21:26:05,226 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:26:05,226 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-29 21:26:05,227 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:26:07,378 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From e26288d696f2/172.19.0.7 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-29 21:26:10,613 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-29 21:26:12,623 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1             | 2023-06-29 21:26:14,626 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:6618082c-1093-41cf-8a52-54b5243d044c is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9;layoutVersion=6
om_1                | 2023-06-29 21:26:16,582 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 8726e8df25d1/172.19.0.3
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:26:22,880 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 8726e8df25d1/172.19.0.3
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1             | 2023-06-29 21:26:18,240 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 5 pipelines from SCM.
recon_1             | 2023-06-29 21:26:18,249 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:26:18,261 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8 from SCM.
recon_1             | 2023-06-29 21:26:19,028 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff from SCM.
recon_1             | 2023-06-29 21:26:19,031 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 from SCM.
recon_1             | 2023-06-29 21:26:19,037 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf from SCM.
recon_1             | 2023-06-29 21:26:19,039 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a from SCM.
recon_1             | 2023-06-29 21:26:19,042 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-29 21:26:19,049 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:26:19,057 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-29 21:26:19,059 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-29 21:26:19,934 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:56514 / 172.19.0.13:56514: output error
recon_1             | 2023-06-29 21:26:19,954 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:43178 / 172.19.0.9:43178: output error
recon_1             | 2023-06-29 21:26:19,954 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:51192 / 172.19.0.9:51192: output error
recon_1             | 2023-06-29 21:26:19,954 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:36912 / 172.19.0.8:36912: output error
recon_1             | 2023-06-29 21:26:19,954 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:50660 / 172.19.0.8:50660: output error
recon_1             | 2023-06-29 21:26:19,954 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:44266 / 172.19.0.13:44266: output error
recon_1             | 2023-06-29 21:26:19,935 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:36918 / 172.19.0.8:36918: output error
recon_1             | 2023-06-29 21:26:19,935 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:51188 / 172.19.0.9:51188: output error
recon_1             | 2023-06-29 21:26:19,935 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:56516 / 172.19.0.13:56516: output error
recon_1             | 2023-06-29 21:26:19,958 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
om_1                | STARTUP_MSG:   java = 11.0.19
scm_1               | 2023-06-29 21:25:53,664 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-29 21:25:53,676 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:25:53,758 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:25:53,828 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:25:53,835 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-29 21:25:53,839 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:25:53,904 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9
scm_1               | 2023-06-29 21:25:53,924 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:25:53,939 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:25:53,941 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:25:53,956 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-29 21:25:53,966 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:25:53,976 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-29 21:25:53,979 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-29 21:25:53,980 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:25:54,076 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-29 21:25:54,076 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:25:54,371 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-29 21:25:54,376 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-29 21:25:54,377 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-29 21:25:54,473 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:25:54,492 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:25:54,546 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: start as a follower, conf=-1: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:25:54,603 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-29 21:25:54,614 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState
scm_1               | 2023-06-29 21:25:54,695 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:25:54,695 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:25:54,799 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CC3B5E8F8C9,id=6618082c-1093-41cf-8a52-54b5243d044c
scm_1               | 2023-06-29 21:25:54,812 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-29 21:25:54,817 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-29 21:25:54,817 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-29 21:25:54,818 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-29 21:25:54,899 [main] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: start RPC server
scm_1               | 2023-06-29 21:25:55,513 [main] INFO server.GrpcService: 6618082c-1093-41cf-8a52-54b5243d044c: GrpcService started, listening on 9894
scm_1               | 2023-06-29 21:25:55,534 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6618082c-1093-41cf-8a52-54b5243d044c: Started
scm_1               | 2023-06-29 21:25:59,720 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.FollowerState: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5105971882ns, electionTimeout:5013ms
scm_1               | 2023-06-29 21:25:59,721 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState
scm_1               | 2023-06-29 21:25:59,735 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-29 21:25:59,738 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-29 21:25:59,738 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:25:33,969 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-29 21:25:33,983 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-29 21:25:34,181 [main] INFO util.log: Logging initialized @9055ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-29 21:25:34,950 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-29 21:25:35,018 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-29 21:25:35,057 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-29 21:25:35,085 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-29 21:25:35,085 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-29 21:25:35,085 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-29 21:25:35,357 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir5189621344021719363
s3g_1               | 2023-06-29 21:25:35,924 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 0bb263a4c8f8/172.19.0.4
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir5189621344021719363, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:25:35,980 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-29 21:25:36,056 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:25:36,617 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-29 21:25:37,240 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-29 21:25:37,240 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-29 21:25:37,383 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-29 21:25:37,403 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
s3g_1               | 2023-06-29 21:25:37,414 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
s3g_1               | 2023-06-29 21:25:37,619 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-29 21:25:37,634 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-29 21:25:37,636 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-29 21:25:37,741 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6e33c391{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:25:37,760 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@20312893{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
s3g_1               | 2023-06-29 21:25:43,793 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.159s with 0.180s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=180ms
s3g_1               | 2023-06-29 21:25:56,169 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@186295cc{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir5189621344021719363/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-3094790161223150401/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
s3g_1               | 2023-06-29 21:25:56,195 [main] INFO server.AbstractConnector: Started ServerConnector@106faf11{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:25:56,218 [main] INFO server.Server: Started @31070ms
s3g_1               | 2023-06-29 21:25:56,233 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-29 21:25:56,233 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-29 21:25:56,239 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | 2023-06-29 21:25:59,752 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:25:59,753 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
scm_1               | 2023-06-29 21:25:59,755 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:25:59,762 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-29 21:25:59,763 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1
scm_1               | 2023-06-29 21:25:59,763 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-29 21:25:59,763 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: change Leader from null to 6618082c-1093-41cf-8a52-54b5243d044c at term 1 for becomeLeader, leader elected after 8179ms
scm_1               | 2023-06-29 21:25:59,776 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:25:59,785 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:25:59,786 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:25:59,827 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:25:59,831 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:25:59,833 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-29 21:25:59,853 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:25:59,883 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-29 21:25:59,885 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderStateImpl
scm_1               | 2023-06-29 21:25:59,985 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-29 21:26:00,065 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: set configuration 0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:00,279 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/log_inprogress_0
scm_1               | 2023-06-29 21:26:01,546 [main] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: close
scm_1               | 2023-06-29 21:26:01,547 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: shutdown
scm_1               | 2023-06-29 21:26:01,547 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CC3B5E8F8C9,id=6618082c-1093-41cf-8a52-54b5243d044c
scm_1               | 2023-06-29 21:26:01,547 [main] INFO server.GrpcService: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-29 21:26:01,547 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderStateImpl
scm_1               | 2023-06-29 21:26:01,569 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO impl.PendingRequests: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-29 21:26:01,580 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO impl.StateMachineUpdater: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-29 21:26:01,581 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO impl.StateMachineUpdater: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-29 21:26:01,584 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO impl.StateMachineUpdater: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-29 21:26:01,593 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: closes. applyIndex: 0
scm_1               | 2023-06-29 21:26:01,604 [main] INFO server.GrpcService: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-29 21:26:02,299 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker close()
scm_1               | 2023-06-29 21:26:02,300 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6618082c-1093-41cf-8a52-54b5243d044c: Stopped
scm_1               | 2023-06-29 21:26:02,300 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:26:02,304 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9; layoutVersion=7; scmId=6618082c-1093-41cf-8a52-54b5243d044c
scm_1               | 2023-06-29 21:26:02,329 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 7601999233fe/172.19.0.5
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:26:05,408 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 7601999233fe/172.19.0.5
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-29 21:26:22,922 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:26:26,386 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:26:27,690 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:26:27,955 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.19.0.3:9862
om_1                | 2023-06-29 21:26:27,955 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:26:27,955 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:26:28,137 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:26:28,343 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-29 21:26:28,796 [main] INFO reflections.Reflections: Reflections took 390 ms to scan 1 urls, producing 139 keys and 399 values [using 2 cores]
om_1                | 2023-06-29 21:26:28,834 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-29 21:26:28,911 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:26:29,661 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-06-29 21:26:29,832 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.19.0.5:9863]
om_1                | 2023-06-29 21:26:30,871 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-29 21:26:30,937 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:26:31,407 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-29 21:26:32,394 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-29 21:26:32,509 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-29 21:26:32,540 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:26:32,626 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-29 21:26:32,632 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-29 21:26:32,976 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:26:33,165 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:26:33,168 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:26:33,232 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-29 21:26:33,251 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:26:33,310 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-29 21:26:33,333 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:19,959 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:26:20,401 [IPC Server handler 99 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7502f754-f707-4011-93cc-35463198dd88
recon_1             | 2023-06-29 21:26:20,448 [IPC Server handler 99 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 7502f754-f707-4011-93cc-35463198dd88{ip: 172.19.0.13, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:26:20,508 [IPC Server handler 6 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adfef5aa-bdc6-4ac3-9280-f2125c45b895
recon_1             | 2023-06-29 21:26:20,521 [IPC Server handler 6 on default port 9891] INFO node.SCMNodeManager: Registered Data node : adfef5aa-bdc6-4ac3-9280-f2125c45b895{ip: 172.19.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:26:20,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:20,645 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:20,645 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 7502f754-f707-4011-93cc-35463198dd88 to Node DB.
recon_1             | 2023-06-29 21:26:20,653 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node adfef5aa-bdc6-4ac3-9280-f2125c45b895 to Node DB.
recon_1             | 2023-06-29 21:26:21,128 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:21,431 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:21,692 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/46a01eeb-eda4-4982-b56b-78fc1e331ee5
recon_1             | 2023-06-29 21:26:21,693 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 46a01eeb-eda4-4982-b56b-78fc1e331ee5{ip: 172.19.0.9, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:26:21,694 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 46a01eeb-eda4-4982-b56b-78fc1e331ee5 to Node DB.
recon_1             | 2023-06-29 21:26:21,704 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8 reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:22,901 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:25,996 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
om_1                | 2023-06-29 21:26:33,436 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-29 21:26:33,450 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:26:33,453 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:26:33,453 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:26:33,453 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:26:33,455 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-29 21:26:33,455 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:26:33,456 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-29 21:26:33,458 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:26:33,459 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-29 21:26:33,460 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:26:33,480 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-29 21:26:33,490 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-29 21:26:33,491 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-29 21:26:34,018 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:26:34,022 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-29 21:26:34,023 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-29 21:26:34,024 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:26:34,024 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:26:34,026 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:26:34,043 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@2da16d31[Not completed]
om_1                | 2023-06-29 21:26:34,043 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-29 21:26:34,046 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-29 21:26:34,092 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-29 21:26:34,102 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-29 21:26:34,103 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-29 21:26:34,104 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-29 21:26:34,104 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:26:34,104 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:26:34,105 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-29 21:26:34,156 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-29 21:26:34,164 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:26:34,197 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-29 21:26:34,198 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-29 21:26:34,244 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-29 21:26:34,255 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-29 21:26:34,285 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-29 21:26:34,286 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-29 21:26:34,476 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-29 21:26:34,647 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:26:34,656 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-29 21:26:34,660 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-29 21:26:34,664 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-29 21:26:34,665 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-29 21:26:34,665 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-29 21:26:35,313 [main] INFO reflections.Reflections: Reflections took 1000 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
om_1                | 2023-06-29 21:26:35,691 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:26:35,705 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-06-29 21:26:35,717 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:26:36,790 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:26:36,816 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:26:36,816 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-29 21:26:36,978 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.19.0.3:9862
om_1                | 2023-06-29 21:26:36,981 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-29 21:26:36,986 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-29 21:26:36,993 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@8726e8df25d1
om_1                | 2023-06-29 21:26:37,007 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
scm_1               | STARTUP_MSG:   java = 11.0.19
recon_1             | 2023-06-29 21:26:25,997 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:26,030 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:26,030 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:26,192 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:26,336 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:26,336 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:26,491 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:26,491 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:27,206 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:27,206 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)
recon_1             | 2023-06-29 21:26:27,543 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:27,544 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)
recon_1             | 2023-06-29 21:26:31,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a reported by 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)
recon_1             | 2023-06-29 21:26:47,280 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-29 21:26:47,574 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:26:49,216 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:26:49,216 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-29 21:26:49,228 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-29 21:26:49,228 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-29 21:26:49,241 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-06-29 21:26:49,241 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-06-29 21:26:49,279 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-29 21:26:49,288 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-06-29 21:26:49,299 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 71 milliseconds.
recon_1             | 2023-06-29 21:26:49,410 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 168 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:26:49,471 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 58 milliseconds for processing 1 containers.
recon_1             | 2023-06-29 21:26:58,499 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:26:58,505 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-29 21:27:05,227 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:27:05,228 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:27:06,547 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688074025228
recon_1             | 2023-06-29 21:27:06,558 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-29 21:27:07,216 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688074025228.
recon_1             | 2023-06-29 21:27:07,275 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:27:07,702 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
recon_1             | 2023-06-29 21:27:07,728 [pool-52-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-29 21:27:07,735 [pool-52-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-29 21:27:07,735 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:27:07,736 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-29 21:27:07,736 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-29 21:27:07,823 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:27:07,823 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.087 seconds to process 3 keys.
recon_1             | 2023-06-29 21:27:07,859 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:27:07,902 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-29 21:27:49,292 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:27:49,336 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:27:49,337 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 44
recon_1             | 2023-06-29 21:28:49,337 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:28:49,337 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:29:49,338 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:29:49,339 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 1
recon_1             | 2023-06-29 21:30:49,342 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:30:49,342 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:31:49,343 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:31:49,343 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:31:49,474 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 2 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:31:49,480 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 6 milliseconds for processing 2 containers.
recon_1             | 2023-06-29 21:31:49,485 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 5 pipelines in house.
recon_1             | 2023-06-29 21:31:49,488 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 17 milliseconds.
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:26:05,426 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:26:05,504 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:26:05,640 [main] INFO reflections.Reflections: Reflections took 99 ms to scan 3 urls, producing 132 keys and 287 values 
scm_1               | 2023-06-29 21:26:05,708 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:26:05,716 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:26:06,273 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:26:06,564 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:26:37,014 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:26:37,042 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-29 21:26:37,043 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:26:37,046 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-29 21:26:37,049 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-29 21:26:37,055 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:26:37,062 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-29 21:26:37,063 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-29 21:26:37,063 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:26:37,069 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-29 21:26:37,069 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:26:37,074 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-29 21:26:37,076 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:26:37,078 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-29 21:26:37,079 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-29 21:26:37,081 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:26:37,081 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-29 21:26:37,082 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-29 21:26:37,102 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-29 21:26:37,104 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:26:37,162 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-29 21:26:37,163 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-29 21:26:37,164 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-29 21:26:37,170 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:26:37,170 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:26:37,172 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:26:37,173 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:26:37,175 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:26:37,194 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-29 21:26:37,200 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-29 21:26:37,203 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-29 21:26:37,204 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-29 21:26:37,207 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-29 21:26:37,214 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-29 21:26:37,215 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-29 21:26:37,220 [main] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-29 21:26:37,381 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-29 21:26:37,400 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-29 21:26:37,405 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-29 21:26:37,501 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-29 21:26:37,503 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-29 21:26:37,573 [main] INFO util.log: Logging initialized @20423ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-29 21:26:37,811 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-29 21:26:37,833 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-29 21:26:37,855 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-29 21:26:37,860 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-29 21:26:37,860 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:26:37,861 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:26:38,014 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
om_1                | 2023-06-29 21:26:38,018 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-29 21:26:38,019 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-29 21:26:06,872 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
scm_1               | 2023-06-29 21:26:06,874 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-29 21:26:06,949 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-29 21:26:07,106 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:6618082c-1093-41cf-8a52-54b5243d044c
scm_1               | 2023-06-29 21:26:07,204 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-29 21:26:07,212 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:26:07,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:26:07,214 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:26:07,215 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:26:07,215 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-29 21:26:07,215 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-29 21:26:07,217 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-29 21:26:07,218 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:26:07,219 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-29 21:26:07,220 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:26:07,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-29 21:26:07,235 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:26:07,236 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-29 21:26:07,424 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-29 21:26:07,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-29 21:26:07,427 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-29 21:26:07,428 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:26:07,428 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:26:07,432 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:26:07,438 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: found a subdirectory /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9
scm_1               | 2023-06-29 21:26:07,443 [main] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: addNew group-3CC3B5E8F8C9:[] returns group-3CC3B5E8F8C9:java.util.concurrent.CompletableFuture@7ed7ae[Not completed]
scm_1               | 2023-06-29 21:26:07,473 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c: new RaftServerImpl for group-3CC3B5E8F8C9:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-29 21:26:07,475 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-29 21:26:07,475 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-29 21:26:07,475 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-29 21:26:07,475 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:26:07,476 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:26:07,476 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-29 21:26:07,483 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-29 21:26:07,483 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:26:07,486 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-29 21:26:07,486 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-29 21:26:07,497 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-29 21:26:07,501 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
scm_1               | 2023-06-29 21:26:07,504 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-29 21:26:07,504 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-29 21:26:07,522 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
scm_1               | 2023-06-29 21:26:07,599 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:26:07,601 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-29 21:26:07,601 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-29 21:26:07,602 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-29 21:26:07,602 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-29 21:26:07,602 [6618082c-1093-41cf-8a52-54b5243d044c-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:26:07,605 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-29 21:26:07,605 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-29 21:26:07,606 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:26:38,046 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:26:38,046 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:26:38,047 [main] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-29 21:26:38,059 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47b67eac{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:26:38,060 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@206769f8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:26:38,253 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c0f82f{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-14896799447764387230/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
om_1                | 2023-06-29 21:26:38,278 [main] INFO server.AbstractConnector: Started ServerConnector@46a04668{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-29 21:26:38,278 [main] INFO server.Server: Started @21129ms
om_1                | 2023-06-29 21:26:38,288 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-29 21:26:38,288 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-29 21:26:38,289 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-29 21:26:38,294 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-29 21:26:38,305 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-29 21:26:38,432 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-29 21:26:38,652 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
om_1                | 2023-06-29 21:26:42,377 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5202010845ns, electionTimeout:5160ms
om_1                | 2023-06-29 21:26:42,378 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:26:42,379 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-29 21:26:42,381 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | 2023-06-29 21:26:42,382 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:26:42,384 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:26:42,385 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-29 21:26:42,387 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:26:42,387 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-29 21:26:42,387 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:26:42,387 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-29 21:26:42,391 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 8143ms
om_1                | 2023-06-29 21:26:42,417 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:26:42,424 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:26:42,424 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:26:42,429 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-29 21:26:42,430 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-29 21:26:42,430 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-29 21:26:42,436 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:26:42,438 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-29 21:26:42,439 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-29 21:26:42,455 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-29 21:26:42,501 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:26:42,558 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-29 21:26:42,653 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-29 21:26:44,099 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-29 21:26:44,200 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-29 21:27:06,108 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
om_1                | 2023-06-29 21:27:06,165 [qtp1966802439-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1688074026114 in 50 milliseconds
om_1                | 2023-06-29 21:27:06,207 [qtp1966802439-54] INFO db.RDBCheckpointUtils: Waited for 35 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1688074026114 availability.
om_1                | 2023-06-29 21:27:06,483 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 253 milliseconds
om_1                | 2023-06-29 21:27:06,484 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
scm_1               | 2023-06-29 21:26:07,645 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
scm_1               | 2023-06-29 21:26:07,693 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-29 21:26:07,694 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-29 21:26:07,701 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-29 21:26:07,703 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-29 21:26:07,777 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-29 21:26:07,799 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
scm_1               | 2023-06-29 21:26:07,803 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-29 21:26:07,814 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-29 21:26:07,835 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-29 21:26:07,835 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-29 21:26:07,844 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-29 21:26:07,844 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:26:07,847 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-29 21:26:07,848 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-29 21:26:07,855 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-29 21:26:07,855 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-29 21:26:07,883 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:26:07,883 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:26:07,904 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-29 21:26:07,978 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-29 21:26:08,000 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-29 21:26:08,000 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-29 21:26:08,017 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-29 21:26:08,022 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:08,027 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:26:08,074 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-29 21:26:08,772 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:26:08,803 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:26:08,841 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-06-29 21:26:08,848 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-29 21:26:08,922 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:26:08,979 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:26:08,979 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-06-29 21:26:08,986 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:26:09,026 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:26:09,035 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:26:09,035 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-06-29 21:26:09,036 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:26:09,181 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-29 21:26:09,182 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-29 21:26:09,184 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-29 21:26:09,191 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:26:09,197 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-29 21:26:09,202 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/in_use.lock acquired by nodename 7@7601999233fe
scm_1               | 2023-06-29 21:26:09,206 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=6618082c-1093-41cf-8a52-54b5243d044c} from /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/raft-meta
scm_1               | 2023-06-29 21:26:09,232 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: set configuration 0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:09,235 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:27:06,484 [qtp1966802439-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074026114
scm_1               | 2023-06-29 21:26:09,243 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:26:09,243 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:26:09,245 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-29 21:26:09,245 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:26:09,248 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:26:09,255 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:26:09,255 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-29 21:26:09,256 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:26:09,261 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9
scm_1               | 2023-06-29 21:26:09,261 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:26:09,263 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:26:09,264 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:26:09,264 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-29 21:26:09,264 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:26:09,265 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-29 21:26:09,266 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-29 21:26:09,266 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:26:09,275 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-29 21:26:09,276 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:26:09,298 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-29 21:26:09,298 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-29 21:26:09,301 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-29 21:26:09,327 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: set configuration 0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:09,328 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/log_inprogress_0
scm_1               | 2023-06-29 21:26:09,336 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:26:09,451 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: start as a follower, conf=0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:09,451 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-29 21:26:09,453 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState
scm_1               | 2023-06-29 21:26:09,466 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:26:09,466 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:26:09,472 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3CC3B5E8F8C9,id=6618082c-1093-41cf-8a52-54b5243d044c
scm_1               | 2023-06-29 21:26:09,476 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-29 21:26:09,477 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-29 21:26:09,478 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-29 21:26:09,479 [6618082c-1093-41cf-8a52-54b5243d044c-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-29 21:26:09,482 [main] INFO server.RaftServer: 6618082c-1093-41cf-8a52-54b5243d044c: start RPC server
scm_1               | 2023-06-29 21:26:09,588 [main] INFO server.GrpcService: 6618082c-1093-41cf-8a52-54b5243d044c: GrpcService started, listening on 9894
scm_1               | 2023-06-29 21:26:09,593 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6618082c-1093-41cf-8a52-54b5243d044c: Started
scm_1               | 2023-06-29 21:26:09,607 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-29 21:26:09,608 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-29 21:26:09,611 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-29 21:26:09,611 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-29 21:26:09,611 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-29 21:26:09,685 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:26:09,696 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:26:09,696 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:26:09,939 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:26:09,940 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:26:09,953 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:26:10,072 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:26:10,073 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:26:10,073 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:26:10,073 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:26:10,199 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:26:10,199 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:26:10,254 [main] INFO util.log: Logging initialized @7217ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:26:10,518 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-29 21:26:10,529 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:26:10,542 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:26:10,544 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:26:10,544 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:26:10,544 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:26:10,633 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-29 21:26:10,634 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-29 21:26:10,635 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-29 21:26:10,675 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:26:10,675 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:26:10,676 [main] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-29 21:26:10,687 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@71acf065{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:26:10,688 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6479b4cb{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:26:10,782 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5035c23c{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-8137863803786517636/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-29 21:26:10,794 [main] INFO server.AbstractConnector: Started ServerConnector@5d5b07e9{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:26:10,794 [main] INFO server.Server: Started @7757ms
scm_1               | 2023-06-29 21:26:10,799 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:26:10,800 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:26:10,801 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:26:14,598 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.FollowerState: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5145036292ns, electionTimeout:5130ms
scm_1               | 2023-06-29 21:26:14,599 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState
scm_1               | 2023-06-29 21:26:14,600 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-29 21:26:14,602 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-29 21:26:14,603 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-FollowerState] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1
scm_1               | 2023-06-29 21:26:14,605 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:14,606 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-29 21:26:14,628 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:14,628 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.LeaderElection: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-29 21:26:14,628 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: shutdown 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1
scm_1               | 2023-06-29 21:26:14,629 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-29 21:26:14,629 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-29 21:26:14,629 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-29 21:26:14,632 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: change Leader from null to 6618082c-1093-41cf-8a52-54b5243d044c at term 2 for becomeLeader, leader elected after 7132ms
scm_1               | 2023-06-29 21:26:14,639 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:26:14,643 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:26:14,643 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:26:14,647 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:26:14,650 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:26:14,651 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-29 21:26:14,656 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:26:14,657 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-29 21:26:14,659 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO impl.RoleInfo: 6618082c-1093-41cf-8a52-54b5243d044c: start 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderStateImpl
scm_1               | 2023-06-29 21:26:14,664 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-29 21:26:14,668 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/log_inprogress_0 to /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/log_0-0
scm_1               | 2023-06-29 21:26:14,688 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-LeaderElection1] INFO server.RaftServer$Division: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9: set configuration 1: peers:[6618082c-1093-41cf-8a52-54b5243d044c|rpc:7601999233fe:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:26:14,694 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/4a6df140-e3ae-4c97-a05a-3cc3b5e8f8c9/current/log_inprogress_1
scm_1               | 2023-06-29 21:26:14,703 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-29 21:26:14,703 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-29 21:26:14,711 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:14,712 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-29 21:26:14,713 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:26:14,714 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:26:14,716 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:26:14,724 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:26:14,893 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:36278 / 172.19.0.8:36278: output error
scm_1               | 2023-06-29 21:26:14,893 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:26:14,895 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:47566 / 172.19.0.13:47566: output error
scm_1               | 2023-06-29 21:26:14,895 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:26:14,901 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:57676 / 172.19.0.9:57676: output error
scm_1               | 2023-06-29 21:26:14,901 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:26:16,827 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7502f754-f707-4011-93cc-35463198dd88
scm_1               | 2023-06-29 21:26:16,861 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7502f754-f707-4011-93cc-35463198dd88{ip: 172.19.0.13, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:26:16,868 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/adfef5aa-bdc6-4ac3-9280-f2125c45b895
scm_1               | 2023-06-29 21:26:16,878 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : adfef5aa-bdc6-4ac3-9280-f2125c45b895{ip: 172.19.0.8, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:26:16,880 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:26:16,888 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:26:16,889 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:26:16,897 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:26:16,900 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:26:16,917 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:26:16,937 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf to datanode:adfef5aa-bdc6-4ac3-9280-f2125c45b895
scm_1               | 2023-06-29 21:26:17,460 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:17,575 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: a5ac173a-04cc-4476-b4a8-7eed12f328bf, Nodes: adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:26:16.935922Z[UTC]]
scm_1               | 2023-06-29 21:26:17,576 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff to datanode:7502f754-f707-4011-93cc-35463198dd88
scm_1               | 2023-06-29 21:26:17,608 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:17,615 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9308ada9-0866-4271-a96c-9154b422d1ff, Nodes: 7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:26:17.576641Z[UTC]]
scm_1               | 2023-06-29 21:26:17,624 [IPC Server handler 9 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/46a01eeb-eda4-4982-b56b-78fc1e331ee5
scm_1               | 2023-06-29 21:26:17,627 [IPC Server handler 9 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 46a01eeb-eda4-4982-b56b-78fc1e331ee5{ip: 172.19.0.9, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:26:17,629 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:26:17,629 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:26:17,629 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:26:17,629 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-29 21:26:17,630 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:26:17,632 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:26:17,633 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8 to datanode:46a01eeb-eda4-4982-b56b-78fc1e331ee5
scm_1               | 2023-06-29 21:26:17,643 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:17,644 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: fc9e5a89-5206-4c35-b48b-12c321b465f8, Nodes: 46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:26:17.633225Z[UTC]]
scm_1               | 2023-06-29 21:26:17,657 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 to datanode:adfef5aa-bdc6-4ac3-9280-f2125c45b895
scm_1               | 2023-06-29 21:26:17,664 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 to datanode:46a01eeb-eda4-4982-b56b-78fc1e331ee5
scm_1               | 2023-06-29 21:26:17,664 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 to datanode:7502f754-f707-4011-93cc-35463198dd88
scm_1               | 2023-06-29 21:26:17,674 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:17,675 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 621848c6-0b0a-4f59-bb77-3b5b76324720, Nodes: adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9)7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:26:17.657527Z[UTC]]
scm_1               | 2023-06-29 21:26:17,685 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a to datanode:adfef5aa-bdc6-4ac3-9280-f2125c45b895
scm_1               | 2023-06-29 21:26:17,685 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a to datanode:7502f754-f707-4011-93cc-35463198dd88
scm_1               | 2023-06-29 21:26:17,685 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a to datanode:46a01eeb-eda4-4982-b56b-78fc1e331ee5
scm_1               | 2023-06-29 21:26:17,696 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:17,704 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a contains same datanodes as previous pipelines: PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720 nodeIds: adfef5aa-bdc6-4ac3-9280-f2125c45b895, 7502f754-f707-4011-93cc-35463198dd88, 46a01eeb-eda4-4982-b56b-78fc1e331ee5
scm_1               | 2023-06-29 21:26:17,706 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: a80bcd66-eb5a-4504-8ea9-d4362cae379a, Nodes: adfef5aa-bdc6-4ac3-9280-f2125c45b895(xcompat_datanode_1.xcompat_default/172.19.0.8)7502f754-f707-4011-93cc-35463198dd88(xcompat_datanode_3.xcompat_default/172.19.0.13)46a01eeb-eda4-4982-b56b-78fc1e331ee5(xcompat_datanode_2.xcompat_default/172.19.0.9), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:26:17.685154Z[UTC]]
scm_1               | 2023-06-29 21:26:17,706 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-29 21:26:17,716 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-29 21:26:20,422 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:20,468 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9308ada9-0866-4271-a96c-9154b422d1ff
scm_1               | 2023-06-29 21:26:20,477 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:20,564 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:20,565 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=a5ac173a-04cc-4476-b4a8-7eed12f328bf
scm_1               | 2023-06-29 21:26:20,569 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:21,123 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:21,428 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:21,869 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:21,915 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=fc9e5a89-5206-4c35-b48b-12c321b465f8
scm_1               | 2023-06-29 21:26:21,928 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:22,263 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:25,998 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:26,034 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:26,197 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:26,337 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:26,464 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:27,182 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:27,576 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:26:27,580 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=621848c6-0b0a-4f59-bb77-3b5b76324720
scm_1               | 2023-06-29 21:26:27,581 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:26:27,582 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:26:27,583 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:26:27,584 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-29 21:26:27,584 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-29 21:26:27,584 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:26:27,586 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:26:27,588 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-29 21:26:27,661 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-29 21:26:27,668 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-29 21:26:31,311 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=a80bcd66-eb5a-4504-8ea9-d4362cae379a
scm_1               | 2023-06-29 21:26:44,369 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-29 21:26:44,410 [6618082c-1093-41cf-8a52-54b5243d044c@group-3CC3B5E8F8C9-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-29 21:26:44,416 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-29 21:27:20,206 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:27:30,641 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:28:17,717 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-29 21:28:25,189 [IPC Server handler 96 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:28:34,831 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:29:30,620 [IPC Server handler 61 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:29:39,905 [IPC Server handler 59 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:30:17,718 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 0. Excluded 3.
scm_1               | 2023-06-29 21:30:38,053 [IPC Server handler 53 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:30:47,515 [IPC Server handler 50 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:31:08,003 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-29 21:31:47,584 [IPC Server handler 62 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
scm_1               | 2023-06-29 21:31:56,800 [IPC Server handler 55 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.19.0.10
Attaching to xcompat_s3g_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_old_client_1_1_0_1, xcompat_datanode_3, xcompat_old_client_1_0_0_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_new_client_1, xcompat_scm_1, xcompat_old_client_1_2_1_1, xcompat_recon_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:32:24 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = eac9a2a3fa2f/172.20.0.13
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.0.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_1          | STARTUP_MSG:   java = 11.0.3
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:32:24 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:32:25 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:32:26 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:32:27 INFO  HddsDatanodeService:209 - HddsDatanodeService host:eac9a2a3fa2f ip:172.20.0.13
datanode_1          | 2023-06-29 21:32:28 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:32:28 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-29 21:32:28 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:32:28 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:32:28 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:32:28 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:32:28 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:32:28 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:32:32 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:32:33 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:32:33 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_1          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:32:34 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:32:34 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:32:35 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_1          | 2023-06-29 21:32:35 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:32:35 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:32:35 INFO  log:169 - Logging initialized @15773ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:32:36 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-29 21:32:36 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:32:36 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:32:36 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:32:36 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_1          | 2023-06-29 21:32:36 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_1          | 2023-06-29 21:32:36 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:32:36 INFO  session:338 - No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:32:36 INFO  session:140 - node0 Scavenging every 600000ms
datanode_1          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:32:37 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-5357921279505280202.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:32:37 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:32:37 INFO  Server:399 - Started @18236ms
datanode_1          | 2023-06-29 21:32:37 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_1          | 2023-06-29 21:32:37 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_1          | 2023-06-29 21:32:37 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:32:38 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_1          | 2023-06-29 21:32:38 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_1          | 2023-06-29 21:32:38 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:32:41 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:32:42 WARN  EndpointStateMachine:217 - Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From eac9a2a3fa2f/172.20.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.13:39592 remote=scm/172.20.0.3:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
datanode_1          | 	at com.sun.proxy.$Proxy37.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:116)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:132)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:70)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:41)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_1          | Caused by: java.net.SocketTimeoutException: 1000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.20.0.13:39592 remote=scm/172.20.0.3:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)
datanode_1          | 2023-06-29 21:32:42 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_1          | 2023-06-29 21:32:42 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_1          | 2023-06-29 21:32:42 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis 6bdda10c-e08d-4091-bed4-5af1eefff6f7 at port 9858
datanode_1          | 2023-06-29 21:32:42 INFO  RaftServerProxy:304 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start RPC server
datanode_1          | 2023-06-29 21:32:43 INFO  GrpcService:160 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_1          | 2023-06-29 21:32:46 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_1          | 2023-06-29 21:33:06 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_1          | 2023-06-29 21:33:26 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_1          | 2023-06-29 21:33:40 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerProxy:89 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: addNew group-F43302CAA785:[6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] returns group-F43302CAA785:java.util.concurrent.CompletableFuture@389e38c2[Not completed]
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:107 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: new RaftServerImpl for group-F43302CAA785:[6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:103 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: ConfigurationManager, init=-1: [6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/5d9399ba-c25d-492e-af9f-f43302caa785 does not exist. Creating ...
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/5d9399ba-c25d-492e-af9f-f43302caa785/in_use.lock acquired by nodename 7@eac9a2a3fa2f
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/5d9399ba-c25d-492e-af9f-f43302caa785 has been successfully formatted.
datanode_1          | 2023-06-29 21:33:43 INFO  ContainerStateMachine:225 - group-F43302CAA785: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:180 - new 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/5d9399ba-c25d-492e-af9f-f43302caa785
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:196 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: start as a follower, conf=-1: [6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:185 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:33:43 INFO  RoleInfo:143 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start FollowerState
datanode_1          | 2023-06-29 21:33:43 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-F43302CAA785,id=6bdda10c-e08d-4091-bed4-5af1eefff6f7
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785
datanode_1          | 2023-06-29 21:33:43 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "5d9399ba-c25d-492e-af9f-f43302caa785"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 6742902094507624750
datanode_1          |   leastSigBits: -5791642095847823483
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerProxy:89 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: addNew group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] returns group-E566274547CE:java.util.concurrent.CompletableFuture@57f00e4a[Not completed]
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:107 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: new RaftServerImpl for group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:103 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: ConfigurationManager, init=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce does not exist. Creating ...
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/in_use.lock acquired by nodename 7@eac9a2a3fa2f
datanode_1          | 2023-06-29 21:33:43 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce has been successfully formatted.
datanode_1          | 2023-06-29 21:33:43 INFO  ContainerStateMachine:225 - group-E566274547CE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:180 - new 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:196 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: start as a follower, conf=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_1          | 2023-06-29 21:33:43 INFO  RaftServerImpl:185 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:33:43 INFO  RoleInfo:143 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start FollowerState
datanode_1          | 2023-06-29 21:33:43 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E566274547CE,id=6bdda10c-e08d-4091-bed4-5af1eefff6f7
datanode_1          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE
datanode_1          | 2023-06-29 21:33:44 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "1b4e442b-1aab-42ee-b03b-e566274547ce"
datanode_1          | uuid128 {
datanode_1          |   mostSigBits: 1967585039129199342
datanode_1          |   leastSigBits: -5747748272500029490
datanode_1          | }
datanode_1          | .
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:121 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: shutdown FollowerState
datanode_1          | 2023-06-29 21:33:48 INFO  FollowerState:117 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start FollowerState
datanode_1          | 2023-06-29 21:33:48 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-E566274547CE with new leaderId: b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:255 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: change Leader from null to b144bd75-2038-4955-ba48-81a35f17a5f3 at term 1 for appendEntries, leader elected after 4825ms
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:356 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE: set configuration 0: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null at 0
datanode_1          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:397 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:596 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-E566274547CE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/current/log_inprogress_0
datanode_1          | 2023-06-29 21:33:48 INFO  FollowerState:108 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-FollowerState: change to CANDIDATE, lastRpcTime:5129ms, electionTimeout:5113ms
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:121 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: shutdown FollowerState
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start LeaderElection
datanode_1          | 2023-06-29 21:33:48 INFO  LeaderElection:209 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-LeaderElection1: begin an election at term 1 for -1: [6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:134 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: shutdown LeaderElection
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:33:48 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-F43302CAA785 with new leaderId: 6bdda10c-e08d-4091-bed4-5af1eefff6f7
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:255 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: change Leader from null to 6bdda10c-e08d-4091-bed4-5af1eefff6f7 at term 1 for becomeLeader, leader elected after 5354ms
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:33:48 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7: start LeaderState
datanode_1          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:397 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:596 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5d9399ba-c25d-492e-af9f-f43302caa785/current/log_inprogress_0
datanode_1          | 2023-06-29 21:33:48 INFO  RaftServerImpl:356 - 6bdda10c-e08d-4091-bed4-5af1eefff6f7@group-F43302CAA785: set configuration 0: [6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null at 0
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:32:24 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 0ba1728083ae/172.20.0.10
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.0.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_2          | STARTUP_MSG:   java = 11.0.3
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:32:24 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:32:25 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:32:26 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:32:27 INFO  HddsDatanodeService:209 - HddsDatanodeService host:0ba1728083ae ip:172.20.0.10
datanode_2          | 2023-06-29 21:32:28 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:32:28 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-29 21:32:28 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:32:28 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:32:28 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:32:29 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:32:29 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:32:29 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:32:33 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:32:33 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:32:33 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_2          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:32:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:32:35 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:32:35 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_2          | 2023-06-29 21:32:35 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:32:35 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:32:35 INFO  log:169 - Logging initialized @15903ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:32:36 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-29 21:32:36 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:32:36 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:32:36 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:32:36 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_2          | 2023-06-29 21:32:36 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_2          | 2023-06-29 21:32:36 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:32:36 INFO  session:338 - No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:32:36 INFO  session:140 - node0 Scavenging every 660000ms
datanode_2          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:32:38 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-12541022251291765014.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-29 21:32:38 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:32:38 INFO  Server:399 - Started @18563ms
datanode_2          | 2023-06-29 21:32:38 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_2          | 2023-06-29 21:32:38 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_2          | 2023-06-29 21:32:38 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:32:38 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_2          | 2023-06-29 21:32:38 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_2          | 2023-06-29 21:32:39 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:32:41 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:32:42 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_2          | 2023-06-29 21:32:42 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_2          | 2023-06-29 21:32:42 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis b93369e5-9e0d-4f8b-a23c-f02b9bda1293 at port 9858
datanode_2          | 2023-06-29 21:32:42 INFO  RaftServerProxy:304 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start RPC server
datanode_2          | 2023-06-29 21:32:43 INFO  GrpcService:160 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_2          | 2023-06-29 21:32:46 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_2          | 2023-06-29 21:33:06 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_2          | 2023-06-29 21:33:26 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_2          | 2023-06-29 21:33:40 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerProxy:89 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: addNew group-932412ADCDA5:[b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858] returns group-932412ADCDA5:java.util.concurrent.CompletableFuture@39a8bf13[Not completed]
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerImpl:107 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: new RaftServerImpl for group-932412ADCDA5:[b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerImpl:103 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: ConfigurationManager, init=-1: [b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/6b3b5230-96ba-443e-ad87-932412adcda5 does not exist. Creating ...
datanode_2          | 2023-06-29 21:33:43 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/6b3b5230-96ba-443e-ad87-932412adcda5/in_use.lock acquired by nodename 7@0ba1728083ae
datanode_2          | 2023-06-29 21:33:43 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/6b3b5230-96ba-443e-ad87-932412adcda5 has been successfully formatted.
datanode_2          | 2023-06-29 21:33:43 INFO  ContainerStateMachine:225 - group-932412ADCDA5: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:180 - new b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/6b3b5230-96ba-443e-ad87-932412adcda5
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:33:43 INFO  SegmentedRaftLogWorker:129 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5
datanode_2          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerImpl:196 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: start as a follower, conf=-1: [b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858], old=null
datanode_2          | 2023-06-29 21:33:43 INFO  RaftServerImpl:185 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:33:43 INFO  RoleInfo:143 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start FollowerState
datanode_2          | 2023-06-29 21:33:43 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-932412ADCDA5,id=b93369e5-9e0d-4f8b-a23c-f02b9bda1293
datanode_2          | 2023-06-29 21:33:43 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5
datanode_2          | 2023-06-29 21:33:44 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "6b3b5230-96ba-443e-ad87-932412adcda5"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 7726859954324915262
datanode_2          |   leastSigBits: -5942619400150594139
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerProxy:89 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: addNew group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] returns group-E566274547CE:java.util.concurrent.CompletableFuture@7014b685[Not completed]
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerImpl:107 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: new RaftServerImpl for group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerImpl:103 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: ConfigurationManager, init=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce does not exist. Creating ...
datanode_2          | 2023-06-29 21:33:44 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/in_use.lock acquired by nodename 7@0ba1728083ae
datanode_2          | 2023-06-29 21:33:44 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce has been successfully formatted.
datanode_2          | 2023-06-29 21:33:44 INFO  ContainerStateMachine:225 - group-E566274547CE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  SegmentedRaftLogWorker:180 - new b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  SegmentedRaftLogWorker:129 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:33:44 INFO  SegmentedRaftLogWorker:129 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:33:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE
datanode_2          | 2023-06-29 21:33:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerImpl:196 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: start as a follower, conf=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_2          | 2023-06-29 21:33:44 INFO  RaftServerImpl:185 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:33:44 INFO  RoleInfo:143 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start FollowerState
datanode_2          | 2023-06-29 21:33:44 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E566274547CE,id=b93369e5-9e0d-4f8b-a23c-f02b9bda1293
datanode_2          | 2023-06-29 21:33:44 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE
datanode_2          | 2023-06-29 21:33:44 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "1b4e442b-1aab-42ee-b03b-e566274547ce"
datanode_2          | uuid128 {
datanode_2          |   mostSigBits: 1967585039129199342
datanode_2          |   leastSigBits: -5747748272500029490
datanode_2          | }
datanode_2          | .
datanode_2          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: changes role from  FOLLOWER to FOLLOWER at term 1 for recognizeCandidate:b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_2          | 2023-06-29 21:33:48 INFO  RoleInfo:121 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: shutdown FollowerState
datanode_2          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start FollowerState
datanode_2          | 2023-06-29 21:33:48 INFO  FollowerState:117 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-FollowerState was interrupted: java.lang.InterruptedException: sleep interrupted
datanode_2          | 2023-06-29 21:33:48 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-E566274547CE with new leaderId: b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_2          | 2023-06-29 21:33:48 INFO  RaftServerImpl:255 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: change Leader from null to b144bd75-2038-4955-ba48-81a35f17a5f3 at term 1 for appendEntries, leader elected after 4290ms
datanode_2          | 2023-06-29 21:33:48 INFO  RaftServerImpl:356 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE: set configuration 0: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null at 0
datanode_2          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:397 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:596 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-E566274547CE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/current/log_inprogress_0
datanode_2          | 2023-06-29 21:33:49 INFO  FollowerState:108 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-FollowerState: change to CANDIDATE, lastRpcTime:5203ms, electionTimeout:5176ms
datanode_2          | 2023-06-29 21:33:49 INFO  RoleInfo:121 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: shutdown FollowerState
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerImpl:185 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:33:49 INFO  RoleInfo:143 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start LeaderElection
datanode_2          | 2023-06-29 21:33:49 INFO  LeaderElection:209 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-LeaderElection1: begin an election at term 1 for -1: [b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858], old=null
datanode_2          | 2023-06-29 21:33:49 INFO  RoleInfo:134 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: shutdown LeaderElection
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerImpl:185 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:33:49 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-932412ADCDA5 with new leaderId: b93369e5-9e0d-4f8b-a23c-f02b9bda1293
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerImpl:255 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: change Leader from null to b93369e5-9e0d-4f8b-a23c-f02b9bda1293 at term 1 for becomeLeader, leader elected after 5431ms
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:33:49 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:33:49 INFO  RoleInfo:143 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293: start LeaderState
datanode_2          | 2023-06-29 21:33:49 INFO  SegmentedRaftLogWorker:397 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:33:49 INFO  RaftServerImpl:356 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5: set configuration 0: [b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858], old=null at 0
datanode_2          | 2023-06-29 21:33:49 INFO  SegmentedRaftLogWorker:596 - b93369e5-9e0d-4f8b-a23c-f02b9bda1293@group-932412ADCDA5-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b3b5230-96ba-443e-ad87-932412adcda5/current/log_inprogress_0
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:32:23 INFO  HddsDatanodeService:112 - STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = d983dd42edbc/172.20.0.9
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.0.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.0.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
datanode_3          | STARTUP_MSG:   java = 11.0.3
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:32:24 INFO  HddsDatanodeService:90 - registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:32:26 INFO  MetricRegistries:64 - Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:32:26 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:32:27 INFO  MetricsSystemImpl:191 - HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:32:27 INFO  HddsDatanodeService:209 - HddsDatanodeService host:d983dd42edbc ip:172.20.0.9
datanode_3          | 2023-06-29 21:32:28 INFO  SaveSpaceUsageToFile:94 - Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:32:28 INFO  HddsVolume:176 - Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-29 21:32:28 INFO  MutableVolumeSet:179 - Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:32:28 INFO  ThrottledAsyncChecker:141 - Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:32:28 INFO  HddsVolumeChecker:200 - Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:32:29 INFO  ContainerReader:123 - Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:32:29 INFO  ContainerReader:148 - Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:32:29 INFO  OzoneContainer:196 - Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:32:33 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:32:33 INFO  RaftServerProxy:44 - raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:32:33 INFO  GrpcConfigKeys:44 - raft.grpc.server.port = 9858 (custom)
datanode_3          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:32:33 INFO  GrpcService:44 - raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:32:33 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:32:35 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:32:35 WARN  ServerUtils:237 - Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:32:35 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
datanode_3          | 2023-06-29 21:32:35 INFO  BaseHttpServer:207 - Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:32:35 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:32:35 INFO  log:169 - Logging initialized @16124ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:32:36 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-29 21:32:36 INFO  HttpRequestLog:86 - Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:32:36 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:32:36 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:32:36 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:32:36 INFO  HttpServer2:1237 - Jetty bound to port 9882
datanode_3          | 2023-06-29 21:32:36 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
datanode_3          | 2023-06-29 21:32:36 INFO  session:333 - DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:32:36 INFO  session:338 - No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:32:36 INFO  session:140 - node0 Scavenging every 660000ms
datanode_3          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@455c1d8c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:32:36 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@58472096{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:32:37 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@d5af0a5{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_0_0_jar-_-any-2916676615179551872.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:32:37 INFO  AbstractConnector:330 - Started ServerConnector@317a118b{HTTP/1.1,[http/1.1]}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:32:37 INFO  Server:399 - Started @18023ms
datanode_3          | 2023-06-29 21:32:37 INFO  MetricsSinkAdapter:204 - Sink prometheus started
datanode_3          | 2023-06-29 21:32:37 INFO  MetricsSystemImpl:301 - Registered sink prometheus
datanode_3          | 2023-06-29 21:32:37 INFO  BaseHttpServer:327 - HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:32:37 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
datanode_3          | 2023-06-29 21:32:38 INFO  SCMConnectionManager:180 - Adding Recon Server : recon/172.20.0.6:9891
datanode_3          | 2023-06-29 21:32:38 INFO  InitDatanodeState:145 - DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-29 21:32:40 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:32:41 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=2147483647, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:32:42 INFO  OzoneContainer:245 - Attempting to start container services.
datanode_3          | 2023-06-29 21:32:42 INFO  OzoneContainer:209 - Background container scanner has been disabled.
datanode_3          | 2023-06-29 21:32:42 INFO  XceiverServerRatis:440 - Starting XceiverServerRatis b144bd75-2038-4955-ba48-81a35f17a5f3 at port 9858
datanode_3          | 2023-06-29 21:32:42 INFO  RaftServerProxy:304 - b144bd75-2038-4955-ba48-81a35f17a5f3: start RPC server
datanode_3          | 2023-06-29 21:32:43 INFO  GrpcService:160 - b144bd75-2038-4955-ba48-81a35f17a5f3: GrpcService started, listening on 0.0.0.0/0.0.0.0:9858
datanode_3          | 2023-06-29 21:32:45 WARN  StateContext:442 - No available thread in pool for past 2 seconds.
datanode_3          | 2023-06-29 21:33:05 WARN  StateContext:442 - No available thread in pool for past 22 seconds.
datanode_3          | 2023-06-29 21:33:25 WARN  StateContext:442 - No available thread in pool for past 42 seconds.
datanode_3          | 2023-06-29 21:33:39 INFO  Client:958 - Retrying connect to server: recon/172.20.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=60000 MILLISECONDS)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerProxy:89 - b144bd75-2038-4955-ba48-81a35f17a5f3: addNew group-51C27043FEC7:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858] returns group-51C27043FEC7:java.util.concurrent.CompletableFuture@c71b165[Not completed]
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:107 - b144bd75-2038-4955-ba48-81a35f17a5f3: new RaftServerImpl for group-51C27043FEC7:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:103 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: ConfigurationManager, init=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/4a53da16-4e4e-4fc2-bd46-51c27043fec7 does not exist. Creating ...
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/4a53da16-4e4e-4fc2-bd46-51c27043fec7/in_use.lock acquired by nodename 6@d983dd42edbc
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/4a53da16-4e4e-4fc2-bd46-51c27043fec7 has been successfully formatted.
datanode_3          | 2023-06-29 21:33:42 INFO  ContainerStateMachine:225 - group-51C27043FEC7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:180 - new b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/4a53da16-4e4e-4fc2-bd46-51c27043fec7
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:129 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:129 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:196 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: start as a follower, conf=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858], old=null
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:33:42 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start FollowerState
datanode_3          | 2023-06-29 21:33:42 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-51C27043FEC7,id=b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7
datanode_3          | 2023-06-29 21:33:42 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS ONE #id: "4a53da16-4e4e-4fc2-bd46-51c27043fec7"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 5355864171211542466
datanode_3          |   leastSigBits: -4808065656622416185
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerProxy:89 - b144bd75-2038-4955-ba48-81a35f17a5f3: addNew group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] returns group-E566274547CE:java.util.concurrent.CompletableFuture@78dd2b39[Not completed]
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:107 - b144bd75-2038-4955-ba48-81a35f17a5f3: new RaftServerImpl for group-E566274547CE:[b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.rpcslowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.sleep.deviation.threshold = 300 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:103 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: ConfigurationManager, init=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorageDirectory:261 - The storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce does not exist. Creating ...
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorageDirectory:343 - Lock on /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/in_use.lock acquired by nodename 6@d983dd42edbc
datanode_3          | 2023-06-29 21:33:42 INFO  RaftStorage:84 - Storage directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce has been successfully formatted.
datanode_3          | 2023-06-29 21:33:42 INFO  ContainerStateMachine:225 - group-E566274547CE: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_worker.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:180 - new b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLogWorker for RaftStorage:Storage Directory /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:129 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:33:42 INFO  SegmentedRaftLogWorker:129 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerConfigKeys:44 - raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.leader_election.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.server.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:196 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: start as a follower, conf=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_3          | 2023-06-29 21:33:42 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:33:42 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start FollowerState
datanode_3          | 2023-06-29 21:33:42 INFO  JmxRegister:44 - Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E566274547CE,id=b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_3          | 2023-06-29 21:33:42 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.state_machine.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:44 INFO  CreatePipelineCommandHandler:110 - Created Pipeline RATIS THREE #id: "1b4e442b-1aab-42ee-b03b-e566274547ce"
datanode_3          | uuid128 {
datanode_3          |   mostSigBits: 1967585039129199342
datanode_3          |   leastSigBits: -5747748272500029490
datanode_3          | }
datanode_3          | .
datanode_3          | 2023-06-29 21:33:47 INFO  FollowerState:108 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-FollowerState: change to CANDIDATE, lastRpcTime:5097ms, electionTimeout:5096ms
datanode_3          | 2023-06-29 21:33:47 INFO  RoleInfo:121 - b144bd75-2038-4955-ba48-81a35f17a5f3: shutdown FollowerState
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start LeaderElection
datanode_3          | 2023-06-29 21:33:48 INFO  LeaderElection:209 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-LeaderElection1: begin an election at term 1 for -1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858], old=null
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:134 - b144bd75-2038-4955-ba48-81a35f17a5f3: shutdown LeaderElection
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:33:48 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-51C27043FEC7 with new leaderId: b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:255 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: change Leader from null to b144bd75-2038-4955-ba48-81a35f17a5f3 at term 1 for becomeLeader, leader elected after 5223ms
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start LeaderState
datanode_3          | 2023-06-29 21:33:48 INFO  FollowerState:108 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-FollowerState: change to CANDIDATE, lastRpcTime:5080ms, electionTimeout:5079ms
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:121 - b144bd75-2038-4955-ba48-81a35f17a5f3: shutdown FollowerState
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start LeaderElection
datanode_3          | 2023-06-29 21:33:48 INFO  LeaderElection:209 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-LeaderElection2: begin an election at term 1 for -1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_3          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:397 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:356 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7: set configuration 0: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858], old=null at 0
datanode_3          | 2023-06-29 21:33:48 INFO  LeaderElection:61 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-LeaderElection2: Election PASSED; received 1 response(s) [b144bd75-2038-4955-ba48-81a35f17a5f3<-b93369e5-9e0d-4f8b-a23c-f02b9bda1293#0:OK-t1] and 0 exception(s); b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE:t1, leader=null, voted=b144bd75-2038-4955-ba48-81a35f17a5f3, raftlog=b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:134 - b144bd75-2038-4955-ba48-81a35f17a5f3: shutdown LeaderElection
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:185 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:33:48 INFO  XceiverServerRatis:822 - Leader change notification received for group: group-E566274547CE with new leaderId: b144bd75-2038-4955-ba48-81a35f17a5f3
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:255 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: change Leader from null to b144bd75-2038-4955-ba48-81a35f17a5f3 at term 1 for becomeLeader, leader elected after 5265ms
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis.log_appender.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RatisMetrics:36 - Creating Metrics Registry : ratis_grpc.log_appender.b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  GrpcConfigKeys:44 - raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerConfigKeys:44 - raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:33:48 INFO  RoleInfo:143 - b144bd75-2038-4955-ba48-81a35f17a5f3: start LeaderState
datanode_3          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:397 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:596 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1b4e442b-1aab-42ee-b03b-e566274547ce/current/log_inprogress_0
datanode_3          | 2023-06-29 21:33:48 INFO  SegmentedRaftLogWorker:596 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-51C27043FEC7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4a53da16-4e4e-4fc2-bd46-51c27043fec7/current/log_inprogress_0
datanode_3          | 2023-06-29 21:33:48 INFO  RaftServerImpl:356 - b144bd75-2038-4955-ba48-81a35f17a5f3@group-E566274547CE: set configuration 0: [b144bd75-2038-4955-ba48-81a35f17a5f3:172.20.0.9:9858, b93369e5-9e0d-4f8b-a23c-f02b9bda1293:172.20.0.10:9858, 6bdda10c-e08d-4091-bed4-5af1eefff6f7:172.20.0.13:9858], old=null at 0
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:32:24 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4c9d0ae87370/172.20.0.7
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-29 21:32:24 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:32:29 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:32:29 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.7:9862
om_1                | 2023-06-29 21:32:29 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:32:29 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:32:29 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-29 21:32:31 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:32 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:33 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:34 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:35 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:36 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:37 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:38 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:39 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:40 INFO  Client:958 - Retrying connect to server: scm/172.20.0.3:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:32:40 INFO  RetriableTask:62 - Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-eae7dbfb-6e39-4100-a04c-3a4697ed42e2;layoutVersion=0
om_1                | 2023-06-29 21:32:45 INFO  OzoneManagerStarter:124 - SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 4c9d0ae87370/172.20.0.7
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:32:46 INFO  OzoneManagerStarter:112 - STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 4c9d0ae87370/172.20.0.7
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.0.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
om_1                | STARTUP_MSG:   java = 11.0.3
om_1                | ************************************************************/
om_1                | 2023-06-29 21:32:46 INFO  OzoneManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:32:48 INFO  OMHANodeDetails:104 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:32:48 INFO  OMHANodeDetails:213 - Configuration either no ozone.om.address set. Falling back to the default OM address om/172.20.0.7:9862
om_1                | 2023-06-29 21:32:48 INFO  OMHANodeDetails:241 - OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:32:48 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:32:48 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:32:48 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
om_1                | 2023-06-29 21:32:49 WARN  ServerUtils:225 - ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:32:49 INFO  OzoneManager:3574 - Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:32:49 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:32:49 INFO  Server:1219 - Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:32:49 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:32:49 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:32:49 INFO  MetricsSystemImpl:191 - OzoneManager metrics system started
om_1                | 2023-06-29 21:32:49 INFO  OzoneManager:1114 - OzoneManager RPC server is listening at om/172.20.0.7:9862
om_1                | 2023-06-29 21:32:49 INFO  BaseHttpServer:207 - Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-29 21:32:49 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-29 21:32:49 INFO  log:169 - Logging initialized @4371ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-29 21:32:50 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-29 21:32:50 INFO  HttpRequestLog:86 - Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-29 21:32:50 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-29 21:32:50 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-29 21:32:50 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:32:50 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:32:50 INFO  HttpServer2:1237 - Jetty bound to port 9874
om_1                | 2023-06-29 21:32:50 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
om_1                | 2023-06-29 21:32:50 INFO  session:333 - DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:32:50 INFO  session:338 - No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:32:50 INFO  session:140 - node0 Scavenging every 600000ms
om_1                | 2023-06-29 21:32:50 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@2100d047{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:32:50 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@47be0f9b{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:32:50 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@1d247525{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_0_0_jar-_-any-11945793044662923461.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-29 21:32:50 INFO  AbstractConnector:330 - Started ServerConnector@f4a3a8d{HTTP/1.1,[http/1.1]}{0.0.0.0:9874}
om_1                | 2023-06-29 21:32:50 INFO  Server:399 - Started @4857ms
om_1                | 2023-06-29 21:32:50 INFO  MetricsSinkAdapter:204 - Sink prometheus started
om_1                | 2023-06-29 21:32:50 INFO  MetricsSystemImpl:301 - Registered sink prometheus
om_1                | 2023-06-29 21:32:50 INFO  BaseHttpServer:327 - HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-29 21:32:50 INFO  Server:1460 - IPC Server Responder: starting
om_1                | 2023-06-29 21:32:50 INFO  Server:1298 - IPC Server listener on 9862: starting
om_1                | 2023-06-29 21:32:50 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
om_1                | 2023-06-29 21:33:44 INFO  OMDBCheckpointServlet:101 - Received request to obtain OM DB checkpoint snapshot
om_1                | 2023-06-29 21:33:44 INFO  RDBCheckpointManager:86 - Created checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1688074424515 in 7 milliseconds
om_1                | 2023-06-29 21:33:44 INFO  OMDBCheckpointServlet:144 - Time taken to write the checkpoint to response output stream: 20 milliseconds
om_1                | 2023-06-29 21:33:44 INFO  RocksDBCheckpoint:78 - Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/rdb_rdb_checkpoint_1688074424515
om_1                | 2023-06-29 21:33:53 INFO  OMVolumeCreateRequest:195 - created volume:vol1 for user:hadoop
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:32:22 INFO  ReconServer:112 - STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 0d1e6c1aa2ba/172.20.0.6
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.0.0
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:32:24 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
s3g_1               | 2023-06-29 21:32:24 INFO  BaseHttpServer:207 - Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-29 21:32:24 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-29 21:32:25 INFO  log:169 - Logging initialized @6215ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-29 21:32:25 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-29 21:32:25 INFO  HttpRequestLog:86 - Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-29 21:32:25 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-29 21:32:25 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-29 21:32:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-29 21:32:25 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-29 21:32:26 INFO  Gateway:112 - STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 2d9be02e67b0/172.20.0.12
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:32:24 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = ae5de228b3ce/172.20.0.3
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.0.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:32:24 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:32:24 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:32:25 INFO  StorageContainerManager:644 - SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-eae7dbfb-6e39-4100-a04c-3a4697ed42e2;layoutVersion=0
scm_1               | 2023-06-29 21:32:25 INFO  StorageContainerManagerStarter:124 - SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at ae5de228b3ce/172.20.0.3
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the HADOOP_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:32:35 INFO  StorageContainerManagerStarter:112 - STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = ae5de228b3ce/172.20.0.3
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.0.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.5.RELEASE.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.0.0.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
recon_1             | STARTUP_MSG:   java = 11.0.3
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:32:22 INFO  ReconServer:90 - registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:32:25 INFO  ReconRestServletModule:75 - rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-29 21:32:27 INFO  ReconServer:93 - Initializing Recon server...
recon_1             | 2023-06-29 21:32:28 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:32:33 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:32:35 INFO  DerbyDataSourceProvider:50 - JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:32:35 INFO  SqlDbUtils:67 - Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:32:35 INFO  ReconServer:101 - Creating Recon Schema.
recon_1             | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
recon_1             | 2023-06-29 21:32:39 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
recon_1             | 2023-06-29 21:32:39 INFO  BaseHttpServer:207 - Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:32:39 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:32:39 INFO  log:169 - Logging initialized @20613ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:32:39 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-29 21:32:39 WARN  HttpRequestLog:103 - Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-29 21:32:39 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:32:39 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:32:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:32:39 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:32:39 INFO  ReconTaskControllerImpl:79 - Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:32:40 INFO  ReconTaskControllerImpl:79 - Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:32:40 INFO  OmUtils:550 - ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:32:40 INFO  OmUtils:569 - No OzoneManager ServiceID configured.
recon_1             | 2023-06-29 21:32:40 INFO  deprecation:1395 - No unit for recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-29 21:32:41 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:32:41 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:32:41 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@5002fde9
recon_1             | 2023-06-29 21:32:41 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
recon_1             | 2023-06-29 21:32:41 WARN  DBStoreBuilder:277 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:32:41 INFO  SCMNodeManager:116 - Entering startup safe mode.
recon_1             | 2023-06-29 21:32:41 WARN  ReconUtils:85 - ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:32:41 INFO  ReconNodeManager:100 - Loaded 0 nodes from node DB.
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0-tests.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jettison-1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jersey-json-1.19.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:04Z
scm_1               | STARTUP_MSG:   java = 11.0.3
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:32:35 INFO  StorageContainerManagerStarter:90 - registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:32:36 WARN  ServerUtils:148 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:32:37 WARN  DBStoreBuilder:277 - ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:32:38 INFO  NodeSchemaLoader:126 - Loading file from java.lang.CompoundEnumeration@7b4c50bc
scm_1               | 2023-06-29 21:32:38 INFO  NodeSchemaLoader:172 - Loading network topology layer schema file
scm_1               | 2023-06-29 21:32:38 INFO  SCMNodeManager:116 - Entering startup safe mode.
scm_1               | 2023-06-29 21:32:39 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-29 21:32:39 INFO  SCMPipelineManager:161 - No pipeline exists in current db
scm_1               | 2023-06-29 21:32:39 INFO  HealthyPipelineSafeModeRule:89 - Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:32:39 INFO  OneReplicaPipelineSafeModeRule:79 - Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:32:39 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
scm_1               | 2023-06-29 21:32:40 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:32:40 INFO  Server:1219 - Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-29 21:32:40 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:32:40 INFO  Server:1219 - Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:32:40 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:32:40 INFO  Server:1219 - Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:32:40 INFO  BaseHttpServer:207 - Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:32:40 INFO  BaseHttpServer:106 - Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:32:40 INFO  log:169 - Logging initialized @14491ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:32:41 INFO  AuthenticationFilter:240 - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-29 21:32:41 INFO  HttpRequestLog:86 - Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:32:41 INFO  HttpServer2:1019 - Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:32:41 INFO  HttpServer2:995 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:32:41 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:32:41 INFO  HttpServer2:1003 - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:32:41 INFO  StorageContainerManager:784 - StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:32:41 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:32:41 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:32:41 INFO  MetricsSystemImpl:191 - StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:32:41 INFO  SCMClientProtocolServer:156 - RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:32:41 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-29 21:32:41 INFO  Server:1298 - IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:32:42 INFO  StorageContainerManager:796 - ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:32:42 INFO  SCMBlockProtocolServer:149 - RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:32:42 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-29 21:32:42 INFO  Server:1298 - IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:32:42 INFO  StorageContainerManager:802 - ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:32:42 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:32:42 INFO  Server:1460 - IPC Server Responder: starting
scm_1               | 2023-06-29 21:32:42 INFO  Server:1298 - IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:32:42 INFO  HttpServer2:1237 - Jetty bound to port 9876
scm_1               | 2023-06-29 21:32:42 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
scm_1               | 2023-06-29 21:32:42 INFO  session:333 - DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:32:42 INFO  session:338 - No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:32:42 INFO  session:140 - node0 Scavenging every 660000ms
scm_1               | 2023-06-29 21:32:42 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@66ba7e45{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:32:42 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7573e12f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:32:42 WARN  Server:1670 - IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.20.0.13:39592: output error
scm_1               | 2023-06-29 21:32:42 INFO  Server:2928 - IPC Server handler 2 on default port 9861 caught an exception
scm_1               | java.nio.channels.AsynchronousCloseException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3550)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:139)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1620)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1690)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2785)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1762)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1081)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:873)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:859)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1016)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
scm_1               | 2023-06-29 21:32:43 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@e9ef5b6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_0_0_jar-_-any-1010725713166169748.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.0.0.jar!/webapps/scm}
scm_1               | 2023-06-29 21:32:43 INFO  AbstractConnector:330 - Started ServerConnector@5f80fa43{HTTP/1.1,[http/1.1]}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:32:43 INFO  Server:399 - Started @17029ms
scm_1               | 2023-06-29 21:32:43 INFO  MetricsSinkAdapter:204 - Sink prometheus started
scm_1               | 2023-06-29 21:32:43 INFO  MetricsSystemImpl:301 - Registered sink prometheus
scm_1               | 2023-06-29 21:32:43 INFO  BaseHttpServer:327 - HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:32:43 INFO  JvmPauseMonitor:188 - Starting JVM pause monitor
scm_1               | 2023-06-29 21:33:39 INFO  NetworkTopology:111 - Added a new node: /default-rack/b144bd75-2038-4955-ba48-81a35f17a5f3
scm_1               | 2023-06-29 21:33:39 INFO  SCMNodeManager:273 - Registered Data node : b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-29 21:33:39 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=4a53da16-4e4e-4fc2-bd46-51c27043fec7 to datanode:b144bd75-2038-4955-ba48-81a35f17a5f3
scm_1               | 2023-06-29 21:33:39 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 4a53da16-4e4e-4fc2-bd46-51c27043fec7, Nodes: b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:39.961057Z]
scm_1               | 2023-06-29 21:33:39 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:33:39 INFO  SCMSafeModeManager:71 - SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:33:40 INFO  NetworkTopology:111 - Added a new node: /default-rack/6bdda10c-e08d-4091-bed4-5af1eefff6f7
scm_1               | 2023-06-29 21:33:40 INFO  SCMNodeManager:273 - Registered Data node : 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:71 - SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:33:40 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=5d9399ba-c25d-492e-af9f-f43302caa785 to datanode:6bdda10c-e08d-4091-bed4-5af1eefff6f7
scm_1               | 2023-06-29 21:33:40 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 5d9399ba-c25d-492e-af9f-f43302caa785, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:40.277317Z]
scm_1               | 2023-06-29 21:33:40 INFO  NetworkTopology:111 - Added a new node: /default-rack/b93369e5-9e0d-4f8b-a23c-f02b9bda1293
scm_1               | 2023-06-29 21:33:40 INFO  SCMNodeManager:273 - Registered Data node : b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
scm_1               | 2023-06-29 21:33:40 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=6b3b5230-96ba-443e-ad87-932412adcda5 to datanode:b93369e5-9e0d-4f8b-a23c-f02b9bda1293
scm_1               | 2023-06-29 21:33:40 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 6b3b5230-96ba-443e-ad87-932412adcda5, Nodes: b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:40.716070Z]
recon_1             | 2023-06-29 21:32:41 INFO  ContainerPlacementPolicyFactory:60 - Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-29 21:32:41 INFO  CallQueueManager:84 - Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:32:41 INFO  Server:1219 - Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-29 21:32:41 INFO  SCMPipelineManager:161 - No pipeline exists in current db
recon_1             | 2023-06-29 21:32:41 INFO  ReconServer:109 - Recon server initialized successfully!
recon_1             | 2023-06-29 21:32:41 INFO  ReconServer:134 - Starting Recon server
recon_1             | 2023-06-29 21:32:41 INFO  MetricsConfig:118 - Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-29 21:32:41 INFO  MetricsSystemImpl:374 - Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-29 21:32:41 INFO  MetricsSystemImpl:191 - Recon metrics system started
recon_1             | 2023-06-29 21:32:42 INFO  HttpServer2:1237 - Jetty bound to port 9888
recon_1             | 2023-06-29 21:32:42 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
recon_1             | 2023-06-29 21:32:42 INFO  session:333 - DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-29 21:32:42 INFO  session:338 - No SessionScavenger set, using defaults
recon_1             | 2023-06-29 21:32:42 INFO  session:140 - node0 Scavenging every 600000ms
recon_1             | 2023-06-29 21:32:42 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@723877dd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-29 21:32:42 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@6d229b1c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-29 21:32:44 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@40b01718{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_0_0_jar-_-any-3536525717873591971.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.0.0.jar!/webapps/recon}
recon_1             | 2023-06-29 21:32:44 INFO  AbstractConnector:330 - Started ServerConnector@62765aec{HTTP/1.1,[http/1.1]}{0.0.0.0:9888}
recon_1             | 2023-06-29 21:32:44 INFO  Server:399 - Started @25754ms
recon_1             | 2023-06-29 21:32:44 INFO  MetricsSinkAdapter:204 - Sink prometheus started
recon_1             | 2023-06-29 21:32:44 INFO  MetricsSystemImpl:301 - Registered sink prometheus
recon_1             | 2023-06-29 21:32:44 INFO  BaseHttpServer:327 - HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-29 21:32:44 INFO  OzoneManagerServiceProviderImpl:198 - Starting Ozone Manager Service Provider.
recon_1             | 2023-06-29 21:32:44 INFO  OzoneManagerServiceProviderImpl:176 - Registered OmDeltaRequest task 
recon_1             | 2023-06-29 21:32:44 INFO  OzoneManagerServiceProviderImpl:186 - Registered OmSnapshotRequest task 
recon_1             | 2023-06-29 21:32:44 INFO  ReconOmMetadataManagerImpl:65 - Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-29 21:32:44 WARN  ReconUtils:85 - ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:32:44 INFO  ReconTaskControllerImpl:221 - Starting Recon Task Controller.
recon_1             | 2023-06-29 21:32:44 INFO  ReconStorageContainerManagerFacade:206 - Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:32:44 INFO  ReconStorageContainerManagerFacade:256 - Obtained 0 pipelines from SCM.
recon_1             | 2023-06-29 21:32:44 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:32:44 INFO  SCMDatanodeProtocolServer:172 - RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:32:44 INFO  Server:1460 - IPC Server Responder: starting
recon_1             | 2023-06-29 21:32:44 INFO  Server:1298 - IPC Server listener on 9891: starting
recon_1             | 2023-06-29 21:32:44 INFO  ReconScmTask:46 - Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:32:44 INFO  ReconScmTask:56 - Starting PipelineSyncTask Thread.
recon_1             | 2023-06-29 21:32:44 INFO  ReconScmTask:46 - Registered ContainerHealthTask task 
recon_1             | 2023-06-29 21:32:44 INFO  ReconScmTask:56 - Starting ContainerHealthTask Thread.
recon_1             | 2023-06-29 21:32:44 INFO  ReconPipelineManager:83 - Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:32:44 INFO  PipelineSyncTask:61 - Pipeline sync Thread took 8 milliseconds.
recon_1             | 2023-06-29 21:32:44 INFO  ContainerHealthTask:77 - Container Health task thread took 57 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:32:44 INFO  ContainerHealthTask:86 - Container Health task thread took 4 milliseconds for processing 0 containers.
recon_1             | 2023-06-29 21:33:41 INFO  NetworkTopology:111 - Added a new node: /default-rack/b144bd75-2038-4955-ba48-81a35f17a5f3
recon_1             | 2023-06-29 21:33:41 INFO  SCMNodeManager:273 - Registered Data node : b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:41 INFO  ReconNodeManager:116 - Adding new node b144bd75-2038-4955-ba48-81a35f17a5f3 to Node DB.
recon_1             | 2023-06-29 21:33:42 INFO  NetworkTopology:111 - Added a new node: /default-rack/6bdda10c-e08d-4091-bed4-5af1eefff6f7
recon_1             | 2023-06-29 21:33:42 INFO  SCMNodeManager:273 - Registered Data node : 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:42 INFO  ReconNodeManager:116 - Adding new node 6bdda10c-e08d-4091-bed4-5af1eefff6f7 to Node DB.
recon_1             | 2023-06-29 21:33:42 INFO  NetworkTopology:111 - Added a new node: /default-rack/b93369e5-9e0d-4f8b-a23c-f02b9bda1293
recon_1             | 2023-06-29 21:33:42 INFO  SCMNodeManager:273 - Registered Data node : b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:42 INFO  ReconNodeManager:116 - Adding new node b93369e5-9e0d-4f8b-a23c-f02b9bda1293 to Node DB.
recon_1             | 2023-06-29 21:33:42 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=4a53da16-4e4e-4fc2-bd46-51c27043fec7. Trying to get from SCM.
recon_1             | 2023-06-29 21:33:42 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 4a53da16-4e4e-4fc2-bd46-51c27043fec7, Nodes: b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b144bd75-2038-4955-ba48-81a35f17a5f3, CreationTimestamp2023-06-29T21:33:39.961Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:33:42 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 4a53da16-4e4e-4fc2-bd46-51c27043fec7, Nodes: b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b144bd75-2038-4955-ba48-81a35f17a5f3, CreationTimestamp2023-06-29T21:33:39.961Z]
recon_1             | 2023-06-29 21:33:42 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce. Trying to get from SCM.
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:71 - SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:214 - DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:242 - All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:33:40 INFO  SCMSafeModeManager:214 - ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:33:40 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce to datanode:6bdda10c-e08d-4091-bed4-5af1eefff6f7
scm_1               | 2023-06-29 21:33:40 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce to datanode:b144bd75-2038-4955-ba48-81a35f17a5f3
scm_1               | 2023-06-29 21:33:40 INFO  RatisPipelineProvider:138 - Sending CreatePipelineCommand for pipeline:PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce to datanode:b93369e5-9e0d-4f8b-a23c-f02b9bda1293
scm_1               | 2023-06-29 21:33:40 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 1b4e442b-1aab-42ee-b03b-e566274547ce, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:40.730380Z]
scm_1               | 2023-06-29 21:33:42 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 4a53da16-4e4e-4fc2-bd46-51c27043fec7, Nodes: b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:b144bd75-2038-4955-ba48-81a35f17a5f3, CreationTimestamp2023-06-29T21:33:39.961057Z] moved to OPEN state
scm_1               | 2023-06-29 21:33:42 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:33:42 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:33:43 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 5d9399ba-c25d-492e-af9f-f43302caa785, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6bdda10c-e08d-4091-bed4-5af1eefff6f7, CreationTimestamp2023-06-29T21:33:40.277317Z] moved to OPEN state
scm_1               | 2023-06-29 21:33:43 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:33:43 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:33:43 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 6b3b5230-96ba-443e-ad87-932412adcda5, Nodes: b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:b93369e5-9e0d-4f8b-a23c-f02b9bda1293, CreationTimestamp2023-06-29T21:33:40.716070Z] moved to OPEN state
scm_1               | 2023-06-29 21:33:43 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:33:43 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:33:48 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 1b4e442b-1aab-42ee-b03b-e566274547ce, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:b144bd75-2038-4955-ba48-81a35f17a5f3, CreationTimestamp2023-06-29T21:33:40.730380Z] moved to OPEN state
scm_1               | 2023-06-29 21:33:48 INFO  SCMSafeModeManager:129 - SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:33:48 INFO  SCMSafeModeManager:214 - AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:33:48 INFO  SCMSafeModeManager:214 - HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:33:48 INFO  SCMSafeModeManager:228 - ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:33:48 INFO  SCMSafeModeManager:257 - SCM exiting safe mode.
scm_1               | 2023-06-29 21:34:21 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-29 21:34:30 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-29 21:35:17 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
scm_1               | 2023-06-29 21:35:25 WARN  SCMNodeManager:653 - Cannot find node for address 172.20.0.4
recon_1             | 2023-06-29 21:33:42 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 1b4e442b-1aab-42ee-b03b-e566274547ce, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:40.730Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:33:42 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 1b4e442b-1aab-42ee-b03b-e566274547ce, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:33:40.730Z]
recon_1             | 2023-06-29 21:33:42 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce reported by b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=5d9399ba-c25d-492e-af9f-f43302caa785. Trying to get from SCM.
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 5d9399ba-c25d-492e-af9f-f43302caa785, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6bdda10c-e08d-4091-bed4-5af1eefff6f7, CreationTimestamp2023-06-29T21:33:40.277Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:33:43 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 5d9399ba-c25d-492e-af9f-f43302caa785, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6bdda10c-e08d-4091-bed4-5af1eefff6f7, CreationTimestamp2023-06-29T21:33:40.277Z]
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:83 - Pipeline ONE PipelineID=5d9399ba-c25d-492e-af9f-f43302caa785 reported by 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:43 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 5d9399ba-c25d-492e-af9f-f43302caa785, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:6bdda10c-e08d-4091-bed4-5af1eefff6f7, CreationTimestamp2023-06-29T21:33:40.277Z] moved to OPEN state
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce reported by 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:63 - Unknown pipeline PipelineID=6b3b5230-96ba-443e-ad87-932412adcda5. Trying to get from SCM.
recon_1             | 2023-06-29 21:33:43 INFO  ReconPipelineReportHandler:66 - Adding new pipeline Pipeline[ Id: 6b3b5230-96ba-443e-ad87-932412adcda5, Nodes: b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b93369e5-9e0d-4f8b-a23c-f02b9bda1293, CreationTimestamp2023-06-29T21:33:40.716Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:33:43 INFO  PipelineStateManager:54 - Created pipeline Pipeline[ Id: 6b3b5230-96ba-443e-ad87-932412adcda5, Nodes: b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:b93369e5-9e0d-4f8b-a23c-f02b9bda1293, CreationTimestamp2023-06-29T21:33:40.716Z]
recon_1             | 2023-06-29 21:33:44 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce reported by b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:44 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:33:44 INFO  OzoneManagerServiceProviderImpl:409 - Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:33:44 INFO  OzoneManagerServiceProviderImpl:316 - Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688074424426
recon_1             | 2023-06-29 21:33:44 INFO  ReconOmMetadataManagerImpl:91 - Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688074424426.
recon_1             | 2023-06-29 21:33:44 INFO  OzoneManagerServiceProviderImpl:421 - Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:33:44 INFO  ContainerKeyMapperTask:73 - Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:33:44 INFO  ContainerDBServiceProviderImpl:117 - Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1688074424696
recon_1             | 2023-06-29 21:33:44 INFO  ContainerDBServiceProviderImpl:122 - Cleaning up old Recon Container DB at /data/metadata/recon/recon-container-key.db_1688074347695.
recon_1             | 2023-06-29 21:33:44 INFO  ContainerKeyMapperTask:89 - Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:33:44 INFO  ContainerKeyMapperTask:92 - It took me 0.096 seconds to process 0 keys.
recon_1             | 2023-06-29 21:33:44 INFO  FileSizeCountTask:102 - Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-29 21:33:48 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce reported by b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:48 INFO  ReconPipelineReportHandler:83 - Pipeline THREE PipelineID=1b4e442b-1aab-42ee-b03b-e566274547ce reported by b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}
recon_1             | 2023-06-29 21:33:48 INFO  PipelineStateManager:131 - Pipeline Pipeline[ Id: 1b4e442b-1aab-42ee-b03b-e566274547ce, Nodes: 6bdda10c-e08d-4091-bed4-5af1eefff6f7{ip: 172.20.0.13, host: xcompat_datanode_1.xcompat_default, networkLocation: /default-rack, certSerialId: null}b144bd75-2038-4955-ba48-81a35f17a5f3{ip: 172.20.0.9, host: xcompat_datanode_3.xcompat_default, networkLocation: /default-rack, certSerialId: null}b93369e5-9e0d-4f8b-a23c-f02b9bda1293{ip: 172.20.0.10, host: xcompat_datanode_2.xcompat_default, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:b144bd75-2038-4955-ba48-81a35f17a5f3, CreationTimestamp2023-06-29T21:33:40.730Z] moved to OPEN state
recon_1             | 2023-06-29 21:33:55 INFO  ReconContainerManager:89 - New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-29 21:33:55 INFO  ReconContainerManager:157 - Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:34:04 INFO  ReconContainerManager:89 - New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-29 21:34:04 INFO  ReconContainerManager:157 - Successfully added container #2 to Recon.
recon_1             | 2023-06-29 21:34:44 INFO  OzoneManagerServiceProviderImpl:374 - Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:34:44 INFO  OzoneManagerServiceProviderImpl:384 - Obtaining delta updates from Ozone Manager
recon_1             | 2023-06-29 21:34:44 INFO  OzoneManagerServiceProviderImpl:350 - Number of updates received from OM : 16
recon_1             | 2023-06-29 21:34:44 INFO  ContainerKeyMapperTask:151 - ContainerKeyMapperTask successfully processed 11 OM DB update event(s).
recon_1             | 2023-06-29 21:34:45 INFO  FileSizeCountTask:159 - Completed a 'process' run of FileSizeCountTask.
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/hadoop/common/*:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.10.3.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-3.10.5.Final.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.3.50.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-1.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/ratis-netty-1.0.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.11.0.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-server-1.0.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.60.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.0.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.2.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.10.3.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.11.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.3.50.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.10.3.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.16.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.26.v20200117.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/picocli-3.9.6.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-all-4.1.48.Final.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-1.0.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.10.3.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.25.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-1.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.4.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/hadoop-ozone.git/28d372ca903b4741131bace09e0339e9161257bb ; compiled by 'sammi' on 2020-08-25T13:05Z
s3g_1               | STARTUP_MSG:   java = 11.0.3
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:32:26 INFO  Gateway:90 - registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-29 21:32:26 INFO  Gateway:68 - Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:32:26 INFO  HttpServer2:1237 - Jetty bound to port 9878
s3g_1               | 2023-06-29 21:32:26 INFO  Server:359 - jetty-9.4.26.v20200117; built: 2020-01-17T12:35:33.676Z; git: 7b38981d25d14afb4a12ff1f2596756144edf695; jvm 11.0.3+7-LTS
s3g_1               | 2023-06-29 21:32:26 INFO  session:333 - DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-29 21:32:26 INFO  session:338 - No SessionScavenger set, using defaults
s3g_1               | 2023-06-29 21:32:26 INFO  session:140 - node0 Scavenging every 660000ms
s3g_1               | 2023-06-29 21:32:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@7dc19a70{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:32:26 INFO  ContextHandler:825 - Started o.e.j.s.ServletContextHandler@23941fb4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/static,AVAILABLE}
s3g_1               | ERROR StatusLogger No Log4j 2 configuration file found. Using default configuration (logging only errors to the console), or user programmatically provided configurations. Set system property 'log4j2.debug' to show Log4j 2 internal initialization logging. See https://logging.apache.org/log4j/2.x/manual/configuration.html for instructions on how to configure Log4j 2
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 29, 2023 9:32:40 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-29 21:32:40 INFO  ContextHandler:825 - Started o.e.j.w.WebAppContext@93501be{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_0_0_jar-_-any-7218009148890713402.dir/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.0.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-29 21:32:40 INFO  AbstractConnector:330 - Started ServerConnector@81d9a72{HTTP/1.1,[http/1.1]}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:32:40 INFO  Server:399 - Started @21508ms
s3g_1               | 2023-06-29 21:32:40 INFO  BaseHttpServer:327 - HTTP server of s3gateway listening at http://0.0.0.0:9878
Attaching to xcompat_old_client_1_1_0_1, xcompat_datanode_3, xcompat_datanode_2, xcompat_old_client_1_0_0_1, xcompat_new_client_1, xcompat_datanode_1, xcompat_recon_1, xcompat_scm_1, xcompat_om_1, xcompat_old_client_1_3_0_1, xcompat_s3g_1, xcompat_old_client_1_2_1_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:35:43,910 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = ed11f18edca2/172.21.0.7
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.1.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_1          | STARTUP_MSG:   java = 11.0.10
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:35:43,940 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:35:45,442 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:35:45,786 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:35:46,464 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:35:46,464 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:35:47,007 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ed11f18edca2 ip:172.21.0.7
datanode_1          | 2023-06-29 21:35:48,049 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:35:48,056 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_1          | 2023-06-29 21:35:48,071 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:35:48,112 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:35:48,275 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:35:48,436 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:35:48,449 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:35:48,449 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:35:55,530 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:35:55,781 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:35:56,298 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-29 21:35:56,299 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-29 21:35:56,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-29 21:35:56,304 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:35:56,324 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:35:56,324 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:35:56,347 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:35:58,113 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-29 21:35:58,160 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:35:58,165 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:35:58,304 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:35:58,318 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:35:58,846 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:35:58,994 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:35:59,139 [main] INFO util.log: Logging initialized @20267ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:35:59,656 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-29 21:35:59,665 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:35:59,708 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:35:59,718 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:35:59,722 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:35:59,722 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:35:59,950 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-29 21:35:59,951 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_1          | 2023-06-29 21:36:00,087 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:36:00,087 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:36:00,096 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-29 21:36:00,136 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:36:00,137 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:36:01,734 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-13873213924252051419/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:36:01,852 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:36:01,853 [main] INFO server.Server: Started @22993ms
datanode_1          | 2023-06-29 21:36:01,856 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-29 21:36:01,856 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-29 21:36:01,861 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:36:01,956 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5bad0af7] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-29 21:36:02,338 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.5:9891
datanode_1          | 2023-06-29 21:36:02,544 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:36:06,241 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-29 21:36:06,242 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-29 21:36:06,622 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:06,713 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.RaftServer: d8c9d548-bd9b-475e-a5cc-64194115352c: start RPC server
datanode_1          | 2023-06-29 21:36:06,734 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: d8c9d548-bd9b-475e-a5cc-64194115352c: GrpcService started, listening on 9856
datanode_1          | 2023-06-29 21:36:06,735 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: d8c9d548-bd9b-475e-a5cc-64194115352c: GrpcService started, listening on 9857
datanode_1          | 2023-06-29 21:36:06,740 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: d8c9d548-bd9b-475e-a5cc-64194115352c: GrpcService started, listening on 9858
datanode_1          | 2023-06-29 21:36:06,772 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$283/0x000000084046ec40@5fe8ed44] INFO util.JvmPauseMonitor: JvmPauseMonitor-d8c9d548-bd9b-475e-a5cc-64194115352c: Started
datanode_1          | 2023-06-29 21:36:06,774 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d8c9d548-bd9b-475e-a5cc-64194115352c is started using port 9858 for RATIS
datanode_1          | 2023-06-29 21:36:06,774 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d8c9d548-bd9b-475e-a5cc-64194115352c is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-29 21:36:06,774 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis d8c9d548-bd9b-475e-a5cc-64194115352c is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-29 21:36:10,992 [Command processor thread] INFO server.RaftServer: d8c9d548-bd9b-475e-a5cc-64194115352c: addNew group-B62C0B0740E2:[d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] returns group-B62C0B0740E2:java.util.concurrent.CompletableFuture@7e1059ae[Not completed]
datanode_1          | 2023-06-29 21:36:11,011 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c: new RaftServerImpl for group-B62C0B0740E2:[d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:36:11,027 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:36:11,033 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:36:11,033 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:36:11,036 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:11,036 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:36:11,036 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:36:11,036 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:36:11,049 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: ConfigurationManager, init=-1: [d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:36:11,064 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:36:11,074 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:36:11,088 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2 does not exist. Creating ...
datanode_1          | 2023-06-29 21:36:11,099 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2/in_use.lock acquired by nodename 7@ed11f18edca2
datanode_1          | 2023-06-29 21:36:11,135 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2 has been successfully formatted.
datanode_1          | 2023-06-29 21:36:11,151 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-B62C0B0740E2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:36:11,191 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:11,206 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:36:11,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:36:11,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:36:11,268 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2
datanode_1          | 2023-06-29 21:36:11,288 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,319 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:36:11,332 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:36:11,354 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2
datanode_1          | 2023-06-29 21:36:11,355 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:36:11,378 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:36:11,379 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,388 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:36:11,389 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:36:11,391 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:36:11,392 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:36:11,393 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:36:11,410 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,415 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:11,433 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:36:11,435 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:35:44,099 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 307fd7392124/172.21.0.13
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.1.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_3          | STARTUP_MSG:   java = 11.0.10
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:35:44,139 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:35:45,535 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:35:46,003 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:35:46,812 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:35:46,812 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:35:47,306 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:307fd7392124 ip:172.21.0.13
datanode_3          | 2023-06-29 21:35:48,315 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:35:48,326 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_3          | 2023-06-29 21:35:48,330 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:35:48,390 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:35:48,453 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:35:48,712 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:35:48,712 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:35:48,719 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:35:55,500 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:35:55,865 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:35:56,432 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-29 21:35:56,433 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-29 21:35:56,434 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-29 21:35:56,444 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:35:56,446 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:35:56,462 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:35:56,481 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:35:58,138 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-29 21:35:58,200 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:35:58,216 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:35:58,383 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:35:58,457 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:35:58,949 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:35:59,090 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:35:59,243 [main] INFO util.log: Logging initialized @20082ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:35:59,817 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-29 21:35:59,857 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:35:59,899 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:35:59,911 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:35:59,932 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:35:59,932 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:36:00,229 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-29 21:36:00,231 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_3          | 2023-06-29 21:36:00,425 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:36:00,425 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:36:00,441 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-29 21:36:00,618 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:36:00,621 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:36:01,812 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-849566332645863353/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:36:01,865 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:36:01,866 [main] INFO server.Server: Started @22705ms
datanode_3          | 2023-06-29 21:36:01,891 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-29 21:36:01,897 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-29 21:36:01,899 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:36:02,013 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@54249e03] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-29 21:36:02,360 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.5:9891
datanode_3          | 2023-06-29 21:36:02,548 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-29 21:36:06,317 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-29 21:36:06,329 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-29 21:36:06,608 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:06,662 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.RaftServer: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start RPC server
datanode_3          | 2023-06-29 21:36:06,670 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: GrpcService started, listening on 9856
datanode_3          | 2023-06-29 21:36:06,672 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:36:06,672 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: GrpcService started, listening on 9858
datanode_3          | 2023-06-29 21:36:06,684 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22521fb4-36a7-41c6-aa9d-d3f5477b102c is started using port 9858 for RATIS
datanode_3          | 2023-06-29 21:36:06,684 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22521fb4-36a7-41c6-aa9d-d3f5477b102c is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-29 21:36:06,684 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@65229a42] INFO util.JvmPauseMonitor: JvmPauseMonitor-22521fb4-36a7-41c6-aa9d-d3f5477b102c: Started
datanode_3          | 2023-06-29 21:36:06,684 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 22521fb4-36a7-41c6-aa9d-d3f5477b102c is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-29 21:36:08,031 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_3          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_3          | Caused by: java.util.concurrent.TimeoutException
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	... 1 more
datanode_3          | 2023-06-29 21:36:11,084 [Command processor thread] INFO server.RaftServer: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: addNew group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-2BB9FCDAB2FB:java.util.concurrent.CompletableFuture@4db73328[Not completed]
datanode_3          | 2023-06-29 21:36:11,117 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: new RaftServerImpl for group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:36:11,131 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:36:11,148 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:36:11,148 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:36:11,148 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:11,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:36:11,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:36:11,149 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:36:11,167 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:36:11,172 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:36:11,175 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:36:11,183 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb does not exist. Creating ...
datanode_3          | 2023-06-29 21:36:11,221 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/in_use.lock acquired by nodename 7@307fd7392124
datanode_3          | 2023-06-29 21:36:11,238 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb has been successfully formatted.
datanode_3          | 2023-06-29 21:36:11,253 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-2BB9FCDAB2FB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:36:11,295 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:11,313 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:36:11,341 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:36:11,348 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:36:11,363 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB
datanode_3          | 2023-06-29 21:36:11,398 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:11,427 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:36:11,443 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:36:11,462 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb
datanode_3          | 2023-06-29 21:36:11,463 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:36:11,468 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:36:11,468 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:11,469 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:36:11,470 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:36:11,471 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:36:11,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:36:11,473 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:36:11,496 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:11,498 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:11,517 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:11,521 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:11,527 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:11,533 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:36:11,535 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:36:11,538 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:36:11,540 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:36:11,543 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:36:11,606 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB
datanode_3          | 2023-06-29 21:36:11,623 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB
datanode_3          | 2023-06-29 21:36:11,642 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:11,644 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:36:11,652 [pool-19-thread-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:11,664 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2BB9FCDAB2FB,id=22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:11,668 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB
datanode_3          | 2023-06-29 21:36:11,851 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-4EA4D984B684->43d92334-42eb-45fb-a952-310405367e2b
datanode_3          | 2023-06-29 21:36:13,771 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-C91EE4D212FD->d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_3          | 2023-06-29 21:36:14,002 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:14,084 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb.
datanode_3          | 2023-06-29 21:36:14,085 [Command processor thread] INFO server.RaftServer: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: addNew group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-CF122FADA941:java.util.concurrent.CompletableFuture@3e634393[Not completed]
datanode_3          | 2023-06-29 21:36:14,090 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: new RaftServerImpl for group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:36:14,091 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:36:14,092 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:36:14,092 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:36:14,092 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:14,092 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:36:11,454 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:11,471 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:36:11,472 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:36:11,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:36:11,492 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:36:11,495 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:36:11,562 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2
datanode_1          | 2023-06-29 21:36:11,567 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2
datanode_1          | 2023-06-29 21:36:11,591 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: start as a follower, conf=-1: [d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:36:11,594 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:36:11,596 [pool-19-thread-1] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState
datanode_1          | 2023-06-29 21:36:11,612 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B62C0B0740E2,id=d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:11,617 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2
datanode_1          | 2023-06-29 21:36:11,685 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2.
datanode_1          | 2023-06-29 21:36:11,688 [Command processor thread] INFO server.RaftServer: d8c9d548-bd9b-475e-a5cc-64194115352c: addNew group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-2BB9FCDAB2FB:java.util.concurrent.CompletableFuture@428747ba[Not completed]
datanode_1          | 2023-06-29 21:36:11,712 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c: new RaftServerImpl for group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:36:11,718 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:36:11,718 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:36:11,718 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:36:11,718 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:11,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:36:11,719 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:36:11,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:36:11,722 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:36:11,722 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:36:11,723 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:36:11,725 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb does not exist. Creating ...
datanode_1          | 2023-06-29 21:36:11,738 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/in_use.lock acquired by nodename 7@ed11f18edca2
datanode_1          | 2023-06-29 21:36:11,740 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb has been successfully formatted.
datanode_1          | 2023-06-29 21:36:11,753 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-2BB9FCDAB2FB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:36:11,758 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:11,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:36:11,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:36:11,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:36:11,759 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB
datanode_1          | 2023-06-29 21:36:11,759 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:36:11,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:36:11,761 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb
datanode_1          | 2023-06-29 21:36:11,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:36:11,761 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:36:11,762 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,762 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:36:11,764 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:36:11,768 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:36:11,768 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:36:11,768 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:36:11,769 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:11,773 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:11,773 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:36:11,774 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:36:11,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:11,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:36:11,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:36:11,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:36:11,780 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:36:11,782 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:36:11,782 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB
datanode_1          | 2023-06-29 21:36:11,783 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB
datanode_1          | 2023-06-29 21:36:11,788 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:11,789 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:36:11,789 [pool-19-thread-1] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:11,792 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2BB9FCDAB2FB,id=d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:11,792 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB
datanode_1          | 2023-06-29 21:36:11,904 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-B48FA25ADA1C->43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:13,559 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:13,874 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-3FE9DEAA6CB8->22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_1          | 2023-06-29 21:36:14,197 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb.
datanode_1          | 2023-06-29 21:36:14,199 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c: new RaftServerImpl for group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:36:14,199 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:36:14,199 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:36:14,199 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:36:14,199 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:36:14,199 [Command processor thread] INFO server.RaftServer: d8c9d548-bd9b-475e-a5cc-64194115352c: addNew group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-CF122FADA941:java.util.concurrent.CompletableFuture@354a67de[Not completed]
datanode_1          | 2023-06-29 21:36:14,201 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:36:14,201 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:36:14,201 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:36:14,201 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 does not exist. Creating ...
datanode_1          | 2023-06-29 21:36:14,203 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/in_use.lock acquired by nodename 7@ed11f18edca2
datanode_1          | 2023-06-29 21:36:14,204 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 has been successfully formatted.
datanode_1          | 2023-06-29 21:36:14,206 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-CF122FADA941: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:36:14,223 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:36:14,224 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:36:14,225 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:36:14,225 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:36:14,226 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941
datanode_1          | 2023-06-29 21:36:14,226 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:14,231 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:36:14,234 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:36:14,234 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941
datanode_1          | 2023-06-29 21:36:14,234 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:36:14,092 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:36:14,094 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:36:14,094 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:36:14,095 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:36:14,095 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:36:14,096 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 does not exist. Creating ...
datanode_3          | 2023-06-29 21:36:14,098 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/in_use.lock acquired by nodename 7@307fd7392124
datanode_3          | 2023-06-29 21:36:14,116 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 has been successfully formatted.
datanode_3          | 2023-06-29 21:36:14,117 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-CF122FADA941: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:36:14,118 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:14,118 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:36:14,123 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:36:14,123 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:36:14,124 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:14,124 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,126 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:36:14,126 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:36:14,127 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941
datanode_3          | 2023-06-29 21:36:14,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:36:14,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:36:14,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:36:14,129 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:36:14,135 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:36:14,137 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:36:14,138 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:36:14,169 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,187 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:14,188 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:14,188 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:14,198 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:36:14,200 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:14,202 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:14,232 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:14,232 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:36:14,232 [pool-19-thread-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:14,233 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CF122FADA941,id=22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:14,233 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:14,239 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A51FE580D3B8->d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_3          | 2023-06-29 21:36:14,386 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-D27FEC0B053F->43d92334-42eb-45fb-a952-310405367e2b
datanode_3          | 2023-06-29 21:36:14,576 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941.
datanode_3          | 2023-06-29 21:36:14,576 [Command processor thread] INFO server.RaftServer: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: addNew group-0C3BAE832E5B:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] returns group-0C3BAE832E5B:java.util.concurrent.CompletableFuture@672c30a0[Not completed]
datanode_3          | 2023-06-29 21:36:14,578 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: new RaftServerImpl for group-0C3BAE832E5B:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:36:14,578 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:36:14,582 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:36:14,587 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:36:14,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:36:14,588 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:36:14,592 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/19fe7a4f-ef93-44ba-ba95-0c3bae832e5b does not exist. Creating ...
datanode_3          | 2023-06-29 21:36:14,597 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/19fe7a4f-ef93-44ba-ba95-0c3bae832e5b/in_use.lock acquired by nodename 7@307fd7392124
datanode_3          | 2023-06-29 21:36:14,600 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/19fe7a4f-ef93-44ba-ba95-0c3bae832e5b has been successfully formatted.
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-0C3BAE832E5B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:36:14,601 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B
datanode_3          | 2023-06-29 21:36:14,602 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,613 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:36:14,614 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:36:14,614 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/19fe7a4f-ef93-44ba-ba95-0c3bae832e5b
datanode_3          | 2023-06-29 21:36:14,615 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:36:14,615 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:36:14,615 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,615 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:36:14,615 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:36:14,616 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:36:14,616 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:36:14,616 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:36:14,616 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:36:14,617 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:14,618 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:14,618 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:36:14,619 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:36:14,619 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:36:14,623 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:36:14,623 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:36:14,623 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:36:14,623 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:36:14,625 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B
datanode_3          | 2023-06-29 21:36:14,632 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B
datanode_3          | 2023-06-29 21:36:14,632 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:36:14,639 [pool-19-thread-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:36:14,639 [pool-19-thread-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState
datanode_3          | 2023-06-29 21:36:14,640 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0C3BAE832E5B,id=22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:14,640 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B
datanode_3          | 2023-06-29 21:36:14,641 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=19fe7a4f-ef93-44ba-ba95-0c3bae832e5b.
datanode_3          | 2023-06-29 21:36:16,867 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5215277286ns, electionTimeout:5198ms
datanode_3          | 2023-06-29 21:36:16,868 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:16,868 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:36:16,871 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:36:16,873 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1
datanode_3          | 2023-06-29 21:36:16,906 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:16,908 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:16,952 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-CANDIDATE: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: already has voted for 22521fb4-36a7-41c6-aa9d-d3f5477b102c at current term 1
datanode_3          | 2023-06-29 21:36:16,963 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t1. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB:t1, leader=null, voted=22521fb4-36a7-41c6-aa9d-d3f5477b102c, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:16,979 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:16,979 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-CANDIDATE: reject ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: already has voted for 22521fb4-36a7-41c6-aa9d-d3f5477b102c at current term 1
datanode_3          | 2023-06-29 21:36:16,986 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t1. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB:t1, leader=null, voted=22521fb4-36a7-41c6-aa9d-d3f5477b102c, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:16,998 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:36:16,998 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection:   Response 0: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1
datanode_3          | 2023-06-29 21:36:17,000 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-29 21:36:17,001 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-29 21:36:17,001 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1
datanode_3          | 2023-06-29 21:36:17,002 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:19,300 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5067288311ns, electionTimeout:5066ms
datanode_3          | 2023-06-29 21:36:19,300 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:36:14,236 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:36:14,237 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:36:14,241 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:14,244 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:36:14,244 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:36:14,244 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:36:14,245 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:36:14,245 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:36:14,245 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:36:14,245 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:36:14,247 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:36:14,248 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941
datanode_1          | 2023-06-29 21:36:14,249 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941
datanode_1          | 2023-06-29 21:36:14,252 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:14,252 [pool-19-thread-1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:36:14,256 [pool-19-thread-1] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:14,266 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CF122FADA941,id=d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:14,266 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941
datanode_1          | 2023-06-29 21:36:14,268 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-1AA14D99F573->22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_1          | 2023-06-29 21:36:14,437 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-27BFCBBACBEC->43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:14,605 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941.
datanode_1          | 2023-06-29 21:36:16,710 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5114701362ns, electionTimeout:5067ms
datanode_1          | 2023-06-29 21:36:16,711 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState
datanode_1          | 2023-06-29 21:36:16,711 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:36:16,718 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:36:19,301 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:36:19,301 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:36:19,301 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2
datanode_3          | 2023-06-29 21:36:19,308 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:19,322 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:19,327 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-CANDIDATE: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: already has voted for 22521fb4-36a7-41c6-aa9d-d3f5477b102c at current term 1
datanode_3          | 2023-06-29 21:36:19,328 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t1. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941:t1, leader=null, voted=22521fb4-36a7-41c6-aa9d-d3f5477b102c, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:19,355 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:36:19,355 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.LeaderElection:   Response 0: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1
datanode_3          | 2023-06-29 21:36:19,355 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.LeaderElection:   Response 1: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:FAIL-t1
datanode_3          | 2023-06-29 21:36:19,356 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-29 21:36:19,357 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-29 21:36:19,358 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2
datanode_3          | 2023-06-29 21:36:19,358 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection2] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:19,779 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5139260345ns, electionTimeout:5138ms
datanode_3          | 2023-06-29 21:36:19,779 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState
datanode_3          | 2023-06-29 21:36:19,779 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:36:19,780 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:36:19,780 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3
datanode_3          | 2023-06-29 21:36:19,783 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:36:19,783 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-29 21:36:19,783 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3
datanode_3          | 2023-06-29 21:36:19,783 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:36:19,783 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-0C3BAE832E5B with new leaderId: 22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:19,785 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: change Leader from null to 22521fb4-36a7-41c6-aa9d-d3f5477b102c at term 1 for becomeLeader, leader elected after 5182ms
datanode_3          | 2023-06-29 21:36:19,794 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:36:19,805 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B
datanode_3          | 2023-06-29 21:36:19,809 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:36:19,809 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-29 21:36:16,718 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1
datanode_1          | 2023-06-29 21:36:16,723 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:36:16,724 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-29 21:36:16,724 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1
datanode_1          | 2023-06-29 21:36:16,724 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:36:16,724 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B62C0B0740E2 with new leaderId: d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_1          | 2023-06-29 21:36:16,725 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: change Leader from null to d8c9d548-bd9b-475e-a5cc-64194115352c at term 1 for becomeLeader, leader elected after 5540ms
datanode_1          | 2023-06-29 21:36:16,734 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:36:16,736 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2
datanode_1          | 2023-06-29 21:36:16,737 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:36:16,737 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_1          | 2023-06-29 21:36:16,745 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:36:16,745 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:36:16,745 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:36:16,752 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderStateImpl
datanode_1          | 2023-06-29 21:36:16,762 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:36:16,788 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-LeaderElection1] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2: set configuration 0: [d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-29 21:36:16,828 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-B62C0B0740E2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2/current/log_inprogress_0
datanode_1          | 2023-06-29 21:36:16,836 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5046180044ns, electionTimeout:5032ms
datanode_1          | 2023-06-29 21:36:16,836 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:16,836 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:36:16,837 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:36:16,837 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2
datanode_1          | 2023-06-29 21:36:16,843 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:16,947 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:16,948 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-CANDIDATE: reject ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: already has voted for d8c9d548-bd9b-475e-a5cc-64194115352c at current term 1
datanode_1          | 2023-06-29 21:36:16,958 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:FAIL-t1. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB:t1, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:16,965 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:36:19,817 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:36:19,817 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:36:19,818 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:36:19,842 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderStateImpl
datanode_3          | 2023-06-29 21:36:19,857 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:36:19,900 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-LeaderElection3] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-29 21:36:19,933 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-0C3BAE832E5B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/19fe7a4f-ef93-44ba-ba95-0c3bae832e5b/current/log_inprogress_0
datanode_3          | 2023-06-29 21:36:22,002 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-2BB9FCDAB2FB, 2, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:22,002 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FOLLOWER: accept ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-29 21:36:22,002 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_3          | 2023-06-29 21:36:22,003 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:22,003 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:22,003 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:22,009 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:OK-t2. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB:t2, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:24,450 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 2, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:24,450 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FOLLOWER: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-29 21:36:24,450 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_3          | 2023-06-29 21:36:24,450 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:24,450 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:24,450 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:24,458 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t2. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941:t2, leader=null, voted=null, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:27,060 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-2BB9FCDAB2FB, 3, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:27,060 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FOLLOWER: accept ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:36:27,061 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:16,965 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.LeaderElection:   Response 0: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1
datanode_1          | 2023-06-29 21:36:16,967 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:36:16,967 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-29 21:36:16,968 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2
datanode_1          | 2023-06-29 21:36:16,968 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection2] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:16,997 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:16,998 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FOLLOWER: reject ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: already has voted for d8c9d548-bd9b-475e-a5cc-64194115352c at current term 1
datanode_1          | 2023-06-29 21:36:16,998 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:FAIL-t1. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB:t1, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:19,282 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5026192080ns, electionTimeout:5015ms
datanode_1          | 2023-06-29 21:36:19,282 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:19,282 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:36:19,283 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:36:19,283 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3
datanode_1          | 2023-06-29 21:36:19,290 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:19,332 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:36:19,332 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.LeaderElection:   Response 0: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t1
datanode_1          | 2023-06-29 21:36:19,332 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:36:19,332 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-29 21:36:19,333 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3
datanode_1          | 2023-06-29 21:36:19,333 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection3] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:19,339 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-CF122FADA941, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:19,340 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FOLLOWER: reject ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: already has voted for d8c9d548-bd9b-475e-a5cc-64194115352c at current term 1
datanode_1          | 2023-06-29 21:36:19,344 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941 replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:FAIL-t1. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941:t1, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:21,990 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5022520347ns, electionTimeout:5011ms
datanode_1          | 2023-06-29 21:36:21,991 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:21,991 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-29 21:36:21,991 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:36:21,991 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4
datanode_1          | 2023-06-29 21:36:21,996 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.LeaderElection:   Response 0: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t2
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4
datanode_1          | 2023-06-29 21:36:22,010 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-LeaderElection4] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:24,420 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5087584650ns, electionTimeout:5087ms
datanode_1          | 2023-06-29 21:36:24,421 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:24,421 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-29 21:36:24,421 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:36:24,421 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5
datanode_1          | 2023-06-29 21:36:24,423 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5 ELECTION round 0: submit vote requests at term 2 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.LeaderElection:   Response 0: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t2
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.LeaderElection:   Response 1: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:OK-t2
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-29 21:36:24,460 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5
datanode_1          | 2023-06-29 21:36:24,461 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection5] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:27,080 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-2BB9FCDAB2FB, 3, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:27,080 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FOLLOWER: accept ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:36:27,080 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:27,080 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:27,080 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState
datanode_1          | 2023-06-29 21:36:27,081 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:27,061 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:27,061 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState
datanode_3          | 2023-06-29 21:36:27,061 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:27,073 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:OK-t3. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB:t3, leader=null, voted=43d92334-42eb-45fb-a952-310405367e2b, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:27,206 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2BB9FCDAB2FB with new leaderId: 43d92334-42eb-45fb-a952-310405367e2b
datanode_3          | 2023-06-29 21:36:27,206 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: change Leader from null to 43d92334-42eb-45fb-a952-310405367e2b at term 3 for appendEntries, leader elected after 15910ms
datanode_3          | 2023-06-29 21:36:27,256 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-06-29 21:36:27,274 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:36:27,280 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/current/log_inprogress_0
datanode_3          | 2023-06-29 21:36:29,496 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 3, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:29,496 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FOLLOWER: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-29 21:36:29,496 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_3          | 2023-06-29 21:36:29,496 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:29,496 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:29,496 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:29,504 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t3. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941:t3, leader=null, voted=null, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:34,573 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-CF122FADA941, 4, (t:0, i:0))
datanode_3          | 2023-06-29 21:36:34,574 [grpc-default-executor-1] INFO impl.VoteContext: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FOLLOWER: reject ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-29 21:36:34,574 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:43d92334-42eb-45fb-a952-310405367e2b
datanode_3          | 2023-06-29 21:36:34,575 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:34,578 [grpc-default-executor-1] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:34,578 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 2023-06-29 21:36:27,101 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:OK-t3. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB:t3, leader=null, voted=43d92334-42eb-45fb-a952-310405367e2b, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:27,198 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2BB9FCDAB2FB with new leaderId: 43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:27,199 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: change Leader from null to 43d92334-42eb-45fb-a952-310405367e2b at term 3 for appendEntries, leader elected after 15440ms
datanode_1          | 2023-06-29 21:36:27,265 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-29 21:36:27,269 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:36:27,284 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/current/log_inprogress_0
datanode_1          | 2023-06-29 21:36:29,485 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5024658340ns, electionTimeout:5018ms
datanode_1          | 2023-06-29 21:36:29,485 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:29,486 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_1          | 2023-06-29 21:36:29,486 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:36:29,486 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6
datanode_1          | 2023-06-29 21:36:29,492 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6 ELECTION round 0: submit vote requests at term 3 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:29,514 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:36:29,514 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.LeaderElection:   Response 0: d8c9d548-bd9b-475e-a5cc-64194115352c<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t3
datanode_1          | 2023-06-29 21:36:29,514 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.LeaderElection: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:36:29,514 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_1          | 2023-06-29 21:36:29,516 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6
datanode_1          | 2023-06-29 21:36:29,516 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-LeaderElection6] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:34,551 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: receive requestVote(ELECTION, 43d92334-42eb-45fb-a952-310405367e2b, group-CF122FADA941, 4, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:34,553 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FOLLOWER: accept ELECTION from 43d92334-42eb-45fb-a952-310405367e2b: our priority 0 <= candidate's priority 0
datanode_1          | 2023-06-29 21:36:34,553 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:43d92334-42eb-45fb-a952-310405367e2b
datanode_1          | 2023-06-29 21:36:34,553 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:34,553 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-29 21:36:34,554 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:36:34,584 [grpc-default-executor-1] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941 replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t4. Peer's state: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941:t4, leader=null, voted=null, raftlog=22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:39,704 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5125949857ns, electionTimeout:5112ms
datanode_3          | 2023-06-29 21:36:39,705 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState
datanode_3          | 2023-06-29 21:36:39,705 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
datanode_3          | 2023-06-29 21:36:39,705 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:36:39,705 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4
datanode_3          | 2023-06-29 21:36:39,709 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4 ELECTION round 0: submit vote requests at term 5 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection:   Response 0: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:OK-t5
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4 ELECTION round 0: result PASSED
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: shutdown 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: changes role from CANDIDATE to LEADER at term 5 for changeToLeader
datanode_3          | 2023-06-29 21:36:39,727 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CF122FADA941 with new leaderId: 22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_3          | 2023-06-29 21:36:39,728 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: change Leader from null to 22521fb4-36a7-41c6-aa9d-d3f5477b102c at term 5 for becomeLeader, leader elected after 25609ms
datanode_3          | 2023-06-29 21:36:39,730 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:36:39,730 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:39,731 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:36:39,731 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_3          | 2023-06-29 21:36:39,733 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:36:39,733 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:36:39,733 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:36:39,737 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:36:39,738 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:36:39,739 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:36:39,754 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:36:39,757 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:36:39,757 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:36:39,758 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941
datanode_3          | 2023-06-29 21:36:39,761 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:36:39,767 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:36:39,767 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:36:39,767 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:36:39,767 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:36:39,768 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:36:39,769 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO impl.RoleInfo: 22521fb4-36a7-41c6-aa9d-d3f5477b102c: start 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderStateImpl
datanode_3          | 2023-06-29 21:36:39,769 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:36:39,773 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/current/log_inprogress_0
datanode_3          | 2023-06-29 21:36:39,797 [22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941-LeaderElection4] INFO server.RaftServer$Division: 22521fb4-36a7-41c6-aa9d-d3f5477b102c@group-CF122FADA941: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:35:43,846 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 594367ec49ff/172.21.0.11
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.1.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-datanode-1.1.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
datanode_2          | STARTUP_MSG:   java = 11.0.10
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:35:43,889 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:35:45,387 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:35:45,823 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:35:46,348 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:35:46,348 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:35:46,916 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:594367ec49ff ip:172.21.0.11
datanode_2          | 2023-06-29 21:35:48,068 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:35:48,080 [main] INFO volume.HddsVolume: Creating Volume: /data/hdds/hdds of storage type : DISK and capacity : 89297309696
datanode_2          | 2023-06-29 21:35:48,081 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:35:48,107 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:35:48,214 [main] INFO volume.HddsVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:35:48,405 [Thread-4] INFO ozoneimpl.ContainerReader: Start to verify containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:35:48,423 [Thread-4] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:35:48,423 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:35:55,470 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:35:55,853 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:35:56,407 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-29 21:35:56,419 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-29 21:35:56,419 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-29 21:35:56,435 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:35:56,452 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:35:56,452 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:35:56,453 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:35:58,022 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-29 21:35:58,055 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:35:58,056 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:35:58,144 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:35:58,154 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:35:58,777 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:35:58,944 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:35:59,174 [main] INFO util.log: Logging initialized @20033ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:35:59,812 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-29 21:35:59,822 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:35:59,874 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:35:59,883 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:35:59,883 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:35:59,889 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-29 21:36:00,092 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-29 21:36:00,117 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
datanode_2          | 2023-06-29 21:36:00,324 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:36:00,324 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:36:00,330 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_2          | 2023-06-29 21:36:00,374 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@37ad042b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:36:00,380 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f9fc8bd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:36:01,882 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@d902300{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hadoop-hdds-container-service-1_1_0_jar-_-any-6187581363105561833/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-29 21:36:01,923 [main] INFO server.AbstractConnector: Started ServerConnector@167381c7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:36:01,937 [main] INFO server.Server: Started @22795ms
datanode_2          | 2023-06-29 21:36:01,958 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-29 21:36:01,958 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-29 21:36:01,960 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:36:02,063 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7759e432] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-29 21:36:02,422 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.21.0.5:9891
datanode_2          | 2023-06-29 21:36:02,634 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:36:06,272 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-29 21:36:06,276 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-29 21:36:06,564 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:06,680 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.RaftServer: 43d92334-42eb-45fb-a952-310405367e2b: start RPC server
datanode_2          | 2023-06-29 21:36:06,701 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 43d92334-42eb-45fb-a952-310405367e2b: GrpcService started, listening on 9856
datanode_2          | 2023-06-29 21:36:06,704 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 43d92334-42eb-45fb-a952-310405367e2b: GrpcService started, listening on 9857
datanode_2          | 2023-06-29 21:36:06,704 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO server.GrpcService: 43d92334-42eb-45fb-a952-310405367e2b: GrpcService started, listening on 9858
datanode_2          | 2023-06-29 21:36:06,735 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 43d92334-42eb-45fb-a952-310405367e2b is started using port 9858 for RATIS
datanode_2          | 2023-06-29 21:36:06,735 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 43d92334-42eb-45fb-a952-310405367e2b is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-29 21:36:06,736 [EndpointStateMachine task thread for scm/172.21.0.6:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 43d92334-42eb-45fb-a952-310405367e2b is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-29 21:36:06,739 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$282/0x000000084046e840@c49bfb2] INFO util.JvmPauseMonitor: JvmPauseMonitor-43d92334-42eb-45fb-a952-310405367e2b: Started
datanode_2          | 2023-06-29 21:36:08,086 [Datanode State Machine Thread - 0] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:231)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:565)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.start(DatanodeStateMachine.java:233)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:410)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:834)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:149)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-29 21:36:11,130 [Command processor thread] INFO server.RaftServer: 43d92334-42eb-45fb-a952-310405367e2b: addNew group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-2BB9FCDAB2FB:java.util.concurrent.CompletableFuture@24c00282[Not completed]
datanode_2          | 2023-06-29 21:36:11,182 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b: new RaftServerImpl for group-2BB9FCDAB2FB:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:36:11,202 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:36:11,203 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:36:11,204 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:36:11,204 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:11,205 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:36:11,206 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:36:11,207 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:36:11,220 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:36:11,223 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:36:11,233 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:36:11,246 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb does not exist. Creating ...
datanode_2          | 2023-06-29 21:36:11,267 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/in_use.lock acquired by nodename 7@594367ec49ff
datanode_2          | 2023-06-29 21:36:11,306 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb has been successfully formatted.
datanode_2          | 2023-06-29 21:36:11,334 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-2BB9FCDAB2FB: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:36:11,352 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:11,372 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:36:11,407 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:36:34,558 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941 replies to ELECTION vote request: 43d92334-42eb-45fb-a952-310405367e2b<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:OK-t4. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941:t4, leader=null, voted=43d92334-42eb-45fb-a952-310405367e2b, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:39,719 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-CF122FADA941, 5, (t:0, i:0))
datanode_1          | 2023-06-29 21:36:39,720 [grpc-default-executor-0] INFO impl.VoteContext: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FOLLOWER: accept ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:36:39,720 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_1          | 2023-06-29 21:36:39,720 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: shutdown d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:39,720 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState] INFO impl.FollowerState: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-29 21:36:39,721 [grpc-default-executor-0] INFO impl.RoleInfo: d8c9d548-bd9b-475e-a5cc-64194115352c: start d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-FollowerState
datanode_1          | 2023-06-29 21:36:39,722 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941 replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:OK-t5. Peer's state: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941:t5, leader=null, voted=22521fb4-36a7-41c6-aa9d-d3f5477b102c, raftlog=d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:36:39,813 [grpc-default-executor-0] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CF122FADA941 with new leaderId: 22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_1          | 2023-06-29 21:36:39,813 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: change Leader from null to 22521fb4-36a7-41c6-aa9d-d3f5477b102c at term 5 for appendEntries, leader elected after 25590ms
datanode_1          | 2023-06-29 21:36:39,855 [grpc-default-executor-0] INFO server.RaftServer$Division: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-29 21:36:39,855 [grpc-default-executor-0] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:36:39,857 [d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d8c9d548-bd9b-475e-a5cc-64194115352c@group-CF122FADA941-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/current/log_inprogress_0
datanode_2          | 2023-06-29 21:36:11,413 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:36:11,425 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:11,467 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:11,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:36:11,487 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:36:11,524 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb
datanode_2          | 2023-06-29 21:36:11,526 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:36:11,539 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:36:11,544 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:11,548 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:36:11,548 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:36:11,550 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:36:11,554 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:36:11,555 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:36:11,576 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:11,583 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:11,599 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:11,599 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:11,631 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:11,632 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:36:11,634 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:36:11,641 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:36:11,659 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:36:11,659 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:36:11,726 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:11,731 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:11,777 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:11,778 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:36:11,785 [pool-19-thread-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:11,825 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-2BB9FCDAB2FB,id=43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:11,826 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:12,060 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-A3AEAA15776F->d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:13,324 [grpc-default-executor-0] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server_message_metrics.43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:13,914 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-FEBDB1C1EF3C->22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_2          | 2023-06-29 21:36:14,218 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb.
datanode_2          | 2023-06-29 21:36:14,220 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b: new RaftServerImpl for group-9C6C3364D118:[43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:36:14,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:36:14,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:36:14,220 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:36:14,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:14,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:36:14,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:36:14,222 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:36:14,223 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: ConfigurationManager, init=-1: [43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:36:14,224 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:36:14,224 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:36:14,224 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9d0ea578-3f9a-4558-a542-9c6c3364d118 does not exist. Creating ...
datanode_2          | 2023-06-29 21:36:14,225 [Command processor thread] INFO server.RaftServer: 43d92334-42eb-45fb-a952-310405367e2b: addNew group-9C6C3364D118:[43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1] returns group-9C6C3364D118:java.util.concurrent.CompletableFuture@ef87c6e[Not completed]
datanode_2          | 2023-06-29 21:36:14,230 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9d0ea578-3f9a-4558-a542-9c6c3364d118/in_use.lock acquired by nodename 7@594367ec49ff
datanode_2          | 2023-06-29 21:36:14,234 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9d0ea578-3f9a-4558-a542-9c6c3364d118 has been successfully formatted.
datanode_2          | 2023-06-29 21:36:14,235 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-9C6C3364D118: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:36:14,235 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:14,237 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:36:14,239 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:36:14,239 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:36:14,260 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118
datanode_2          | 2023-06-29 21:36:14,260 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:36:14,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:36:14,261 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9d0ea578-3f9a-4558-a542-9c6c3364d118
datanode_2          | 2023-06-29 21:36:14,261 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:36:14,262 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:36:14,262 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,265 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:36:14,266 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:36:14,270 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:36:14,270 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:36:14,270 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:36:14,271 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,272 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:36:14,273 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:36:14,274 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:36:14,274 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:36:14,275 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118
datanode_2          | 2023-06-29 21:36:14,276 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118
datanode_2          | 2023-06-29 21:36:14,280 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: start as a follower, conf=-1: [43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:36:14,282 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:36:14,282 [pool-19-thread-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState
datanode_2          | 2023-06-29 21:36:14,283 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9C6C3364D118,id=43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:14,283 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118
datanode_2          | 2023-06-29 21:36:14,328 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=9d0ea578-3f9a-4558-a542-9c6c3364d118.
datanode_2          | 2023-06-29 21:36:14,329 [Command processor thread] INFO server.RaftServer: 43d92334-42eb-45fb-a952-310405367e2b: addNew group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] returns group-CF122FADA941:java.util.concurrent.CompletableFuture@5cc09be9[Not completed]
datanode_2          | 2023-06-29 21:36:14,338 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b: new RaftServerImpl for group-CF122FADA941:[22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:36:14,340 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:36:14,341 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:36:14,342 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:36:14,343 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:14,343 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:36:14,343 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:36:14,345 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:36:14,347 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: ConfigurationManager, init=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:36:14,347 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:36:14,347 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:36:14,376 [pool-19-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 does not exist. Creating ...
datanode_2          | 2023-06-29 21:36:14,377 [pool-19-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/in_use.lock acquired by nodename 7@594367ec49ff
datanode_2          | 2023-06-29 21:36:14,383 [pool-19-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941 has been successfully formatted.
datanode_2          | 2023-06-29 21:36:14,384 [pool-19-thread-1] INFO ratis.ContainerStateMachine: group-CF122FADA941: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:36:14,387 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:36:14,389 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:36:14,393 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:36:14,393 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:36:14,393 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941
datanode_2          | 2023-06-29 21:36:14,399 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,404 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:36:14,404 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:36:14,404 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: new 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941
datanode_2          | 2023-06-29 21:36:14,404 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:36:14,405 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:36:14,408 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,408 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:36:14,412 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:36:14,412 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:36:14,412 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:36:14,412 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:36:14,413 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:36:14,414 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:14,421 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:14,445 [pool-19-thread-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:36:14,466 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:36:14,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:36:14,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:36:14,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:36:14,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:36:14,484 [pool-19-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:36:14,485 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941
datanode_2          | 2023-06-29 21:36:14,485 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941
datanode_2          | 2023-06-29 21:36:14,486 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: start as a follower, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:14,486 [pool-19-thread-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:36:14,486 [pool-19-thread-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:14,507 [pool-19-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-CF122FADA941,id=43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:14,508 [pool-19-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941
datanode_2          | 2023-06-29 21:36:14,510 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-53E5E37C152B->d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:14,620 [Command processor thread] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.client_message_metrics.client-14A6B3CEF65E->22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_2          | 2023-06-29 21:36:14,683 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941.
datanode_2          | 2023-06-29 21:36:16,864 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5078867494ns, electionTimeout:5073ms
datanode_2          | 2023-06-29 21:36:16,864 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:16,865 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:36:16,867 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:36:16,868 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1
datanode_2          | 2023-06-29 21:36:16,894 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:16,921 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:16,923 [grpc-default-executor-0] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-CANDIDATE: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: already has voted for 43d92334-42eb-45fb-a952-310405367e2b at current term 1
datanode_2          | 2023-06-29 21:36:16,933 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB:t1, leader=null, voted=43d92334-42eb-45fb-a952-310405367e2b, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:16,991 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-2BB9FCDAB2FB, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:16,992 [grpc-default-executor-0] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-CANDIDATE: reject ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: already has voted for 43d92334-42eb-45fb-a952-310405367e2b at current term 1
datanode_2          | 2023-06-29 21:36:16,992 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB:t1, leader=null, voted=43d92334-42eb-45fb-a952-310405367e2b, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:17,002 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:36:17,006 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection:   Response 0: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t1
datanode_2          | 2023-06-29 21:36:17,006 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection:   Response 1: 43d92334-42eb-45fb-a952-310405367e2b<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:FAIL-t1
datanode_2          | 2023-06-29 21:36:17,006 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-29 21:36:17,008 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-06-29 21:36:17,011 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1
datanode_2          | 2023-06-29 21:36:17,011 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:19,310 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 1, (t:0, i:0))
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:35:42,356 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = e2710229275f/172.21.0.5
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.1.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-reconcodegen-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.27.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-tools-1.1.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.27.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.27.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | STARTUP_MSG:   java = 11.0.10
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:35:42,419 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:35:45,843 [main] INFO recon.ReconRestServletModule: rest([/api/v1/*]).packages(org.apache.hadoop.ozone.recon.api)
recon_1             | 2023-06-29 21:35:47,278 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-29 21:35:48,568 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:35:54,245 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:35:56,063 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:35:56,232 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:35:56,233 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-29 21:36:00,503 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:36:00,601 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:36:00,639 [main] INFO util.log: Logging initialized @21765ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:36:01,031 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-29 21:36:01,052 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-29 21:36:01,076 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:36:01,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:36:01,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:36:01,096 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:36:01,602 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:36:02,645 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:36:02,660 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-29 21:36:02,694 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:36:02,694 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-29 21:36:02,957 [main] INFO Configuration.deprecation: No unit for ozone.recon.om.connection.request.timeout(5000) assuming MILLISECONDS
recon_1             | 2023-06-29 21:36:03,300 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:36:03,496 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:36:03,523 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-29 21:36:03,532 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-29 21:36:03,657 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:36:03,786 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-29 21:36:03,810 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-29 21:36:03,813 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
datanode_2          | 2023-06-29 21:36:19,311 [grpc-default-executor-0] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FOLLOWER: accept ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:36:19,311 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:19,311 [grpc-default-executor-0] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:19,312 [grpc-default-executor-0] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:19,312 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:36:19,337 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-CF122FADA941, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:19,340 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057657237ns, electionTimeout:5056ms
datanode_2          | 2023-06-29 21:36:19,340 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState
datanode_2          | 2023-06-29 21:36:19,340 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:36:19,340 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:36:19,340 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2
datanode_2          | 2023-06-29 21:36:19,341 [grpc-default-executor-0] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:OK-t1. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941:t1, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:19,341 [grpc-default-executor-1] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FOLLOWER: reject ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: already has voted for d8c9d548-bd9b-475e-a5cc-64194115352c at current term 1
datanode_2          | 2023-06-29 21:36:19,352 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941 replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t1. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941:t1, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:19,359 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:36:19,364 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-29 21:36:19,364 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2
datanode_2          | 2023-06-29 21:36:19,364 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:36:19,364 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9C6C3364D118 with new leaderId: 43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:19,365 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: change Leader from null to 43d92334-42eb-45fb-a952-310405367e2b at term 1 for becomeLeader, leader elected after 5129ms
datanode_2          | 2023-06-29 21:36:19,381 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:36:19,382 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118
datanode_2          | 2023-06-29 21:36:19,385 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:36:19,390 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-29 21:36:19,399 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:36:19,399 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:36:19,400 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:35:43,821 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 83ff0bac7c87/172.21.0.9
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-29 21:35:43,854 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:35:50,801 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:35:51,136 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-29 21:35:51,159 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:35:51,160 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:35:51,216 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:35:53,749 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:54,750 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:55,751 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:56,752 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:57,763 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:58,764 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:35:59,765 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:36:00,766 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:36:01,767 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:36:02,768 [main] INFO ipc.Client: Retrying connect to server: scm/172.21.0.6:9863. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
om_1                | 2023-06-29 21:36:02,771 [main] INFO utils.RetriableTask: Execution of task OM#getScmInfo failed, will be retried in 5000 ms
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-992a5c27-d477-4bb7-8860-05dc77cc875e;layoutVersion=0
om_1                | 2023-06-29 21:36:07,883 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 83ff0bac7c87/172.21.0.9
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:36:08,900 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 83ff0bac7c87/172.21.0.9
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.1.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-storage-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
om_1                | STARTUP_MSG:   java = 11.0.10
om_1                | ************************************************************/
om_1                | 2023-06-29 21:36:08,910 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:36:10,531 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:36:10,604 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.21.0.9:9862
om_1                | 2023-06-29 21:36:10,604 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:36:10,607 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:36:10,618 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:36:10,633 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:36:13,717 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:36:14,027 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:36:14,032 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:36:14,378 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:36:14,398 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:36:14,399 [main] WARN ratis.OzoneManagerRatisServer: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:36:14,426 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-29 21:36:14,498 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:36:14,664 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with GroupID: omServiceIdDefault and Raft Peers: om:9872
om_1                | 2023-06-29 21:36:14,698 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-29 21:36:14,755 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-29 21:36:14,879 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-29 21:36:14,883 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:36:14,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-29 21:36:14,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:36:14,884 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:36:14,885 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-29 21:36:14,888 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:36:14,889 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-29 21:36:14,892 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
datanode_2          | 2023-06-29 21:36:19,417 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderStateImpl
datanode_2          | 2023-06-29 21:36:19,437 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:36:19,464 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-LeaderElection2] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118: set configuration 0: [43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-29 21:36:19,519 [43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-9C6C3364D118-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9d0ea578-3f9a-4558-a542-9c6c3364d118/current/log_inprogress_0
datanode_2          | 2023-06-29 21:36:22,001 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-2BB9FCDAB2FB, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:22,001 [grpc-default-executor-1] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FOLLOWER: reject ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-29 21:36:22,003 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:22,003 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:22,003 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:36:22,004 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:22,007 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:FAIL-t2. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB:t2, leader=null, voted=null, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:24,430 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:24,430 [grpc-default-executor-1] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FOLLOWER: accept ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:36:24,430 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:24,430 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:24,430 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:36:24,431 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:24,433 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:OK-t2. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941:t2, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:27,041 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5037010384ns, electionTimeout:5033ms
datanode_2          | 2023-06-29 21:36:27,041 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState
datanode_2          | 2023-06-29 21:36:27,041 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-06-29 21:36:27,041 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:36:27,041 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3
recon_1             | 2023-06-29 21:36:03,845 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:36:03,863 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-29 21:36:03,905 [Listener at 0.0.0.0/9891] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
recon_1             | 2023-06-29 21:36:03,956 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-29 21:36:03,957 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-29 21:36:04,251 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-29 21:36:04,326 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-29 21:36:04,326 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-29 21:36:04,881 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-29 21:36:04,882 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
recon_1             | 2023-06-29 21:36:04,981 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-29 21:36:04,983 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-29 21:36:04,986 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1             | 2023-06-29 21:36:05,029 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4978777f{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-29 21:36:05,033 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@75b38c36{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-29 21:36:07,609 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6e355249{recon,/,file:///tmp/jetty-0_0_0_0-9888-hadoop-ozone-recon-1_1_0_jar-_-any-5378601020733549335/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-recon-1.1.0.jar!/webapps/recon}
recon_1             | 2023-06-29 21:36:07,616 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@76ff68c5{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-29 21:36:07,616 [Listener at 0.0.0.0/9891] INFO server.Server: Started @28743ms
recon_1             | 2023-06-29 21:36:07,619 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-29 21:36:07,619 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-29 21:36:07,622 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-29 21:36:07,622 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-29 21:36:07,627 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-29 21:36:07,630 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-29 21:36:07,630 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-29 21:36:07,631 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:36:07,631 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-29 21:36:07,632 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:36:07,729 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-29 21:36:07,729 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:36:07,729 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:36:07,731 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-29 21:36:07,743 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-29 21:36:07,805 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-29 21:36:07,806 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-29 21:36:07,861 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:36:07,862 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-29 21:36:07,897 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:36:07,911 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 43 milliseconds.
recon_1             | 2023-06-29 21:36:07,946 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 138 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:36:08,018 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 71 milliseconds for processing 0 containers.
recon_1             | 2023-06-29 21:36:09,980 [IPC Server handler 0 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d8c9d548-bd9b-475e-a5cc-64194115352c
recon_1             | 2023-06-29 21:36:09,984 [IPC Server handler 0 on default port 9891] INFO node.SCMNodeManager: Registered Data node : d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:10,004 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node d8c9d548-bd9b-475e-a5cc-64194115352c to Node DB.
recon_1             | 2023-06-29 21:36:10,035 [IPC Server handler 1 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22521fb4-36a7-41c6-aa9d-d3f5477b102c
recon_1             | 2023-06-29 21:36:10,038 [IPC Server handler 1 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:10,039 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 22521fb4-36a7-41c6-aa9d-d3f5477b102c to Node DB.
recon_1             | 2023-06-29 21:36:10,092 [IPC Server handler 2 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/43d92334-42eb-45fb-a952-310405367e2b
recon_1             | 2023-06-29 21:36:10,092 [IPC Server handler 2 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om_1                | 2023-06-29 21:36:15,248 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:36:15,253 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:36:15,253 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:36:15,271 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:36:15,283 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@3f598450[Not completed]
om_1                | 2023-06-29 21:36:15,283 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-29 21:36:15,327 [pool-17-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-29 21:36:15,330 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-29 21:36:15,330 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-29 21:36:15,334 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:36:15,335 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-29 21:36:15,338 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:36:15,338 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:36:15,338 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-29 21:36:15,339 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-29 21:36:15,342 [pool-17-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-29 21:36:15,348 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:36:15,354 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-29 21:36:15,355 [pool-17-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-29 21:36:15,377 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:36:15,384 [pool-17-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@83ff0bac7c87
om_1                | 2023-06-29 21:36:15,411 [pool-17-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-29 21:36:15,417 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-29 21:36:15,426 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:36:15,451 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-29 21:36:15,452 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:36:15,462 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_worker.om1@group-C5BA1605619E
om_1                | 2023-06-29 21:36:15,492 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:36:15,515 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-29 21:36:15,515 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-29 21:36:15,519 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-29 21:36:15,519 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:36:15,536 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-29 21:36:15,537 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:36:15,537 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-29 21:36:15,537 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-29 21:36:15,588 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:36:15,588 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-29 21:36:15,589 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-29 21:36:15,608 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-29 21:36:15,609 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-29 21:36:15,614 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:36:15,614 [pool-17-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:36:15,617 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-29 21:36:15,618 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-29 21:36:15,618 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-29 21:36:15,619 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-29 21:36:15,620 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-29 21:36:15,624 [pool-17-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-29 21:36:15,650 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:36:15,657 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.leader_election.om1@group-C5BA1605619E
om_1                | 2023-06-29 21:36:15,659 [pool-17-thread-1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.server.om1@group-C5BA1605619E
datanode_2          | 2023-06-29 21:36:27,055 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:27,079 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:36:27,079 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.LeaderElection:   Response 0: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:OK-t3
datanode_2          | 2023-06-29 21:36:27,080 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-06-29 21:36:27,080 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3
datanode_2          | 2023-06-29 21:36:27,080 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_2          | 2023-06-29 21:36:27,080 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-2BB9FCDAB2FB with new leaderId: 43d92334-42eb-45fb-a952-310405367e2b
datanode_2          | 2023-06-29 21:36:27,086 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: change Leader from null to 43d92334-42eb-45fb-a952-310405367e2b at term 3 for becomeLeader, leader elected after 15728ms
datanode_2          | 2023-06-29 21:36:27,087 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:36:27,087 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:27,087 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:36:27,088 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1073741824 (custom)
datanode_2          | 2023-06-29 21:36:27,088 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:36:27,088 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:36:27,088 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:36:27,120 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:36:27,124 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:36:27,124 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:36:27,131 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:36:27,131 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:36:27,132 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:36:27,132 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis_grpc.log_appender.43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB
datanode_2          | 2023-06-29 21:36:27,144 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:36:27,144 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:36:27,154 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:36:27,155 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:36:27,155 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:36:27,155 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:36:27,158 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderStateImpl
datanode_2          | 2023-06-29 21:36:27,159 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:36:27,161 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d25069e2-5091-4b21-9dcc-2bb9fcdab2fb/current/log_inprogress_0
datanode_2          | 2023-06-29 21:36:27,179 [43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB-LeaderElection3] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-2BB9FCDAB2FB: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:0, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:1, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-29 21:36:29,505 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: receive requestVote(ELECTION, d8c9d548-bd9b-475e-a5cc-64194115352c, group-CF122FADA941, 3, (t:0, i:0))
om_1                | 2023-06-29 21:36:15,676 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:36:15,677 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-29 21:36:15,697 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.21.0.9:9862
om_1                | 2023-06-29 21:36:15,697 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-29 21:36:15,699 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-29 21:36:15,700 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:36:15,702 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:36:15,703 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-29 21:36:15,707 [Listener at om/9862] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.state_machine.om1@group-C5BA1605619E
om_1                | 2023-06-29 21:36:15,710 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-29 21:36:15,745 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-29 21:36:15,752 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$354/0x0000000840484440@c1050f2] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-29 21:36:15,786 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-29 21:36:15,787 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-29 21:36:15,805 [Listener at om/9862] INFO util.log: Logging initialized @7656ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-29 21:36:15,885 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-29 21:36:15,888 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-29 21:36:15,893 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-29 21:36:15,894 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-29 21:36:15,895 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:36:15,895 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:36:15,929 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-29 21:36:15,930 [Listener at om/9862] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
om_1                | 2023-06-29 21:36:15,954 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:36:15,954 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:36:15,956 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-29 21:36:16,008 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3c25cfe1{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:36:16,015 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a083b96{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:36:16,253 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@740a0d5e{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-hadoop-ozone-ozone-manager-1_1_0_jar-_-any-8402197890519808299/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-ozone-manager-1.1.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-29 21:36:16,271 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7c96c85{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-29 21:36:16,271 [Listener at om/9862] INFO server.Server: Started @8122ms
om_1                | 2023-06-29 21:36:16,275 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-29 21:36:16,276 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-29 21:36:16,277 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-29 21:36:16,285 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-29 21:36:16,286 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-29 21:36:16,305 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-29 21:36:16,311 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5c313224] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-29 21:36:20,841 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5138917311ns, electionTimeout:5126ms
om_1                | 2023-06-29 21:36:20,842 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:36:20,843 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-29 21:36:20,849 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-29 21:36:20,855 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:36:20,868 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-29 21:36:20,870 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-29 21:36:20,870 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:36:20,871 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-29 21:36:20,871 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5454ms
om_1                | 2023-06-29 21:36:20,879 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:36:20,881 [om1@group-C5BA1605619E-LeaderElection1] INFO metrics.RatisMetrics: Creating Metrics Registry : ratis.log_appender.om1@group-C5BA1605619E
datanode_2          | 2023-06-29 21:36:29,505 [grpc-default-executor-1] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FOLLOWER: accept ELECTION from d8c9d548-bd9b-475e-a5cc-64194115352c: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:36:29,505 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:d8c9d548-bd9b-475e-a5cc-64194115352c
datanode_2          | 2023-06-29 21:36:29,505 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:29,505 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:29,505 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:36:29,508 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941 replies to ELECTION vote request: d8c9d548-bd9b-475e-a5cc-64194115352c<-43d92334-42eb-45fb-a952-310405367e2b#0:OK-t3. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941:t3, leader=null, voted=d8c9d548-bd9b-475e-a5cc-64194115352c, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:34,527 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5019091254ns, electionTimeout:5019ms
datanode_2          | 2023-06-29 21:36:34,527 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:34,530 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_2          | 2023-06-29 21:36:34,530 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:36:34,530 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4
datanode_2          | 2023-06-29 21:36:34,534 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4 ELECTION round 0: submit vote requests at term 4 for -1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:34,596 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:36:34,597 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection:   Response 0: 43d92334-42eb-45fb-a952-310405367e2b<-22521fb4-36a7-41c6-aa9d-d3f5477b102c#0:FAIL-t4
datanode_2          | 2023-06-29 21:36:34,597 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection:   Response 1: 43d92334-42eb-45fb-a952-310405367e2b<-d8c9d548-bd9b-475e-a5cc-64194115352c#0:OK-t4
datanode_2          | 2023-06-29 21:36:34,597 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.LeaderElection: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-29 21:36:34,597 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
datanode_2          | 2023-06-29 21:36:34,597 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4
datanode_2          | 2023-06-29 21:36:34,598 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-LeaderElection4] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:39,713 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: receive requestVote(ELECTION, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, group-CF122FADA941, 5, (t:0, i:0))
datanode_2          | 2023-06-29 21:36:39,714 [grpc-default-executor-1] INFO impl.VoteContext: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FOLLOWER: accept ELECTION from 22521fb4-36a7-41c6-aa9d-d3f5477b102c: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:36:39,714 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_2          | 2023-06-29 21:36:39,714 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: shutdown 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:39,715 [grpc-default-executor-1] INFO impl.RoleInfo: 43d92334-42eb-45fb-a952-310405367e2b: start 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState
datanode_2          | 2023-06-29 21:36:39,715 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState] INFO impl.FollowerState: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:339)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
recon_1             | 2023-06-29 21:36:10,092 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 43d92334-42eb-45fb-a952-310405367e2b to Node DB.
recon_1             | 2023-06-29 21:36:11,190 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2. Trying to get from SCM.
recon_1             | 2023-06-29 21:36:11,230 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.131Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:36:11,238 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.131Z]
recon_1             | 2023-06-29 21:36:11,256 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2 reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:11,260 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:d8c9d548-bd9b-475e-a5cc-64194115352c, CreationTimestamp2023-06-29T21:36:08.131Z] moved to OPEN state
recon_1             | 2023-06-29 21:36:11,293 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb. Trying to get from SCM.
recon_1             | 2023-06-29 21:36:11,302 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d25069e2-5091-4b21-9dcc-2bb9fcdab2fb, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.164Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:36:11,319 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: d25069e2-5091-4b21-9dcc-2bb9fcdab2fb, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.164Z]
recon_1             | 2023-06-29 21:36:11,323 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:11,371 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:11,748 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,124 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,124 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=1f17b98f-1336-47b2-9769-cf122fada941. Trying to get from SCM.
om_1                | 2023-06-29 21:36:20,883 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:36:20,883 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:36:20,888 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-29 21:36:20,888 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-29 21:36:20,889 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-29 21:36:20,897 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-29 21:36:20,927 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-29 21:36:20,986 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-29 21:36:21,034 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-29 21:36:32,004 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-29 21:37:08,010 [qtp628402659-39] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-29 21:37:08,039 [qtp628402659-39] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074628012 in 26 milliseconds
om_1                | 2023-06-29 21:37:08,077 [qtp628402659-39] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 36 milliseconds
om_1                | 2023-06-29 21:37:08,079 [qtp628402659-39] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074628012
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:35:41,964 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-29 21:35:41,964 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-29 21:35:42,092 [main] INFO util.log: Logging initialized @4892ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-29 21:35:42,704 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-29 21:35:42,821 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-29 21:35:42,839 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-29 21:35:42,856 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-29 21:35:42,871 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-29 21:35:42,872 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-29 21:35:43,224 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = f82f3df66b39/172.21.0.3
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.1.0
recon_1             | 2023-06-29 21:36:14,151 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 1f17b98f-1336-47b2-9769-cf122fada941, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.177Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:36:14,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1f17b98f-1336-47b2-9769-cf122fada941, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.177Z]
recon_1             | 2023-06-29 21:36:14,152 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,222 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,223 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,260 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,260 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9d0ea578-3f9a-4558-a542-9c6c3364d118. Trying to get from SCM.
recon_1             | 2023-06-29 21:36:14,281 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9d0ea578-3f9a-4558-a542-9c6c3364d118, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.170Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:36:14,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 9d0ea578-3f9a-4558-a542-9c6c3364d118, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.170Z]
recon_1             | 2023-06-29 21:36:14,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline ONE PipelineID=9d0ea578-3f9a-4558-a542-9c6c3364d118 reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,282 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9d0ea578-3f9a-4558-a542-9c6c3364d118, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:43d92334-42eb-45fb-a952-310405367e2b, CreationTimestamp2023-06-29T21:36:08.170Z] moved to OPEN state
recon_1             | 2023-06-29 21:36:14,388 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,389 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
datanode_2          | 2023-06-29 21:36:39,724 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941 replies to ELECTION vote request: 22521fb4-36a7-41c6-aa9d-d3f5477b102c<-43d92334-42eb-45fb-a952-310405367e2b#0:OK-t5. Peer's state: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941:t5, leader=null, voted=22521fb4-36a7-41c6-aa9d-d3f5477b102c, raftlog=43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLog:OPENED:c-1,f-1,i0, conf=-1: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:36:39,826 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-CF122FADA941 with new leaderId: 22521fb4-36a7-41c6-aa9d-d3f5477b102c
datanode_2          | 2023-06-29 21:36:39,837 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: change Leader from null to 22521fb4-36a7-41c6-aa9d-d3f5477b102c at term 5 for appendEntries, leader elected after 25439ms
datanode_2          | 2023-06-29 21:36:39,865 [grpc-default-executor-1] INFO server.RaftServer$Division: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941: set configuration 0: [22521fb4-36a7-41c6-aa9d-d3f5477b102c|rpc:172.21.0.13:9856|admin:172.21.0.13:9857|client:172.21.0.13:9858|dataStream:|priority:1, 43d92334-42eb-45fb-a952-310405367e2b|rpc:172.21.0.11:9856|admin:172.21.0.11:9857|client:172.21.0.11:9858|dataStream:|priority:0, d8c9d548-bd9b-475e-a5cc-64194115352c|rpc:172.21.0.7:9856|admin:172.21.0.7:9857|client:172.21.0.7:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-29 21:36:39,865 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:36:39,868 [43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 43d92334-42eb-45fb-a952-310405367e2b@group-CF122FADA941-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/1f17b98f-1336-47b2-9769-cf122fada941/current/log_inprogress_0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jaxb-impl-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.22.0-CR2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.5.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/validation-api-1.1.0.Final.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.27.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.27.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.27.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.27.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/javax.ws.rs-api-2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.5.0-b42.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-ozone-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.27.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.27.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.27.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
s3g_1               | STARTUP_MSG:   java = 11.0.10
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:35:43,251 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-29 21:35:43,466 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:35:43,513 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-29 21:35:43,529 [main] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
s3g_1               | 2023-06-29 21:35:43,789 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-29 21:35:43,804 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-29 21:35:43,810 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-29 21:35:43,965 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e13529a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:35:44,021 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@e25951c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 29, 2023 9:35:59 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-29 21:35:59,791 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@19ae2ee5{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-hadoop-ozone-s3gateway-1_1_0_jar-_-any-7091416621794643230/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-ozone-s3gateway-1.1.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-29 21:35:59,880 [main] INFO server.AbstractConnector: Started ServerConnector@28b46423{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:35:59,881 [main] INFO server.Server: Started @22681ms
s3g_1               | 2023-06-29 21:35:59,882 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
recon_1             | 2023-06-29 21:36:14,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:14,615 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=19fe7a4f-ef93-44ba-ba95-0c3bae832e5b. Trying to get from SCM.
recon_1             | 2023-06-29 21:36:14,646 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 19fe7a4f-ef93-44ba-ba95-0c3bae832e5b, Nodes: 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:22521fb4-36a7-41c6-aa9d-d3f5477b102c, CreationTimestamp2023-06-29T21:36:08.181Z] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:36:14,647 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 19fe7a4f-ef93-44ba-ba95-0c3bae832e5b, Nodes: 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:OPEN, leaderId:22521fb4-36a7-41c6-aa9d-d3f5477b102c, CreationTimestamp2023-06-29T21:36:08.181Z]
recon_1             | 2023-06-29 21:36:16,728 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:16,731 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:19,367 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:19,371 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:19,790 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:19,791 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:27,111 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:27,112 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: d25069e2-5091-4b21-9dcc-2bb9fcdab2fb, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:43d92334-42eb-45fb-a952-310405367e2b, CreationTimestamp2023-06-29T21:36:08.164Z] moved to OPEN state
recon_1             | 2023-06-29 21:36:27,113 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:34,431 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:34,433 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_2.xcompat_default.
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:35:45,608 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 0fbbcfbae23b/172.21.0.6
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
recon_1             | 2023-06-29 21:36:34,485 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:36:34,607 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:34,824 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:39,755 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline THREE PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 reported by 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:36:39,756 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1f17b98f-1336-47b2-9769-cf122fada941, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:22521fb4-36a7-41c6-aa9d-d3f5477b102c, CreationTimestamp2023-06-29T21:36:08.177Z] moved to OPEN state
recon_1             | 2023-06-29 21:36:42,763 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:36:42,768 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-29 21:37:07,632 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:37:07,632 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:37:08,147 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688074627632
recon_1             | 2023-06-29 21:37:08,158 [pool-14-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:37:08,159 [pool-14-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:37:08,236 [pool-14-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688074627632.
recon_1             | 2023-06-29 21:37:08,265 [pool-14-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:37:08,462 [pool-15-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-29 21:37:08,470 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:37:08,494 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Creating new Recon Container DB at /data/metadata/recon/recon-container-key.db_1688074628472
recon_1             | 2023-06-29 21:37:08,495 [pool-15-thread-1] INFO impl.ContainerDBServiceProviderImpl: Cleaning up old Recon Container key DB at /data/metadata/recon/recon-container-key.db_1688074547656.
recon_1             | 2023-06-29 21:37:08,735 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:37:08,735 [pool-15-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.264 seconds to process 4 keys.
recon_1             | 2023-06-29 21:37:08,762 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:37:08,795 [pool-15-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-29 21:37:19,832 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-29 21:37:19,851 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:35:45,695 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:35:46,406 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:35:46,801 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm;cid=CID-992a5c27-d477-4bb7-8860-05dc77cc875e;layoutVersion=0
scm_1               | 2023-06-29 21:35:46,936 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 0fbbcfbae23b/172.21.0.6
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:35:59,078 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 0fbbcfbae23b/172.21.0.6
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.1.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.2.2.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.13.3.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.0.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/picocli-4.4.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.2.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-container-service-1.1.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.2.0.jar:/opt/hadoop/share/ozone/lib/javax.activation-api-1.2.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-admin-1.1.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.0.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.2.0.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-hadoop-dependency-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.0.0.jar:/opt/hadoop/share/ozone/lib/okio-2.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/guava-28.2-jre.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/xz-1.0.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/libthrift-0.13.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.13.3.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.4.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.0.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.8.1.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.0.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-config-1.1.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-client-1.1.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.2.2.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.0.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-framework-1.1.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.5.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-interface-server-1.1.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/checker-qual-2.10.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.0.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.2.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.35.v20201120.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/f2406c4b4eb2b1295d06ec379cfe684ed9a16637 ; compiled by 'ppogde' on 2021-04-09T04:10Z
scm_1               | STARTUP_MSG:   java = 11.0.10
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:35:59,150 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:35:59,825 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:36:00,469 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:36:00,848 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-common-1.1.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-29 21:36:00,856 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-29 21:36:01,207 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-29 21:36:01,854 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-29 21:36:01,930 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-29 21:36:01,942 [main] INFO pipeline.SCMPipelineManager: No pipeline exists in current db
scm_1               | 2023-06-29 21:36:01,991 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:36:02,134 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:36:02,148 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:36:03,846 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:36:03,899 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-29 21:36:03,947 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:36:03,960 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:36:03,981 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:36:03,983 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:36:04,254 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:36:04,526 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:36:04,597 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:36:04,598 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:36:05,135 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:36:05,136 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:36:05,148 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:36:05,233 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:36:05,296 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:36:05,308 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:36:05,310 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:36:05,329 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:36:05,329 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:36:05,331 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:36:05,331 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:36:05,444 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@77d680e6] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-29 21:36:05,512 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:36:05,512 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:36:05,561 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @16588ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:36:05,927 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-29 21:36:05,954 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:36:06,012 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:36:06,018 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:36:06,018 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:36:06,018 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:36:06,429 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-29 21:36:06,430 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.35.v20201120; built: 2020-11-20T21:17:03.964Z; git: bdc54f03a5e0a7e280fab27f55c3c75ee8da89fb; jvm 11.0.10+9-LTS
scm_1               | 2023-06-29 21:36:06,690 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:36:06,691 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:36:06,702 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm_1               | 2023-06-29 21:36:06,873 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7e7f0216{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:36:06,874 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@52e04737{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:36:07,315 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@60e9c3a5{scm,/,file:///tmp/jetty-0_0_0_0-9876-hadoop-hdds-server-scm-1_1_0_jar-_-any-4897824382658361499/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hadoop-hdds-server-scm-1.1.0.jar!/webapps/scm}
scm_1               | 2023-06-29 21:36:07,337 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@e84fb85{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:36:07,337 [Listener at 0.0.0.0/9860] INFO server.Server: Started @18377ms
scm_1               | 2023-06-29 21:36:07,347 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:36:07,348 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:36:07,349 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:36:08,047 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/d8c9d548-bd9b-475e-a5cc-64194115352c
scm_1               | 2023-06-29 21:36:08,058 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:36:08,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:36:08,105 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/22521fb4-36a7-41c6-aa9d-d3f5477b102c
scm_1               | 2023-06-29 21:36:08,110 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/43d92334-42eb-45fb-a952-310405367e2b
scm_1               | 2023-06-29 21:36:08,122 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:36:08,123 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:36:08,128 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:36:08,128 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:36:08,135 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2 to datanode:d8c9d548-bd9b-475e-a5cc-64194115352c
scm_1               | 2023-06-29 21:36:08,128 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,137 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,137 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,137 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,137 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:36:08,138 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,138 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,138 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:08,147 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.131944Z]
scm_1               | 2023-06-29 21:36:08,164 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb to datanode:43d92334-42eb-45fb-a952-310405367e2b
scm_1               | 2023-06-29 21:36:08,164 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb to datanode:d8c9d548-bd9b-475e-a5cc-64194115352c
scm_1               | 2023-06-29 21:36:08,164 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb to datanode:22521fb4-36a7-41c6-aa9d-d3f5477b102c
scm_1               | 2023-06-29 21:36:08,166 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: d25069e2-5091-4b21-9dcc-2bb9fcdab2fb, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.164321Z]
scm_1               | 2023-06-29 21:36:08,170 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9d0ea578-3f9a-4558-a542-9c6c3364d118 to datanode:43d92334-42eb-45fb-a952-310405367e2b
scm_1               | 2023-06-29 21:36:08,176 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 9d0ea578-3f9a-4558-a542-9c6c3364d118, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.170870Z]
scm_1               | 2023-06-29 21:36:08,177 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 to datanode:d8c9d548-bd9b-475e-a5cc-64194115352c
scm_1               | 2023-06-29 21:36:08,177 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 to datanode:22521fb4-36a7-41c6-aa9d-d3f5477b102c
scm_1               | 2023-06-29 21:36:08,178 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 to datanode:43d92334-42eb-45fb-a952-310405367e2b
scm_1               | 2023-06-29 21:36:08,179 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 1f17b98f-1336-47b2-9769-cf122fada941, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.177722Z]
scm_1               | 2023-06-29 21:36:08,180 [RatisPipelineUtilsThread] INFO pipeline.SCMPipelineManager: Pipeline: PipelineID=1f17b98f-1336-47b2-9769-cf122fada941 contains same datanodes as previous pipelines: PipelineID=d25069e2-5091-4b21-9dcc-2bb9fcdab2fb nodeIds: d8c9d548-bd9b-475e-a5cc-64194115352c, 22521fb4-36a7-41c6-aa9d-d3f5477b102c, 43d92334-42eb-45fb-a952-310405367e2b
scm_1               | 2023-06-29 21:36:08,181 [RatisPipelineUtilsThread] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=19fe7a4f-ef93-44ba-ba95-0c3bae832e5b to datanode:22521fb4-36a7-41c6-aa9d-d3f5477b102c
scm_1               | 2023-06-29 21:36:08,182 [RatisPipelineUtilsThread] INFO pipeline.PipelineStateManager: Created pipeline Pipeline[ Id: 19fe7a4f-ef93-44ba-ba95-0c3bae832e5b, Nodes: 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:36:08.181525Z]
scm_1               | 2023-06-29 21:36:11,224 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:11,234 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: c0bc27f8-ee8a-4c9f-a490-b62c0b0740e2, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:d8c9d548-bd9b-475e-a5cc-64194115352c, CreationTimestamp2023-06-29T21:36:08.131944Z] moved to OPEN state
scm_1               | 2023-06-29 21:36:11,262 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:11,311 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:11,400 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:11,754 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:11,755 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:14,141 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:14,247 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:14,248 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:14,284 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:14,284 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 9d0ea578-3f9a-4558-a542-9c6c3364d118, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:43d92334-42eb-45fb-a952-310405367e2b, CreationTimestamp2023-06-29T21:36:08.170870Z] moved to OPEN state
scm_1               | 2023-06-29 21:36:14,284 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:14,395 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:14,408 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:14,622 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:14,623 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 19fe7a4f-ef93-44ba-ba95-0c3bae832e5b, Nodes: 22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:ONE, State:ALLOCATED, leaderId:22521fb4-36a7-41c6-aa9d-d3f5477b102c, CreationTimestamp2023-06-29T21:36:08.181525Z] moved to OPEN state
scm_1               | 2023-06-29 21:36:14,624 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:16,729 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:16,730 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:19,374 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:19,375 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:19,795 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:19,796 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:27,097 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:36:27,097 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: d25069e2-5091-4b21-9dcc-2bb9fcdab2fb, Nodes: 43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:43d92334-42eb-45fb-a952-310405367e2b, CreationTimestamp2023-06-29T21:36:08.164321Z] moved to OPEN state
scm_1               | 2023-06-29 21:36:27,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:36:27,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:36:27,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:36:27,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-29 21:36:39,730 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineStateManager: Pipeline Pipeline[ Id: 1f17b98f-1336-47b2-9769-cf122fada941, Nodes: d8c9d548-bd9b-475e-a5cc-64194115352c{ip: 172.21.0.7, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}22521fb4-36a7-41c6-aa9d-d3f5477b102c{ip: 172.21.0.13, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}43d92334-42eb-45fb-a952-310405367e2b{ip: 172.21.0.11, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:22521fb4-36a7-41c6-aa9d-d3f5477b102c, CreationTimestamp2023-06-29T21:36:08.177722Z] moved to OPEN state
scm_1               | 2023-06-29 21:37:00,428 [IPC Server handler 94 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.8
scm_1               | 2023-06-29 21:37:10,022 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.8
scm_1               | 2023-06-29 21:38:00,569 [IPC Server handler 0 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.8
scm_1               | 2023-06-29 21:38:08,968 [IPC Server handler 5 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.21.0.8
Attaching to xcompat_datanode_3, xcompat_datanode_2, xcompat_old_client_1_1_0_1, xcompat_datanode_1, xcompat_new_client_1, xcompat_recon_1, xcompat_s3g_1, xcompat_old_client_1_2_1_1, xcompat_om_1, xcompat_old_client_1_0_0_1, xcompat_old_client_1_3_0_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:38:29,865 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 52422f59a607/172.22.0.13
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.2.1
datanode_1          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_1          | STARTUP_MSG:   java = 11.0.13
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:38:29,924 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:38:31,326 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:38:31,751 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:38:32,416 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:38:32,419 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:38:33,422 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:52422f59a607 ip:172.22.0.13
datanode_1          | 2023-06-29 21:38:34,612 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_1          | 2023-06-29 21:38:35,283 [main] INFO reflections.Reflections: Reflections took 575 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_1          | 2023-06-29 21:38:36,503 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:38:36,531 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-29 21:38:36,533 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:38:36,534 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:38:36,766 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:38:36,870 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:38:36,884 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-29 21:38:36,890 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-29 21:38:36,895 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-29 21:38:36,899 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:38:37,035 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:38:37,043 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:38:43,337 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:38:43,636 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:38:44,325 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-29 21:38:44,334 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-29 21:38:44,335 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-29 21:38:44,354 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:38:44,358 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:38:44,359 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:38:44,360 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:38:45,888 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-29 21:38:45,899 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:38:45,899 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:38:45,994 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:38:46,663 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:38:46,857 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:38:47,105 [main] INFO util.log: Logging initialized @22207ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:38:47,677 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_1          | 2023-06-29 21:38:47,708 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:38:47,740 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:38:47,746 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:38:47,763 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:38:47,764 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:38:48,057 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-29 21:38:48,061 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_1          | 2023-06-29 21:38:48,241 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:38:48,247 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:38:48,249 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_1          | 2023-06-29 21:38:48,332 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6579cdbb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:38:48,340 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@64bebd55{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:38:49,701 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3e1f1046{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-9404655503467423572/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:38:49,768 [main] INFO server.AbstractConnector: Started ServerConnector@78b7f805{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:38:49,771 [main] INFO server.Server: Started @24873ms
datanode_1          | 2023-06-29 21:38:49,784 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-29 21:38:49,784 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-29 21:38:49,788 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:38:49,806 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-29 21:38:50,076 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5d0998aa] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-29 21:38:50,345 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.10:9891
datanode_1          | 2023-06-29 21:38:50,530 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:38:53,355 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:38:54,356 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:38:55,357 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:38:56,837 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-29 21:38:56,838 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-29 21:38:57,215 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:38:57,293 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.RaftServer: 572b7029-abfb-4d84-a537-3845bbccad20: start RPC server
datanode_1          | 2023-06-29 21:38:57,306 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 572b7029-abfb-4d84-a537-3845bbccad20: GrpcService started, listening on 9856
datanode_1          | 2023-06-29 21:38:57,307 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 572b7029-abfb-4d84-a537-3845bbccad20: GrpcService started, listening on 9857
datanode_1          | 2023-06-29 21:38:57,308 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 572b7029-abfb-4d84-a537-3845bbccad20: GrpcService started, listening on 9858
datanode_1          | 2023-06-29 21:38:57,332 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 572b7029-abfb-4d84-a537-3845bbccad20 is started using port 9858 for RATIS
datanode_1          | 2023-06-29 21:38:57,332 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 572b7029-abfb-4d84-a537-3845bbccad20 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-29 21:38:57,332 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 572b7029-abfb-4d84-a537-3845bbccad20 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-29 21:38:57,333 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@71696c43] INFO util.JvmPauseMonitor: JvmPauseMonitor-572b7029-abfb-4d84-a537-3845bbccad20: Started
datanode_1          | 2023-06-29 21:38:57,378 [EndpointStateMachine task thread for recon/172.22.0.10:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 52422f59a607/172.22.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.13:40266 remote=recon/172.22.0.10:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.13:40266 remote=recon/172.22.0.10:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_1          | 2023-06-29 21:39:01,150 [Command processor thread] INFO server.RaftServer: 572b7029-abfb-4d84-a537-3845bbccad20: addNew group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] returns group-06F746AEF7C1:java.util.concurrent.CompletableFuture@49d37159[Not completed]
datanode_1          | 2023-06-29 21:39:01,280 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20: new RaftServerImpl for group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:39:01,289 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:39:01,290 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:39:01,298 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:39:01,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:01,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:39:01,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:39:01,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:39:01,327 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:39:01,332 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:39:01,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:39:01,344 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:39:01,353 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 does not exist. Creating ...
datanode_1          | 2023-06-29 21:39:01,417 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/in_use.lock acquired by nodename 7@52422f59a607
datanode_1          | 2023-06-29 21:39:01,528 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 has been successfully formatted.
datanode_1          | 2023-06-29 21:39:01,555 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:01,555 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06F746AEF7C1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:39:01,627 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:01,649 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:39:01,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:39:01,747 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:39:01,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:01,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:39:01,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:39:01,861 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_1          | 2023-06-29 21:39:01,861 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:39:01,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:01,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:01,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:39:01,865 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:39:01,868 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:39:01,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:39:01,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:39:01,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:01,916 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:01,953 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:01,953 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:01,957 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:01,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:39:01,971 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:39:01,972 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:39:01,979 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:39:01,982 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:39:02,095 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:02,116 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:39:02,117 [pool-22-thread-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:02,151 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06F746AEF7C1,id=572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:39:02,193 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_1          | 2023-06-29 21:39:04,349 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1.
datanode_1          | 2023-06-29 21:39:04,351 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20: new RaftServerImpl for group-13166AACF0A7:[572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:39:04,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:39:04,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:39:04,355 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: ConfigurationManager, init=-1: [572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:39:04,355 [Command processor thread] INFO server.RaftServer: 572b7029-abfb-4d84-a537-3845bbccad20: addNew group-13166AACF0A7:[572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1] returns group-13166AACF0A7:java.util.concurrent.CompletableFuture@77c03c30[Not completed]
datanode_1          | 2023-06-29 21:39:04,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:39:04,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:39:04,356 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:39:04,356 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d774fccb-ebaf-434f-9da2-13166aacf0a7 does not exist. Creating ...
datanode_1          | 2023-06-29 21:39:04,358 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d774fccb-ebaf-434f-9da2-13166aacf0a7/in_use.lock acquired by nodename 7@52422f59a607
datanode_1          | 2023-06-29 21:39:04,375 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d774fccb-ebaf-434f-9da2-13166aacf0a7 has been successfully formatted.
datanode_1          | 2023-06-29 21:39:04,376 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:04,394 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-13166AACF0A7: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:39:04,395 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:04,396 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:39:04,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:39:04,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:39:04,400 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,400 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:39:04,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:39:04,401 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d774fccb-ebaf-434f-9da2-13166aacf0a7
datanode_1          | 2023-06-29 21:39:04,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:39:04,401 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:04,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:39:04,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:39:04,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:39:04,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:39:04,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:39:04,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:04,418 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:04,418 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:04,418 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:04,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:39:04,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:39:04,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:39:04,423 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:39:04,423 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:39:04,424 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: start as a follower, conf=-1: [572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:39:04,424 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:39:04,425 [pool-22-thread-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState
datanode_1          | 2023-06-29 21:39:04,428 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-13166AACF0A7,id=572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:39:04,435 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d774fccb-ebaf-434f-9da2-13166aacf0a7
datanode_1          | 2023-06-29 21:39:04,435 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=d774fccb-ebaf-434f-9da2-13166aacf0a7.
datanode_1          | 2023-06-29 21:39:04,436 [Command processor thread] INFO server.RaftServer: 572b7029-abfb-4d84-a537-3845bbccad20: addNew group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] returns group-12A6932B9835:java.util.concurrent.CompletableFuture@486f168c[Not completed]
datanode_1          | 2023-06-29 21:39:04,440 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20: new RaftServerImpl for group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:39:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:39:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:39:04,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:39:04,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:04,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:39:04,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:39:04,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:39:04,443 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:39:04,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:39:04,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:39:04,445 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:39:04,447 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 does not exist. Creating ...
datanode_1          | 2023-06-29 21:39:04,449 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/in_use.lock acquired by nodename 7@52422f59a607
datanode_1          | 2023-06-29 21:39:04,450 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 has been successfully formatted.
datanode_1          | 2023-06-29 21:39:04,451 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:04,481 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-12A6932B9835: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:39:04,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:39:04,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:39:04,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:39:04,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:39:04,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:39:04,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:39:04,483 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835
datanode_1          | 2023-06-29 21:39:04,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_1          | 2023-06-29 21:39:04,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:04,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:39:04,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:38:30,073 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 4c640ae30598/172.22.0.11
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.2.1
datanode_3          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_3          | STARTUP_MSG:   java = 11.0.13
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:38:30,142 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:38:31,530 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:38:31,976 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:38:32,692 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:38:32,692 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:38:33,529 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4c640ae30598 ip:172.22.0.11
datanode_3          | 2023-06-29 21:38:34,802 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_3          | 2023-06-29 21:38:35,388 [main] INFO reflections.Reflections: Reflections took 493 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_3          | 2023-06-29 21:38:36,526 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:38:36,596 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-29 21:38:36,616 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:38:36,627 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:38:36,718 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:38:36,889 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:38:36,900 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-29 21:38:36,916 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-29 21:38:36,916 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-29 21:38:36,930 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-29 21:38:37,031 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:38:37,032 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:38:42,853 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:38:43,209 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:38:44,056 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-29 21:38:44,057 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-29 21:38:44,058 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-29 21:38:44,060 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:38:44,064 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:38:44,065 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:38:44,080 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:38:45,061 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-29 21:38:45,077 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:38:45,079 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:38:45,140 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:38:46,008 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:38:46,112 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:38:46,219 [main] INFO util.log: Logging initialized @21139ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:38:46,822 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_3          | 2023-06-29 21:38:46,846 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:38:46,887 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:38:46,912 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:38:46,913 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:38:46,913 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:38:47,226 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-29 21:38:47,284 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_3          | 2023-06-29 21:38:47,493 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:38:47,518 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:38:47,520 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_3          | 2023-06-29 21:38:47,636 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7a1b8a46{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:38:47,637 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@40d52be7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:38:49,170 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@45a1d057{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-3528673380690800368/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:38:49,288 [main] INFO server.AbstractConnector: Started ServerConnector@322e49ee{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:38:49,299 [main] INFO server.Server: Started @24219ms
datanode_3          | 2023-06-29 21:38:49,310 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-29 21:38:49,310 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-29 21:38:49,317 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:38:49,384 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-29 21:38:49,957 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@64fd99e8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-29 21:38:50,228 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.10:9891
datanode_3          | 2023-06-29 21:38:50,487 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-29 21:38:53,329 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:38:54,330 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:38:55,331 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:38:56,824 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-29 21:38:56,825 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-29 21:38:57,064 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_3          | 2023-06-29 21:38:57,196 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.RaftServer: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start RPC server
datanode_3          | 2023-06-29 21:38:57,205 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: GrpcService started, listening on 9856
datanode_3          | 2023-06-29 21:38:57,206 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:38:57,212 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: GrpcService started, listening on 9858
datanode_3          | 2023-06-29 21:38:57,237 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 520bd49a-4ee1-4e6e-9505-d3fcf658e00e is started using port 9858 for RATIS
datanode_3          | 2023-06-29 21:38:57,246 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 520bd49a-4ee1-4e6e-9505-d3fcf658e00e is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-29 21:38:57,246 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 520bd49a-4ee1-4e6e-9505-d3fcf658e00e is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-29 21:38:57,237 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$309/0x00000008404bec40@3c335edc] INFO util.JvmPauseMonitor: JvmPauseMonitor-520bd49a-4ee1-4e6e-9505-d3fcf658e00e: Started
datanode_3          | 2023-06-29 21:38:57,439 [EndpointStateMachine task thread for recon/172.22.0.10:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 4c640ae30598/172.22.0.11 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.11:42454 remote=recon/172.22.0.10:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.11:42454 remote=recon/172.22.0.10:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_3          | 2023-06-29 21:39:00,996 [Command processor thread] INFO server.RaftServer: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: addNew group-3446AB55CE0B:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:1] returns group-3446AB55CE0B:java.util.concurrent.CompletableFuture@34b919b1[Not completed]
datanode_3          | 2023-06-29 21:39:01,103 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: new RaftServerImpl for group-3446AB55CE0B:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:39:01,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:39:01,134 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:39:01,134 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:39:01,140 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:01,140 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:39:01,146 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:39:01,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:39:01,188 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:39:01,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:39:01,225 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:39:01,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:39:01,245 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/0dbc6dab-65b5-4deb-807e-3446ab55ce0b does not exist. Creating ...
datanode_3          | 2023-06-29 21:39:01,293 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/0dbc6dab-65b5-4deb-807e-3446ab55ce0b/in_use.lock acquired by nodename 6@4c640ae30598
datanode_3          | 2023-06-29 21:39:01,320 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/0dbc6dab-65b5-4deb-807e-3446ab55ce0b has been successfully formatted.
datanode_3          | 2023-06-29 21:39:01,335 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:01,341 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-3446AB55CE0B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:39:01,342 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:01,346 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:39:01,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:39:01,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:39:01,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:01,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:39:01,506 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:39:01,529 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/0dbc6dab-65b5-4deb-807e-3446ab55ce0b
datanode_3          | 2023-06-29 21:39:01,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:39:01,534 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:39:01,576 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:01,586 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:39:01,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:39:01,605 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:39:01,605 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:39:01,606 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:39:01,660 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:01,661 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:01,696 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:01,697 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:01,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:01,711 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:39:01,711 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:39:01,713 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:39:01,714 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:39:01,714 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:39:01,999 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:39:02,012 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:38:29,808 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = e43ca4451925/172.22.0.9
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.2.1
datanode_2          | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.2.1.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
datanode_2          | STARTUP_MSG:   java = 11.0.13
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:38:29,857 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:38:31,192 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:38:31,642 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:38:32,447 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:38:32,447 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:38:33,490 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:e43ca4451925 ip:172.22.0.9
datanode_2          | 2023-06-29 21:38:34,757 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
datanode_2          | 2023-06-29 21:38:35,591 [main] INFO reflections.Reflections: Reflections took 668 ms to scan 2 urls, producing 84 keys and 167 values 
datanode_2          | 2023-06-29 21:38:36,902 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:38:36,923 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-29 21:38:36,948 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:38:36,960 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:38:37,126 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:38:37,304 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:38:37,332 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-29 21:38:37,343 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-29 21:38:37,344 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-29 21:38:37,344 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-29 21:38:37,524 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:38:37,525 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:38:43,875 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:38:44,269 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:38:44,970 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-29 21:38:44,984 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-29 21:38:44,993 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-29 21:38:44,994 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:38:44,996 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:38:45,014 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:38:45,020 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:38:45,883 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-29 21:38:45,887 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:38:45,889 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:38:45,926 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:38:46,673 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:38:46,813 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:38:47,039 [main] INFO util.log: Logging initialized @22203ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:38:47,687 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
datanode_2          | 2023-06-29 21:38:47,735 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:38:47,787 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:38:47,805 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:38:47,805 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:38:47,814 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-29 21:38:48,110 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-29 21:38:48,118 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
datanode_2          | 2023-06-29 21:38:48,442 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:38:48,443 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:38:48,453 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-29 21:38:48,496 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2921199d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:38:48,509 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@23ad71bf{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:38:49,771 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@51a16adf{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_2_1_jar-_-any-8205711202282128292/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:39:02,022 [pool-22-thread-1] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState
datanode_3          | 2023-06-29 21:39:02,041 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-3446AB55CE0B,id=520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_3          | 2023-06-29 21:39:02,109 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=0dbc6dab-65b5-4deb-807e-3446ab55ce0b
datanode_3          | 2023-06-29 21:39:02,110 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=0dbc6dab-65b5-4deb-807e-3446ab55ce0b.
datanode_3          | 2023-06-29 21:39:02,110 [Command processor thread] INFO server.RaftServer: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: addNew group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] returns group-06F746AEF7C1:java.util.concurrent.CompletableFuture@5cfd8f0d[Not completed]
datanode_3          | 2023-06-29 21:39:02,130 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: new RaftServerImpl for group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:39:02,139 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:39:02,143 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:39:02,144 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:39:02,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:02,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:39:02,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:39:02,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:39:02,158 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:39:02,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:39:02,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:39:02,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:39:02,160 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 does not exist. Creating ...
datanode_3          | 2023-06-29 21:39:02,173 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/in_use.lock acquired by nodename 6@4c640ae30598
datanode_3          | 2023-06-29 21:39:02,176 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 has been successfully formatted.
datanode_3          | 2023-06-29 21:39:02,176 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:02,176 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06F746AEF7C1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:39:02,187 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:02,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:39:02,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:39:02,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:39:02,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:02,218 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:39:02,219 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:39:02,221 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_3          | 2023-06-29 21:39:02,232 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:39:02,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:39:02,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:02,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:39:02,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:39:02,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:39:02,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:39:02,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:39:02,239 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:02,242 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:02,260 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:02,266 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:02,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:02,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:39:02,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:39:02,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:39:02,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:39:02,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:39:02,316 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:02,316 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:39:02,316 [pool-22-thread-1] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:02,331 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06F746AEF7C1,id=520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_3          | 2023-06-29 21:39:02,333 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_3          | 2023-06-29 21:39:04,422 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1.
datanode_3          | 2023-06-29 21:39:04,432 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: new RaftServerImpl for group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:39:04,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:39:04,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:39:04,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:39:04,436 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:04,437 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:39:04,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:39:04,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:39:04,441 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:39:04,443 [Command processor thread] INFO server.RaftServer: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: addNew group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] returns group-12A6932B9835:java.util.concurrent.CompletableFuture@4626ae51[Not completed]
datanode_3          | 2023-06-29 21:39:04,444 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:39:04,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:39:04,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:39:04,453 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 does not exist. Creating ...
datanode_3          | 2023-06-29 21:39:04,463 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/in_use.lock acquired by nodename 6@4c640ae30598
datanode_3          | 2023-06-29 21:39:04,476 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 has been successfully formatted.
datanode_3          | 2023-06-29 21:39:04,478 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:04,478 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-12A6932B9835: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:39:04,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:39:04,503 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:39:04,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:39:04,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:39:04,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:04,504 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:39:04,505 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:39:04,507 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835
datanode_3          | 2023-06-29 21:39:04,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_3          | 2023-06-29 21:39:04,507 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:39:04,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:39:04,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:39:04,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:39:04,523 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:04,532 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:04,532 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:39:04,533 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:39:04,533 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:39:04,533 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:39:04,533 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:39:04,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:39:04,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:39:04,537 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:39:04,537 [pool-22-thread-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:39:04,538 [pool-22-thread-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FollowerState
datanode_1          | 2023-06-29 21:39:04,538 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12A6932B9835,id=572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:39:04,539 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=23a15059-7de0-4dec-8658-12a6932b9835
datanode_1          | 2023-06-29 21:39:04,795 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835.
datanode_1          | 2023-06-29 21:39:07,272 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5155403810ns, electionTimeout:5111ms
datanode_1          | 2023-06-29 21:39:07,273 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:07,273 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:39:07,283 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: receive requestVote(ELECTION, b4562603-a319-4320-8669-6b5a2bb28a32, group-06F746AEF7C1, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:39:07,288 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:39:07,288 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection1
datanode_1          | 2023-06-29 21:39:07,300 [grpc-default-executor-1] INFO impl.VoteContext: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-CANDIDATE: reject ELECTION from b4562603-a319-4320-8669-6b5a2bb28a32: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-29 21:39:07,308 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:b4562603-a319-4320-8669-6b5a2bb28a32
datanode_1          | 2023-06-29 21:39:07,308 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection1
datanode_1          | 2023-06-29 21:39:07,308 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:07,322 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection1] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection1: skip running since this is already CLOSING
datanode_1          | 2023-06-29 21:39:07,361 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1 replies to ELECTION vote request: b4562603-a319-4320-8669-6b5a2bb28a32<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t1. Peer's state: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1:t1, leader=null, voted=null, raftlog=572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:09,555 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5130284783ns, electionTimeout:5120ms
datanode_1          | 2023-06-29 21:39:09,556 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState
datanode_1          | 2023-06-29 21:39:09,556 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:39:09,556 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:39:04,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:39:04,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:04,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:04,536 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:04,536 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:39:04,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:39:04,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:39:04,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:39:04,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:39:04,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:39:04,541 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:39:04,542 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:39:04,542 [pool-22-thread-1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:39:04,542 [pool-22-thread-1] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FollowerState
datanode_3          | 2023-06-29 21:39:04,558 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12A6932B9835,id=520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_3          | 2023-06-29 21:39:04,577 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=23a15059-7de0-4dec-8658-12a6932b9835
datanode_3          | 2023-06-29 21:39:04,792 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835.
datanode_3          | 2023-06-29 21:39:07,061 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5039514652ns, electionTimeout:5037ms
datanode_3          | 2023-06-29 21:39:07,061 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState
datanode_3          | 2023-06-29 21:39:07,062 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:39:07,064 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:39:07,064 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1
datanode_3          | 2023-06-29 21:39:07,080 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:39:07,081 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-29 21:39:07,081 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1
datanode_3          | 2023-06-29 21:39:07,082 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:39:07,082 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-3446AB55CE0B with new leaderId: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_3          | 2023-06-29 21:39:07,083 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:07,086 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: change Leader from null to 520bd49a-4ee1-4e6e-9505-d3fcf658e00e at term 1 for becomeLeader, leader elected after 5740ms
datanode_3          | 2023-06-29 21:39:07,109 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:39:07,120 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:39:07,128 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:39:07,141 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:39:07,141 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:39:07,141 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:39:09,556 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2
datanode_1          | 2023-06-29 21:39:09,563 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:39:09,564 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-29 21:39:09,564 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2
datanode_1          | 2023-06-29 21:39:09,564 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:39:09,564 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-13166AACF0A7 with new leaderId: 572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:39:09,565 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:09,568 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: change Leader from null to 572b7029-abfb-4d84-a537-3845bbccad20 at term 1 for becomeLeader, leader elected after 5169ms
datanode_1          | 2023-06-29 21:39:09,575 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:39:09,580 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:09,581 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:39:09,586 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:39:09,586 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:39:09,589 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:39:09,598 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:09,612 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-29 21:39:09,616 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderStateImpl
datanode_1          | 2023-06-29 21:39:09,622 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: receive requestVote(ELECTION, b4562603-a319-4320-8669-6b5a2bb28a32, group-12A6932B9835, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:39:09,623 [grpc-default-executor-1] INFO impl.VoteContext: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FOLLOWER: accept ELECTION from b4562603-a319-4320-8669-6b5a2bb28a32: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:39:09,624 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b4562603-a319-4320-8669-6b5a2bb28a32
datanode_1          | 2023-06-29 21:39:09,624 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FollowerState
datanode_1          | 2023-06-29 21:39:09,624 [572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-29 21:39:09,629 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-FollowerState
datanode_1          | 2023-06-29 21:39:09,647 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835 replies to ELECTION vote request: b4562603-a319-4320-8669-6b5a2bb28a32<-572b7029-abfb-4d84-a537-3845bbccad20#0:OK-t1. Peer's state: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835:t1, leader=null, voted=b4562603-a319-4320-8669-6b5a2bb28a32, raftlog=572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_1          | 2023-06-29 21:39:09,710 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:39:09,775 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-LeaderElection2] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7: set configuration 0: [572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-29 21:39:09,838 [572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-13166AACF0A7-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d774fccb-ebaf-434f-9da2-13166aacf0a7/current/log_inprogress_0
datanode_1          | 2023-06-29 21:39:10,069 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12A6932B9835 with new leaderId: b4562603-a319-4320-8669-6b5a2bb28a32
datanode_1          | 2023-06-29 21:39:10,070 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: change Leader from null to b4562603-a319-4320-8669-6b5a2bb28a32 at term 1 for appendEntries, leader elected after 5588ms
datanode_2          | 2023-06-29 21:38:49,872 [main] INFO server.AbstractConnector: Started ServerConnector@4aeb0e2b{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:38:49,879 [main] INFO server.Server: Started @25043ms
datanode_2          | 2023-06-29 21:38:49,908 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-29 21:38:49,908 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-29 21:38:49,910 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:38:49,943 [Datanode State Machine Thread - 0] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-29 21:38:50,111 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1400e203] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-29 21:38:50,421 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.22.0.10:9891
datanode_2          | 2023-06-29 21:38:50,730 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:38:53,373 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:38:54,374 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:38:55,375 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.22.0.5:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:38:56,797 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-29 21:38:56,798 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-29 21:38:57,057 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:38:57,194 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.RaftServer: b4562603-a319-4320-8669-6b5a2bb28a32: start RPC server
datanode_2          | 2023-06-29 21:38:57,216 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: b4562603-a319-4320-8669-6b5a2bb28a32: GrpcService started, listening on 9856
datanode_2          | 2023-06-29 21:38:57,226 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: b4562603-a319-4320-8669-6b5a2bb28a32: GrpcService started, listening on 9857
datanode_2          | 2023-06-29 21:38:57,228 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO server.GrpcService: b4562603-a319-4320-8669-6b5a2bb28a32: GrpcService started, listening on 9858
datanode_2          | 2023-06-29 21:38:57,245 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b4562603-a319-4320-8669-6b5a2bb28a32 is started using port 9858 for RATIS
datanode_2          | 2023-06-29 21:38:57,246 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b4562603-a319-4320-8669-6b5a2bb28a32 is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-29 21:38:57,246 [EndpointStateMachine task thread for scm/172.22.0.5:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b4562603-a319-4320-8669-6b5a2bb28a32 is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-29 21:38:57,279 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$310/0x00000008404be840@2fc05e12] INFO util.JvmPauseMonitor: JvmPauseMonitor-b4562603-a319-4320-8669-6b5a2bb28a32: Started
datanode_2          | 2023-06-29 21:38:57,427 [EndpointStateMachine task thread for recon/172.22.0.10:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From e43ca4451925/172.22.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.9:40080 remote=recon/172.22.0.10:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1577)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1519)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1416)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:236)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:123)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:71)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:42)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.22.0.9:40080 remote=recon/172.22.0.10:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1892)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)
datanode_2          | 2023-06-29 21:39:01,194 [Command processor thread] INFO server.RaftServer: b4562603-a319-4320-8669-6b5a2bb28a32: addNew group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] returns group-06F746AEF7C1:java.util.concurrent.CompletableFuture@1b0bd90[Not completed]
datanode_2          | 2023-06-29 21:39:01,222 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32: new RaftServerImpl for group-06F746AEF7C1:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:39:01,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:39:01,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:39:01,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:39:01,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:01,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:39:01,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:39:01,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:39:01,243 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:39:01,245 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:39:01,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:39:01,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:39:01,259 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 does not exist. Creating ...
datanode_2          | 2023-06-29 21:39:01,266 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/in_use.lock acquired by nodename 7@e43ca4451925
datanode_2          | 2023-06-29 21:39:01,325 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 has been successfully formatted.
datanode_2          | 2023-06-29 21:39:01,374 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:01,409 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-06F746AEF7C1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:39:01,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:01,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:39:01,512 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:39:01,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:39:01,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:01,685 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:39:01,685 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:39:01,734 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_2          | 2023-06-29 21:39:01,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:39:01,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:01,784 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:01,787 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:39:01,788 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:39:01,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:39:01,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:39:01,793 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:39:01,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:01,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:01,902 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:01,902 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:01,921 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:01,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:39:01,934 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:39:01,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:39:01,942 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:39:07,160 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:39:07,164 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:39:07,175 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderStateImpl
datanode_3          | 2023-06-29 21:39:07,215 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:39:07,311 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: receive requestVote(ELECTION, b4562603-a319-4320-8669-6b5a2bb28a32, group-06F746AEF7C1, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:39:07,312 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-LeaderElection1] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-29 21:39:07,318 [grpc-default-executor-2] INFO impl.VoteContext: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FOLLOWER: accept ELECTION from b4562603-a319-4320-8669-6b5a2bb28a32: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-29 21:39:07,331 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b4562603-a319-4320-8669-6b5a2bb28a32
datanode_3          | 2023-06-29 21:39:07,331 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:07,332 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:39:07,340 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:07,360 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1 replies to ELECTION vote request: b4562603-a319-4320-8669-6b5a2bb28a32<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:OK-t1. Peer's state: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1:t1, leader=null, voted=b4562603-a319-4320-8669-6b5a2bb28a32, raftlog=520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:07,413 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-3446AB55CE0B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/0dbc6dab-65b5-4deb-807e-3446ab55ce0b/current/log_inprogress_0
datanode_3          | 2023-06-29 21:39:09,634 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: receive requestVote(ELECTION, b4562603-a319-4320-8669-6b5a2bb28a32, group-12A6932B9835, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:39:09,636 [grpc-default-executor-2] INFO impl.VoteContext: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FOLLOWER: accept ELECTION from b4562603-a319-4320-8669-6b5a2bb28a32: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:39:09,637 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b4562603-a319-4320-8669-6b5a2bb28a32
datanode_3          | 2023-06-29 21:39:09,637 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FollowerState
datanode_3          | 2023-06-29 21:39:09,637 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FollowerState
datanode_3          | 2023-06-29 21:39:09,637 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:39:09,647 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835 replies to ELECTION vote request: b4562603-a319-4320-8669-6b5a2bb28a32<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:OK-t1. Peer's state: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835:t1, leader=null, voted=b4562603-a319-4320-8669-6b5a2bb28a32, raftlog=520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_3          | 2023-06-29 21:39:10,088 [grpc-default-executor-2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12A6932B9835 with new leaderId: b4562603-a319-4320-8669-6b5a2bb28a32
datanode_3          | 2023-06-29 21:39:10,088 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: change Leader from null to b4562603-a319-4320-8669-6b5a2bb28a32 at term 1 for appendEntries, leader elected after 5605ms
datanode_1          | 2023-06-29 21:39:10,142 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:1], old=null
datanode_1          | 2023-06-29 21:39:10,143 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:39:10,144 [572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-12A6932B9835-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/current/log_inprogress_0
datanode_1          | 2023-06-29 21:39:12,445 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5137050091ns, electionTimeout:5115ms
datanode_1          | 2023-06-29 21:39:12,446 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:12,467 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-29 21:39:12,467 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:39:12,467 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3
datanode_1          | 2023-06-29 21:39:12,541 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: receive requestVote(ELECTION, 520bd49a-4ee1-4e6e-9505-d3fcf658e00e, group-06F746AEF7C1, 2, (t:0, i:0))
datanode_1          | 2023-06-29 21:39:12,548 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:12,548 [grpc-default-executor-1] INFO impl.VoteContext: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-CANDIDATE: reject ELECTION from 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: already has voted for 572b7029-abfb-4d84-a537-3845bbccad20 at current term 2
datanode_1          | 2023-06-29 21:39:12,548 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1 replies to ELECTION vote request: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t2. Peer's state: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1:t2, leader=null, voted=572b7029-abfb-4d84-a537-3845bbccad20, raftlog=572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:12,659 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:39:12,659 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection:   Response 0: 572b7029-abfb-4d84-a537-3845bbccad20<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:FAIL-t2
datanode_1          | 2023-06-29 21:39:12,659 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection:   Response 1: 572b7029-abfb-4d84-a537-3845bbccad20<-b4562603-a319-4320-8669-6b5a2bb28a32#0:FAIL-t2
datanode_1          | 2023-06-29 21:39:12,659 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:39:12,660 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-29 21:39:12,660 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3
datanode_1          | 2023-06-29 21:39:12,660 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection3] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:17,572 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:17,655 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: receive requestVote(ELECTION, 520bd49a-4ee1-4e6e-9505-d3fcf658e00e, group-06F746AEF7C1, 3, (t:0, i:0))
datanode_1          | 2023-06-29 21:39:17,655 [grpc-default-executor-1] INFO impl.VoteContext: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FOLLOWER: reject ELECTION from 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: our priority 1 > candidate's priority 0
datanode_1          | 2023-06-29 21:39:17,656 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_1          | 2023-06-29 21:39:17,656 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:17,656 [grpc-default-executor-1] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:17,657 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_1          | java.lang.InterruptedException: sleep interrupted
datanode_1          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_1          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_1          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_1          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 2023-06-29 21:39:01,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:39:02,051 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:02,074 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:39:02,080 [pool-22-thread-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:02,113 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-06F746AEF7C1,id=b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:39:02,185 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1
datanode_2          | 2023-06-29 21:39:04,366 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1.
datanode_2          | 2023-06-29 21:39:04,369 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32: new RaftServerImpl for group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:39:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:39:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:39:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:39:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:39:04,373 [Command processor thread] INFO server.RaftServer: b4562603-a319-4320-8669-6b5a2bb28a32: addNew group-12A6932B9835:[520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] returns group-12A6932B9835:java.util.concurrent.CompletableFuture@379c7c91[Not completed]
datanode_2          | 2023-06-29 21:39:04,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:39:04,378 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:39:04,378 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: ConfigurationManager, init=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:39:04,379 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:39:04,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:39:04,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:39:04,380 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 does not exist. Creating ...
datanode_2          | 2023-06-29 21:39:04,387 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/in_use.lock acquired by nodename 7@e43ca4451925
datanode_2          | 2023-06-29 21:39:04,398 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835 has been successfully formatted.
datanode_2          | 2023-06-29 21:39:04,399 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-12A6932B9835: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:39:04,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:04,400 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:04,425 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:39:04,425 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:39:04,425 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:39:04,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:04,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:39:04,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:39:04,426 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835
datanode_2          | 2023-06-29 21:39:04,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:39:04,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:04,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:04,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:39:04,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:39:04,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:39:04,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:39:04,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:39:04,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:04,454 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:04,458 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:04,459 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:04,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:04,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:39:04,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:39:04,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:39:04,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:39:04,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:39:04,468 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: start as a follower, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:39:04,477 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:39:04,483 [pool-22-thread-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState
datanode_2          | 2023-06-29 21:39:04,484 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-12A6932B9835,id=b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:39:04,485 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=23a15059-7de0-4dec-8658-12a6932b9835
datanode_2          | 2023-06-29 21:39:04,771 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835.
datanode_2          | 2023-06-29 21:39:04,773 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32: new RaftServerImpl for group-4D612BF98E7D:[b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:39:04,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:39:04,774 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:39:04,775 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: ConfigurationManager, init=-1: [b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:39:04,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:39:04,775 [Command processor thread] INFO server.RaftServer: b4562603-a319-4320-8669-6b5a2bb28a32: addNew group-4D612BF98E7D:[b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1] returns group-4D612BF98E7D:java.util.concurrent.CompletableFuture@3f911cb3[Not completed]
datanode_2          | 2023-06-29 21:39:04,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:39:04,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:39:04,776 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b627f98d-aae1-4e2f-abc9-4d612bf98e7d does not exist. Creating ...
datanode_2          | 2023-06-29 21:39:04,777 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b627f98d-aae1-4e2f-abc9-4d612bf98e7d/in_use.lock acquired by nodename 7@e43ca4451925
datanode_2          | 2023-06-29 21:39:04,783 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b627f98d-aae1-4e2f-abc9-4d612bf98e7d has been successfully formatted.
datanode_2          | 2023-06-29 21:39:04,788 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-4D612BF98E7D: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:39:04,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:39:04,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:39:04,793 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:39:04,788 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:04,795 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:39:04,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:04,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:39:04,796 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:39:04,796 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b627f98d-aae1-4e2f-abc9-4d612bf98e7d
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 2147483647 (custom)
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:39:04,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:39:04,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:39:04,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:39:04,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:39:10,156 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:1], old=null
datanode_3          | 2023-06-29 21:39:10,157 [grpc-default-executor-2] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:39:10,158 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-12A6932B9835-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/current/log_inprogress_0
datanode_3          | 2023-06-29 21:39:12,438 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5089322806ns, electionTimeout:5088ms
datanode_3          | 2023-06-29 21:39:12,439 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:12,439 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-29 21:39:12,439 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:39:12,439 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2
datanode_3          | 2023-06-29 21:39:12,446 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:12,552 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:39:12,552 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.LeaderElection:   Response 0: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t2
datanode_3          | 2023-06-29 21:39:12,560 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-29 21:39:12,561 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_3          | 2023-06-29 21:39:12,561 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2
datanode_3          | 2023-06-29 21:39:12,562 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:12,625 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: receive requestVote(ELECTION, 572b7029-abfb-4d84-a537-3845bbccad20, group-06F746AEF7C1, 2, (t:0, i:0))
datanode_3          | 2023-06-29 21:39:12,625 [grpc-default-executor-2] INFO impl.VoteContext: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FOLLOWER: reject ELECTION from 572b7029-abfb-4d84-a537-3845bbccad20: already has voted for 520bd49a-4ee1-4e6e-9505-d3fcf658e00e at current term 2
datanode_3          | 2023-06-29 21:39:12,626 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1 replies to ELECTION vote request: 572b7029-abfb-4d84-a537-3845bbccad20<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:FAIL-t2. Peer's state: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1:t2, leader=null, voted=520bd49a-4ee1-4e6e-9505-d3fcf658e00e, raftlog=520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:17,599 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5037563835ns, electionTimeout:5015ms
datanode_3          | 2023-06-29 21:39:17,600 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:17,600 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_3          | 2023-06-29 21:39:17,600 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:39:17,600 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3
datanode_3          | 2023-06-29 21:39:17,635 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:17,679 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:39:17,679 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection:   Response 0: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t3
datanode_3          | 2023-06-29 21:39:17,679 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection:   Response 1: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-b4562603-a319-4320-8669-6b5a2bb28a32#0:OK-t3
datanode_2          | 2023-06-29 21:39:04,813 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:04,815 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:04,816 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:39:04,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:39:04,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:39:04,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:39:04,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:39:04,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:39:04,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:39:04,836 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: start as a follower, conf=-1: [b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:39:04,837 [pool-22-thread-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:39:04,837 [pool-22-thread-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState
datanode_2          | 2023-06-29 21:39:04,837 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-4D612BF98E7D,id=b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:39:04,841 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b627f98d-aae1-4e2f-abc9-4d612bf98e7d
datanode_2          | 2023-06-29 21:39:04,841 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b627f98d-aae1-4e2f-abc9-4d612bf98e7d.
datanode_2          | 2023-06-29 21:39:07,179 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5099136046ns, electionTimeout:5078ms
datanode_2          | 2023-06-29 21:39:07,180 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:07,180 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:39:07,183 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:39:07,186 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1
datanode_2          | 2023-06-29 21:39:07,201 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:07,378 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:39:07,378 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.LeaderElection:   Response 0: b4562603-a319-4320-8669-6b5a2bb28a32<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t1
datanode_2          | 2023-06-29 21:39:07,379 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-29 21:39:07,380 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_2          | 2023-06-29 21:39:07,380 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1
datanode_2          | 2023-06-29 21:39:07,383 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-LeaderElection1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:09,601 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5123583104ns, electionTimeout:5116ms
datanode_2          | 2023-06-29 21:39:09,602 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState
datanode_2          | 2023-06-29 21:39:09,602 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:39:09,602 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:39:09,602 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2
datanode_2          | 2023-06-29 21:39:09,610 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:39:09,651 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:39:09,651 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.LeaderElection:   Response 0: b4562603-a319-4320-8669-6b5a2bb28a32<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:OK-t1
datanode_2          | 2023-06-29 21:39:09,652 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2 ELECTION round 0: result PASSED
datanode_2          | 2023-06-29 21:39:09,652 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2
datanode_2          | 2023-06-29 21:39:09,652 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:39:09,653 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-12A6932B9835 with new leaderId: b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:39:09,656 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: change Leader from null to b4562603-a319-4320-8669-6b5a2bb28a32 at term 1 for becomeLeader, leader elected after 5236ms
datanode_2          | 2023-06-29 21:39:09,684 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:39:09,740 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:09,741 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:39:09,765 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:39:09,765 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:39:09,766 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:39:09,783 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:09,803 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:39:09,823 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:39:09,830 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:39:09,830 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:39:09,845 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:39:09,846 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:39:09,846 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:39:09,865 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:39:09,865 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:39:09,866 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:39:09,889 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:39:09,891 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:39:09,892 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:39:09,897 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderStateImpl
datanode_2          | 2023-06-29 21:39:09,952 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:39:09,956 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5119250710ns, electionTimeout:5115ms
datanode_2          | 2023-06-29 21:39:09,957 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState
datanode_2          | 2023-06-29 21:39:09,957 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_1          | 2023-06-29 21:39:17,676 [grpc-default-executor-1] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1 replies to ELECTION vote request: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-572b7029-abfb-4d84-a537-3845bbccad20#0:FAIL-t3. Peer's state: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1:t3, leader=null, voted=null, raftlog=572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:22,865 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5208743511ns, electionTimeout:5176ms
datanode_1          | 2023-06-29 21:39:22,866 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState
datanode_1          | 2023-06-29 21:39:22,866 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
datanode_1          | 2023-06-29 21:39:22,866 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:39:22,867 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-FollowerState] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4
datanode_1          | 2023-06-29 21:39:22,877 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4 ELECTION round 0: submit vote requests at term 4 for -1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_1          | 2023-06-29 21:39:22,900 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:39:22,900 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.LeaderElection:   Response 0: 572b7029-abfb-4d84-a537-3845bbccad20<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:OK-t4
datanode_1          | 2023-06-29 21:39:22,900 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.LeaderElection: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4 ELECTION round 0: result PASSED
datanode_1          | 2023-06-29 21:39:22,900 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: shutdown 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4
datanode_1          | 2023-06-29 21:39:22,901 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: changes role from CANDIDATE to LEADER at term 4 for changeToLeader
datanode_1          | 2023-06-29 21:39:22,901 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06F746AEF7C1 with new leaderId: 572b7029-abfb-4d84-a537-3845bbccad20
datanode_1          | 2023-06-29 21:39:22,901 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_1          | 2023-06-29 21:39:22,902 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: change Leader from null to 572b7029-abfb-4d84-a537-3845bbccad20 at term 4 for becomeLeader, leader elected after 21274ms
datanode_1          | 2023-06-29 21:39:22,905 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:39:22,910 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:22,911 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:39:22,911 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:39:22,911 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:39:22,911 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:39:22,912 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:39:22,912 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-29 21:39:22,937 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-29 21:39:22,937 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:39:22,938 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-29 21:39:22,940 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-29 21:39:22,944 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:39:22,944 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:39:22,947 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_1          | 2023-06-29 21:39:22,949 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:39:22,950 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_1          | 2023-06-29 21:39:22,950 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_1          | 2023-06-29 21:39:22,953 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:39:22,953 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:39:22,954 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO impl.RoleInfo: 572b7029-abfb-4d84-a537-3845bbccad20: start 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderStateImpl
datanode_1          | 2023-06-29 21:39:22,955 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:39:22,957 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/current/log_inprogress_0
datanode_1          | 2023-06-29 21:39:22,974 [572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1-LeaderElection4] INFO server.RaftServer$Division: 572b7029-abfb-4d84-a537-3845bbccad20@group-06F746AEF7C1: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:0], old=null
datanode_1          | 2023-06-29 21:39:34,967 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:17,679 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.LeaderElection: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-29 21:39:17,707 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_3          | 2023-06-29 21:39:17,720 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
datanode_3          | 2023-06-29 21:39:17,733 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3
datanode_3          | 2023-06-29 21:39:17,734 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-LeaderElection3] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:22,881 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: receive requestVote(ELECTION, 572b7029-abfb-4d84-a537-3845bbccad20, group-06F746AEF7C1, 4, (t:0, i:0))
datanode_3          | 2023-06-29 21:39:22,884 [grpc-default-executor-2] INFO impl.VoteContext: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FOLLOWER: accept ELECTION from 572b7029-abfb-4d84-a537-3845bbccad20: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:39:22,885 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:572b7029-abfb-4d84-a537-3845bbccad20
datanode_3          | 2023-06-29 21:39:22,885 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: shutdown 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:22,886 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_3          | java.lang.InterruptedException: sleep interrupted
datanode_3          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_3          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_3          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_3          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_3          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_3          | 2023-06-29 21:39:22,890 [grpc-default-executor-2] INFO impl.RoleInfo: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: start 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-FollowerState
datanode_3          | 2023-06-29 21:39:22,893 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1 replies to ELECTION vote request: 572b7029-abfb-4d84-a537-3845bbccad20<-520bd49a-4ee1-4e6e-9505-d3fcf658e00e#0:OK-t4. Peer's state: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1:t4, leader=null, voted=572b7029-abfb-4d84-a537-3845bbccad20, raftlog=520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_3          | 2023-06-29 21:39:23,044 [grpc-default-executor-2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06F746AEF7C1 with new leaderId: 572b7029-abfb-4d84-a537-3845bbccad20
datanode_3          | 2023-06-29 21:39:23,044 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: change Leader from null to 572b7029-abfb-4d84-a537-3845bbccad20 at term 4 for appendEntries, leader elected after 20856ms
datanode_3          | 2023-06-29 21:39:23,057 [grpc-default-executor-2] INFO server.RaftServer$Division: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:0], old=null
datanode_3          | 2023-06-29 21:39:23,057 [grpc-default-executor-2] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:39:23,060 [520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e@group-06F746AEF7C1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/current/log_inprogress_0
datanode_3          | 2023-06-29 21:39:34,926 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:09,958 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:39:09,958 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-FollowerState] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3
datanode_2          | 2023-06-29 21:39:09,988 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: [b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:1], old=null
datanode_2          | 2023-06-29 21:39:09,988 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO impl.LeaderElection: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-29 21:39:09,988 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3
datanode_2          | 2023-06-29 21:39:09,988 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:39:09,988 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-4D612BF98E7D with new leaderId: b4562603-a319-4320-8669-6b5a2bb28a32
datanode_2          | 2023-06-29 21:39:09,988 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:09,999 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: change Leader from null to b4562603-a319-4320-8669-6b5a2bb28a32 at term 1 for becomeLeader, leader elected after 5195ms
datanode_2          | 2023-06-29 21:39:10,001 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:39:10,008 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:10,009 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:39:10,009 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:39:10,009 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:39:10,009 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:39:10,010 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:39:10,010 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:39:10,010 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderStateImpl
datanode_2          | 2023-06-29 21:39:10,010 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:39:10,052 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-LeaderElection2] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:0, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-29 21:39:10,053 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-LeaderElection3] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D: set configuration 0: [b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:1], old=null
datanode_2          | 2023-06-29 21:39:10,181 [b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-4D612BF98E7D-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b627f98d-aae1-4e2f-abc9-4d612bf98e7d/current/log_inprogress_0
datanode_2          | 2023-06-29 21:39:10,181 [b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-12A6932B9835-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/23a15059-7de0-4dec-8658-12a6932b9835/current/log_inprogress_0
datanode_2          | 2023-06-29 21:39:12,542 [grpc-default-executor-0] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: receive requestVote(ELECTION, 520bd49a-4ee1-4e6e-9505-d3fcf658e00e, group-06F746AEF7C1, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:39:12,543 [grpc-default-executor-0] INFO impl.VoteContext: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FOLLOWER: accept ELECTION from 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:39:12,543 [grpc-default-executor-0] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_2          | 2023-06-29 21:39:12,543 [grpc-default-executor-0] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:12,544 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:39:12,558 [grpc-default-executor-0] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:12,567 [grpc-default-executor-0] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1 replies to ELECTION vote request: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-b4562603-a319-4320-8669-6b5a2bb28a32#0:OK-t2. Peer's state: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1:t2, leader=null, voted=520bd49a-4ee1-4e6e-9505-d3fcf658e00e, raftlog=b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:12,639 [grpc-default-executor-0] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: receive requestVote(ELECTION, 572b7029-abfb-4d84-a537-3845bbccad20, group-06F746AEF7C1, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:39:12,639 [grpc-default-executor-0] INFO impl.VoteContext: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FOLLOWER: reject ELECTION from 572b7029-abfb-4d84-a537-3845bbccad20: already has voted for 520bd49a-4ee1-4e6e-9505-d3fcf658e00e at current term 2
datanode_2          | 2023-06-29 21:39:12,639 [grpc-default-executor-0] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1 replies to ELECTION vote request: 572b7029-abfb-4d84-a537-3845bbccad20<-b4562603-a319-4320-8669-6b5a2bb28a32#0:FAIL-t2. Peer's state: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1:t2, leader=null, voted=520bd49a-4ee1-4e6e-9505-d3fcf658e00e, raftlog=b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:17,574 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:17,656 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: receive requestVote(ELECTION, 520bd49a-4ee1-4e6e-9505-d3fcf658e00e, group-06F746AEF7C1, 3, (t:0, i:0))
datanode_2          | 2023-06-29 21:39:17,662 [grpc-default-executor-1] INFO impl.VoteContext: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FOLLOWER: accept ELECTION from 520bd49a-4ee1-4e6e-9505-d3fcf658e00e: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:39:17,663 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
datanode_2          | 2023-06-29 21:39:17,663 [grpc-default-executor-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:17,663 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:39:17,663 [grpc-default-executor-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:17,669 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1 replies to ELECTION vote request: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e<-b4562603-a319-4320-8669-6b5a2bb28a32#0:OK-t3. Peer's state: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1:t3, leader=null, voted=520bd49a-4ee1-4e6e-9505-d3fcf658e00e, raftlog=b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:22,884 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: receive requestVote(ELECTION, 572b7029-abfb-4d84-a537-3845bbccad20, group-06F746AEF7C1, 4, (t:0, i:0))
datanode_2          | 2023-06-29 21:39:22,884 [grpc-default-executor-1] INFO impl.VoteContext: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FOLLOWER: accept ELECTION from 572b7029-abfb-4d84-a537-3845bbccad20: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:39:22,885 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:572b7029-abfb-4d84-a537-3845bbccad20
datanode_2          | 2023-06-29 21:39:22,885 [grpc-default-executor-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: shutdown b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:22,885 [grpc-default-executor-1] INFO impl.RoleInfo: b4562603-a319-4320-8669-6b5a2bb28a32: start b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState
datanode_2          | 2023-06-29 21:39:22,886 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState] INFO impl.FollowerState: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-FollowerState was interrupted: {}
datanode_2          | java.lang.InterruptedException: sleep interrupted
datanode_2          | 	at java.base/java.lang.Thread.sleep(Native Method)
datanode_2          | 	at java.base/java.lang.Thread.sleep(Thread.java:334)
datanode_2          | 	at java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:324)
datanode_2          | 	at org.apache.ratis.util.TimeDuration.sleep(TimeDuration.java:309)
datanode_2          | 	at org.apache.ratis.server.impl.FollowerState.run(FollowerState.java:118)
datanode_2          | 2023-06-29 21:39:22,892 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1 replies to ELECTION vote request: 572b7029-abfb-4d84-a537-3845bbccad20<-b4562603-a319-4320-8669-6b5a2bb28a32#0:OK-t4. Peer's state: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1:t4, leader=null, voted=572b7029-abfb-4d84-a537-3845bbccad20, raftlog=b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLog:OPENED:c-1, conf=-1: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|priority:0], old=null
datanode_2          | 2023-06-29 21:39:23,000 [grpc-default-executor-1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-06F746AEF7C1 with new leaderId: 572b7029-abfb-4d84-a537-3845bbccad20
datanode_2          | 2023-06-29 21:39:23,000 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: change Leader from null to 572b7029-abfb-4d84-a537-3845bbccad20 at term 4 for appendEntries, leader elected after 21590ms
datanode_2          | 2023-06-29 21:39:23,020 [grpc-default-executor-1] INFO server.RaftServer$Division: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1: set configuration 0: [520bd49a-4ee1-4e6e-9505-d3fcf658e00e|rpc:172.22.0.11:9856|admin:172.22.0.11:9857|client:172.22.0.11:9858|dataStream:|priority:0, 572b7029-abfb-4d84-a537-3845bbccad20|rpc:172.22.0.13:9856|admin:172.22.0.13:9857|client:172.22.0.13:9858|dataStream:|priority:1, b4562603-a319-4320-8669-6b5a2bb28a32|rpc:172.22.0.9:9856|admin:172.22.0.9:9857|client:172.22.0.9:9858|dataStream:|priority:0], old=null
datanode_2          | 2023-06-29 21:39:23,021 [grpc-default-executor-1] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:39:23,025 [b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b4562603-a319-4320-8669-6b5a2bb28a32@group-06F746AEF7C1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/ea72719d-0c1b-4ae0-b1b4-06f746aef7c1/current/log_inprogress_0
datanode_2          | 2023-06-29 21:39:26,405 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
datanode_2          | 2023-06-29 21:39:34,902 [Datanode State Machine Thread - 0] WARN statemachine.StateContext: No available thread in pool for past 30 seconds.
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:38:29,582 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = ff21264fe19a/172.22.0.4
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-29 21:38:29,646 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:38:36,580 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:38:37,064 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.4:9862
om_1                | 2023-06-29 21:38:37,080 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:38:37,081 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:38:37,156 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:38:41,131 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:43,132 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:45,134 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:47,135 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:49,137 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:51,139 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:38:53,140 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From ff21264fe19a/172.22.0.4 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.22.0.5:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-5443b70b-cf66-449c-a079-505ae8a7b481;layoutVersion=0
om_1                | 2023-06-29 21:38:56,930 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at ff21264fe19a/172.22.0.4
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:38:59,030 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = ff21264fe19a/172.22.0.4
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.2.1
om_1                | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
om_1                | STARTUP_MSG:   java = 11.0.13
om_1                | ************************************************************/
om_1                | 2023-06-29 21:38:59,035 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:39:00,449 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:39:00,560 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.22.0.4:9862
om_1                | 2023-06-29 21:39:00,560 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:39:00,561 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:39:00,578 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:39:00,619 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = INITIAL_VERSION (version = 0), software layout = INITIAL_VERSION (version = 0)
om_1                | 2023-06-29 21:39:01,764 [main] INFO reflections.Reflections: Reflections took 1031 ms to scan 1 urls, producing 95 keys and 258 values [using 2 cores]
om_1                | 2023-06-29 21:39:01,849 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:39:04,975 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:39:05,206 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:39:05,208 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:39:05,471 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:39:05,526 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:39:05,527 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:39:05,564 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-29 21:39:05,571 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:39:05,617 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-29 21:39:05,629 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-29 21:39:05,666 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-29 21:39:05,907 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = -1 (default)
om_1                | 2023-06-29 21:39:05,912 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:39:05,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = -1 (default)
om_1                | 2023-06-29 21:39:05,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:39:05,913 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:39:05,914 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-29 21:39:05,915 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:39:05,919 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-29 21:39:05,919 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:39:06,290 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:39:06,292 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:39:06,294 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:39:06,315 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:39:06,333 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@628b819d[Not completed]
om_1                | 2023-06-29 21:39:06,333 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-29 21:39:06,366 [pool-23-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-29 21:39:06,370 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-29 21:39:06,370 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-29 21:39:06,371 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-29 21:39:06,373 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:39:06,373 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:39:06,373 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-29 21:39:06,374 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-29 21:39:06,387 [pool-23-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: [om1|rpc:om:9872|priority:0], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-29 21:39:06,387 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:39:06,392 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:39:06,402 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-29 21:39:06,403 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-29 21:39:06,404 [pool-23-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-29 21:39:06,470 [pool-23-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@ff21264fe19a
om_1                | 2023-06-29 21:39:06,478 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:39:06,540 [pool-23-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-29 21:39:06,554 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-29 21:39:06,555 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:39:06,591 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-29 21:39:06,600 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:39:06,656 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:39:06,658 [Listener at om/9862] INFO om.OzoneManager: Configured ozone.om.metadata.layout=SIMPLE and disabled optimized OM FS operations
om_1                | 2023-06-29 21:39:06,692 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-29 21:39:06,692 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-29 21:39:06,731 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-29 21:39:06,732 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:39:06,737 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-29 21:39:06,738 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:39:06,739 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-29 21:39:06,739 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-29 21:39:06,740 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:39:06,740 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-29 21:39:06,741 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-29 21:39:06,770 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-29 21:39:06,770 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-29 21:39:06,800 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:39:06,800 [pool-23-thread-1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:39:06,827 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-29 21:39:06,828 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-29 21:39:06,828 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om_1                | 2023-06-29 21:39:06,830 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-29 21:39:06,831 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-29 21:39:06,831 [pool-23-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-29 21:39:06,914 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:39:06,970 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:39:06,970 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-29 21:39:07,129 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.22.0.4:9862
om_1                | 2023-06-29 21:39:07,130 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-29 21:39:07,134 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-29 21:39:07,171 [Listener at om/9862] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:39:07,172 [Listener at om/9862] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:39:07,179 [Listener at om/9862] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-29 21:39:07,212 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-29 21:39:07,389 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-29 21:39:07,399 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (0) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-29 21:39:07,400 [org.apache.ratis.util.JvmPauseMonitor$$Lambda$370/0x00000008404df040@110b7837] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-29 21:39:07,449 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-29 21:39:07,450 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-29 21:39:07,488 [Listener at om/9862] INFO util.log: Logging initialized @9906ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-29 21:39:07,612 [Listener at om/9862] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
om_1                | 2023-06-29 21:39:07,622 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-29 21:39:07,626 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-29 21:39:07,629 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-29 21:39:07,629 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:39:07,632 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:39:07,673 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-29 21:39:07,674 [Listener at om/9862] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
om_1                | 2023-06-29 21:39:07,713 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:39:07,714 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:39:07,721 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-29 21:39:07,737 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c951ada{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:39:07,737 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11f23038{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:39:08,005 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@ad3f70a{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_2_1_jar-_-any-1658386391591659489/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar!/webapps/ozoneManager}
om_1                | 2023-06-29 21:39:08,013 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@2eb0cefe{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-29 21:39:08,014 [Listener at om/9862] INFO server.Server: Started @10431ms
om_1                | 2023-06-29 21:39:08,015 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-29 21:39:08,016 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-29 21:39:08,017 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-29 21:39:08,018 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-29 21:39:08,052 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-29 21:39:08,065 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted will not move to trash
om_1                | 2023-06-29 21:39:08,069 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@45bbc52f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-29 21:39:12,266 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5094285257ns, electionTimeout:5049ms
om_1                | 2023-06-29 21:39:12,267 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:38:28,176 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = bf4a3d8d5728/172.22.0.10
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.2.1
recon_1             | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.2.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.2.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.33.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.33.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.33.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.2.1.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.11.RELEASE.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
recon_1             | STARTUP_MSG:   java = 11.0.13
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:38:28,228 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:38:31,706 [main] INFO reflections.Reflections: Reflections took 326 ms to scan 1 urls, producing 13 keys and 35 values 
recon_1             | 2023-06-29 21:38:33,622 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-29 21:38:34,712 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:38:40,464 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:38:41,784 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:38:41,932 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:38:41,937 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-29 21:38:46,623 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:38:46,719 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:38:46,774 [main] INFO util.log: Logging initialized @22427ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:38:47,258 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
recon_1             | 2023-06-29 21:38:47,295 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-29 21:38:47,318 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:38:47,344 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:38:47,344 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:38:47,352 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:38:47,915 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:38:48,952 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:38:48,988 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-29 21:38:48,996 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-29 21:38:49,108 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:38:49,108 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-29 21:38:50,835 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:38:51,120 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:38:51,154 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
recon_1             | 2023-06-29 21:38:51,157 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-29 21:38:51,284 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:38:51,415 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
recon_1             | 2023-06-29 21:38:51,518 [main] INFO reflections.Reflections: Reflections took 97 ms to scan 3 urls, producing 103 keys and 211 values 
recon_1             | 2023-06-29 21:38:51,594 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-29 21:38:51,628 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-29 21:38:51,636 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-29 21:38:51,639 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-29 21:38:51,730 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:38:51,785 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-29 21:38:51,861 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-29 21:38:51,953 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-29 21:38:51,953 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-29 21:38:52,130 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-29 21:38:52,180 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-29 21:38:52,180 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-29 21:38:52,951 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-29 21:38:52,963 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
recon_1             | 2023-06-29 21:38:53,065 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-29 21:38:53,067 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-29 21:38:53,078 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-29 21:38:53,157 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6da4feeb{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-29 21:38:53,160 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@498a612d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-29 21:38:56,279 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e764e5c{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_2_1_jar-_-any-4562392528660318331/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.2.1.jar!/webapps/recon}
recon_1             | 2023-06-29 21:38:56,285 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@1e98b788{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-29 21:38:56,286 [Listener at 0.0.0.0/9891] INFO server.Server: Started @31939ms
recon_1             | 2023-06-29 21:38:56,291 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-29 21:38:56,291 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-29 21:38:56,293 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-29 21:38:56,293 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-29 21:38:56,308 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-29 21:38:56,316 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-29 21:38:56,316 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-29 21:38:56,316 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:38:56,316 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-29 21:38:56,317 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:38:57,387 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-29 21:38:57,395 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:38:57,395 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:38:57,411 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-29 21:38:57,401 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-29 21:38:57,624 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:38:57,629 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-29 21:38:57,714 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-29 21:38:57,714 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-29 21:38:57,720 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:38:57,735 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 93 milliseconds.
recon_1             | 2023-06-29 21:38:57,946 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 217 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:38:58,030 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 85 milliseconds for processing 0 containers.
recon_1             | 2023-06-29 21:38:58,196 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.11:42454: output error
recon_1             | 2023-06-29 21:38:58,196 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.13:40266: output error
recon_1             | 2023-06-29 21:38:58,204 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.22.0.9:40080: output error
recon_1             | 2023-06-29 21:38:58,208 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-06-29 21:39:12,268 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-29 21:39:12,270 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-29 21:39:12,270 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:39:12,296 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: [om1|rpc:om:9872|priority:0], old=null
om_1                | 2023-06-29 21:39:12,297 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-29 21:39:12,298 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:39:12,298 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-29 21:39:12,303 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 5749ms
om_1                | 2023-06-29 21:39:12,316 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:39:12,321 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:39:12,333 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:39:12,340 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-29 21:39:12,341 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om_1                | 2023-06-29 21:39:12,342 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-29 21:39:12,345 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:39:12,348 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-29 21:39:12,353 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-29 21:39:12,376 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-29 21:39:12,402 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: [om1|rpc:om:9872|admin:|client:|dataStream:|priority:0], old=null
om_1                | 2023-06-29 21:39:12,730 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-29 21:39:12,870 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | ]
om_1                | 2023-06-29 21:39:15,860 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-29 21:39:56,602 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-29 21:39:56,636 [qtp2081751971-43] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074796605 in 30 milliseconds
om_1                | 2023-06-29 21:39:56,672 [qtp2081751971-43] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 34 milliseconds
om_1                | 2023-06-29 21:39:56,672 [qtp2081751971-43] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074796605
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-29 21:38:58,208 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-29 21:38:58,208 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3605)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:141)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1667)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1737)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2837)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1809)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1117)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:909)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:895)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1052)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:963)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2966)
recon_1             | 2023-06-29 21:38:59,970 [IPC Server handler 5 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/520bd49a-4ee1-4e6e-9505-d3fcf658e00e
recon_1             | 2023-06-29 21:38:59,975 [IPC Server handler 5 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:00,028 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 520bd49a-4ee1-4e6e-9505-d3fcf658e00e to Node DB.
recon_1             | 2023-06-29 21:39:00,098 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b4562603-a319-4320-8669-6b5a2bb28a32
recon_1             | 2023-06-29 21:39:00,099 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:00,099 [IPC Server handler 8 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/572b7029-abfb-4d84-a537-3845bbccad20
recon_1             | 2023-06-29 21:39:00,099 [IPC Server handler 8 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:00,107 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 572b7029-abfb-4d84-a537-3845bbccad20 to Node DB.
recon_1             | 2023-06-29 21:39:00,108 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node b4562603-a319-4320-8669-6b5a2bb28a32 to Node DB.
recon_1             | 2023-06-29 21:39:01,434 [IPC Server handler 10 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-29 21:39:01,435 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1. Trying to get from SCM.
recon_1             | 2023-06-29 21:39:01,451 [IPC Server handler 4 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-29 21:39:01,479 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: ea72719d-0c1b-4ae0-b1b4-06f746aef7c1, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.328Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:39:01,644 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-29 21:39:01,933 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ea72719d-0c1b-4ae0-b1b4-06f746aef7c1, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.328Z[UTC]].
recon_1             | 2023-06-29 21:39:01,938 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 249549.586us
recon_1             | 2023-06-29 21:39:01,948 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:01,948 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=0dbc6dab-65b5-4deb-807e-3446ab55ce0b. Trying to get from SCM.
recon_1             | 2023-06-29 21:39:01,953 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 0dbc6dab-65b5-4deb-807e-3446ab55ce0b, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:520bd49a-4ee1-4e6e-9505-d3fcf658e00e, CreationTimestamp2023-06-29T21:38:58.292Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:39:01,962 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0dbc6dab-65b5-4deb-807e-3446ab55ce0b, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:520bd49a-4ee1-4e6e-9505-d3fcf658e00e, CreationTimestamp2023-06-29T21:38:58.292Z[UTC]].
recon_1             | 2023-06-29 21:39:01,967 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 1296.214us
recon_1             | 2023-06-29 21:39:01,968 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:02,196 [IPC Server handler 1 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-29 21:39:02,199 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,386 [IPC Server handler 14 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-29 21:39:04,387 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d774fccb-ebaf-434f-9da2-13166aacf0a7. Trying to get from SCM.
recon_1             | 2023-06-29 21:39:04,391 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d774fccb-ebaf-434f-9da2-13166aacf0a7, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.335Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:39:04,391 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d774fccb-ebaf-434f-9da2-13166aacf0a7, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.335Z[UTC]].
recon_1             | 2023-06-29 21:39:04,392 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 651.707us
recon_1             | 2023-06-29 21:39:04,392 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=d774fccb-ebaf-434f-9da2-13166aacf0a7 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,392 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d774fccb-ebaf-434f-9da2-13166aacf0a7, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:572b7029-abfb-4d84-a537-3845bbccad20, CreationTimestamp2023-06-29T21:38:58.335Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:39:04,393 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 816.309us
recon_1             | 2023-06-29 21:39:04,393 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,436 [IPC Server handler 12 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-29 21:39:04,443 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=23a15059-7de0-4dec-8658-12a6932b9835. Trying to get from SCM.
recon_1             | 2023-06-29 21:39:04,453 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 23a15059-7de0-4dec-8658-12a6932b9835, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.338Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:39:04,456 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 23a15059-7de0-4dec-8658-12a6932b9835, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.338Z[UTC]].
recon_1             | 2023-06-29 21:39:04,457 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 3326.434us
recon_1             | 2023-06-29 21:39:04,457 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,457 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,524 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,529 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,529 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,530 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,793 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,794 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:04,794 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=b627f98d-aae1-4e2f-abc9-4d612bf98e7d. Trying to get from SCM.
recon_1             | 2023-06-29 21:39:04,815 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: b627f98d-aae1-4e2f-abc9-4d612bf98e7d, Nodes: b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:b4562603-a319-4320-8669-6b5a2bb28a32, CreationTimestamp2023-06-29T21:38:58.340Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:39:04,815 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b627f98d-aae1-4e2f-abc9-4d612bf98e7d, Nodes: b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:OPEN, leaderId:b4562603-a319-4320-8669-6b5a2bb28a32, CreationTimestamp2023-06-29T21:38:58.340Z[UTC]].
recon_1             | 2023-06-29 21:39:04,816 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.addPipeline(org.apache.hadoop.hdds.protocol.proto.HddsProtos$Pipeline) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 741.807us
recon_1             | 2023-06-29 21:39:07,088 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:07,088 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:09,567 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:09,568 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:09,689 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:09,689 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 23a15059-7de0-4dec-8658-12a6932b9835, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b4562603-a319-4320-8669-6b5a2bb28a32, CreationTimestamp2023-06-29T21:38:58.338Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:39:09,689 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 524.905us
recon_1             | 2023-06-29 21:39:09,689 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:09,994 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:17,606 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-29 21:39:17,676 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 23844.435us
recon_1             | 2023-06-29 21:39:17,681 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:39:22,906 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 reported by 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:39:22,908 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ea72719d-0c1b-4ae0-b1b4-06f746aef7c1, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:572b7029-abfb-4d84-a537-3845bbccad20, CreationTimestamp2023-06-29T21:38:58.328Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:39:22,910 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.pipeline.PipelineStateManager.updatePipelineState(org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineID,org.apache.hadoop.hdds.protocol.proto.HddsProtos$PipelineState) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 541.506us
recon_1             | 2023-06-29 21:39:26,411 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:39:26,415 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 394.704us
recon_1             | 2023-06-29 21:39:26,415 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-29 21:39:34,908 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:39:34,913 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO ha.SCMHAInvocationHandler: Invoking method public abstract void org.apache.hadoop.hdds.scm.container.ContainerStateManagerV2.addContainer(org.apache.hadoop.hdds.protocol.proto.HddsProtos$ContainerInfoProto) throws java.io.IOException on target org.apache.hadoop.hdds.scm.ha.MockSCMHAManager$MockRatisServer@11903b10, cost 389.804us
recon_1             | 2023-06-29 21:39:34,913 [EventQueue-IncrementalContainerReportForReconIncrementalContainerReportHandler] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-29 21:39:56,317 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:39:56,318 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:39:56,764 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688074796318
recon_1             | 2023-06-29 21:39:56,769 [pool-16-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:39:56,770 [pool-16-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:39:56,839 [pool-16-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688074796318.
recon_1             | 2023-06-29 21:39:56,859 [pool-16-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:39:56,876 [pool-17-thread-1] INFO tasks.NSSummaryTask: Completed a reprocess run of NSSummaryTask
recon_1             | 2023-06-29 21:39:57,136 [pool-17-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-29 21:39:57,136 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:39:57,252 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:39:57,252 [pool-17-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.115 seconds to process 4 keys.
recon_1             | 2023-06-29 21:39:57,265 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:39:57,289 [pool-17-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:38:29,585 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-29 21:38:29,586 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-29 21:38:29,722 [main] INFO util.log: Logging initialized @5757ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-29 21:38:30,424 [main] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
s3g_1               | 2023-06-29 21:38:30,566 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-29 21:38:30,609 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-29 21:38:30,623 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-29 21:38:30,630 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-29 21:38:30,630 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-29 21:38:30,941 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = a3fba620aa37/172.22.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.2.1
s3g_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.12.1.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.33.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.33.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.33.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.33.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.33.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.33.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:28Z
s3g_1               | STARTUP_MSG:   java = 11.0.13
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:38:30,971 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-29 21:38:31,268 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:38:31,334 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-29 21:38:31,336 [main] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
s3g_1               | 2023-06-29 21:38:31,584 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-29 21:38:31,584 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-29 21:38:31,586 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-29 21:38:31,665 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f284218{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:38:31,669 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@723e88f9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.classfilewriter.ClassFile$1 (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.classfilewriter.ClassFile$1
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 29, 2023 9:38:48 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-29 21:38:48,908 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5fb3111a{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_2_1_jar-_-any-7012976170356706827/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.2.1.jar!/webapps/s3gateway}
s3g_1               | 2023-06-29 21:38:48,955 [main] INFO server.AbstractConnector: Started ServerConnector@5495333e{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:38:48,955 [main] INFO server.Server: Started @24991ms
s3g_1               | 2023-06-29 21:38:48,970 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:38:32,183 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = f3338c874d6f/172.22.0.5
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:38:32,306 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:38:33,082 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:38:33,398 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:38:33,423 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:38:34,130 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-5443b70b-cf66-449c-a079-505ae8a7b481; layoutVersion=2; scmId=89f60c75-862f-4b76-9524-fb3cf7ffeb8f
scm_1               | 2023-06-29 21:38:34,282 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at f3338c874d6f/172.22.0.5
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:38:47,422 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = f3338c874d6f/172.22.0.5
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.2.1
scm_1               | STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop/share/ozone/lib/jackson-annotations-2.12.1.jar:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-log4j12-1.7.30.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-6.20.3.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop/share/ozone/lib/commons-io-2.8.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.31.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.8.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.2.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.16.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.12.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.26.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-30.1.1-jre.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.12.1.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.2.0.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.1.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.12.1.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.1.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.2.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.3.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-1.2.17.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/accessors-smart-1.2.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.2.1.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.30.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/gson-2.2.4.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.43.v20210629.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.63.Final.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.4.31.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.16.0.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.2.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.0.3.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.2.1.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.2.1.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.2.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.1.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.20.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/76aa27e7c05196ae00cba540efce4bb7529e5d15 ; compiled by 'ethanrose' on 2021-12-15T22:27Z
scm_1               | STARTUP_MSG:   java = 11.0.13
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:38:47,495 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:38:48,021 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:38:48,021 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:38:48,913 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:38:49,205 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = SCM_HA (version = 2), software layout = SCM_HA (version = 2)
scm_1               | 2023-06-29 21:38:51,048 [main] INFO reflections.Reflections: Reflections took 442 ms to scan 3 urls, producing 103 keys and 211 values 
scm_1               | 2023-06-29 21:38:51,910 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:38:52,378 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:38:52,841 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/opt/hadoop/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.2.1.jar!/network-topology-default.xml]
scm_1               | 2023-06-29 21:38:52,847 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-29 21:38:53,008 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-29 21:38:53,051 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-29 21:38:53,051 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-29 21:38:53,060 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-29 21:38:53,067 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-29 21:38:53,119 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-29 21:38:53,135 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-29 21:38:53,152 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-29 21:38:53,205 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-29 21:38:53,219 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-29 21:38:53,219 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:38:53,256 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:38:53,277 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-29 21:38:53,298 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-29 21:38:53,306 [main] INFO container.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-29 21:38:53,336 [ReplicationMonitor] INFO container.ReplicationManager: Replication Monitor Thread took 13 milliseconds for processing 0 containers.
scm_1               | 2023-06-29 21:38:53,344 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-29 21:38:53,351 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:38:53,358 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:38:54,734 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:38:54,802 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-29 21:38:54,840 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:38:54,852 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:38:54,872 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:38:54,873 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:38:55,130 [Listener at 0.0.0.0/9860] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-29 21:38:55,153 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          0.1
scm_1               | Max Datanodes to Involve per Iteration(ratio)      0.2
scm_1               | Max Size to Move per Iteration                     32212254720B
scm_1               | 
scm_1               | 2023-06-29 21:38:55,153 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:38:55,153 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-29 21:38:55,177 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:38:55,402 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:38:55,422 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:38:55,422 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:38:55,999 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:38:56,000 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:38:56,062 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:38:56,093 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:38:56,101 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:38:56,114 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:38:56,114 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:38:56,196 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmDatanodeProtocl RPC server is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:38:56,198 [Listener at 0.0.0.0/9860] INFO server.SCMDatanodeProtocolServer: RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:38:56,216 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:38:56,216 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:38:56,355 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3e28fee1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-29 21:38:56,384 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:38:56,384 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:38:56,475 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @19606ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:38:56,695 [Listener at 0.0.0.0/9860] INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
scm_1               | 2023-06-29 21:38:56,701 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:38:56,731 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:38:56,748 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:38:56,748 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:38:56,748 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:38:56,978 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-29 21:38:57,001 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.43.v20210629; built: 2021-06-30T11:07:22.254Z; git: 526006ecfa3af7f1a27ef3a288e2bef7ea9dd7e8; jvm 11.0.13+8-LTS
scm_1               | 2023-06-29 21:38:57,491 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:38:57,499 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:38:57,500 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-29 21:38:57,527 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6babffb5{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:38:57,557 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@606a1bc4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:38:58,068 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/520bd49a-4ee1-4e6e-9505-d3fcf658e00e
scm_1               | 2023-06-29 21:38:58,073 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:38:58,120 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:38:58,126 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b4562603-a319-4320-8669-6b5a2bb28a32
scm_1               | 2023-06-29 21:38:58,227 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:38:58,228 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:38:58,142 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/572b7029-abfb-4d84-a537-3845bbccad20
scm_1               | 2023-06-29 21:38:58,230 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:38:58,231 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:38:58,231 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:38:58,238 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:38:58,256 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:38:58,256 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:38:58,273 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:38:58,273 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:38:58,273 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:38:58,282 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-29 21:38:58,284 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:38:58,285 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:38:58,294 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=0dbc6dab-65b5-4deb-807e-3446ab55ce0b to datanode:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
scm_1               | 2023-06-29 21:38:58,315 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@3cc053{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_2_1_jar-_-any-5336265784330965906/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.2.1.jar!/webapps/scm}
scm_1               | 2023-06-29 21:38:58,315 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 0dbc6dab-65b5-4deb-807e-3446ab55ce0b, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.292Z[UTC]].
scm_1               | 2023-06-29 21:38:58,328 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 to datanode:572b7029-abfb-4d84-a537-3845bbccad20
scm_1               | 2023-06-29 21:38:58,331 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@2f677247{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:38:58,331 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 to datanode:b4562603-a319-4320-8669-6b5a2bb28a32
scm_1               | 2023-06-29 21:38:58,331 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 to datanode:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
scm_1               | 2023-06-29 21:38:58,332 [Listener at 0.0.0.0/9860] INFO server.Server: Started @21462ms
scm_1               | 2023-06-29 21:38:58,334 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: ea72719d-0c1b-4ae0-b1b4-06f746aef7c1, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.328Z[UTC]].
scm_1               | 2023-06-29 21:38:58,335 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d774fccb-ebaf-434f-9da2-13166aacf0a7 to datanode:572b7029-abfb-4d84-a537-3845bbccad20
scm_1               | 2023-06-29 21:38:58,337 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d774fccb-ebaf-434f-9da2-13166aacf0a7, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.335Z[UTC]].
scm_1               | 2023-06-29 21:38:58,338 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 to datanode:520bd49a-4ee1-4e6e-9505-d3fcf658e00e
scm_1               | 2023-06-29 21:38:58,338 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 to datanode:b4562603-a319-4320-8669-6b5a2bb28a32
scm_1               | 2023-06-29 21:38:58,338 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 to datanode:572b7029-abfb-4d84-a537-3845bbccad20
scm_1               | 2023-06-29 21:38:58,339 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 23a15059-7de0-4dec-8658-12a6932b9835, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.338Z[UTC]].
scm_1               | 2023-06-29 21:38:58,339 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=23a15059-7de0-4dec-8658-12a6932b9835 contains same datanodes as previous pipelines: PipelineID=ea72719d-0c1b-4ae0-b1b4-06f746aef7c1 nodeIds: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e, b4562603-a319-4320-8669-6b5a2bb28a32, 572b7029-abfb-4d84-a537-3845bbccad20
scm_1               | 2023-06-29 21:38:58,340 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b627f98d-aae1-4e2f-abc9-4d612bf98e7d to datanode:b4562603-a319-4320-8669-6b5a2bb28a32
scm_1               | 2023-06-29 21:38:58,341 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b627f98d-aae1-4e2f-abc9-4d612bf98e7d, Nodes: b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:38:58.340Z[UTC]].
scm_1               | 2023-06-29 21:38:58,343 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:38:58,347 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:38:58,349 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:39:01,460 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 0dbc6dab-65b5-4deb-807e-3446ab55ce0b, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:520bd49a-4ee1-4e6e-9505-d3fcf658e00e, CreationTimestamp2023-06-29T21:38:58.292Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:39:01,492 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:02,199 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:04,404 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d774fccb-ebaf-434f-9da2-13166aacf0a7, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:572b7029-abfb-4d84-a537-3845bbccad20, CreationTimestamp2023-06-29T21:38:58.335Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:39:04,405 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:04,507 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:04,516 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:04,804 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b627f98d-aae1-4e2f-abc9-4d612bf98e7d, Nodes: b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b4562603-a319-4320-8669-6b5a2bb28a32, CreationTimestamp2023-06-29T21:38:58.340Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:39:04,813 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:07,090 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:09,570 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:09,692 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 23a15059-7de0-4dec-8658-12a6932b9835, Nodes: 520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:b4562603-a319-4320-8669-6b5a2bb28a32, CreationTimestamp2023-06-29T21:38:58.338Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-29 21:39:09,697 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO container.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-29 21:39:15,964 [IPC Server handler 85 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-29 21:39:15,992 [IPC Server handler 85 on default port 9863] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-29 21:39:15,993 [IPC Server handler 85 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-29 21:39:22,907 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: ea72719d-0c1b-4ae0-b1b4-06f746aef7c1, Nodes: 572b7029-abfb-4d84-a537-3845bbccad20{ip: 172.22.0.13, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b4562603-a319-4320-8669-6b5a2bb28a32{ip: 172.22.0.9, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}520bd49a-4ee1-4e6e-9505-d3fcf658e00e{ip: 172.22.0.11, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:572b7029-abfb-4d84-a537-3845bbccad20, CreationTimestamp2023-06-29T21:38:58.328Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:39:44,165 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.7
scm_1               | 2023-06-29 21:39:52,332 [IPC Server handler 2 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.7
scm_1               | 2023-06-29 21:40:45,688 [IPC Server handler 60 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.7
scm_1               | 2023-06-29 21:40:54,118 [IPC Server handler 73 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.22.0.7
Attaching to xcompat_old_client_1_1_0_1, xcompat_old_client_1_0_0_1, xcompat_new_client_1, xcompat_old_client_1_3_0_1, xcompat_om_1, xcompat_recon_1, xcompat_datanode_3, xcompat_s3g_1, xcompat_old_client_1_2_1_1, xcompat_datanode_2, xcompat_datanode_1, xcompat_scm_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:41:18,908 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = 0a959264db04/172.23.0.5
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.3.0
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_1          | STARTUP_MSG:   java = 11.0.14.1
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:41:18,956 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:41:19,268 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:41:19,663 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:41:20,464 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:41:20,465 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:41:21,145 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:0a959264db04 ip:172.23.0.5
datanode_1          | 2023-06-29 21:41:22,383 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_1          | 2023-06-29 21:41:23,107 [main] INFO reflections.Reflections: Reflections took 603 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_1          | 2023-06-29 21:41:23,840 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-29 21:41:24,526 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:41:24,664 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-29 21:41:24,665 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:41:24,666 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:41:24,721 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:41:24,804 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:41:24,806 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-29 21:41:24,808 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-29 21:41:24,808 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-29 21:41:24,826 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:41:24,961 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:41:24,961 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:41:31,134 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-29 21:41:31,406 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:41:31,650 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:41:32,023 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:41:32,061 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-29 21:41:32,072 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:41:32,075 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-29 21:41:32,080 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-29 21:41:32,080 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-29 21:41:32,081 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:41:32,086 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:41:32,086 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:41:32,096 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:41:32,161 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-29 21:41:32,185 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-29 21:41:32,192 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-29 21:41:33,875 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-29 21:41:33,913 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-29 21:41:33,913 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-29 21:41:33,914 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:33,914 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:41:33,945 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:41:34,045 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-29 21:41:34,760 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:41:34,857 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:41:35,040 [main] INFO util.log: Logging initialized @22763ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:41:35,724 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-29 21:41:35,741 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:41:35,770 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:41:35,779 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:41:35,789 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:41:35,792 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:41:36,358 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-29 21:41:36,382 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_1          | 2023-06-29 21:41:36,608 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:41:36,608 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:41:36,610 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-29 21:41:36,710 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7af0affa{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:41:36,723 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5b251fb9{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:41:38,420 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7f0f84d4{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-90531534128323040/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:41:38,465 [main] INFO server.AbstractConnector: Started ServerConnector@1f10fec6{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:41:38,466 [main] INFO server.Server: Started @26189ms
datanode_1          | 2023-06-29 21:41:38,517 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-29 21:41:38,518 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-29 21:41:38,520 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:41:38,569 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-29 21:41:38,705 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3f701ed7] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_1          | 2023-06-29 21:41:39,168 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_1          | 2023-06-29 21:41:39,378 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:41:41,881 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:41,884 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:42,883 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:42,886 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:43,886 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:44,887 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:45,888 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:46,889 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:41:47,934 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 0a959264db04/172.23.0.5 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:40488 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:40488 remote=recon/172.23.0.6:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-29 21:41:51,900 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From 0a959264db04/172.23.0.5 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:55300 remote=scm/172.23.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_1          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.5:55300 remote=scm/172.23.0.4:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_1          | 2023-06-29 21:41:52,124 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-13221bfe-bb80-4e22-b1db-03c005082ce5/container.db to cache
datanode_1          | 2023-06-29 21:41:52,124 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-13221bfe-bb80-4e22-b1db-03c005082ce5/container.db for volume DS-13221bfe-bb80-4e22-b1db-03c005082ce5
datanode_1          | 2023-06-29 21:41:52,126 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-29 21:41:52,127 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_1          | 2023-06-29 21:41:52,324 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_1          | 2023-06-29 21:41:52,409 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.RaftServer: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start RPC server
datanode_1          | 2023-06-29 21:41:52,413 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: GrpcService started, listening on 9858
datanode_1          | 2023-06-29 21:41:52,414 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: GrpcService started, listening on 9856
datanode_1          | 2023-06-29 21:41:52,418 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: GrpcService started, listening on 9857
datanode_1          | 2023-06-29 21:41:52,469 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5569f9f3-6953-4e70-aaf7-1b773b1f5661: Started
datanode_1          | 2023-06-29 21:41:52,479 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5569f9f3-6953-4e70-aaf7-1b773b1f5661 is started using port 9858 for RATIS
datanode_1          | 2023-06-29 21:41:52,479 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5569f9f3-6953-4e70-aaf7-1b773b1f5661 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-29 21:41:52,479 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5569f9f3-6953-4e70-aaf7-1b773b1f5661 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-29 21:41:55,886 [Command processor thread] INFO server.RaftServer: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: addNew group-BB8C98830DA2:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER] returns group-BB8C98830DA2:java.util.concurrent.CompletableFuture@44e29d44[Not completed]
datanode_1          | 2023-06-29 21:41:55,970 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: new RaftServerImpl for group-BB8C98830DA2:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:41:55,973 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:41:55,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:41:55,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:41:55,978 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:55,979 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:41:55,979 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:41:55,997 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: ConfigurationManager, init=-1: peers:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:41:56,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:41:56,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:41:56,052 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:41:56,082 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:56,084 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:41:56,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:41:56,217 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:41:56,217 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:41:56,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:41:56,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:41:56,220 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:41:56,221 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/051de25a-6894-4194-92a4-bb8c98830da2 does not exist. Creating ...
datanode_1          | 2023-06-29 21:41:56,230 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/051de25a-6894-4194-92a4-bb8c98830da2/in_use.lock acquired by nodename 7@0a959264db04
datanode_1          | 2023-06-29 21:41:56,254 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/051de25a-6894-4194-92a4-bb8c98830da2 has been successfully formatted.
datanode_1          | 2023-06-29 21:41:56,280 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-BB8C98830DA2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:41:56,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:41:56,321 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:41:56,329 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:41:56,332 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:41:56,333 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:41:56,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,373 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:41:56,384 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:41:56,414 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/051de25a-6894-4194-92a4-bb8c98830da2
datanode_1          | 2023-06-29 21:41:56,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:41:56,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:41:56,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:41:56,432 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:41:56,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:41:56,438 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:41:56,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:41:56,481 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,494 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:56,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:56,498 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:56,549 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:56,549 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:56,567 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: start as a follower, conf=-1: peers:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:41:56,567 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:41:56,571 [pool-22-thread-1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState
datanode_1          | 2023-06-29 21:41:56,590 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BB8C98830DA2,id=5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_1          | 2023-06-29 21:41:56,593 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:56,604 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:41:56,605 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:41:56,606 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:41:56,614 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:41:56,619 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:41:56,688 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=051de25a-6894-4194-92a4-bb8c98830da2
datanode_1          | 2023-06-29 21:41:56,691 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=051de25a-6894-4194-92a4-bb8c98830da2.
datanode_1          | 2023-06-29 21:41:56,692 [Command processor thread] INFO server.RaftServer: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: addNew group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] returns group-DA220F9029B2:java.util.concurrent.CompletableFuture@6a8f7eb7[Not completed]
datanode_1          | 2023-06-29 21:41:56,715 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: new RaftServerImpl for group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:41:56,721 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:41:56,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:41:56,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:41:56,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:56,740 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:41:56,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:41:56,741 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:41:56,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:41:56,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:41:56,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:41:56,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:56,743 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:41:56,743 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:41:56,748 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:41:56,753 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:41:56,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:41:56,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:41:56,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:41:56,761 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 does not exist. Creating ...
datanode_1          | 2023-06-29 21:41:56,768 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/in_use.lock acquired by nodename 7@0a959264db04
datanode_1          | 2023-06-29 21:41:56,779 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 has been successfully formatted.
datanode_1          | 2023-06-29 21:41:56,780 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-DA220F9029B2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:41:56,780 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:41:56,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:41:56,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:41:56,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:41:56,791 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:41:56,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:41:56,793 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:41:56,794 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:41:56,795 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:41:56,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:41:56,798 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:41:56,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:56,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:56,800 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:56,802 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:56,802 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:56,802 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:56,804 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:41:56,817 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:41:56,817 [pool-22-thread-1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState
datanode_1          | 2023-06-29 21:41:56,836 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:41:56,836 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA220F9029B2,id=5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_1          | 2023-06-29 21:41:56,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:56,843 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:41:56,844 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:41:56,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:41:56,841 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:41:56,850 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_1          | 2023-06-29 21:41:59,829 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2.
datanode_1          | 2023-06-29 21:41:59,830 [Command processor thread] INFO server.RaftServer: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: addNew group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] returns group-98AA515D663B:java.util.concurrent.CompletableFuture@368a4484[Not completed]
datanode_1          | 2023-06-29 21:41:59,832 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: new RaftServerImpl for group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:41:59,832 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_1          | 2023-06-29 21:41:59,833 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:41:59,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:41:59,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:41:59,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:41:59,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:41:59,834 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:41:59,835 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:41:59,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:41:59,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:41:59,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:41:59,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:41:59,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:41:59,842 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b does not exist. Creating ...
datanode_1          | 2023-06-29 21:41:59,844 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/in_use.lock acquired by nodename 7@0a959264db04
datanode_1          | 2023-06-29 21:41:59,847 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b has been successfully formatted.
datanode_1          | 2023-06-29 21:41:59,848 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-98AA515D663B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:41:59,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:41:59,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:41:59,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:41:59,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:41:59,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:41:59,856 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:59,856 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:41:59,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:41:59,857 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b
datanode_1          | 2023-06-29 21:41:59,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:41:59,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:41:59,857 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:59,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:41:59,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:41:59,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:41:59,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:41:59,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:41:59,866 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:41:59,866 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:59,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:41:59,874 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:59,875 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:59,875 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:41:59,875 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:41:59,876 [pool-22-thread-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:41:59,876 [pool-22-thread-1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:41:59,877 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-98AA515D663B,id=5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_1          | 2023-06-29 21:41:59,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:41:59,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:41:59,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:41:59,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:41:59,889 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:41:59,932 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b
datanode_1          | 2023-06-29 21:41:59,932 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:00,312 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b.
datanode_1          | 2023-06-29 21:42:01,789 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5220462028ns, electionTimeout:5168ms
datanode_1          | 2023-06-29 21:42:01,789 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState
datanode_1          | 2023-06-29 21:42:01,790 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:42:01,793 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:42:01,793 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1
datanode_1          | 2023-06-29 21:42:01,799 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:01,799 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-29 21:42:01,800 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1
datanode_1          | 2023-06-29 21:42:01,800 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:42:01,800 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BB8C98830DA2 with new leaderId: 5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_1          | 2023-06-29 21:42:01,803 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: change Leader from null to 5569f9f3-6953-4e70-aaf7-1b773b1f5661 at term 1 for becomeLeader, leader elected after 5721ms
datanode_1          | 2023-06-29 21:42:01,816 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:42:01,826 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:42:01,827 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:42:01,835 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:42:01,836 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:42:01,836 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:42:01,846 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:42:01,848 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-29 21:42:01,852 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderStateImpl
datanode_1          | 2023-06-29 21:42:01,892 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:42:01,937 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-LeaderElection1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2: set configuration 0: peers:[5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:02,046 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5228337937ns, electionTimeout:5197ms
datanode_1          | 2023-06-29 21:42:02,055 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState
datanode_1          | 2023-06-29 21:42:02,055 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:42:02,056 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:42:02,056 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2
datanode_1          | 2023-06-29 21:42:02,078 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:02,113 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_1          | 2023-06-29 21:42:02,130 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:02,131 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:02,156 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_1          | 2023-06-29 21:42:02,258 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-BB8C98830DA2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/051de25a-6894-4194-92a4-bb8c98830da2/current/log_inprogress_0
datanode_1          | 2023-06-29 21:42:02,265 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:42:02,267 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.LeaderElection:   Response 0: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t1
datanode_1          | 2023-06-29 21:42:02,269 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:42:02,269 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-29 21:42:02,270 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2
datanode_1          | 2023-06-29 21:42:02,270 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-LeaderElection2] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState
datanode_1          | 2023-06-29 21:42:02,298 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:02,299 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:05,067 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5190256568ns, electionTimeout:5134ms
datanode_1          | 2023-06-29 21:42:05,067 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:05,067 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:42:05,068 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:42:05,068 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3
datanode_1          | 2023-06-29 21:42:05,071 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:05,072 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:05,072 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection:   Response 0: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t1
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection:   Response 1: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t1
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3
datanode_1          | 2023-06-29 21:42:05,092 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection3] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:05,103 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:05,104 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:05,104 [grpc-default-executor-0] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: receive requestVote(ELECTION, 0d3a8303-04f4-4b15-9343-c46934377c95, group-98AA515D663B, 1, (t:0, i:0))
datanode_1          | 2023-06-29 21:42:05,106 [grpc-default-executor-0] INFO impl.VoteContext: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FOLLOWER: reject ELECTION from 0d3a8303-04f4-4b15-9343-c46934377c95: already has voted for 5569f9f3-6953-4e70-aaf7-1b773b1f5661 at current term 1
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:41:19,489 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 6d049e7eb0fe/172.23.0.9
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.3.0
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_3          | STARTUP_MSG:   java = 11.0.14.1
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:41:19,506 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:41:19,783 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:41:20,136 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:41:20,829 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:41:20,839 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:41:21,489 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:6d049e7eb0fe ip:172.23.0.9
datanode_3          | 2023-06-29 21:41:22,648 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_3          | 2023-06-29 21:41:23,375 [main] INFO reflections.Reflections: Reflections took 601 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_3          | 2023-06-29 21:41:23,968 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-29 21:41:24,653 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:41:24,723 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-29 21:41:24,733 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:41:24,794 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:41:24,919 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:41:24,990 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:41:24,995 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-29 21:41:25,000 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-29 21:41:25,001 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-29 21:41:25,001 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 2023-06-29 21:41:25,165 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:41:25,166 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:41:31,646 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-29 21:41:32,127 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:41:32,361 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:41:32,633 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:41:32,649 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-29 21:41:32,650 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:41:32,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-29 21:41:32,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-29 21:41:32,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-29 21:41:32,653 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:41:32,660 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:41:32,661 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:41:32,662 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:41:32,705 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:41:32,721 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-29 21:41:32,745 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-29 21:41:34,504 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-29 21:41:34,509 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-29 21:41:34,552 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-29 21:41:34,555 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:34,555 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:41:34,557 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:41:34,732 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-29 21:41:35,344 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:41:35,457 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:41:35,649 [main] INFO util.log: Logging initialized @22920ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:41:36,534 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-29 21:41:36,604 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:41:36,624 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:41:36,645 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:41:36,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:41:36,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:41:36,947 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-29 21:41:36,949 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_3          | 2023-06-29 21:41:37,156 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:41:37,156 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:41:37,161 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-29 21:41:37,243 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b9ac754{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:41:37,272 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:41:38,982 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1d2fb82{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-1384248537152960087/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:41:39,046 [main] INFO server.AbstractConnector: Started ServerConnector@7302ff13{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:41:39,052 [main] INFO server.Server: Started @26323ms
datanode_3          | 2023-06-29 21:41:39,071 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-29 21:41:39,072 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-29 21:41:39,073 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:41:39,097 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-29 21:41:39,333 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@745809c1] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_3          | 2023-06-29 21:41:39,551 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_3          | 2023-06-29 21:41:39,834 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:42:05,109 [grpc-default-executor-0] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B replies to ELECTION vote request: 0d3a8303-04f4-4b15-9343-c46934377c95<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:FAIL-t1. Peer's state: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B:t1, leader=null, voted=5569f9f3-6953-4e70-aaf7-1b773b1f5661, raftlog=Memoized:5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:07,335 [grpc-default-executor-0] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: receive requestVote(ELECTION, 0d3a8303-04f4-4b15-9343-c46934377c95, group-DA220F9029B2, 2, (t:0, i:0))
datanode_1          | 2023-06-29 21:42:07,336 [grpc-default-executor-0] INFO impl.VoteContext: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FOLLOWER: accept ELECTION from 0d3a8303-04f4-4b15-9343-c46934377c95: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:42:07,336 [grpc-default-executor-0] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:0d3a8303-04f4-4b15-9343-c46934377c95
datanode_1          | 2023-06-29 21:42:07,336 [grpc-default-executor-0] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState
datanode_1          | 2023-06-29 21:42:07,336 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState was interrupted
datanode_1          | 2023-06-29 21:42:07,338 [grpc-default-executor-0] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState
datanode_1          | 2023-06-29 21:42:07,340 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:07,340 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:07,342 [grpc-default-executor-0] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2 replies to ELECTION vote request: 0d3a8303-04f4-4b15-9343-c46934377c95<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:OK-t2. Peer's state: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2:t2, leader=null, voted=0d3a8303-04f4-4b15-9343-c46934377c95, raftlog=Memoized:5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:07,527 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA220F9029B2 with new leaderId: 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_1          | 2023-06-29 21:42:07,528 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: change Leader from null to 0d3a8303-04f4-4b15-9343-c46934377c95 at term 2 for appendEntries, leader elected after 10785ms
datanode_1          | 2023-06-29 21:42:07,541 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:07,546 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:42:07,547 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-DA220F9029B2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/current/log_inprogress_0
datanode_1          | 2023-06-29 21:42:10,118 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5025505558ns, electionTimeout:5014ms
datanode_1          | 2023-06-29 21:42:10,118 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:10,118 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_1          | 2023-06-29 21:42:10,118 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_1          | 2023-06-29 21:42:10,118 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4
datanode_1          | 2023-06-29 21:42:10,126 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4 ELECTION round 0: submit vote requests at term 2 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:10,149 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:10,149 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:10,168 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_1          | 2023-06-29 21:42:10,168 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.LeaderElection:   Response 0: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:OK-t2
datanode_1          | 2023-06-29 21:42:10,168 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.LeaderElection:   Response 1: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t2
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:41:18,415 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = 679bbafe669a/172.23.0.3
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.3.0
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
datanode_2          | STARTUP_MSG:   java = 11.0.14.1
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:41:18,447 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:41:18,741 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:41:19,235 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:41:20,070 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:41:20,072 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:41:20,681 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:679bbafe669a ip:172.23.0.3
datanode_2          | 2023-06-29 21:41:21,692 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
datanode_2          | 2023-06-29 21:41:22,507 [main] INFO reflections.Reflections: Reflections took 658 ms to scan 2 urls, producing 92 keys and 204 values 
datanode_2          | 2023-06-29 21:41:23,080 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-29 21:41:23,706 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:41:23,746 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-29 21:41:23,766 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:41:23,786 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:41:23,862 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:41:23,906 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:41:23,926 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-29 21:41:23,927 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-29 21:41:23,927 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-29 21:41:23,938 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-29 21:41:24,057 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:41:24,057 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:41:30,445 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-29 21:41:30,954 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:41:31,328 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:41:31,814 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:41:31,831 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-29 21:41:31,852 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:41:31,863 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-29 21:41:31,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-29 21:41:31,871 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-29 21:41:31,872 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:41:31,873 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:41:31,875 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:41:31,883 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:41:31,934 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:41:31,974 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-29 21:41:31,975 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-29 21:41:33,788 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-29 21:41:33,814 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-29 21:41:33,820 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-29 21:41:33,820 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:33,820 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:41:33,822 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:41:33,972 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-29 21:41:34,573 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:41:34,633 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:41:34,740 [main] INFO util.log: Logging initialized @22892ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:41:35,169 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-29 21:41:35,188 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:41:35,227 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:41:35,228 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:41:35,233 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:42:10,168 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.LeaderElection: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4 ELECTION round 0: result REJECTED
datanode_1          | 2023-06-29 21:42:10,169 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_1          | 2023-06-29 21:42:10,169 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4
datanode_1          | 2023-06-29 21:42:10,169 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-LeaderElection4] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:10,176 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:10,176 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:10,218 [grpc-default-executor-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: receive requestVote(ELECTION, 0217402b-914d-4687-b2b8-79d59c463fcd, group-98AA515D663B, 2, (t:0, i:0))
datanode_1          | 2023-06-29 21:42:10,218 [grpc-default-executor-1] INFO impl.VoteContext: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FOLLOWER: reject ELECTION from 0217402b-914d-4687-b2b8-79d59c463fcd: already has voted for 5569f9f3-6953-4e70-aaf7-1b773b1f5661 at current term 2
datanode_1          | 2023-06-29 21:42:10,221 [grpc-default-executor-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B replies to ELECTION vote request: 0217402b-914d-4687-b2b8-79d59c463fcd<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:FAIL-t2. Peer's state: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B:t2, leader=null, voted=5569f9f3-6953-4e70-aaf7-1b773b1f5661, raftlog=Memoized:5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:15,242 [grpc-default-executor-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: receive requestVote(ELECTION, 0217402b-914d-4687-b2b8-79d59c463fcd, group-98AA515D663B, 3, (t:0, i:0))
datanode_1          | 2023-06-29 21:42:15,243 [grpc-default-executor-1] INFO impl.VoteContext: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FOLLOWER: accept ELECTION from 0217402b-914d-4687-b2b8-79d59c463fcd: our priority 0 <= candidate's priority 1
datanode_1          | 2023-06-29 21:42:15,243 [grpc-default-executor-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:0217402b-914d-4687-b2b8-79d59c463fcd
datanode_1          | 2023-06-29 21:42:15,243 [grpc-default-executor-1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: shutdown 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:15,243 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState was interrupted
datanode_1          | 2023-06-29 21:42:15,244 [grpc-default-executor-1] INFO impl.RoleInfo: 5569f9f3-6953-4e70-aaf7-1b773b1f5661: start 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState
datanode_1          | 2023-06-29 21:42:15,246 [grpc-default-executor-1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B replies to ELECTION vote request: 0217402b-914d-4687-b2b8-79d59c463fcd<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:OK-t3. Peer's state: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B:t3, leader=null, voted=0217402b-914d-4687-b2b8-79d59c463fcd, raftlog=Memoized:5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:15,250 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:42:15,251 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:42:15,328 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-98AA515D663B with new leaderId: 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_1          | 2023-06-29 21:42:15,329 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread1] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: change Leader from null to 0217402b-914d-4687-b2b8-79d59c463fcd at term 3 for appendEntries, leader elected after 15494ms
datanode_1          | 2023-06-29 21:42:15,344 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread2] INFO server.RaftServer$Division: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:42:15,344 [5569f9f3-6953-4e70-aaf7-1b773b1f5661-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:42:15,353 [5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5569f9f3-6953-4e70-aaf7-1b773b1f5661@group-98AA515D663B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/current/log_inprogress_0
datanode_3          | 2023-06-29 21:41:42,428 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:42,429 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:43,429 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:44,430 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:45,431 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:46,431 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:41:47,474 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 6d049e7eb0fe/172.23.0.9 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:38392 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:38392 remote=recon/172.23.0.6:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-06-29 21:41:51,438 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 6d049e7eb0fe/172.23.0.9 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:54868 remote=scm/172.23.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_3          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.9:54868 remote=scm/172.23.0.4:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_3          | 2023-06-29 21:41:52,157 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-0ca10aaa-d3f5-4e72-a36e-32debdf1187f/container.db to cache
datanode_3          | 2023-06-29 21:41:52,157 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-0ca10aaa-d3f5-4e72-a36e-32debdf1187f/container.db for volume DS-0ca10aaa-d3f5-4e72-a36e-32debdf1187f
datanode_3          | 2023-06-29 21:41:52,158 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-29 21:41:52,159 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_3          | 2023-06-29 21:41:52,385 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:41:52,451 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.RaftServer: 0d3a8303-04f4-4b15-9343-c46934377c95: start RPC server
datanode_3          | 2023-06-29 21:41:52,453 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0d3a8303-04f4-4b15-9343-c46934377c95: GrpcService started, listening on 9858
datanode_3          | 2023-06-29 21:41:52,454 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0d3a8303-04f4-4b15-9343-c46934377c95: GrpcService started, listening on 9856
datanode_3          | 2023-06-29 21:41:52,455 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0d3a8303-04f4-4b15-9343-c46934377c95: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:41:52,519 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0d3a8303-04f4-4b15-9343-c46934377c95: Started
datanode_3          | 2023-06-29 21:41:52,542 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0d3a8303-04f4-4b15-9343-c46934377c95 is started using port 9858 for RATIS
datanode_3          | 2023-06-29 21:41:52,542 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0d3a8303-04f4-4b15-9343-c46934377c95 is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-29 21:41:52,542 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0d3a8303-04f4-4b15-9343-c46934377c95 is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-29 21:41:56,437 [Command processor thread] INFO server.RaftServer: 0d3a8303-04f4-4b15-9343-c46934377c95: addNew group-44463133A719:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER] returns group-44463133A719:java.util.concurrent.CompletableFuture@7bd8840a[Not completed]
datanode_3          | 2023-06-29 21:41:56,503 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95: new RaftServerImpl for group-44463133A719:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:41:56,520 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:41:56,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:41:56,521 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:41:56,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:56,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:41:56,522 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:41:56,550 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:41:56,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:41:56,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:41:56,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:41:56,665 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:56,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:41:56,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:41:56,842 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:41:56,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:41:56,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:41:56,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:41:56,856 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:41:56,856 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/4b0d94af-634a-4894-8f42-44463133a719 does not exist. Creating ...
datanode_3          | 2023-06-29 21:41:56,880 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/4b0d94af-634a-4894-8f42-44463133a719/in_use.lock acquired by nodename 7@6d049e7eb0fe
datanode_3          | 2023-06-29 21:41:56,890 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/4b0d94af-634a-4894-8f42-44463133a719 has been successfully formatted.
datanode_3          | 2023-06-29 21:41:56,916 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-44463133A719: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:41:56,937 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:41:57,001 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:41:57,002 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:41:57,003 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:41:57,005 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:41:57,015 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,049 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:41:57,050 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:41:57,081 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/4b0d94af-634a-4894-8f42-44463133a719
datanode_3          | 2023-06-29 21:41:57,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:41:57,085 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:41:57,087 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:41:57,097 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:41:57,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:41:57,103 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:41:57,104 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:41:57,133 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:57,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:57,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:57,171 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:57,172 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:57,173 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:41:57,173 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:41:57,174 [pool-22-thread-1] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState
datanode_3          | 2023-06-29 21:41:57,187 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-44463133A719,id=0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:41:57,189 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:41:35,233 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:41:35,827 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-29 21:41:35,840 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
datanode_2          | 2023-06-29 21:41:36,082 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:41:36,082 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:41:36,118 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-29 21:41:36,183 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@353f472a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:41:36,216 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5c931134{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:41:38,264 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@39832280{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-632795031969421465/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-29 21:41:38,341 [main] INFO server.AbstractConnector: Started ServerConnector@7c46c9c3{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:41:38,355 [main] INFO server.Server: Started @26507ms
datanode_2          | 2023-06-29 21:41:38,377 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-29 21:41:38,377 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-29 21:41:38,384 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:41:38,410 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-29 21:41:38,501 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@55020942] INFO util.JvmPauseMonitor: Starting JVM pause monitor
datanode_2          | 2023-06-29 21:41:39,156 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.23.0.6:9891
datanode_2          | 2023-06-29 21:41:39,532 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:41:41,810 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:41,824 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:42,811 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:42,825 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.23.0.6:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:43,811 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:44,812 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:45,813 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:46,814 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.23.0.4:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:41:47,854 [EndpointStateMachine task thread for recon/172.23.0.6:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 679bbafe669a/172.23.0.3 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.3:58990 remote=recon/172.23.0.6:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.3:58990 remote=recon/172.23.0.6:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-29 21:41:51,828 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From 679bbafe669a/172.23.0.3 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.3:38728 remote=scm/172.23.0.4:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
datanode_2          | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.23.0.3:38728 remote=scm/172.23.0.4:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
datanode_2          | 2023-06-29 21:41:52,188 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-f7411377-5686-4215-9686-2009124f0e4d/container.db to cache
datanode_2          | 2023-06-29 21:41:52,191 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/DS-f7411377-5686-4215-9686-2009124f0e4d/container.db for volume DS-f7411377-5686-4215-9686-2009124f0e4d
datanode_2          | 2023-06-29 21:41:52,202 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-29 21:41:52,205 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
datanode_2          | 2023-06-29 21:41:52,438 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:41:52,530 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.RaftServer: 0217402b-914d-4687-b2b8-79d59c463fcd: start RPC server
datanode_2          | 2023-06-29 21:41:52,569 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0217402b-914d-4687-b2b8-79d59c463fcd: GrpcService started, listening on 9858
datanode_2          | 2023-06-29 21:41:52,589 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0217402b-914d-4687-b2b8-79d59c463fcd: GrpcService started, listening on 9856
datanode_2          | 2023-06-29 21:41:52,591 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO server.GrpcService: 0217402b-914d-4687-b2b8-79d59c463fcd: GrpcService started, listening on 9857
datanode_2          | 2023-06-29 21:41:52,598 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0217402b-914d-4687-b2b8-79d59c463fcd is started using port 9858 for RATIS
datanode_2          | 2023-06-29 21:41:52,598 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0217402b-914d-4687-b2b8-79d59c463fcd is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-29 21:41:52,599 [EndpointStateMachine task thread for scm/172.23.0.4:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0217402b-914d-4687-b2b8-79d59c463fcd is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-29 21:41:52,600 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0217402b-914d-4687-b2b8-79d59c463fcd: Started
datanode_2          | 2023-06-29 21:41:57,781 [Command processor thread] INFO server.RaftServer: 0217402b-914d-4687-b2b8-79d59c463fcd: addNew group-16309B018913:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] returns group-16309B018913:java.util.concurrent.CompletableFuture@55c5b3b[Not completed]
datanode_2          | 2023-06-29 21:41:57,833 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd: new RaftServerImpl for group-16309B018913:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:41:57,844 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:41:57,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:41:57,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:41:57,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:57,852 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:41:57,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:41:57,869 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: ConfigurationManager, init=-1: peers:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:41:57,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:41:57,925 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:41:57,929 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:41:57,957 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:57,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:41:57,965 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:41:58,120 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:41:58,121 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:41:58,126 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:41:58,133 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:41:58,133 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:41:58,137 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/90c00c1a-3550-4281-81b8-16309b018913 does not exist. Creating ...
datanode_2          | 2023-06-29 21:41:58,147 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/90c00c1a-3550-4281-81b8-16309b018913/in_use.lock acquired by nodename 7@679bbafe669a
datanode_2          | 2023-06-29 21:41:58,164 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/90c00c1a-3550-4281-81b8-16309b018913 has been successfully formatted.
datanode_2          | 2023-06-29 21:41:58,207 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-16309B018913: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:41:58,208 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:41:58,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:41:58,233 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:41:58,234 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:41:58,238 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:41:58,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,256 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:41:58,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:41:58,272 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/90c00c1a-3550-4281-81b8-16309b018913
datanode_2          | 2023-06-29 21:41:58,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:41:58,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:41:58,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:41:58,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:41:58,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:41:58,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:41:58,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:41:58,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:41:58,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:41:58,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:41:58,333 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:41:58,338 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:41:58,343 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: start as a follower, conf=-1: peers:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:41:58,343 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:41:58,345 [pool-22-thread-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState
datanode_2          | 2023-06-29 21:41:58,360 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-16309B018913,id=0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:41:58,361 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:41:58,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:41:58,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:41:58,365 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:41:58,366 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:41:58,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:41:58,394 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=90c00c1a-3550-4281-81b8-16309b018913
datanode_2          | 2023-06-29 21:41:58,399 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=90c00c1a-3550-4281-81b8-16309b018913.
datanode_2          | 2023-06-29 21:41:58,400 [Command processor thread] INFO server.RaftServer: 0217402b-914d-4687-b2b8-79d59c463fcd: addNew group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] returns group-DA220F9029B2:java.util.concurrent.CompletableFuture@3932c398[Not completed]
datanode_2          | 2023-06-29 21:41:58,403 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd: new RaftServerImpl for group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:41:58,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:41:58,407 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:41:58,410 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:41:58,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:58,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:41:58,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:41:58,415 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:41:58,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:41:58,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:41:58,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:41:58,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:58,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:41:58,416 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:41:58,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:41:58,422 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:41:58,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:41:58,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:41:58,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:41:58,430 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 does not exist. Creating ...
datanode_2          | 2023-06-29 21:41:58,433 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/in_use.lock acquired by nodename 7@679bbafe669a
datanode_2          | 2023-06-29 21:41:58,434 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 has been successfully formatted.
datanode_2          | 2023-06-29 21:41:58,437 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-DA220F9029B2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:41:58,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:41:58,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:41:58,439 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:41:58,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:41:58,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:41:58,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:41:58,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:41:58,442 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_2          | 2023-06-29 21:41:58,442 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:41:58,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:41:58,453 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:41:58,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:41:58,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:41:58,459 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:41:58,460 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:41:58,467 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:41:58,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:41:58,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:41:58,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:41:58,471 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:41:58,472 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:41:58,476 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:41:58,479 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:41:58,479 [pool-22-thread-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState
datanode_2          | 2023-06-29 21:41:58,482 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA220F9029B2,id=0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:41:58,487 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:41:58,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:41:58,489 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:41:58,490 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:41:58,490 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:41:58,497 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:41:58,497 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_2          | 2023-06-29 21:41:59,976 [grpc-default-executor-0] INFO server.RaftServer: 0217402b-914d-4687-b2b8-79d59c463fcd: addNew group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER] returns group-98AA515D663B:java.util.concurrent.CompletableFuture@57c8f92f[Not completed]
datanode_2          | 2023-06-29 21:41:59,979 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd: new RaftServerImpl for group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:41:59,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:41:59,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:41:59,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:41:59,983 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:59,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:41:59,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:41:59,989 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:41:59,989 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:41:59,989 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:41:59,989 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:41:59,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:41:59,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:41:59,993 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:41:59,994 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:41:59,994 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:41:59,995 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:41:59,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:41:59,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:41:59,996 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b does not exist. Creating ...
datanode_2          | 2023-06-29 21:42:00,002 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/in_use.lock acquired by nodename 7@679bbafe669a
datanode_2          | 2023-06-29 21:42:00,005 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b has been successfully formatted.
datanode_2          | 2023-06-29 21:42:00,023 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-98AA515D663B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:42:00,023 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:42:00,023 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:42:00,024 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:42:00,024 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:42:00,031 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:42:00,032 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:42:00,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:42:00,038 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:42:00,040 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b
datanode_2          | 2023-06-29 21:42:00,040 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:42:00,043 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:42:00,044 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:42:00,047 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:42:00,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:42:00,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:42:00,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:42:00,048 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:42:00,050 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:42:00,054 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:42:00,055 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:42:00,069 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:42:00,087 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:42:00,089 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:42:00,104 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:00,107 [pool-22-thread-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:42:00,115 [pool-22-thread-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:00,116 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-98AA515D663B,id=0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:42:00,116 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:42:00,117 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:42:00,117 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:42:00,117 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:42:00,125 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:00,164 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:00,155 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2.
datanode_2          | 2023-06-29 21:42:02,305 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-DA220F9029B2, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:42:02,307 [grpc-default-executor-0] INFO impl.VoteContext: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FOLLOWER: accept ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: our priority 0 <= candidate's priority 0
datanode_2          | 2023-06-29 21:42:02,307 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_2          | 2023-06-29 21:42:02,307 [grpc-default-executor-0] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState
datanode_2          | 2023-06-29 21:42:02,308 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState was interrupted
datanode_2          | 2023-06-29 21:42:02,309 [grpc-default-executor-0] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState
datanode_2          | 2023-06-29 21:42:02,310 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:02,325 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:02,326 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2 replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0217402b-914d-4687-b2b8-79d59c463fcd#0:OK-t1. Peer's state: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2:t1, leader=null, voted=5569f9f3-6953-4e70-aaf7-1b773b1f5661, raftlog=Memoized:0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:03,464 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5119653319ns, electionTimeout:5097ms
datanode_2          | 2023-06-29 21:42:03,465 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState
datanode_2          | 2023-06-29 21:42:03,465 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:42:03,467 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:42:03,467 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1
datanode_2          | 2023-06-29 21:42:03,476 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:03,493 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-29 21:42:03,494 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1
datanode_2          | 2023-06-29 21:42:03,494 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:42:03,494 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-16309B018913 with new leaderId: 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:42:03,495 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: change Leader from null to 0217402b-914d-4687-b2b8-79d59c463fcd at term 1 for becomeLeader, leader elected after 5551ms
datanode_2          | 2023-06-29 21:42:03,519 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:42:03,544 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:42:03,546 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:42:03,554 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:42:03,555 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:42:03,556 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:42:03,574 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:42:03,579 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:42:03,585 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderStateImpl
datanode_2          | 2023-06-29 21:42:03,602 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:41:57,190 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:41:57,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:57,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:41:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:41:57,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:41:57,247 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=4b0d94af-634a-4894-8f42-44463133a719
datanode_3          | 2023-06-29 21:41:57,248 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=4b0d94af-634a-4894-8f42-44463133a719.
datanode_3          | 2023-06-29 21:41:57,249 [Command processor thread] INFO server.RaftServer: 0d3a8303-04f4-4b15-9343-c46934377c95: addNew group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] returns group-DA220F9029B2:java.util.concurrent.CompletableFuture@73c08108[Not completed]
datanode_3          | 2023-06-29 21:41:57,282 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95: new RaftServerImpl for group-DA220F9029B2:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:41:57,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:41:57,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:41:57,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:41:57,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:57,283 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:41:57,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:41:57,289 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:41:57,289 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:41:57,289 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:41:57,290 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:41:57,290 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:57,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:41:57,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:41:57,303 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:41:57,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:41:57,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:41:57,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:41:57,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:41:57,334 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 does not exist. Creating ...
datanode_3          | 2023-06-29 21:41:57,337 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/in_use.lock acquired by nodename 7@6d049e7eb0fe
datanode_3          | 2023-06-29 21:41:57,339 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2 has been successfully formatted.
datanode_3          | 2023-06-29 21:41:57,340 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-DA220F9029B2: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:41:57,352 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:41:57,352 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:41:57,352 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:41:57,353 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:41:57,354 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:41:57,362 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,362 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:41:57,362 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:41:57,363 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_3          | 2023-06-29 21:41:57,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:41:57,364 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:41:57,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:41:57,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:41:57,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:41:57,365 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:41:57,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:41:57,366 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:57,367 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:57,368 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:57,369 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:57,369 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:57,369 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:57,379 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:41:57,379 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:41:57,379 [pool-22-thread-1] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState
datanode_3          | 2023-06-29 21:41:57,384 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-DA220F9029B2,id=0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:41:57,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:57,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:41:57,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:41:57,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:41:57,389 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:41:57,390 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:41:57,395 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2
datanode_3          | 2023-06-29 21:41:59,879 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2.
datanode_3          | 2023-06-29 21:41:59,879 [Command processor thread] INFO server.RaftServer: 0d3a8303-04f4-4b15-9343-c46934377c95: addNew group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] returns group-98AA515D663B:java.util.concurrent.CompletableFuture@54f75ead[Not completed]
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95: new RaftServerImpl for group-98AA515D663B:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: ConfigurationManager, init=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:41:59,882 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:41:59,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:41:59,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:41:59,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:41:59,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:41:59,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:41:59,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:41:59,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:41:59,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:41:59,889 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:41:59,890 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:41:59,890 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b does not exist. Creating ...
datanode_3          | 2023-06-29 21:41:59,894 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/in_use.lock acquired by nodename 7@6d049e7eb0fe
datanode_3          | 2023-06-29 21:41:59,896 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b has been successfully formatted.
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-98AA515D663B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:41:59,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:41:59,898 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:59,899 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:41:59,899 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:41:59,902 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b
datanode_3          | 2023-06-29 21:41:59,903 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:41:59,903 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:41:59,903 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:59,903 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:41:59,907 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:41:59,909 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:41:59,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:41:59,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:41:59,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:41:59,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:59,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:41:59,915 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:59,915 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:59,915 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:41:59,915 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: start as a follower, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:41:59,916 [pool-22-thread-1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:41:59,916 [pool-22-thread-1] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:41:59,918 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-98AA515D663B,id=0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:41:59,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:41:59,923 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:41:59,923 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:41:59,923 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:41:59,924 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:41:59,966 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b
datanode_3          | 2023-06-29 21:41:59,967 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:00,178 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b.
datanode_3          | 2023-06-29 21:42:02,210 [grpc-default-executor-0] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-DA220F9029B2, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:42:02,212 [grpc-default-executor-0] INFO impl.VoteContext: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FOLLOWER: reject ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: our priority 1 > candidate's priority 0
datanode_3          | 2023-06-29 21:42:02,212 [grpc-default-executor-0] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_3          | 2023-06-29 21:42:02,217 [grpc-default-executor-0] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState
datanode_3          | 2023-06-29 21:42:02,218 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState was interrupted
datanode_3          | 2023-06-29 21:42:02,218 [grpc-default-executor-0] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState
datanode_3          | 2023-06-29 21:42:02,228 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:02,228 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:02,239 [grpc-default-executor-0] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2 replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t1. Peer's state: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2:t1, leader=null, voted=null, raftlog=Memoized:0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:02,330 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5156147892ns, electionTimeout:5139ms
datanode_3          | 2023-06-29 21:42:02,331 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState
datanode_3          | 2023-06-29 21:42:02,331 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:42:02,334 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:42:02,334 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1
datanode_3          | 2023-06-29 21:42:02,358 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:02,359 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-29 21:42:02,361 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1
datanode_3          | 2023-06-29 21:42:02,361 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:42:02,361 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-44463133A719 with new leaderId: 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:42:02,362 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: change Leader from null to 0d3a8303-04f4-4b15-9343-c46934377c95 at term 1 for becomeLeader, leader elected after 5696ms
datanode_3          | 2023-06-29 21:42:02,384 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:42:02,402 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:42:02,403 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:42:02,413 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:42:02,413 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:42:02,414 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:42:02,425 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:42:02,435 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:42:02,451 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderStateImpl
datanode_3          | 2023-06-29 21:42:02,486 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:42:02,585 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-LeaderElection1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:02,691 [0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-44463133A719-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/4b0d94af-634a-4894-8f42-44463133a719/current/log_inprogress_0
datanode_3          | 2023-06-29 21:42:05,045 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5129390124ns, electionTimeout:5078ms
datanode_3          | 2023-06-29 21:42:05,046 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:05,046 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:42:05,046 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:42:05,046 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2
datanode_3          | 2023-06-29 21:42:05,049 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:05,052 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:05,053 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:05,053 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_3          | 2023-06-29 21:42:05,054 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_3          | 2023-06-29 21:42:05,081 [grpc-default-executor-0] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-98AA515D663B, 1, (t:0, i:0))
datanode_3          | 2023-06-29 21:42:05,081 [grpc-default-executor-0] INFO impl.VoteContext: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-CANDIDATE: reject ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: already has voted for 0d3a8303-04f4-4b15-9343-c46934377c95 at current term 1
datanode_3          | 2023-06-29 21:42:05,081 [grpc-default-executor-0] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t1. Peer's state: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B:t1, leader=null, voted=0d3a8303-04f4-4b15-9343-c46934377c95, raftlog=Memoized:0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:05,096 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2: ELECTION REJECTED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:42:05,098 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection:   Response 0: 0d3a8303-04f4-4b15-9343-c46934377c95<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t1
datanode_3          | 2023-06-29 21:42:05,098 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2 ELECTION round 0: result REJECTED
datanode_3          | 2023-06-29 21:42:05,099 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
datanode_3          | 2023-06-29 21:42:05,099 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2
datanode_3          | 2023-06-29 21:42:05,099 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-LeaderElection2] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:05,099 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:05,101 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:07,321 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5103086111ns, electionTimeout:5093ms
datanode_3          | 2023-06-29 21:42:07,322 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState
datanode_3          | 2023-06-29 21:42:07,322 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_3          | 2023-06-29 21:42:07,322 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_3          | 2023-06-29 21:42:07,323 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-FollowerState] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3
datanode_3          | 2023-06-29 21:42:07,329 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:07,332 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:07,350 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:07,352 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_3          | 2023-06-29 21:42:07,352 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.LeaderElection:   Response 0: 0d3a8303-04f4-4b15-9343-c46934377c95<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:OK-t2
datanode_3          | 2023-06-29 21:42:07,352 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.LeaderElection: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3 ELECTION round 0: result PASSED
datanode_3          | 2023-06-29 21:42:07,352 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3
datanode_3          | 2023-06-29 21:42:07,352 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
datanode_3          | 2023-06-29 21:42:07,353 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA220F9029B2 with new leaderId: 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_3          | 2023-06-29 21:42:07,353 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: change Leader from null to 0d3a8303-04f4-4b15-9343-c46934377c95 at term 2 for becomeLeader, leader elected after 10062ms
datanode_3          | 2023-06-29 21:42:07,353 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:42:07,354 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:42:07,354 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:42:07,356 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:42:07,356 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:42:07,356 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:42:07,356 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:42:07,357 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:42:07,376 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:42:07,377 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:42:07,377 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:42:07,387 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:42:07,389 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:42:07,389 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:42:07,389 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:42:07,390 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-29 21:42:07,411 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_3          | 2023-06-29 21:42:07,412 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:42:07,412 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_3          | 2023-06-29 21:42:07,412 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_3          | 2023-06-29 21:42:07,412 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:42:07,412 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:42:07,413 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:42:07,413 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_3          | 2023-06-29 21:42:07,414 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderStateImpl
datanode_3          | 2023-06-29 21:42:07,417 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:42:07,420 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/current/log_inprogress_0
datanode_2          | 2023-06-29 21:42:03,620 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-LeaderElection1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913: set configuration 0: peers:[0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:03,709 [0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-16309B018913-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/90c00c1a-3550-4281-81b8-16309b018913/current/log_inprogress_0
datanode_2          | 2023-06-29 21:42:05,076 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-98AA515D663B, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:42:05,077 [grpc-default-executor-0] INFO impl.VoteContext: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FOLLOWER: reject ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-29 21:42:05,077 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_2          | 2023-06-29 21:42:05,077 [grpc-default-executor-0] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:05,077 [grpc-default-executor-0] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:05,078 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState was interrupted
datanode_2          | 2023-06-29 21:42:05,082 [grpc-default-executor-0] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t1. Peer's state: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B:t1, leader=null, voted=null, raftlog=Memoized:0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:05,082 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: receive requestVote(ELECTION, 0d3a8303-04f4-4b15-9343-c46934377c95, group-98AA515D663B, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:42:05,083 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:05,083 [grpc-default-executor-1] INFO impl.VoteContext: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FOLLOWER: reject ELECTION from 0d3a8303-04f4-4b15-9343-c46934377c95: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-29 21:42:05,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:0d3a8303-04f4-4b15-9343-c46934377c95
datanode_2          | 2023-06-29 21:42:05,084 [grpc-default-executor-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:05,084 [grpc-default-executor-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:05,084 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:05,084 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState was interrupted
datanode_2          | 2023-06-29 21:42:05,085 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B replies to ELECTION vote request: 0d3a8303-04f4-4b15-9343-c46934377c95<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t1. Peer's state: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B:t1, leader=null, voted=null, raftlog=Memoized:0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:05,094 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:05,094 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:07,372 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: receive requestVote(ELECTION, 0d3a8303-04f4-4b15-9343-c46934377c95, group-DA220F9029B2, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:42:07,372 [grpc-default-executor-1] INFO impl.VoteContext: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FOLLOWER: accept ELECTION from 0d3a8303-04f4-4b15-9343-c46934377c95: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:42:07,372 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:0d3a8303-04f4-4b15-9343-c46934377c95
datanode_2          | 2023-06-29 21:42:07,372 [grpc-default-executor-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState
datanode_2          | 2023-06-29 21:42:07,372 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState was interrupted
datanode_2          | 2023-06-29 21:42:07,373 [grpc-default-executor-1] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState
datanode_2          | 2023-06-29 21:42:07,376 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:07,436 [0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2-LeaderElection3] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-DA220F9029B2: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:10,134 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-98AA515D663B, 2, (t:0, i:0))
datanode_3          | 2023-06-29 21:42:10,135 [grpc-default-executor-4] INFO impl.VoteContext: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FOLLOWER: accept ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: our priority 0 <= candidate's priority 0
datanode_3          | 2023-06-29 21:42:10,135 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_3          | 2023-06-29 21:42:10,135 [grpc-default-executor-4] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:10,135 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState was interrupted
datanode_3          | 2023-06-29 21:42:10,135 [grpc-default-executor-4] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:10,146 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:10,154 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0d3a8303-04f4-4b15-9343-c46934377c95#0:OK-t2. Peer's state: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B:t2, leader=null, voted=5569f9f3-6953-4e70-aaf7-1b773b1f5661, raftlog=Memoized:0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:10,160 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:10,208 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: receive requestVote(ELECTION, 0217402b-914d-4687-b2b8-79d59c463fcd, group-98AA515D663B, 2, (t:0, i:0))
datanode_3          | 2023-06-29 21:42:10,208 [grpc-default-executor-4] INFO impl.VoteContext: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FOLLOWER: reject ELECTION from 0217402b-914d-4687-b2b8-79d59c463fcd: already has voted for 5569f9f3-6953-4e70-aaf7-1b773b1f5661 at current term 2
datanode_3          | 2023-06-29 21:42:10,208 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B replies to ELECTION vote request: 0217402b-914d-4687-b2b8-79d59c463fcd<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t2. Peer's state: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B:t2, leader=null, voted=5569f9f3-6953-4e70-aaf7-1b773b1f5661, raftlog=Memoized:0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:15,247 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: receive requestVote(ELECTION, 0217402b-914d-4687-b2b8-79d59c463fcd, group-98AA515D663B, 3, (t:0, i:0))
datanode_3          | 2023-06-29 21:42:15,247 [grpc-default-executor-4] INFO impl.VoteContext: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FOLLOWER: accept ELECTION from 0217402b-914d-4687-b2b8-79d59c463fcd: our priority 0 <= candidate's priority 1
datanode_3          | 2023-06-29 21:42:15,248 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:0217402b-914d-4687-b2b8-79d59c463fcd
datanode_3          | 2023-06-29 21:42:15,249 [grpc-default-executor-4] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: shutdown 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:15,249 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState was interrupted
datanode_3          | 2023-06-29 21:42:15,249 [grpc-default-executor-4] INFO impl.RoleInfo: 0d3a8303-04f4-4b15-9343-c46934377c95: start 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState
datanode_3          | 2023-06-29 21:42:15,253 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:42:15,253 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:42:15,254 [grpc-default-executor-4] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B replies to ELECTION vote request: 0217402b-914d-4687-b2b8-79d59c463fcd<-0d3a8303-04f4-4b15-9343-c46934377c95#0:OK-t3. Peer's state: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B:t3, leader=null, voted=0217402b-914d-4687-b2b8-79d59c463fcd, raftlog=Memoized:0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:15,349 [0d3a8303-04f4-4b15-9343-c46934377c95-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-98AA515D663B with new leaderId: 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:42:07,378 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2 replies to ELECTION vote request: 0d3a8303-04f4-4b15-9343-c46934377c95<-0217402b-914d-4687-b2b8-79d59c463fcd#0:OK-t2. Peer's state: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2:t2, leader=null, voted=0d3a8303-04f4-4b15-9343-c46934377c95, raftlog=Memoized:0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:07,383 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:07,469 [0217402b-914d-4687-b2b8-79d59c463fcd-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-DA220F9029B2 with new leaderId: 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_2          | 2023-06-29 21:42:07,471 [0217402b-914d-4687-b2b8-79d59c463fcd-server-thread1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: change Leader from null to 0d3a8303-04f4-4b15-9343-c46934377c95 at term 2 for appendEntries, leader elected after 9053ms
datanode_2          | 2023-06-29 21:42:07,525 [0217402b-914d-4687-b2b8-79d59c463fcd-server-thread2] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:1|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:07,529 [0217402b-914d-4687-b2b8-79d59c463fcd-server-thread2] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:42:07,531 [0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-DA220F9029B2-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/d24dfc41-67be-4d61-9d74-da220f9029b2/current/log_inprogress_0
datanode_2          | 2023-06-29 21:42:10,142 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5057527675ns, electionTimeout:5047ms
datanode_2          | 2023-06-29 21:42:10,143 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:10,144 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
datanode_2          | 2023-06-29 21:42:10,144 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:42:10,144 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2
datanode_2          | 2023-06-29 21:42:10,161 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: receive requestVote(ELECTION, 5569f9f3-6953-4e70-aaf7-1b773b1f5661, group-98AA515D663B, 2, (t:0, i:0))
datanode_2          | 2023-06-29 21:42:10,162 [grpc-default-executor-1] INFO impl.VoteContext: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-CANDIDATE: reject ELECTION from 5569f9f3-6953-4e70-aaf7-1b773b1f5661: already has voted for 0217402b-914d-4687-b2b8-79d59c463fcd at current term 2
datanode_2          | 2023-06-29 21:42:10,163 [grpc-default-executor-1] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B replies to ELECTION vote request: 5569f9f3-6953-4e70-aaf7-1b773b1f5661<-0217402b-914d-4687-b2b8-79d59c463fcd#0:FAIL-t2. Peer's state: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B:t2, leader=null, voted=0217402b-914d-4687-b2b8-79d59c463fcd, raftlog=Memoized:0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:10,162 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:10,175 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0d3a8303-04f4-4b15-9343-c46934377c95
datanode_2          | 2023-06-29 21:42:10,177 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:10,183 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:10,181 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 5569f9f3-6953-4e70-aaf7-1b773b1f5661
datanode_2          | 2023-06-29 21:42:10,224 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection:   Response 0: 0217402b-914d-4687-b2b8-79d59c463fcd<-0d3a8303-04f4-4b15-9343-c46934377c95#0:FAIL-t2
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection:   Response 1: 0217402b-914d-4687-b2b8-79d59c463fcd<-5569f9f3-6953-4e70-aaf7-1b773b1f5661#0:FAIL-t2
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2 ELECTION round 0: result REJECTED
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2
datanode_2          | 2023-06-29 21:42:10,225 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection2] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:10,229 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:10,230 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:15,234 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.FollowerState: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5008926910ns, electionTimeout:5004ms
datanode_2          | 2023-06-29 21:42:15,235 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState
datanode_2          | 2023-06-29 21:42:15,235 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
datanode_2          | 2023-06-29 21:42:15,235 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
datanode_2          | 2023-06-29 21:42:15,236 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-FollowerState] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3
datanode_2          | 2023-06-29 21:42:15,238 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3 ELECTION round 0: submit vote requests at term 3 for -1: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:42:15,238 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:42:15,239 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:42:15,259 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:42:15,259 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection:   Response 0: 0217402b-914d-4687-b2b8-79d59c463fcd<-0d3a8303-04f4-4b15-9343-c46934377c95#0:OK-t3
datanode_2          | 2023-06-29 21:42:15,259 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.LeaderElection: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3 ELECTION round 0: result PASSED
datanode_2          | 2023-06-29 21:42:15,259 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: shutdown 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3
datanode_2          | 2023-06-29 21:42:15,259 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: changes role from CANDIDATE to LEADER at term 3 for changeToLeader
datanode_2          | 2023-06-29 21:42:15,260 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-98AA515D663B with new leaderId: 0217402b-914d-4687-b2b8-79d59c463fcd
datanode_2          | 2023-06-29 21:42:15,260 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: change Leader from null to 0217402b-914d-4687-b2b8-79d59c463fcd at term 3 for becomeLeader, leader elected after 15270ms
datanode_2          | 2023-06-29 21:42:15,260 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:42:15,261 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:42:15,261 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:42:15,261 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:42:15,261 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:42:15,262 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:42:15,262 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:42:15,262 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:42:15,281 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:42:15,282 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:42:15,282 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:42:15,284 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:42:15,285 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:42:15,285 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:42:15,351 [0d3a8303-04f4-4b15-9343-c46934377c95-server-thread1] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: change Leader from null to 0217402b-914d-4687-b2b8-79d59c463fcd at term 3 for appendEntries, leader elected after 15465ms
datanode_3          | 2023-06-29 21:42:15,355 [0d3a8303-04f4-4b15-9343-c46934377c95-server-thread2] INFO server.RaftServer$Division: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:42:15,363 [0d3a8303-04f4-4b15-9343-c46934377c95-server-thread2] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:42:15,364 [0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0d3a8303-04f4-4b15-9343-c46934377c95@group-98AA515D663B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/current/log_inprogress_0
datanode_2          | 2023-06-29 21:42:15,285 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:42:15,285 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:42:15,287 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:42:15,288 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:42:15,288 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-29 21:42:15,288 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO impl.RoleInfo: 0217402b-914d-4687-b2b8-79d59c463fcd: start 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderStateImpl
datanode_2          | 2023-06-29 21:42:15,289 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:42:15,290 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e0e001cb-db02-4e27-95aa-98aa515d663b/current/log_inprogress_0
datanode_2          | 2023-06-29 21:42:15,297 [0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B-LeaderElection3] INFO server.RaftServer$Division: 0217402b-914d-4687-b2b8-79d59c463fcd@group-98AA515D663B: set configuration 0: peers:[0d3a8303-04f4-4b15-9343-c46934377c95|rpc:172.23.0.9:9856|admin:172.23.0.9:9857|client:172.23.0.9:9858|dataStream:|priority:0|startupRole:FOLLOWER, 5569f9f3-6953-4e70-aaf7-1b773b1f5661|rpc:172.23.0.5:9856|admin:172.23.0.5:9857|client:172.23.0.5:9858|dataStream:|priority:0|startupRole:FOLLOWER, 0217402b-914d-4687-b2b8-79d59c463fcd|rpc:172.23.0.3:9856|admin:172.23.0.3:9857|client:172.23.0.3:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:41:18,387 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f8be3a2f452c/172.23.0.10
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-29 21:41:18,427 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:41:23,343 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:41:25,231 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:41:25,566 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.10:9862
om_1                | 2023-06-29 21:41:25,566 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:41:25,577 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:41:25,645 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:41:26,459 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863]
om_1                | 2023-06-29 21:41:29,707 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:31,708 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:33,710 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:35,711 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:37,713 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:39,715 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:41,716 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:43,717 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:45,722 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From f8be3a2f452c/172.23.0.10 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:47,907 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fd53d016-86ea-45d7-ba8d-ac5e060a3c4d is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | 2023-06-29 21:41:49,917 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fd53d016-86ea-45d7-ba8d-ac5e060a3c4d is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om_1                | , while invoking $Proxy31.send over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14;layoutVersion=3
om_1                | 2023-06-29 21:41:52,136 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at f8be3a2f452c/172.23.0.10
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:41:54,221 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = f8be3a2f452c/172.23.0.10
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.3.0
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om_1                | STARTUP_MSG:   java = 11.0.14.1
om_1                | ************************************************************/
om_1                | 2023-06-29 21:41:54,228 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:41:55,190 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:41:55,685 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:41:55,849 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.23.0.10:9862
om_1                | 2023-06-29 21:41:55,849 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:41:55,849 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:41:55,925 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:41:55,969 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om_1                | 2023-06-29 21:41:57,063 [main] INFO reflections.Reflections: Reflections took 967 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om_1                | 2023-06-29 21:41:57,125 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:41:58,143 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863]
om_1                | 2023-06-29 21:41:58,359 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9863]
om_1                | 2023-06-29 21:42:00,531 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:42:00,703 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:42:00,704 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om_1                | 2023-06-29 21:42:01,043 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-29 21:42:01,138 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:42:01,196 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:42:01,196 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:42:01,226 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-29 21:42:01,247 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:42:01,299 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-29 21:42:01,317 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om_1                | 2023-06-29 21:42:01,382 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-29 21:42:01,514 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:42:01,528 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:42:01,528 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:42:01,529 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:42:01,529 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-29 21:42:01,530 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:42:01,530 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-29 21:42:01,535 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:42:01,536 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-29 21:42:01,536 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:42:01,551 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-29 21:42:01,559 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-29 21:42:01,562 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-29 21:42:01,983 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:42:01,992 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-29 21:42:02,002 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-29 21:42:02,003 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:42:02,007 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:42:02,028 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:42:02,060 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@364fd4ae[Not completed]
om_1                | 2023-06-29 21:42:02,060 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-29 21:42:02,105 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-29 21:42:02,111 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-29 21:42:02,118 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-29 21:42:02,118 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-29 21:42:02,121 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-29 21:42:02,123 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:42:02,127 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:42:02,127 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-29 21:42:02,156 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-29 21:42:02,157 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:42:02,181 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-29 21:42:02,183 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-29 21:42:02,272 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-29 21:42:02,300 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-29 21:42:02,308 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-29 21:42:02,863 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-29 21:42:02,863 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-29 21:42:02,864 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-29 21:42:02,904 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-29 21:42:02,904 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-29 21:42:03,566 [main] INFO reflections.Reflections: Reflections took 1367 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om_1                | 2023-06-29 21:42:03,694 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:42:03,708 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:42:03,863 [Listener at om/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:42:03,874 [Listener at om/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:42:03,874 [Listener at om/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-29 21:42:03,929 [Listener at om/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.23.0.10:9862
om_1                | 2023-06-29 21:42:03,929 [Listener at om/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-29 21:42:03,932 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-29 21:42:03,936 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 6@f8be3a2f452c
om_1                | 2023-06-29 21:42:03,948 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
om_1                | 2023-06-29 21:42:03,951 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:42:03,957 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om_1                | 2023-06-29 21:42:03,957 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:42:03,958 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om_1                | 2023-06-29 21:42:03,959 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om_1                | 2023-06-29 21:42:03,961 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:42:03,966 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om_1                | 2023-06-29 21:42:03,966 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om_1                | 2023-06-29 21:42:03,970 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
om_1                | 2023-06-29 21:42:03,971 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:42:03,971 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-29 21:42:03,972 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:42:03,973 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om_1                | 2023-06-29 21:42:03,973 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-29 21:42:03,974 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:42:03,974 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om_1                | 2023-06-29 21:42:03,974 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om_1                | 2023-06-29 21:42:03,993 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om_1                | 2023-06-29 21:42:03,993 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-29 21:42:03,994 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-29 21:42:03,994 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-29 21:42:04,000 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:42:04,001 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:42:04,005 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:42:04,005 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:42:04,006 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:42:04,012 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
om_1                | 2023-06-29 21:42:04,014 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om_1                | 2023-06-29 21:42:04,016 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om_1                | 2023-06-29 21:42:04,017 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:41:17,946 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = 2864f539970a/172.23.0.6
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.3.0
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1             | STARTUP_MSG:   java = 11.0.14.1
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:41:18,009 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | WARNING: An illegal reflective access operation has occurred
recon_1             | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1             | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:41:20,874 [main] INFO reflections.Reflections: Reflections took 369 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1             | 2023-06-29 21:41:23,923 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1             | 2023-06-29 21:41:25,100 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:41:30,807 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:41:32,233 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-29 21:41:32,240 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-29 21:41:32,269 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:41:32,406 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | 2023-06-29 21:41:32,432 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1             | 2023-06-29 21:41:34,620 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1             | 2023-06-29 21:41:36,969 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:41:37,051 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:41:37,106 [main] INFO util.log: Logging initialized @24525ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:41:37,608 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-29 21:41:37,627 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1             | 2023-06-29 21:41:37,680 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:41:37,684 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:41:37,689 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:41:37,690 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:41:38,462 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:41:39,424 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:41:39,460 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1             | 2023-06-29 21:41:39,480 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-29 21:41:39,544 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:41:39,550 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
recon_1             | 2023-06-29 21:41:40,794 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:41:41,071 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:41:41,135 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1             | 2023-06-29 21:41:41,137 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 2023-06-29 21:41:41,288 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:41:41,598 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1             | 2023-06-29 21:41:41,705 [main] INFO reflections.Reflections: Reflections took 99 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1             | 2023-06-29 21:41:41,777 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-29 21:41:41,813 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-29 21:41:41,819 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-29 21:41:41,829 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1             | 2023-06-29 21:41:41,886 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-29 21:41:41,923 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:41:42,007 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1             | 2023-06-29 21:41:42,036 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-29 21:41:42,141 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1             | 2023-06-29 21:41:42,141 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1             | 2023-06-29 21:41:42,254 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1             | 2023-06-29 21:41:42,317 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1             | 2023-06-29 21:41:42,317 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1             | 2023-06-29 21:41:42,565 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1             | 2023-06-29 21:41:42,568 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1             | 2023-06-29 21:41:42,637 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 2023-06-29 21:41:42,639 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1             | 2023-06-29 21:41:42,641 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1             | 2023-06-29 21:41:42,683 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1             | 2023-06-29 21:41:42,684 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1             | 2023-06-29 21:41:45,891 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@518b0c12{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-7867948443816600295/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1             | 2023-06-29 21:41:45,905 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1             | 2023-06-29 21:41:45,905 [Listener at 0.0.0.0/9891] INFO server.Server: Started @33325ms
recon_1             | 2023-06-29 21:41:45,909 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 2023-06-29 21:41:45,909 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1             | 2023-06-29 21:41:45,911 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1             | 2023-06-29 21:41:45,911 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1             | 2023-06-29 21:41:45,917 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1             | 2023-06-29 21:41:45,924 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1             | 2023-06-29 21:41:45,924 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1             | 2023-06-29 21:41:45,924 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 2023-06-29 21:41:45,924 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1             | 2023-06-29 21:41:45,925 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:41:48,117 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fd53d016-86ea-45d7-ba8d-ac5e060a3c4d is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-29 21:41:50,126 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:fd53d016-86ea-45d7-ba8d-ac5e060a3c4d is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.23.0.4:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1             | 2023-06-29 21:41:52,956 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 0 pipelines from SCM.
recon_1             | 2023-06-29 21:41:52,956 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:41:52,957 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1             | 2023-06-29 21:41:52,957 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:41:52,959 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1             | 2023-06-29 21:41:52,960 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1             | 2023-06-29 21:41:53,065 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1             | 2023-06-29 21:41:53,065 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1             | 2023-06-29 21:41:53,102 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:41:53,103 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1             | 2023-06-29 21:41:53,140 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1             | 2023-06-29 21:41:53,146 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 23 milliseconds.
recon_1             | 2023-06-29 21:41:53,485 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.3:58994: output error
recon_1             | 2023-06-29 21:41:53,486 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.9:38392: output error
recon_1             | 2023-06-29 21:41:53,490 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.5:40488: output error
recon_1             | 2023-06-29 21:41:53,492 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.5:40496: output error
recon_1             | 2023-06-29 21:41:53,492 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.3:58990: output error
recon_1             | 2023-06-29 21:41:53,493 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 172.23.0.9:38398: output error
recon_1             | 2023-06-29 21:41:53,493 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:53,497 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:53,497 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:53,497 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:53,497 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:53,496 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-06-29 21:42:04,018 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om_1                | 2023-06-29 21:42:04,022 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om_1                | 2023-06-29 21:42:04,022 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om_1                | 2023-06-29 21:42:04,026 [Listener at om/9862] INFO server.RaftServer: om1: start RPC server
om_1                | 2023-06-29 21:42:04,064 [Listener at om/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om_1                | 2023-06-29 21:42:04,067 [Listener at om/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om_1                | 2023-06-29 21:42:04,068 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om_1                | 2023-06-29 21:42:04,097 [Listener at om/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om_1                | 2023-06-29 21:42:04,098 [Listener at om/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om_1                | 2023-06-29 21:42:04,117 [Listener at om/9862] INFO util.log: Logging initialized @11335ms to org.eclipse.jetty.util.log.Slf4jLog
om_1                | 2023-06-29 21:42:04,204 [Listener at om/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om_1                | 2023-06-29 21:42:04,210 [Listener at om/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om_1                | 2023-06-29 21:42:04,218 [Listener at om/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om_1                | 2023-06-29 21:42:04,221 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om_1                | 2023-06-29 21:42:04,221 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:42:04,222 [Listener at om/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:42:04,255 [Listener at om/9862] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-29 21:42:04,256 [Listener at om/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om_1                | 2023-06-29 21:42:04,283 [Listener at om/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:42:04,284 [Listener at om/9862] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:42:04,286 [Listener at om/9862] INFO server.session: node0 Scavenging every 600000ms
om_1                | 2023-06-29 21:42:04,299 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@440ef8d{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:42:04,300 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b170235{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:42:04,597 [Listener at om/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6a7a1a0d{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-11390721490267437464/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om_1                | 2023-06-29 21:42:04,604 [Listener at om/9862] INFO server.AbstractConnector: Started ServerConnector@7d66a126{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-29 21:42:04,604 [Listener at om/9862] INFO server.Server: Started @11823ms
om_1                | 2023-06-29 21:42:04,606 [Listener at om/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om_1                | 2023-06-29 21:42:04,606 [Listener at om/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om_1                | 2023-06-29 21:42:04,608 [Listener at om/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om_1                | 2023-06-29 21:42:04,609 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-29 21:42:04,627 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om_1                | 2023-06-29 21:42:04,637 [Listener at om/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om_1                | 2023-06-29 21:42:04,658 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@919086] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om_1                | 2023-06-29 21:42:09,041 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5035272603ns, electionTimeout:5018ms
om_1                | 2023-06-29 21:42:09,043 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:42:09,044 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | 2023-06-29 21:42:09,057 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om_1                | 2023-06-29 21:42:09,057 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:42:09,077 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:42:09,078 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om_1                | 2023-06-29 21:42:09,078 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
om_1                | 2023-06-29 21:42:09,079 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om_1                | 2023-06-29 21:42:09,079 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 6806ms
om_1                | 2023-06-29 21:42:09,088 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:42:09,092 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:42:09,092 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:42:09,096 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om_1                | 2023-06-29 21:42:09,096 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:41:18,380 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1               | 2023-06-29 21:41:18,387 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1               | 2023-06-29 21:41:18,549 [main] INFO util.log: Logging initialized @6137ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1               | 2023-06-29 21:41:19,402 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1               | 2023-06-29 21:41:19,592 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1               | 2023-06-29 21:41:19,641 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1               | 2023-06-29 21:41:19,651 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1               | 2023-06-29 21:41:19,663 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1               | 2023-06-29 21:41:19,663 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1               | 2023-06-29 21:41:20,063 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1               | /************************************************************
s3g_1               | STARTUP_MSG: Starting Gateway
s3g_1               | STARTUP_MSG:   host = 84c2983a80ac/172.23.0.8
s3g_1               | STARTUP_MSG:   args = []
s3g_1               | STARTUP_MSG:   version = 1.3.0
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1               | STARTUP_MSG:   java = 11.0.14.1
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:41:20,100 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1               | 2023-06-29 21:41:20,221 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:41:20,645 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1               | 2023-06-29 21:41:21,448 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1               | 2023-06-29 21:41:21,452 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-29 21:41:21,637 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-29 21:41:21,640 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1               | 2023-06-29 21:41:21,896 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1               | 2023-06-29 21:41:21,896 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1               | 2023-06-29 21:41:21,898 [main] INFO server.session: node0 Scavenging every 600000ms
s3g_1               | 2023-06-29 21:41:22,088 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:41:22,111 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1               | WARNING: An illegal reflective access operation has occurred
s3g_1               | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1               | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1               | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1               | WARNING: All illegal access operations will be denied in a future release
s3g_1               | Jun 29, 2023 9:41:39 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1               | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1               | 
s3g_1               | 2023-06-29 21:41:39,412 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-2474667153722373133/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
s3g_1               | 2023-06-29 21:41:39,475 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:41:39,476 [main] INFO server.Server: Started @27064ms
s3g_1               | 2023-06-29 21:41:39,480 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1               | 2023-06-29 21:41:39,483 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | 2023-06-29 21:41:39,488 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:41:16,685 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 4ab3452317e5/172.23.0.4
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.3.0
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1             | 2023-06-29 21:41:54,592 [IPC Server handler 11 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0217402b-914d-4687-b2b8-79d59c463fcd
recon_1             | 2023-06-29 21:41:54,611 [IPC Server handler 11 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:54,634 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 0217402b-914d-4687-b2b8-79d59c463fcd to Node DB.
recon_1             | 2023-06-29 21:41:54,723 [IPC Server handler 1 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5569f9f3-6953-4e70-aaf7-1b773b1f5661
recon_1             | 2023-06-29 21:41:54,724 [IPC Server handler 1 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:54,739 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5569f9f3-6953-4e70-aaf7-1b773b1f5661 to Node DB.
recon_1             | 2023-06-29 21:41:55,236 [IPC Server handler 9 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0d3a8303-04f4-4b15-9343-c46934377c95
recon_1             | 2023-06-29 21:41:55,236 [IPC Server handler 9 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:55,237 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 0d3a8303-04f4-4b15-9343-c46934377c95 to Node DB.
recon_1             | 2023-06-29 21:41:56,325 [IPC Server handler 0 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-29 21:41:56,328 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=051de25a-6894-4194-92a4-bb8c98830da2. Trying to get from SCM.
recon_1             | 2023-06-29 21:41:56,504 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 051de25a-6894-4194-92a4-bb8c98830da2, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5569f9f3-6953-4e70-aaf7-1b773b1f5661, CreationTimestamp2023-06-29T21:41:52.959Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:41:56,566 [IPC Server handler 11 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-29 21:41:56,636 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 051de25a-6894-4194-92a4-bb8c98830da2, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5569f9f3-6953-4e70-aaf7-1b773b1f5661, CreationTimestamp2023-06-29T21:41:52.959Z[UTC]].
recon_1             | 2023-06-29 21:41:56,676 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=051de25a-6894-4194-92a4-bb8c98830da2 reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:56,684 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 051de25a-6894-4194-92a4-bb8c98830da2, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5569f9f3-6953-4e70-aaf7-1b773b1f5661, CreationTimestamp2023-06-29T21:41:52.959Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:41:56,791 [IPC Server handler 12 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_1.xcompat_default
recon_1             | 2023-06-29 21:41:56,794 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2. Trying to get from SCM.
recon_1             | 2023-06-29 21:41:56,826 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: d24dfc41-67be-4d61-9d74-da220f9029b2, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.606Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:41:56,827 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d24dfc41-67be-4d61-9d74-da220f9029b2, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.606Z[UTC]].
recon_1             | 2023-06-29 21:41:56,828 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:56,954 [IPC Server handler 13 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-29 21:41:56,956 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=4b0d94af-634a-4894-8f42-44463133a719. Trying to get from SCM.
recon_1             | 2023-06-29 21:41:56,968 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 4b0d94af-634a-4894-8f42-44463133a719, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:53.463Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:41:56,972 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4b0d94af-634a-4894-8f42-44463133a719, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:53.463Z[UTC]].
recon_1             | 2023-06-29 21:41:56,976 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=4b0d94af-634a-4894-8f42-44463133a719 reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:56,979 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4b0d94af-634a-4894-8f42-44463133a719, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0d3a8303-04f4-4b15-9343-c46934377c95, CreationTimestamp2023-06-29T21:41:53.463Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:41:57,353 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_3.xcompat_default
recon_1             | 2023-06-29 21:41:57,354 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:58,182 [IPC Server handler 9 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for xcompat_datanode_2.xcompat_default
recon_1             | 2023-06-29 21:41:58,183 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=90c00c1a-3550-4281-81b8-16309b018913. Trying to get from SCM.
recon_1             | 2023-06-29 21:41:58,187 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 90c00c1a-3550-4281-81b8-16309b018913, Nodes: 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.584Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:41:58,188 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 90c00c1a-3550-4281-81b8-16309b018913, Nodes: 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.584Z[UTC]].
recon_1             | 2023-06-29 21:41:58,188 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=90c00c1a-3550-4281-81b8-16309b018913 reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:58,189 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 90c00c1a-3550-4281-81b8-16309b018913, Nodes: 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0217402b-914d-4687-b2b8-79d59c463fcd, CreationTimestamp2023-06-29T21:41:54.584Z[UTC]] moved to OPEN state
om_1                | 2023-06-29 21:42:09,097 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-29 21:42:09,118 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:42:09,119 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om_1                | 2023-06-29 21:42:09,128 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
om_1                | 2023-06-29 21:42:09,256 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
om_1                | 2023-06-29 21:42:09,335 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:42:09,456 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
om_1                | 2023-06-29 21:42:09,571 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om_1                | [id: "om1"
om_1                | address: "om:9872"
om_1                | startupRole: FOLLOWER
om_1                | ]
om_1                | 2023-06-29 21:42:12,650 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:vol1 for user:hadoop
om_1                | 2023-06-29 21:42:12,698 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: bucket1 of layout LEGACY in volume: vol1
om_1                | 2023-06-29 21:42:32,406 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-29 21:42:32,407 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-29 21:42:32,407 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om_1                | 2023-06-29 21:42:46,255 [qtp1436610577-47] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om_1                | 2023-06-29 21:42:46,290 [qtp1436610577-47] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074966256 in 32 milliseconds
om_1                | 2023-06-29 21:42:46,351 [qtp1436610577-47] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 60 milliseconds
om_1                | 2023-06-29 21:42:46,357 [qtp1436610577-47] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688074966256
om_1                | 2023-06-29 21:42:53,919 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:41:16,732 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:41:17,271 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:41:17,658 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:41:17,690 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:41:18,988 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-29 21:41:20,221 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:41:20,223 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:41:20,223 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:41:20,241 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:41:20,241 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-29 21:41:20,241 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-29 21:41:20,242 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-29 21:41:20,296 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:41:20,296 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-29 21:41:20,306 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:41:20,395 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-29 21:41:20,399 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:41:20,446 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-29 21:41:22,686 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-29 21:41:22,711 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-29 21:41:22,716 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-29 21:41:22,729 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:41:22,732 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:41:22,738 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:41:22,924 [main] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: addNew group-0A5B3F9EBF14:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|priority:0|startupRole:FOLLOWER] returns group-0A5B3F9EBF14:java.util.concurrent.CompletableFuture@6e16b8b5[Not completed]
scm_1               | 2023-06-29 21:41:23,208 [pool-2-thread-1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: new RaftServerImpl for group-0A5B3F9EBF14:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-29 21:41:23,229 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-29 21:41:23,240 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-29 21:41:23,250 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-29 21:41:23,251 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:41:23,251 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:41:23,252 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-29 21:41:23,287 [pool-2-thread-1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: ConfigurationManager, init=-1: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-29 21:41:23,312 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:41:23,394 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-29 21:41:23,396 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-29 21:41:23,579 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-29 21:41:23,690 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-29 21:41:23,734 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-29 21:41:24,084 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-29 21:41:25,734 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-29 21:41:25,836 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-29 21:41:25,862 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-29 21:41:25,881 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-29 21:41:25,885 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:41:25,896 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14 does not exist. Creating ...
scm_1               | 2023-06-29 21:41:25,919 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/in_use.lock acquired by nodename 13@4ab3452317e5
scm_1               | 2023-06-29 21:41:26,060 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14 has been successfully formatted.
scm_1               | 2023-06-29 21:41:26,109 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-29 21:41:26,295 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:41:26,299 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:41:26,336 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-29 21:41:26,346 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:41:26,349 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:41:26,521 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:41:26,524 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-29 21:41:26,585 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14
scm_1               | 2023-06-29 21:41:26,588 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:41:26,628 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:26,670 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:41:26,670 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-29 21:41:26,688 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:41:26,708 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-29 21:41:26,722 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-29 21:41:26,723 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:41:26,926 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-29 21:41:26,930 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-29 21:41:26,949 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-29 21:41:26,986 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-29 21:41:27,070 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:41:27,070 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:41:27,093 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: start as a follower, conf=-1: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:27,302 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm_1               | 2023-06-29 21:41:27,304 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState
scm_1               | 2023-06-29 21:41:27,352 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:41:27,353 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:41:27,352 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0A5B3F9EBF14,id=fd53d016-86ea-45d7-ba8d-ac5e060a3c4d
scm_1               | 2023-06-29 21:41:27,355 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-29 21:41:27,368 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-29 21:41:27,369 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-29 21:41:27,371 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-29 21:41:27,417 [main] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start RPC server
scm_1               | 2023-06-29 21:41:28,001 [main] INFO server.GrpcService: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: GrpcService started, listening on 9894
scm_1               | 2023-06-29 21:41:28,033 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: Started
scm_1               | 2023-06-29 21:41:32,376 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.FollowerState: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5072924269ns, electionTimeout:5015ms
scm_1               | 2023-06-29 21:41:32,378 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState
scm_1               | 2023-06-29 21:41:32,400 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm_1               | 2023-06-29 21:41:32,403 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-29 21:41:32,411 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1
scm_1               | 2023-06-29 21:41:32,477 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.LeaderElection: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:32,477 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.LeaderElection: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm_1               | 2023-06-29 21:41:32,479 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1
scm_1               | 2023-06-29 21:41:32,488 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm_1               | 2023-06-29 21:41:32,499 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: change Leader from null to fd53d016-86ea-45d7-ba8d-ac5e060a3c4d at term 1 for becomeLeader, leader elected after 8920ms
scm_1               | 2023-06-29 21:41:32,544 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:41:32,619 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:32,620 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:41:32,698 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:41:32,699 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:41:32,701 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-29 21:41:32,777 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:32,831 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-29 21:41:32,884 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderStateImpl
scm_1               | 2023-06-29 21:41:33,130 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: Starting segment from index:0
scm_1               | 2023-06-29 21:41:33,517 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: set configuration 0: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:33,816 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/log_inprogress_0
scm_1               | 2023-06-29 21:41:34,041 [main] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: close
scm_1               | 2023-06-29 21:41:34,056 [main] INFO server.GrpcService: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown server GrpcServerProtocolService now
scm_1               | 2023-06-29 21:41:34,092 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: shutdown
scm_1               | 2023-06-29 21:41:34,092 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-0A5B3F9EBF14,id=fd53d016-86ea-45d7-ba8d-ac5e060a3c4d
scm_1               | 2023-06-29 21:41:34,092 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderStateImpl
scm_1               | 2023-06-29 21:41:34,132 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO impl.PendingRequests: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-29 21:41:34,139 [main] INFO server.GrpcService: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown server GrpcServerProtocolService successfully
scm_1               | 2023-06-29 21:41:34,141 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO impl.StateMachineUpdater: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-29 21:41:34,149 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO impl.StateMachineUpdater: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-29 21:41:34,150 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO impl.StateMachineUpdater: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-29 21:41:34,171 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: closes. applyIndex: 0
scm_1               | 2023-06-29 21:41:34,189 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm_1               | 2023-06-29 21:41:34,191 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker close()
scm_1               | 2023-06-29 21:41:34,220 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: Stopped
scm_1               | 2023-06-29 21:41:34,220 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:41:34,256 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14; layoutVersion=4; scmId=fd53d016-86ea-45d7-ba8d-ac5e060a3c4d
recon_1             | 2023-06-29 21:41:58,457 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:59,857 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:59,857 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b. Trying to get from SCM.
recon_1             | 2023-06-29 21:41:59,874 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: e0e001cb-db02-4e27-95aa-98aa515d663b, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.618Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:41:59,875 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e0e001cb-db02-4e27-95aa-98aa515d663b, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.618Z[UTC]].
recon_1             | 2023-06-29 21:41:59,876 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:59,907 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:41:59,907 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:00,024 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:00,025 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:01,806 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:01,806 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:02,378 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:02,379 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:03,512 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:03,513 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:07,360 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:07,361 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d24dfc41-67be-4d61-9d74-da220f9029b2, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0d3a8303-04f4-4b15-9343-c46934377c95, CreationTimestamp2023-06-29T21:41:54.606Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:42:07,365 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:14,683 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-29 21:42:14,763 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:42:15,264 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b reported by 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1             | 2023-06-29 21:42:15,264 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e0e001cb-db02-4e27-95aa-98aa515d663b, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0217402b-914d-4687-b2b8-79d59c463fcd, CreationTimestamp2023-06-29T21:41:54.618Z[UTC]] moved to OPEN state
recon_1             | 2023-06-29 21:42:23,266 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:42:23,274 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1             | 2023-06-29 21:42:45,925 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:42:45,926 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:42:46,408 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688074965926
recon_1             | 2023-06-29 21:42:46,412 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:42:46,413 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1             | 2023-06-29 21:42:46,512 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688074965926.
recon_1             | 2023-06-29 21:42:46,549 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:42:46,569 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1             | 2023-06-29 21:42:46,586 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-29 21:42:46,841 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1             | 2023-06-29 21:42:46,841 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:42:46,842 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-29 21:42:46,842 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-29 21:42:46,886 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:42:46,886 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.044 seconds to process 4 keys.
recon_1             | 2023-06-29 21:42:46,917 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:42:46,940 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-29 21:42:47,049 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
scm_1               | 2023-06-29 21:41:34,351 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm_1               | /************************************************************
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 4ab3452317e5/172.23.0.4
scm_1               | ************************************************************/
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:41:40,672 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
scm_1               | STARTUP_MSG:   host = 4ab3452317e5/172.23.0.4
scm_1               | STARTUP_MSG:   args = []
scm_1               | STARTUP_MSG:   version = 1.3.0
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm_1               | STARTUP_MSG:   java = 11.0.14.1
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:41:40,704 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:41:40,853 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:41:40,925 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:41:40,931 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm_1               | 2023-06-29 21:41:42,025 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:41:42,269 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:41:42,652 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm_1               | 2023-06-29 21:41:42,657 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm_1               | 2023-06-29 21:41:42,776 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm_1               | 2023-06-29 21:41:42,802 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:fd53d016-86ea-45d7-ba8d-ac5e060a3c4d
scm_1               | 2023-06-29 21:41:42,943 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm_1               | 2023-06-29 21:41:43,045 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:41:43,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:41:43,049 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:41:43,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm_1               | 2023-06-29 21:41:43,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-29 21:41:43,051 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm_1               | 2023-06-29 21:41:43,052 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm_1               | 2023-06-29 21:41:43,054 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:41:43,056 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm_1               | 2023-06-29 21:41:43,057 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:41:43,080 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm_1               | 2023-06-29 21:41:43,086 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:41:43,086 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm_1               | 2023-06-29 21:41:43,481 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm_1               | 2023-06-29 21:41:43,488 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-29 21:41:43,488 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm_1               | 2023-06-29 21:41:43,489 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:41:43,489 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:41:43,495 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:41:43,507 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: found a subdirectory /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14
scm_1               | 2023-06-29 21:41:43,515 [main] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: addNew group-0A5B3F9EBF14:[] returns group-0A5B3F9EBF14:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm_1               | 2023-06-29 21:41:43,548 [pool-16-thread-1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: new RaftServerImpl for group-0A5B3F9EBF14:[] with SCMStateMachine:uninitialized
scm_1               | 2023-06-29 21:41:43,560 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm_1               | 2023-06-29 21:41:43,561 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-29 21:41:43,561 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-29 21:41:43,562 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:41:43,562 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:41:43,563 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm_1               | 2023-06-29 21:41:43,581 [pool-16-thread-1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm_1               | 2023-06-29 21:41:43,581 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:41:43,586 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm_1               | 2023-06-29 21:41:43,588 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-29 21:41:43,616 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm_1               | 2023-06-29 21:41:43,619 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm_1               | 2023-06-29 21:41:43,625 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm_1               | 2023-06-29 21:41:43,928 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-29 21:41:43,929 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm_1               | 2023-06-29 21:41:43,930 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-29 21:41:43,931 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm_1               | 2023-06-29 21:41:43,932 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:41:43,936 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm_1               | 2023-06-29 21:41:43,936 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm_1               | 2023-06-29 21:41:43,936 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm_1               | 2023-06-29 21:41:43,968 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm_1               | 2023-06-29 21:41:44,323 [main] INFO reflections.Reflections: Reflections took 227 ms to scan 3 urls, producing 112 keys and 252 values 
scm_1               | 2023-06-29 21:41:44,423 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm_1               | 2023-06-29 21:41:44,427 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm_1               | 2023-06-29 21:41:44,432 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm_1               | 2023-06-29 21:41:44,433 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm_1               | 2023-06-29 21:41:44,490 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm_1               | 2023-06-29 21:41:44,506 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm_1               | 2023-06-29 21:41:44,511 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-29 21:41:44,521 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm_1               | 2023-06-29 21:41:44,566 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm_1               | 2023-06-29 21:41:44,569 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm_1               | 2023-06-29 21:41:44,578 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm_1               | 2023-06-29 21:41:44,578 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:41:44,589 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm_1               | 2023-06-29 21:41:44,595 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm_1               | 2023-06-29 21:41:44,602 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm_1               | 2023-06-29 21:41:44,606 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm_1               | 2023-06-29 21:41:44,674 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:41:44,701 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-29 21:41:44,756 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-29 21:41:44,799 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-29 21:41:44,802 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-29 21:41:44,810 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-29 21:41:44,819 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:44,823 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:41:45,951 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:41:45,990 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:41:46,061 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm_1               | 2023-06-29 21:41:46,230 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:41:46,233 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:41:46,235 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:41:46,263 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:41:46,268 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:41:46,268 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:41:46,322 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-29 21:41:46,322 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        true
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-29 21:41:46,323 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:41:46,323 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-29 21:41:46,332 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:41:46,337 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-29 21:41:46,345 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/in_use.lock acquired by nodename 7@4ab3452317e5
scm_1               | 2023-06-29 21:41:46,348 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=fd53d016-86ea-45d7-ba8d-ac5e060a3c4d} from /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/raft-meta
scm_1               | 2023-06-29 21:41:46,386 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: set configuration 0: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:46,389 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm_1               | 2023-06-29 21:41:46,397 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:41:46,403 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:41:46,405 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-29 21:41:46,406 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:41:46,411 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:41:46,419 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:41:46,422 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-29 21:41:46,428 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14
scm_1               | 2023-06-29 21:41:46,429 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:41:46,429 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:46,430 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:41:46,430 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-29 21:41:46,431 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:41:46,434 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-29 21:41:46,434 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-29 21:41:46,435 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:41:46,441 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-29 21:41:46,442 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-29 21:41:46,442 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-29 21:41:46,443 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-29 21:41:46,506 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: set configuration 0: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:46,511 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/log_inprogress_0
scm_1               | 2023-06-29 21:41:46,514 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm_1               | 2023-06-29 21:41:46,514 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:41:46,566 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: start as a follower, conf=0: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:46,567 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-29 21:41:46,568 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState
scm_1               | 2023-06-29 21:41:46,569 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:41:46,569 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:41:46,572 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-0A5B3F9EBF14,id=fd53d016-86ea-45d7-ba8d-ac5e060a3c4d
scm_1               | 2023-06-29 21:41:46,574 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-29 21:41:46,574 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-29 21:41:46,575 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-29 21:41:46,576 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-29 21:41:46,578 [Listener at 0.0.0.0/9860] INFO server.RaftServer: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start RPC server
scm_1               | 2023-06-29 21:41:46,611 [Listener at 0.0.0.0/9860] INFO server.GrpcService: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: GrpcService started, listening on 9894
scm_1               | 2023-06-29 21:41:46,613 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: Started
scm_1               | 2023-06-29 21:41:46,619 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-29 21:41:46,619 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-29 21:41:46,674 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:41:46,684 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:41:46,685 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:41:46,982 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:41:46,983 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:41:46,991 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:41:47,006 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:41:47,007 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:41:47,011 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:41:47,015 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:41:47,075 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1c84d80a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm_1               | 2023-06-29 21:41:47,082 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:41:47,083 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:41:47,101 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @10609ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:41:47,237 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-29 21:41:47,241 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:41:47,247 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:41:47,248 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:41:47,248 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:41:47,248 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:41:47,288 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-29 21:41:47,289 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm_1               | 2023-06-29 21:41:47,318 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:41:47,318 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:41:47,319 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-29 21:41:47,328 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@591f6f83{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:41:47,329 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4bf4680c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:41:47,588 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@afee63{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-6757565630080312248/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm_1               | 2023-06-29 21:41:47,595 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@550c973e{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:41:47,596 [Listener at 0.0.0.0/9860] INFO server.Server: Started @11103ms
scm_1               | 2023-06-29 21:41:47,603 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:41:47,603 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:41:47,605 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:41:51,719 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.FollowerState: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5151696826ns, electionTimeout:5149ms
scm_1               | 2023-06-29 21:41:51,721 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState
scm_1               | 2023-06-29 21:41:51,722 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-29 21:41:51,724 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm_1               | 2023-06-29 21:41:51,725 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-FollowerState] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1
scm_1               | 2023-06-29 21:41:51,742 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.LeaderElection: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:51,742 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.LeaderElection: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-29 21:41:51,743 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: shutdown fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1
scm_1               | 2023-06-29 21:41:51,743 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-29 21:41:51,743 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-29 21:41:51,743 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-29 21:41:51,747 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: change Leader from null to fd53d016-86ea-45d7-ba8d-ac5e060a3c4d at term 2 for becomeLeader, leader elected after 8127ms
scm_1               | 2023-06-29 21:41:51,755 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:41:51,761 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:51,762 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:41:51,766 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:41:51,766 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:41:51,766 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-29 21:41:51,770 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:41:51,772 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-29 21:41:51,777 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO impl.RoleInfo: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d: start fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderStateImpl
scm_1               | 2023-06-29 21:41:51,783 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-29 21:41:51,792 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/log_inprogress_0 to /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/log_0-0
scm_1               | 2023-06-29 21:41:51,799 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-LeaderElection1] INFO server.RaftServer$Division: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14: set configuration 1: peers:[fd53d016-86ea-45d7-ba8d-ac5e060a3c4d|rpc:4ab3452317e5:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:41:51,811 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/f4a63518-edb9-4c7a-a31a-0a5b3f9ebf14/current/log_inprogress_1
scm_1               | 2023-06-29 21:41:51,816 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-29 21:41:51,817 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-29 21:41:51,837 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:51,838 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-29 21:41:51,839 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:41:51,839 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:41:51,851 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:41:51,854 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:41:51,983 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.3:38728: output error
scm_1               | 2023-06-29 21:41:52,006 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-29 21:41:51,999 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.5:55300: output error
scm_1               | 2023-06-29 21:41:51,998 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 172.23.0.9:54868: output error
scm_1               | 2023-06-29 21:41:52,030 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-29 21:41:52,031 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm_1               | 2023-06-29 21:41:52,842 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5569f9f3-6953-4e70-aaf7-1b773b1f5661
scm_1               | 2023-06-29 21:41:52,864 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:41:52,930 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:41:52,960 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=051de25a-6894-4194-92a4-bb8c98830da2 to datanode:5569f9f3-6953-4e70-aaf7-1b773b1f5661
scm_1               | 2023-06-29 21:41:52,973 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:41:53,014 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:41:53,112 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:41:53,277 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0d3a8303-04f4-4b15-9343-c46934377c95
scm_1               | 2023-06-29 21:41:53,281 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:41:53,282 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:41:53,295 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:41:53,401 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 051de25a-6894-4194-92a4-bb8c98830da2, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:52.959Z[UTC]].
scm_1               | 2023-06-29 21:41:53,409 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:53,463 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=4b0d94af-634a-4894-8f42-44463133a719 to datanode:0d3a8303-04f4-4b15-9343-c46934377c95
scm_1               | 2023-06-29 21:41:53,491 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 4b0d94af-634a-4894-8f42-44463133a719, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:53.463Z[UTC]].
scm_1               | 2023-06-29 21:41:53,493 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:54,581 [IPC Server handler 21 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0217402b-914d-4687-b2b8-79d59c463fcd
scm_1               | 2023-06-29 21:41:54,582 [IPC Server handler 21 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:41:54,583 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:41:54,584 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=90c00c1a-3550-4281-81b8-16309b018913 to datanode:0217402b-914d-4687-b2b8-79d59c463fcd
scm_1               | 2023-06-29 21:41:54,587 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 90c00c1a-3550-4281-81b8-16309b018913, Nodes: 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.584Z[UTC]].
scm_1               | 2023-06-29 21:41:54,587 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:54,599 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:41:54,599 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:41:54,600 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:41:54,600 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:41:54,601 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-29 21:41:54,601 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:41:54,606 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 to datanode:0d3a8303-04f4-4b15-9343-c46934377c95
scm_1               | 2023-06-29 21:41:54,606 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 to datanode:5569f9f3-6953-4e70-aaf7-1b773b1f5661
scm_1               | 2023-06-29 21:41:54,606 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 to datanode:0217402b-914d-4687-b2b8-79d59c463fcd
scm_1               | 2023-06-29 21:41:54,617 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: d24dfc41-67be-4d61-9d74-da220f9029b2, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.606Z[UTC]].
scm_1               | 2023-06-29 21:41:54,617 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:54,618 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b to datanode:5569f9f3-6953-4e70-aaf7-1b773b1f5661
scm_1               | 2023-06-29 21:41:54,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b to datanode:0217402b-914d-4687-b2b8-79d59c463fcd
scm_1               | 2023-06-29 21:41:54,625 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b to datanode:0d3a8303-04f4-4b15-9343-c46934377c95
scm_1               | 2023-06-29 21:41:54,628 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e0e001cb-db02-4e27-95aa-98aa515d663b, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:41:54.618Z[UTC]].
scm_1               | 2023-06-29 21:41:54,632 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:54,632 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=e0e001cb-db02-4e27-95aa-98aa515d663b contains same datanodes as previous pipelines: PipelineID=d24dfc41-67be-4d61-9d74-da220f9029b2 nodeIds: 5569f9f3-6953-4e70-aaf7-1b773b1f5661, 0217402b-914d-4687-b2b8-79d59c463fcd, 0d3a8303-04f4-4b15-9343-c46934377c95
scm_1               | 2023-06-29 21:41:56,352 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 051de25a-6894-4194-92a4-bb8c98830da2, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:5569f9f3-6953-4e70-aaf7-1b773b1f5661, CreationTimestamp2023-06-29T21:41:52.959Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:41:56,363 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:56,395 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:56,827 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:56,981 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 4b0d94af-634a-4894-8f42-44463133a719, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0d3a8303-04f4-4b15-9343-c46934377c95, CreationTimestamp2023-06-29T21:41:53.463Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:41:56,985 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:56,988 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:57,364 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:58,225 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 90c00c1a-3550-4281-81b8-16309b018913, Nodes: 0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0217402b-914d-4687-b2b8-79d59c463fcd, CreationTimestamp2023-06-29T21:41:54.584Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:41:58,232 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:41:58,234 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:58,454 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:59,925 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:41:59,929 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:00,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:01,803 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:02,393 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:03,510 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:07,365 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: d24dfc41-67be-4d61-9d74-da220f9029b2, Nodes: 0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0d3a8303-04f4-4b15-9343-c46934377c95, CreationTimestamp2023-06-29T21:41:54.606Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:42:07,365 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:07,385 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:42:07,391 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:42:07,391 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:42:07,391 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:42:07,391 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-29 21:42:07,392 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm_1               | 2023-06-29 21:42:07,392 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-29 21:42:07,392 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:42:07,393 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:42:07,407 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-29 21:42:07,409 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-29 21:42:12,758 [IPC Server handler 99 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-29 21:42:12,776 [fd53d016-86ea-45d7-ba8d-ac5e060a3c4d@group-0A5B3F9EBF14-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-29 21:42:12,778 [IPC Server handler 99 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-29 21:42:15,268 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e0e001cb-db02-4e27-95aa-98aa515d663b, Nodes: 5569f9f3-6953-4e70-aaf7-1b773b1f5661{ip: 172.23.0.5, host: xcompat_datanode_1.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0217402b-914d-4687-b2b8-79d59c463fcd{ip: 172.23.0.3, host: xcompat_datanode_2.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0d3a8303-04f4-4b15-9343-c46934377c95{ip: 172.23.0.9, host: xcompat_datanode_3.xcompat_default, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0217402b-914d-4687-b2b8-79d59c463fcd, CreationTimestamp2023-06-29T21:41:54.618Z[UTC]] moved to OPEN state
scm_1               | 2023-06-29 21:42:41,718 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.11
scm_1               | 2023-06-29 21:42:51,008 [IPC Server handler 16 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.11
scm_1               | 2023-06-29 21:43:46,376 [IPC Server handler 1 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.11
scm_1               | 2023-06-29 21:43:55,135 [IPC Server handler 99 on default port 9863] WARN node.SCMNodeManager: Cannot find node for address 172.23.0.11
Attaching to xcompat_om_1, xcompat_scm_1, xcompat_datanode_5, xcompat_datanode_2, xcompat_datanode_3, xcompat_s3g_1, xcompat_new_client_1, xcompat_datanode_1, xcompat_datanode_4, xcompat_old_client_1_1_0_1, xcompat_old_client_1_0_0_1, xcompat_old_client_1_3_0_1, xcompat_recon_1, xcompat_old_client_1_2_1_1
datanode_1          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_1          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_1          | 2023-06-29 21:44:30,067 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_1          | /************************************************************
datanode_1          | STARTUP_MSG: Starting HddsDatanodeService
datanode_1          | STARTUP_MSG:   host = a0f9d6dccb28/172.24.0.7
datanode_1          | STARTUP_MSG:   args = []
datanode_1          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_1          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_1          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_1          | STARTUP_MSG:   java = 11.0.19
datanode_1          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_1          | ************************************************************/
datanode_1          | 2023-06-29 21:44:30,139 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_1          | 2023-06-29 21:44:30,455 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_1          | 2023-06-29 21:44:31,198 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_1          | 2023-06-29 21:44:32,296 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_1          | 2023-06-29 21:44:32,296 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_1          | 2023-06-29 21:44:33,258 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:a0f9d6dccb28 ip:172.24.0.7
datanode_1          | 2023-06-29 21:44:34,518 [main] INFO reflections.Reflections: Reflections took 896 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_1          | 2023-06-29 21:44:38,002 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_1          | 2023-06-29 21:44:38,422 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_1          | 2023-06-29 21:44:39,938 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_1          | 2023-06-29 21:44:40,116 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_1          | 2023-06-29 21:44:40,131 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_1          | 2023-06-29 21:44:40,150 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_1          | 2023-06-29 21:44:40,393 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:44:40,457 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:44:40,461 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_1          | 2023-06-29 21:44:40,488 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_1          | 2023-06-29 21:44:40,489 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_1          | 2023-06-29 21:44:40,492 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:44:40,720 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_1          | 2023-06-29 21:44:40,734 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_1          | 2023-06-29 21:44:50,957 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_1          | 2023-06-29 21:44:51,863 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_1          | 2023-06-29 21:44:52,172 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_1          | 2023-06-29 21:44:52,737 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:44:52,765 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_1          | 2023-06-29 21:44:52,790 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_1          | 2023-06-29 21:44:52,791 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_1          | 2023-06-29 21:44:52,795 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_1          | 2023-06-29 21:44:52,796 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_1          | 2023-06-29 21:44:52,796 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_1          | 2023-06-29 21:44:52,800 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:44:52,808 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_1          | 2023-06-29 21:44:52,810 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:44:53,172 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_1          | 2023-06-29 21:44:53,206 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_1          | 2023-06-29 21:44:53,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_1          | 2023-06-29 21:44:55,698 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_1          | 2023-06-29 21:44:55,731 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_1          | 2023-06-29 21:44:55,740 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_1          | 2023-06-29 21:44:55,740 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:44:55,748 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:44:55,771 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:44:56,353 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_1          | 2023-06-29 21:44:56,839 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_1          | 2023-06-29 21:44:58,200 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:44:58,302 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_1          | 2023-06-29 21:44:58,557 [main] INFO util.log: Logging initialized @40657ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_1          | 2023-06-29 21:44:59,489 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_1          | 2023-06-29 21:44:59,586 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_1          | 2023-06-29 21:44:59,668 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_1          | 2023-06-29 21:44:59,693 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_1          | 2023-06-29 21:44:59,703 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_1          | 2023-06-29 21:44:59,707 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_1          | 2023-06-29 21:45:00,034 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_1          | 2023-06-29 21:45:00,072 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_1          | 2023-06-29 21:45:00,084 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_1          | 2023-06-29 21:45:00,352 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_1          | 2023-06-29 21:45:00,352 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_1          | 2023-06-29 21:45:00,380 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_1          | 2023-06-29 21:45:00,486 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6859bbd4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_1          | 2023-06-29 21:45:00,493 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@62376bdd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_1          | 2023-06-29 21:45:01,308 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1a1dd8eb{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-1524352004890244648/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_1          | 2023-06-29 21:45:01,381 [main] INFO server.AbstractConnector: Started ServerConnector@48d14ea0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_1          | 2023-06-29 21:45:01,381 [main] INFO server.Server: Started @43481ms
datanode_1          | 2023-06-29 21:45:01,397 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_1          | 2023-06-29 21:45:01,397 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_1          | 2023-06-29 21:45:01,403 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_1          | 2023-06-29 21:45:01,663 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_1          | 2023-06-29 21:45:01,859 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_1          | 2023-06-29 21:45:01,894 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_1          | 2023-06-29 21:45:03,822 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_1          | 2023-06-29 21:45:03,852 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_1          | 2023-06-29 21:45:03,854 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_1          | 2023-06-29 21:45:03,893 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_1          | 2023-06-29 21:45:03,905 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_1          | 2023-06-29 21:45:04,584 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.2:9891
datanode_2          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_2          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_2          | 2023-06-29 21:44:29,908 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_2          | /************************************************************
datanode_2          | STARTUP_MSG: Starting HddsDatanodeService
datanode_2          | STARTUP_MSG:   host = c97024849072/172.24.0.15
datanode_2          | STARTUP_MSG:   args = []
datanode_2          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_2          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_2          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_2          | STARTUP_MSG:   java = 11.0.19
datanode_2          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_2          | ************************************************************/
datanode_2          | 2023-06-29 21:44:29,947 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_2          | 2023-06-29 21:44:30,377 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_2          | 2023-06-29 21:44:31,165 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_2          | 2023-06-29 21:44:32,142 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_2          | 2023-06-29 21:44:32,142 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_2          | 2023-06-29 21:44:33,021 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:c97024849072 ip:172.24.0.15
datanode_2          | 2023-06-29 21:44:34,344 [main] INFO reflections.Reflections: Reflections took 964 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_2          | 2023-06-29 21:44:37,789 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_2          | 2023-06-29 21:44:38,192 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_2          | 2023-06-29 21:44:39,745 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_2          | 2023-06-29 21:44:39,882 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_2          | 2023-06-29 21:44:39,954 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_2          | 2023-06-29 21:44:39,955 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_2          | 2023-06-29 21:44:40,231 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:44:40,277 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:44:40,279 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_2          | 2023-06-29 21:44:40,312 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_2          | 2023-06-29 21:44:40,313 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_2          | 2023-06-29 21:44:40,316 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_2          | 2023-06-29 21:44:40,556 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_2          | 2023-06-29 21:44:40,557 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_2          | 2023-06-29 21:44:50,796 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_2          | 2023-06-29 21:44:51,327 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_2          | 2023-06-29 21:44:51,654 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_2          | 2023-06-29 21:44:52,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:44:52,240 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_2          | 2023-06-29 21:44:52,244 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_2          | 2023-06-29 21:44:52,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_2          | 2023-06-29 21:44:52,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_2          | 2023-06-29 21:44:52,258 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_2          | 2023-06-29 21:44:52,259 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_2          | 2023-06-29 21:44:52,292 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:44:52,297 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_2          | 2023-06-29 21:44:52,308 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:44:52,617 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:44:52,758 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_2          | 2023-06-29 21:44:52,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_2          | 2023-06-29 21:44:55,760 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_2          | 2023-06-29 21:44:55,786 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_2          | 2023-06-29 21:44:55,868 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_2          | 2023-06-29 21:44:55,872 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:44:55,875 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:44:55,886 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:44:56,258 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_2          | 2023-06-29 21:44:56,662 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_2          | 2023-06-29 21:44:58,066 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:44:58,184 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_2          | 2023-06-29 21:44:58,397 [main] INFO util.log: Logging initialized @39363ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_2          | 2023-06-29 21:44:59,196 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_2          | 2023-06-29 21:44:59,219 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_2          | 2023-06-29 21:44:59,299 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_2          | 2023-06-29 21:44:59,319 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_2          | 2023-06-29 21:44:59,319 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_2          | 2023-06-29 21:44:59,328 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_2          | 2023-06-29 21:44:59,663 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_2          | 2023-06-29 21:44:59,677 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_2          | 2023-06-29 21:44:59,687 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_2          | 2023-06-29 21:44:59,950 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_2          | 2023-06-29 21:44:59,959 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_2          | 2023-06-29 21:44:59,963 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_2          | 2023-06-29 21:45:00,052 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d836c4a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_2          | 2023-06-29 21:45:00,066 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a36da5e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_2          | 2023-06-29 21:45:00,941 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5e1bfe66{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-16676039703708512677/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_2          | 2023-06-29 21:45:01,031 [main] INFO server.AbstractConnector: Started ServerConnector@677274e7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_2          | 2023-06-29 21:45:01,032 [main] INFO server.Server: Started @41998ms
datanode_2          | 2023-06-29 21:45:01,033 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_2          | 2023-06-29 21:45:01,034 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_2          | 2023-06-29 21:45:01,072 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_2          | 2023-06-29 21:45:01,406 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_2          | 2023-06-29 21:45:01,678 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_2          | 2023-06-29 21:45:01,714 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_2          | 2023-06-29 21:45:03,375 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_2          | 2023-06-29 21:45:03,384 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_2          | 2023-06-29 21:45:03,388 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_2          | 2023-06-29 21:45:03,466 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_2          | 2023-06-29 21:45:03,587 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_2          | 2023-06-29 21:45:04,368 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.2:9891
datanode_1          | 2023-06-29 21:45:04,949 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_1          | 2023-06-29 21:45:07,229 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:07,239 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:08,240 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:09,241 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:10,242 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:11,243 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:12,244 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:12,268 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From a0f9d6dccb28/172.24.0.7 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:40514 remote=recon/172.24.0.2:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:40514 remote=recon/172.24.0.2:9891]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-29 21:45:13,246 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:14,247 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:15,248 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_1          | 2023-06-29 21:45:20,259 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_1          | java.net.SocketTimeoutException: Call From a0f9d6dccb28/172.24.0.7 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:50706 remote=scm/172.24.0.11:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_1          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_1          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_1          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_1          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_1          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_1          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_1          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_1          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_1          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_1          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_1          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_1          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_1          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_1          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.7:50706 remote=scm/172.24.0.11:9861]
datanode_1          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_1          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_1          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_1          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_1          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_1          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_1          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_1          | 2023-06-29 21:45:22,191 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-396e5631-489e-4e50-8bf3-25e6edd950fd/container.db to cache
datanode_1          | 2023-06-29 21:45:22,191 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-396e5631-489e-4e50-8bf3-25e6edd950fd/container.db for volume DS-396e5631-489e-4e50-8bf3-25e6edd950fd
datanode_1          | 2023-06-29 21:45:22,232 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_1          | 2023-06-29 21:45:22,254 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_1          | 2023-06-29 21:45:22,644 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_1          | 2023-06-29 21:45:22,645 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 6922a681-9151-4f6a-8181-db1e5c68da07
datanode_1          | 2023-06-29 21:45:22,698 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.RaftServer: 6922a681-9151-4f6a-8181-db1e5c68da07: start RPC server
datanode_1          | 2023-06-29 21:45:22,705 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 6922a681-9151-4f6a-8181-db1e5c68da07: GrpcService started, listening on 9858
datanode_1          | 2023-06-29 21:45:22,707 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 6922a681-9151-4f6a-8181-db1e5c68da07: GrpcService started, listening on 9856
datanode_1          | 2023-06-29 21:45:22,709 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 6922a681-9151-4f6a-8181-db1e5c68da07: GrpcService started, listening on 9857
datanode_1          | 2023-06-29 21:45:22,724 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6922a681-9151-4f6a-8181-db1e5c68da07 is started using port 9858 for RATIS
datanode_1          | 2023-06-29 21:45:22,724 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6922a681-9151-4f6a-8181-db1e5c68da07 is started using port 9857 for RATIS_ADMIN
datanode_1          | 2023-06-29 21:45:22,725 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 6922a681-9151-4f6a-8181-db1e5c68da07 is started using port 9856 for RATIS_SERVER
datanode_1          | 2023-06-29 21:45:22,728 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-6922a681-9151-4f6a-8181-db1e5c68da07: Started
datanode_1          | 2023-06-29 21:45:22,794 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:45:27,331 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 6922a681-9151-4f6a-8181-db1e5c68da07: addNew group-30FA5CFF24D8:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] returns group-30FA5CFF24D8:java.util.concurrent.CompletableFuture@2839aefa[Not completed]
datanode_1          | 2023-06-29 21:45:27,612 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07: new RaftServerImpl for group-30FA5CFF24D8:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_1          | 2023-06-29 21:45:27,640 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_1          | 2023-06-29 21:45:27,649 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_1          | 2023-06-29 21:45:27,650 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_1          | 2023-06-29 21:45:27,655 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:45:27,658 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_1          | 2023-06-29 21:45:27,658 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:45:04,780 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_2          | 2023-06-29 21:45:06,922 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:06,923 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:07,797 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
datanode_2          | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:666)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:334)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:532)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.util.concurrent.TimeoutException
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	... 1 more
datanode_2          | 2023-06-29 21:45:07,923 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:07,926 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:08,927 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:09,928 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:10,930 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:11,931 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:12,932 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:12,968 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From c97024849072/172.24.0.15 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:38620 remote=recon/172.24.0.2:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:38620 remote=recon/172.24.0.2:9891]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_1          | 2023-06-29 21:45:27,769 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: ConfigurationManager, init=-1: peers:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_1          | 2023-06-29 21:45:27,807 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_1          | 2023-06-29 21:45:27,845 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_1          | 2023-06-29 21:45:27,872 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_1          | 2023-06-29 21:45:28,021 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_1          | 2023-06-29 21:45:28,080 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_1          | 2023-06-29 21:45:28,105 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_1          | 2023-06-29 21:45:28,116 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_1          | 2023-06-29 21:45:28,347 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_1          | 2023-06-29 21:45:28,457 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_1          | 2023-06-29 21:45:28,499 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_1          | 2023-06-29 21:45:28,500 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_1          | 2023-06-29 21:45:28,504 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_1          | 2023-06-29 21:45:28,505 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_1          | 2023-06-29 21:45:28,518 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_1          | 2023-06-29 21:45:28,524 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c1ab9456-2d2a-44b4-88b9-30fa5cff24d8 does not exist. Creating ...
datanode_1          | 2023-06-29 21:45:28,558 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c1ab9456-2d2a-44b4-88b9-30fa5cff24d8/in_use.lock acquired by nodename 7@a0f9d6dccb28
datanode_1          | 2023-06-29 21:45:28,592 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c1ab9456-2d2a-44b4-88b9-30fa5cff24d8 has been successfully formatted.
datanode_1          | 2023-06-29 21:45:28,678 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO ratis.ContainerStateMachine: group-30FA5CFF24D8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_1          | 2023-06-29 21:45:28,777 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_1          | 2023-06-29 21:45:28,906 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_1          | 2023-06-29 21:45:28,913 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:45:28,914 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_1          | 2023-06-29 21:45:28,916 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_1          | 2023-06-29 21:45:28,927 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:45:28,960 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_1          | 2023-06-29 21:45:28,966 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_1          | 2023-06-29 21:45:28,976 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:45:29,027 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c1ab9456-2d2a-44b4-88b9-30fa5cff24d8
datanode_1          | 2023-06-29 21:45:29,036 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_1          | 2023-06-29 21:45:29,037 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:45:29,043 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_1          | 2023-06-29 21:45:29,049 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_1          | 2023-06-29 21:45:29,051 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_1          | 2023-06-29 21:45:29,056 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_1          | 2023-06-29 21:45:29,056 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_1          | 2023-06-29 21:45:29,057 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_1          | 2023-06-29 21:45:29,082 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_1          | 2023-06-29 21:45:29,090 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_1          | 2023-06-29 21:45:29,148 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_1          | 2023-06-29 21:45:29,151 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_5          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_5          | 2023-06-29 21:44:30,308 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_5          | /************************************************************
datanode_5          | STARTUP_MSG: Starting HddsDatanodeService
datanode_5          | STARTUP_MSG:   host = 5d2a01c1f503/172.24.0.13
datanode_5          | STARTUP_MSG:   args = []
datanode_5          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_5          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_5          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_5          | STARTUP_MSG:   java = 11.0.19
datanode_3          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_3          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_3          | 2023-06-29 21:44:29,974 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_3          | /************************************************************
datanode_3          | STARTUP_MSG: Starting HddsDatanodeService
datanode_3          | STARTUP_MSG:   host = 07ec33f7692b/172.24.0.10
datanode_3          | STARTUP_MSG:   args = []
datanode_3          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_3          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_3          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_3          | STARTUP_MSG:   java = 11.0.19
datanode_3          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_3          | ************************************************************/
datanode_3          | 2023-06-29 21:44:30,076 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_3          | 2023-06-29 21:44:30,405 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_3          | 2023-06-29 21:44:31,099 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_3          | 2023-06-29 21:44:32,041 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_3          | 2023-06-29 21:44:32,041 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_3          | 2023-06-29 21:44:32,867 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:07ec33f7692b ip:172.24.0.10
datanode_3          | 2023-06-29 21:44:34,092 [main] INFO reflections.Reflections: Reflections took 839 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_3          | 2023-06-29 21:44:37,434 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_3          | 2023-06-29 21:44:37,830 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_3          | 2023-06-29 21:44:39,364 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_3          | 2023-06-29 21:44:39,617 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_3          | 2023-06-29 21:44:39,629 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_3          | 2023-06-29 21:44:39,656 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_3          | 2023-06-29 21:44:39,939 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:44:40,000 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:44:40,022 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_3          | 2023-06-29 21:44:40,080 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_3          | 2023-06-29 21:44:40,091 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_3          | 2023-06-29 21:44:40,092 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_1          | 2023-06-29 21:45:29,151 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_1          | 2023-06-29 21:45:29,174 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO segmented.SegmentedRaftLogWorker: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:45:29,174 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO segmented.SegmentedRaftLogWorker: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_1          | 2023-06-29 21:45:29,178 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: start as a follower, conf=-1: peers:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:45:29,178 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_1          | 2023-06-29 21:45:29,182 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO impl.RoleInfo: 6922a681-9151-4f6a-8181-db1e5c68da07: start 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState
datanode_1          | 2023-06-29 21:45:29,203 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-30FA5CFF24D8,id=6922a681-9151-4f6a-8181-db1e5c68da07
datanode_1          | 2023-06-29 21:45:29,228 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_1          | 2023-06-29 21:45:29,230 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_1          | 2023-06-29 21:45:29,232 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_1          | 2023-06-29 21:45:29,232 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_1          | 2023-06-29 21:45:29,233 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_1          | 2023-06-29 21:45:29,234 [6922a681-9151-4f6a-8181-db1e5c68da07-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_1          | 2023-06-29 21:45:29,342 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8
datanode_1          | 2023-06-29 21:45:29,344 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8.
datanode_1          | 2023-06-29 21:45:34,373 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO impl.FollowerState: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5188951810ns, electionTimeout:5137ms
datanode_1          | 2023-06-29 21:45:34,375 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO impl.RoleInfo: 6922a681-9151-4f6a-8181-db1e5c68da07: shutdown 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState
datanode_1          | 2023-06-29 21:45:34,380 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_1          | 2023-06-29 21:45:34,401 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_1          | 2023-06-29 21:45:34,402 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-FollowerState] INFO impl.RoleInfo: 6922a681-9151-4f6a-8181-db1e5c68da07: start 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1
datanode_1          | 2023-06-29 21:45:34,415 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.LeaderElection: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:45:34,420 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.LeaderElection: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_1          | 2023-06-29 21:45:34,429 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.LeaderElection: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:45:34,429 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.LeaderElection: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_1          | 2023-06-29 21:45:34,429 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.RoleInfo: 6922a681-9151-4f6a-8181-db1e5c68da07: shutdown 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1
datanode_1          | 2023-06-29 21:45:34,430 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_1          | 2023-06-29 21:45:34,434 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-30FA5CFF24D8 with new leaderId: 6922a681-9151-4f6a-8181-db1e5c68da07
datanode_1          | 2023-06-29 21:45:34,437 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: change Leader from null to 6922a681-9151-4f6a-8181-db1e5c68da07 at term 1 for becomeLeader, leader elected after 6409ms
datanode_1          | 2023-06-29 21:45:34,539 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_1          | 2023-06-29 21:45:34,580 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:45:34,586 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_1          | 2023-06-29 21:45:34,620 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_1          | 2023-06-29 21:45:34,629 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_1          | 2023-06-29 21:45:34,632 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_1          | 2023-06-29 21:45:34,706 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_1          | 2023-06-29 21:45:34,733 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_1          | 2023-06-29 21:45:34,748 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO impl.RoleInfo: 6922a681-9151-4f6a-8181-db1e5c68da07: start 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderStateImpl
datanode_1          | 2023-06-29 21:45:34,913 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker: Starting segment from index:0
datanode_1          | 2023-06-29 21:45:35,137 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-LeaderElection1] INFO server.RaftServer$Division: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8: set configuration 0: peers:[6922a681-9151-4f6a-8181-db1e5c68da07|rpc:172.24.0.7:9856|admin:172.24.0.7:9857|client:172.24.0.7:9858|dataStream:172.24.0.7:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_1          | 2023-06-29 21:45:35,231 [6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 6922a681-9151-4f6a-8181-db1e5c68da07@group-30FA5CFF24D8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c1ab9456-2d2a-44b4-88b9-30fa5cff24d8/current/log_inprogress_0
datanode_1          | 2023-06-29 21:46:22,795 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:47:22,795 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:48:22,796 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:49:22,797 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:50:22,797 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:51:22,797 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_1          | 2023-06-29 21:52:22,798 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
datanode_4          | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
datanode_4          | 2023-06-29 21:44:30,614 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
datanode_4          | /************************************************************
datanode_4          | STARTUP_MSG: Starting HddsDatanodeService
datanode_4          | STARTUP_MSG:   host = 79b5ac674f34/172.24.0.12
datanode_4          | STARTUP_MSG:   args = []
datanode_4          | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4          | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.4.0-SNAPSHOT.jar
datanode_4          | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
datanode_4          | STARTUP_MSG:   java = 11.0.19
datanode_5          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_5          | ************************************************************/
datanode_5          | 2023-06-29 21:44:30,361 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_5          | 2023-06-29 21:44:30,661 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_5          | 2023-06-29 21:44:31,447 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_5          | 2023-06-29 21:44:32,669 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_5          | 2023-06-29 21:44:32,670 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_5          | 2023-06-29 21:44:33,465 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:5d2a01c1f503 ip:172.24.0.13
datanode_5          | 2023-06-29 21:44:34,600 [main] INFO reflections.Reflections: Reflections took 777 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_5          | 2023-06-29 21:44:38,144 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_5          | 2023-06-29 21:44:38,601 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_5          | 2023-06-29 21:44:40,201 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_5          | 2023-06-29 21:44:40,398 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_5          | 2023-06-29 21:44:40,403 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_5          | 2023-06-29 21:44:40,456 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_5          | 2023-06-29 21:44:40,723 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_5          | 2023-06-29 21:44:40,774 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-29 21:44:40,799 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_5          | 2023-06-29 21:44:40,816 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_5          | 2023-06-29 21:44:40,816 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_5          | 2023-06-29 21:44:40,817 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_5          | 2023-06-29 21:44:40,976 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_5          | 2023-06-29 21:44:40,976 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_5          | 2023-06-29 21:44:51,724 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_5          | 2023-06-29 21:44:52,243 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_5          | 2023-06-29 21:44:52,620 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_5          | 2023-06-29 21:44:53,940 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-29 21:44:53,965 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_5          | 2023-06-29 21:44:54,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_5          | 2023-06-29 21:44:54,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_5          | 2023-06-29 21:44:54,044 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_5          | 2023-06-29 21:44:54,045 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_5          | 2023-06-29 21:44:54,045 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_5          | 2023-06-29 21:44:54,046 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:44:54,046 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_5          | 2023-06-29 21:44:54,080 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:44:54,203 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-29 21:44:54,225 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_5          | 2023-06-29 21:44:54,244 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_5          | 2023-06-29 21:44:56,930 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_5          | 2023-06-29 21:44:56,936 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_5          | 2023-06-29 21:44:56,952 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_5          | 2023-06-29 21:44:56,952 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:44:56,955 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-29 21:44:57,004 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-29 21:44:57,530 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_5          | 2023-06-29 21:44:57,968 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_5          | 2023-06-29 21:44:59,304 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_5          | 2023-06-29 21:44:59,407 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_5          | 2023-06-29 21:44:59,682 [main] INFO util.log: Logging initialized @40638ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_5          | 2023-06-29 21:45:00,339 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_5          | 2023-06-29 21:45:00,390 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_5          | 2023-06-29 21:45:00,454 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_5          | 2023-06-29 21:45:00,463 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_5          | 2023-06-29 21:45:00,475 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_5          | 2023-06-29 21:45:00,476 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_5          | 2023-06-29 21:45:00,727 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_5          | 2023-06-29 21:45:00,771 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_5          | 2023-06-29 21:45:00,797 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_5          | 2023-06-29 21:45:01,030 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_5          | 2023-06-29 21:45:01,044 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_5          | 2023-06-29 21:45:01,059 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_5          | 2023-06-29 21:45:01,208 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6859bbd4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_5          | 2023-06-29 21:45:01,212 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@62376bdd{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_5          | 2023-06-29 21:45:02,252 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1a1dd8eb{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-15354515449562582703/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_5          | 2023-06-29 21:45:02,316 [main] INFO server.AbstractConnector: Started ServerConnector@48d14ea0{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_5          | 2023-06-29 21:45:02,335 [main] INFO server.Server: Started @43272ms
datanode_5          | 2023-06-29 21:45:02,349 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_5          | 2023-06-29 21:45:02,349 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_5          | 2023-06-29 21:45:02,353 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_5          | 2023-06-29 21:45:02,712 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_5          | 2023-06-29 21:45:03,255 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_5          | 2023-06-29 21:45:03,308 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_5          | 2023-06-29 21:45:04,646 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_5          | 2023-06-29 21:45:04,646 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_5          | 2023-06-29 21:45:04,663 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_5          | 2023-06-29 21:45:04,710 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_5          | 2023-06-29 21:45:04,832 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_5          | 2023-06-29 21:45:05,205 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.2:9891
datanode_5          | 2023-06-29 21:45:05,429 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_5          | 2023-06-29 21:45:08,009 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-29 21:45:13,932 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:14,934 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:15,935 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_2          | 2023-06-29 21:45:20,947 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | java.net.SocketTimeoutException: Call From c97024849072/172.24.0.15 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:46298 remote=scm/172.24.0.11:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_2          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_2          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_2          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_2          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_2          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_2          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_2          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_2          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_2          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_2          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_2          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_2          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_2          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_2          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.15:46298 remote=scm/172.24.0.11:9861]
datanode_2          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_2          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_2          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_2          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_2          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_2          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_2          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_2          | 2023-06-29 21:45:21,829 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-1446b67b-4965-41a2-9c5f-392144777efc/container.db to cache
datanode_2          | 2023-06-29 21:45:21,832 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-1446b67b-4965-41a2-9c5f-392144777efc/container.db for volume DS-1446b67b-4965-41a2-9c5f-392144777efc
datanode_2          | 2023-06-29 21:45:21,868 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_2          | 2023-06-29 21:45:21,897 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_2          | 2023-06-29 21:45:22,263 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_2          | 2023-06-29 21:45:22,263 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:22,389 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.RaftServer: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start RPC server
datanode_2          | 2023-06-29 21:45:22,408 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 3ea36174-d662-4e6c-bd1c-d614f11906cb: GrpcService started, listening on 9858
datanode_2          | 2023-06-29 21:45:22,413 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 3ea36174-d662-4e6c-bd1c-d614f11906cb: GrpcService started, listening on 9856
datanode_2          | 2023-06-29 21:45:22,416 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 3ea36174-d662-4e6c-bd1c-d614f11906cb: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:44:40,329 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_3          | 2023-06-29 21:44:40,332 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_3          | 2023-06-29 21:44:50,975 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_3          | 2023-06-29 21:44:51,884 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_3          | 2023-06-29 21:44:52,291 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_3          | 2023-06-29 21:44:52,919 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:44:52,924 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_3          | 2023-06-29 21:44:52,929 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_3          | 2023-06-29 21:44:52,930 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_3          | 2023-06-29 21:44:52,930 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_3          | 2023-06-29 21:44:52,930 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_3          | 2023-06-29 21:44:52,930 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_3          | 2023-06-29 21:44:52,935 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:44:52,936 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_3          | 2023-06-29 21:44:52,945 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:44:53,199 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_3          | 2023-06-29 21:44:53,330 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_3          | 2023-06-29 21:44:53,360 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_3          | 2023-06-29 21:44:56,169 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_3          | 2023-06-29 21:44:56,197 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_3          | 2023-06-29 21:44:56,221 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_3          | 2023-06-29 21:44:56,228 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:44:56,229 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_3          | 2023-06-29 21:44:56,281 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:44:56,953 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_3          | 2023-06-29 21:44:57,399 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_3          | 2023-06-29 21:44:58,715 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:44:58,782 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_3          | 2023-06-29 21:44:58,998 [main] INFO util.log: Logging initialized @40441ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_3          | 2023-06-29 21:44:59,818 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_3          | 2023-06-29 21:44:59,850 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_3          | 2023-06-29 21:44:59,927 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_3          | 2023-06-29 21:44:59,941 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_3          | 2023-06-29 21:44:59,966 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_3          | 2023-06-29 21:44:59,966 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_3          | 2023-06-29 21:45:00,251 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_3          | 2023-06-29 21:45:00,304 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_3          | 2023-06-29 21:45:00,333 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_3          | 2023-06-29 21:45:00,616 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_3          | 2023-06-29 21:45:00,616 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_3          | 2023-06-29 21:45:00,626 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_3          | 2023-06-29 21:45:00,745 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a225014{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_3          | 2023-06-29 21:45:00,747 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b8a063d{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_3          | 2023-06-29 21:45:01,610 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4def900a{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-15968320130303922581/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_3          | 2023-06-29 21:45:01,696 [main] INFO server.AbstractConnector: Started ServerConnector@21251e43{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_3          | 2023-06-29 21:45:01,703 [main] INFO server.Server: Started @43139ms
datanode_3          | 2023-06-29 21:45:01,707 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_3          | 2023-06-29 21:45:01,708 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_3          | 2023-06-29 21:45:01,718 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_3          | 2023-06-29 21:45:01,990 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_3          | 2023-06-29 21:45:02,183 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_3          | 2023-06-29 21:45:02,338 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_3          | 2023-06-29 21:45:04,153 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_3          | 2023-06-29 21:45:04,161 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_3          | 2023-06-29 21:45:04,162 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_3          | 2023-06-29 21:45:04,200 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_3          | 2023-06-29 21:45:04,265 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_3          | 2023-06-29 21:45:04,880 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.2:9891
datanode_2          | 2023-06-29 21:45:22,433 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3ea36174-d662-4e6c-bd1c-d614f11906cb is started using port 9858 for RATIS
datanode_2          | 2023-06-29 21:45:22,434 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3ea36174-d662-4e6c-bd1c-d614f11906cb is started using port 9857 for RATIS_ADMIN
datanode_2          | 2023-06-29 21:45:22,434 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3ea36174-d662-4e6c-bd1c-d614f11906cb is started using port 9856 for RATIS_SERVER
datanode_2          | 2023-06-29 21:45:22,436 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3ea36174-d662-4e6c-bd1c-d614f11906cb: Started
datanode_2          | 2023-06-29 21:45:22,536 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:45:27,028 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3ea36174-d662-4e6c-bd1c-d614f11906cb: addNew group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-EECA45441CF9:java.util.concurrent.CompletableFuture@4f2370bb[Not completed]
datanode_2          | 2023-06-29 21:45:27,154 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb: new RaftServerImpl for group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:45:27,163 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:45:27,166 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:45:27,168 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:45:27,171 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:27,172 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:45:27,174 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:45:27,205 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:45:27,214 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:45:27,229 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:45:27,237 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:45:27,309 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:27,325 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:27,344 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:45:27,344 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:45:27,539 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:45:27,890 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:45:27,962 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:45:27,963 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:45:28,040 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:45:28,041 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:45:28,048 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:45:28,048 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 does not exist. Creating ...
datanode_2          | 2023-06-29 21:45:28,059 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/in_use.lock acquired by nodename 6@c97024849072
datanode_2          | 2023-06-29 21:45:28,164 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 has been successfully formatted.
datanode_2          | 2023-06-29 21:45:28,283 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO ratis.ContainerStateMachine: group-EECA45441CF9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:45:28,353 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:45:28,465 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:45:28,465 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:45:05,122 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_3          | 2023-06-29 21:45:07,606 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:07,615 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:08,615 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:09,617 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:10,618 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:11,618 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:12,619 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:12,647 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 07ec33f7692b/172.24.0.10 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:35816 remote=recon/172.24.0.2:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:35816 remote=recon/172.24.0.2:9891]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-29 21:45:13,620 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:14,621 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:15,621 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_3          | 2023-06-29 21:45:20,637 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_3          | java.net.SocketTimeoutException: Call From 07ec33f7692b/172.24.0.10 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:36396 remote=scm/172.24.0.11:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_3          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_3          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 2023-06-29 21:45:08,011 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:09,010 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:10,012 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:11,012 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:12,013 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:13,014 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:13,034 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 5d2a01c1f503/172.24.0.13 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:42286 remote=recon/172.24.0.2:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:42286 remote=recon/172.24.0.2:9891]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-06-29 21:45:14,015 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:15,016 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_5          | 2023-06-29 21:45:20,027 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_5          | java.net.SocketTimeoutException: Call From 5d2a01c1f503/172.24.0.13 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:47166 remote=scm/172.24.0.11:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_5          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_5          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_5          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_5          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
datanode_4          | ************************************************************/
datanode_4          | 2023-06-29 21:44:30,709 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
datanode_4          | 2023-06-29 21:44:30,993 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
datanode_4          | 2023-06-29 21:44:31,795 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-06-29 21:44:32,928 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-06-29 21:44:32,928 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
datanode_4          | 2023-06-29 21:44:33,736 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:79b5ac674f34 ip:172.24.0.12
datanode_4          | 2023-06-29 21:44:34,921 [main] INFO reflections.Reflections: Reflections took 839 ms to scan 2 urls, producing 107 keys and 231 values 
datanode_4          | 2023-06-29 21:44:38,360 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-06-29 21:44:38,750 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 2
datanode_4          | 2023-06-29 21:44:40,316 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
datanode_4          | 2023-06-29 21:44:40,544 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
datanode_4          | 2023-06-29 21:44:40,550 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
datanode_4          | 2023-06-29 21:44:40,570 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
datanode_4          | 2023-06-29 21:44:40,867 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
datanode_4          | 2023-06-29 21:44:40,907 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-29 21:44:40,941 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
datanode_4          | 2023-06-29 21:44:40,988 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
datanode_4          | 2023-06-29 21:44:40,988 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
datanode_4          | 2023-06-29 21:44:40,989 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
datanode_3          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_3          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_3          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_3          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_3          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_3          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_3          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_3          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_3          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_3          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_3          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_3          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.10:36396 remote=scm/172.24.0.11:9861]
datanode_3          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_3          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_3          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_3          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_3          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_3          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_3          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_3          | 2023-06-29 21:45:22,204 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-5d874ca8-d30d-4690-9abb-9e28582622f9/container.db to cache
datanode_3          | 2023-06-29 21:45:22,206 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-5d874ca8-d30d-4690-9abb-9e28582622f9/container.db for volume DS-5d874ca8-d30d-4690-9abb-9e28582622f9
datanode_3          | 2023-06-29 21:45:22,241 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_3          | 2023-06-29 21:45:22,259 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_3          | 2023-06-29 21:45:22,578 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_3          | 2023-06-29 21:45:22,578 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis c8dd7f08-3522-46ce-9dac-414b9693c03d
datanode_3          | 2023-06-29 21:45:22,685 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.RaftServer: c8dd7f08-3522-46ce-9dac-414b9693c03d: start RPC server
datanode_3          | 2023-06-29 21:45:22,725 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: c8dd7f08-3522-46ce-9dac-414b9693c03d: GrpcService started, listening on 9858
datanode_3          | 2023-06-29 21:45:22,730 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: c8dd7f08-3522-46ce-9dac-414b9693c03d: GrpcService started, listening on 9856
datanode_3          | 2023-06-29 21:45:22,736 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: c8dd7f08-3522-46ce-9dac-414b9693c03d: GrpcService started, listening on 9857
datanode_3          | 2023-06-29 21:45:22,749 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c8dd7f08-3522-46ce-9dac-414b9693c03d is started using port 9858 for RATIS
datanode_3          | 2023-06-29 21:45:22,750 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c8dd7f08-3522-46ce-9dac-414b9693c03d is started using port 9857 for RATIS_ADMIN
datanode_3          | 2023-06-29 21:45:22,750 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis c8dd7f08-3522-46ce-9dac-414b9693c03d is started using port 9856 for RATIS_SERVER
datanode_3          | 2023-06-29 21:45:22,750 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-c8dd7f08-3522-46ce-9dac-414b9693c03d: Started
datanode_3          | 2023-06-29 21:45:22,827 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:45:28,094 [PipelineCommandHandlerThread-0] INFO server.RaftServer: c8dd7f08-3522-46ce-9dac-414b9693c03d: addNew group-D93FDE1C1C74:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] returns group-D93FDE1C1C74:java.util.concurrent.CompletableFuture@295956e4[Not completed]
datanode_3          | 2023-06-29 21:45:28,281 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d: new RaftServerImpl for group-D93FDE1C1C74:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_3          | 2023-06-29 21:45:28,288 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_3          | 2023-06-29 21:45:28,292 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_3          | 2023-06-29 21:45:28,292 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_3          | 2023-06-29 21:45:28,297 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:45:28,299 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-29 21:44:41,168 [Thread-7] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
datanode_4          | 2023-06-29 21:44:41,169 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
datanode_4          | 2023-06-29 21:44:51,018 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
datanode_4          | 2023-06-29 21:44:51,710 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
datanode_4          | 2023-06-29 21:44:51,994 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-06-29 21:44:52,484 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-29 21:44:52,784 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
datanode_4          | 2023-06-29 21:44:52,812 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
datanode_4          | 2023-06-29 21:44:52,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
datanode_4          | 2023-06-29 21:44:52,813 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
datanode_4          | 2023-06-29 21:44:52,822 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
datanode_4          | 2023-06-29 21:44:52,844 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
datanode_4          | 2023-06-29 21:44:52,860 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:44:52,861 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
datanode_4          | 2023-06-29 21:44:52,862 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-29 21:44:53,033 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-06-29 21:44:53,097 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
datanode_4          | 2023-06-29 21:44:53,154 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
datanode_4          | 2023-06-29 21:44:55,588 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
datanode_4          | 2023-06-29 21:44:55,590 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
datanode_4          | 2023-06-29 21:44:55,590 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
datanode_4          | 2023-06-29 21:44:55,591 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:44:55,591 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-29 21:44:55,642 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-29 21:44:56,214 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
datanode_4          | 2023-06-29 21:44:56,594 [main] INFO replication.ReplicationSupervisor: Node state updated to IN_SERVICE, scaling executor pool size to 10
datanode_4          | 2023-06-29 21:44:57,700 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
datanode_4          | 2023-06-29 21:44:57,806 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
datanode_4          | 2023-06-29 21:44:58,003 [main] INFO util.log: Logging initialized @38880ms to org.eclipse.jetty.util.log.Slf4jLog
datanode_4          | 2023-06-29 21:44:58,868 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
datanode_4          | 2023-06-29 21:44:58,910 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
datanode_4          | 2023-06-29 21:44:58,953 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
datanode_4          | 2023-06-29 21:44:58,980 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
datanode_4          | 2023-06-29 21:44:58,980 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
datanode_4          | 2023-06-29 21:44:58,980 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
datanode_4          | 2023-06-29 21:44:59,293 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode uses base directory /data/metadata/webserver
datanode_4          | 2023-06-29 21:44:59,332 [main] INFO http.HttpServer2: Jetty bound to port 9882
datanode_4          | 2023-06-29 21:44:59,333 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
datanode_4          | 2023-06-29 21:44:59,594 [main] INFO server.session: DefaultSessionIdManager workerName=node0
datanode_4          | 2023-06-29 21:44:59,599 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-06-29 21:44:59,621 [main] INFO server.session: node0 Scavenging every 660000ms
datanode_4          | 2023-06-29 21:44:59,707 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7d836c4a{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
datanode_4          | 2023-06-29 21:44:59,724 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3a36da5e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-06-29 21:45:00,582 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5e1bfe66{hddsDatanode,/,file:///data/metadata/webserver/jetty-0_0_0_0-9882-hdds-container-service-1_4_0-SNAPSHOT_jar-_-any-4812226409746515793/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar!/webapps/hddsDatanode}
datanode_4          | 2023-06-29 21:45:00,671 [main] INFO server.AbstractConnector: Started ServerConnector@677274e7{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
datanode_4          | 2023-06-29 21:45:00,673 [main] INFO server.Server: Started @41550ms
datanode_4          | 2023-06-29 21:45:00,688 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
datanode_4          | 2023-06-29 21:45:00,688 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-06-29 21:45:00,695 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
datanode_4          | 2023-06-29 21:45:00,975 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 100, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-06-29 21:45:01,192 [main] INFO ipc.Server: Listener at 0.0.0.0:9864
datanode_4          | 2023-06-29 21:45:01,236 [Socket Reader #1 for port 9864] INFO ipc.Server: Starting Socket Reader #1 for port 9864
datanode_4          | 2023-06-29 21:45:02,776 [main] INFO ozone.HddsDatanodeService: Datanode start with admins: [hadoop]
datanode_4          | 2023-06-29 21:45:02,776 [main] INFO ozone.HddsDatanodeClientProtocolServer: RPC server for Client /0.0.0.0:9864
datanode_4          | 2023-06-29 21:45:02,776 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
datanode_4          | 2023-06-29 21:45:02,800 [IPC Server listener on 9864] INFO ipc.Server: IPC Server listener on 9864: starting
datanode_4          | 2023-06-29 21:45:02,834 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
datanode_4          | 2023-06-29 21:45:03,788 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/172.24.0.2:9891
datanode_4          | 2023-06-29 21:45:04,209 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
datanode_4          | 2023-06-29 21:45:06,217 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:06,224 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:07,218 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:07,225 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/172.24.0.2:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:08,219 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:09,220 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:10,221 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:11,222 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:12,223 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:12,278 [EndpointStateMachine task thread for recon/172.24.0.2:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
datanode_4          | java.net.SocketTimeoutException: Call From 79b5ac674f34/172.24.0.12 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:35550 remote=recon/172.24.0.2:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy40.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:35550 remote=recon/172.24.0.2:9891]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_4          | 2023-06-29 21:45:13,224 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:14,225 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:15,226 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm/172.24.0.11:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
datanode_4          | 2023-06-29 21:45:20,241 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm:9861 for past 0 seconds.
datanode_2          | 2023-06-29 21:45:28,478 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:45:28,485 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:45:28,510 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:28,545 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:45:28,545 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:45:28,545 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:28,585 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
datanode_2          | 2023-06-29 21:45:28,586 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:45:28,586 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:28,587 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:28,587 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:45:28,587 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:45:28,588 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:45:28,588 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:28,588 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:45:28,667 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:28,672 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:28,765 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:28,770 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:28,770 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:28,817 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:28,817 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:28,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:28,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:45:28,840 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState
datanode_2          | 2023-06-29 21:45:28,857 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:28,858 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:28,878 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EECA45441CF9,id=3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:28,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:28,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:45:28,887 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:45:28,888 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:45:28,970 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
datanode_2          | 2023-06-29 21:45:33,151 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9.
datanode_2          | 2023-06-29 21:45:33,152 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3ea36174-d662-4e6c-bd1c-d614f11906cb: addNew group-FF44D979937E:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-FF44D979937E:java.util.concurrent.CompletableFuture@3612527e[Not completed]
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb: new RaftServerImpl for group-FF44D979937E:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:45:33,158 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:45:33,161 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: ConfigurationManager, init=-1: peers:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:45:33,161 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_2          | 2023-06-29 21:45:33,161 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:45:33,162 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:45:33,162 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:33,163 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:33,163 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:45:33,163 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:45:33,163 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:45:33,170 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:45:33,171 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:45:33,172 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:45:33,172 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:45:33,172 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:45:33,172 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:45:33,178 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/caf603ee-435e-47a0-8b37-ff44d979937e does not exist. Creating ...
datanode_2          | 2023-06-29 21:45:33,181 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/caf603ee-435e-47a0-8b37-ff44d979937e/in_use.lock acquired by nodename 6@c97024849072
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/caf603ee-435e-47a0-8b37-ff44d979937e has been successfully formatted.
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO ratis.ContainerStateMachine: group-FF44D979937E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,184 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:45:33,185 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:45:33,185 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,216 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:45:33,216 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:45:33,216 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,217 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/caf603ee-435e-47a0-8b37-ff44d979937e
datanode_2          | 2023-06-29 21:45:33,217 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:45:33,217 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:33,217 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,235 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:45:33,235 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:45:33,235 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:45:33,235 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_5          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_5          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_5          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_5          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_5          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_5          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_5          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_5          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_5          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.13:47166 remote=scm/172.24.0.11:9861]
datanode_5          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_5          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_5          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_5          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_5          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_5          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_5          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_5          | 2023-06-29 21:45:22,036 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-c19b342f-7f0e-4daf-8537-1710ece1c0da/container.db to cache
datanode_5          | 2023-06-29 21:45:22,036 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-c19b342f-7f0e-4daf-8537-1710ece1c0da/container.db for volume DS-c19b342f-7f0e-4daf-8537-1710ece1c0da
datanode_5          | 2023-06-29 21:45:22,061 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_5          | 2023-06-29 21:45:22,076 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_5          | 2023-06-29 21:45:22,387 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_5          | 2023-06-29 21:45:22,387 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:45:22,492 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.RaftServer: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start RPC server
datanode_5          | 2023-06-29 21:45:22,520 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: GrpcService started, listening on 9858
datanode_5          | 2023-06-29 21:45:22,523 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: GrpcService started, listening on 9856
datanode_5          | 2023-06-29 21:45:22,529 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: GrpcService started, listening on 9857
datanode_5          | 2023-06-29 21:45:22,544 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5ffc81fa-0a37-43e0-be73-58283f7df8c3 is started using port 9858 for RATIS
datanode_5          | 2023-06-29 21:45:22,545 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5ffc81fa-0a37-43e0-be73-58283f7df8c3 is started using port 9857 for RATIS_ADMIN
datanode_5          | 2023-06-29 21:45:22,546 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 5ffc81fa-0a37-43e0-be73-58283f7df8c3 is started using port 9856 for RATIS_SERVER
datanode_5          | 2023-06-29 21:45:22,547 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-5ffc81fa-0a37-43e0-be73-58283f7df8c3: Started
datanode_5          | 2023-06-29 21:45:22,614 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:45:31,640 [grpc-default-executor-1] INFO server.RaftServer: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: addNew group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-EECA45441CF9:java.util.concurrent.CompletableFuture@7dc7b49f[Not completed]
datanode_5          | 2023-06-29 21:45:31,796 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: new RaftServerImpl for group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-29 21:45:31,804 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-29 21:45:31,812 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-29 21:45:31,812 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-29 21:45:31,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:31,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-29 21:45:31,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-29 21:45:31,887 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-29 21:45:31,888 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-29 21:45:31,908 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-29 21:45:31,908 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-29 21:45:31,954 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:31,971 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-29 21:45:31,988 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-29 21:45:31,990 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-29 21:45:32,088 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-29 21:45:32,141 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:45:32,156 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-29 21:45:32,157 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-29 21:45:32,159 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-29 21:45:32,161 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-29 21:45:32,163 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-29 21:45:32,164 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 does not exist. Creating ...
datanode_5          | 2023-06-29 21:45:32,178 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/in_use.lock acquired by nodename 7@5d2a01c1f503
datanode_5          | 2023-06-29 21:45:32,196 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 has been successfully formatted.
datanode_5          | 2023-06-29 21:45:32,315 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO ratis.ContainerStateMachine: group-EECA45441CF9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-29 21:45:32,334 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-29 21:45:32,431 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-29 21:45:32,432 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:32,440 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-29 21:45:32,451 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-29 21:45:32,471 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:32,511 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-29 21:45:32,516 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-29 21:45:32,517 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:32,545 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
datanode_5          | 2023-06-29 21:45:32,550 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-29 21:45:32,551 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:45:32,552 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:32,557 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-29 21:45:32,558 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-29 21:45:32,562 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-29 21:45:32,563 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-29 21:45:32,564 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-29 21:45:32,594 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | java.net.SocketTimeoutException: Call From 79b5ac674f34/172.24.0.12 to scm:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:49348 remote=scm/172.24.0.11:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
datanode_4          | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
datanode_4          | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
datanode_4          | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:930)
datanode_4          | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:865)
datanode_4          | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1571)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1513)
datanode_4          | 	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:250)
datanode_4          | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:132)
datanode_4          | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
datanode_4          | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
datanode_4          | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
datanode_4          | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
datanode_4          | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
datanode_4          | 	at java.base/java.lang.Thread.run(Thread.java:829)
datanode_4          | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/172.24.0.12:49348 remote=scm/172.24.0.11:9861]
datanode_4          | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
datanode_4          | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
datanode_4          | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
datanode_4          | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:501)
datanode_4          | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
datanode_4          | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1187)
datanode_4          | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1078)
datanode_4          | 2023-06-29 21:45:21,953 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-c7155683-9a04-4095-9728-9badfb33524e/container.db to cache
datanode_4          | 2023-06-29 21:45:21,953 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-2aa45680-139b-4449-b07e-596b1957bdef/DS-c7155683-9a04-4095-9728-9badfb33524e/container.db for volume DS-c7155683-9a04-4095-9728-9badfb33524e
datanode_4          | 2023-06-29 21:45:21,964 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
datanode_4          | 2023-06-29 21:45:21,978 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Scheduled background container scanners and the on-demand container scanner have been disabled.
datanode_4          | 2023-06-29 21:45:22,260 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO replication.ReplicationServer: ReplicationServer is started using port 9886
datanode_4          | 2023-06-29 21:45:22,261 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_4          | 2023-06-29 21:45:22,427 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.RaftServer: 1fef6398-5e0e-4531-8b14-70177bcfa866: start RPC server
datanode_4          | 2023-06-29 21:45:22,438 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 1fef6398-5e0e-4531-8b14-70177bcfa866: GrpcService started, listening on 9858
datanode_4          | 2023-06-29 21:45:22,445 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 1fef6398-5e0e-4531-8b14-70177bcfa866: GrpcService started, listening on 9856
datanode_4          | 2023-06-29 21:45:22,450 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO server.GrpcService: 1fef6398-5e0e-4531-8b14-70177bcfa866: GrpcService started, listening on 9857
datanode_4          | 2023-06-29 21:45:22,468 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1fef6398-5e0e-4531-8b14-70177bcfa866 is started using port 9858 for RATIS
datanode_4          | 2023-06-29 21:45:22,468 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1fef6398-5e0e-4531-8b14-70177bcfa866 is started using port 9857 for RATIS_ADMIN
datanode_4          | 2023-06-29 21:45:22,469 [EndpointStateMachine task thread for scm/172.24.0.11:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 1fef6398-5e0e-4531-8b14-70177bcfa866 is started using port 9856 for RATIS_SERVER
datanode_4          | 2023-06-29 21:45:22,471 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1fef6398-5e0e-4531-8b14-70177bcfa866: Started
datanode_4          | 2023-06-29 21:45:22,556 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-29 21:45:26,292 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1fef6398-5e0e-4531-8b14-70177bcfa866: addNew group-510E705E518C:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] returns group-510E705E518C:java.util.concurrent.CompletableFuture@36b1bd3d[Not completed]
datanode_4          | 2023-06-29 21:45:26,400 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866: new RaftServerImpl for group-510E705E518C:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-29 21:45:26,402 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-29 21:45:26,403 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-29 21:45:32,596 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:32,698 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:32,699 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:32,700 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:32,785 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-29 21:45:32,785 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-29 21:45:32,788 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:32,790 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-29 21:45:32,801 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState
datanode_5          | 2023-06-29 21:45:32,810 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EECA45441CF9,id=5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:45:32,812 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:32,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-29 21:45:32,814 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-29 21:45:32,817 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-29 21:45:32,822 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-29 21:45:32,833 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-29 21:45:34,045 [grpc-default-executor-0] INFO server.RaftServer: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: addNew group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-B9087FDFF180:java.util.concurrent.CompletableFuture@2cc2494f[Not completed]
datanode_5          | 2023-06-29 21:45:34,046 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: new RaftServerImpl for group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-29 21:45:34,047 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-29 21:45:34,048 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-29 21:45:34,048 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-29 21:45:34,048 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-29 21:45:34,048 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:34,048 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-29 21:45:28,301 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_3          | 2023-06-29 21:45:28,357 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: ConfigurationManager, init=-1: peers:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_3          | 2023-06-29 21:45:28,378 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_3          | 2023-06-29 21:45:28,411 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_3          | 2023-06-29 21:45:28,417 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_3          | 2023-06-29 21:45:28,510 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_3          | 2023-06-29 21:45:28,538 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_3          | 2023-06-29 21:45:28,566 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_3          | 2023-06-29 21:45:28,566 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_3          | 2023-06-29 21:45:28,685 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_3          | 2023-06-29 21:45:28,922 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_3          | 2023-06-29 21:45:28,949 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_3          | 2023-06-29 21:45:28,949 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_3          | 2023-06-29 21:45:28,950 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_3          | 2023-06-29 21:45:28,950 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_3          | 2023-06-29 21:45:28,953 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_3          | 2023-06-29 21:45:28,953 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8719d141-45c6-439f-90fe-d93fde1c1c74 does not exist. Creating ...
datanode_3          | 2023-06-29 21:45:28,986 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8719d141-45c6-439f-90fe-d93fde1c1c74/in_use.lock acquired by nodename 7@07ec33f7692b
datanode_3          | 2023-06-29 21:45:29,039 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8719d141-45c6-439f-90fe-d93fde1c1c74 has been successfully formatted.
datanode_3          | 2023-06-29 21:45:29,147 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO ratis.ContainerStateMachine: group-D93FDE1C1C74: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_3          | 2023-06-29 21:45:29,182 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_3          | 2023-06-29 21:45:29,240 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_3          | 2023-06-29 21:45:29,272 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:45:29,274 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_3          | 2023-06-29 21:45:29,280 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_3          | 2023-06-29 21:45:29,296 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:45:29,325 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_3          | 2023-06-29 21:45:29,332 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_3          | 2023-06-29 21:45:29,333 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:45:29,350 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO segmented.SegmentedRaftLogWorker: new c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8719d141-45c6-439f-90fe-d93fde1c1c74
datanode_3          | 2023-06-29 21:45:29,362 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_3          | 2023-06-29 21:45:29,363 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:45:29,371 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_3          | 2023-06-29 21:45:29,372 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_3          | 2023-06-29 21:45:29,373 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_3          | 2023-06-29 21:45:29,376 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_3          | 2023-06-29 21:45:29,379 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_3          | 2023-06-29 21:45:29,380 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_3          | 2023-06-29 21:45:29,437 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_3          | 2023-06-29 21:45:29,438 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_3          | 2023-06-29 21:45:29,508 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:45:29,511 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_3          | 2023-06-29 21:45:29,514 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_3          | 2023-06-29 21:45:29,544 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO segmented.SegmentedRaftLogWorker: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:45:29,544 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO segmented.SegmentedRaftLogWorker: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_3          | 2023-06-29 21:45:29,548 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: start as a follower, conf=-1: peers:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:45:29,549 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_3          | 2023-06-29 21:45:29,561 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO impl.RoleInfo: c8dd7f08-3522-46ce-9dac-414b9693c03d: start c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState
datanode_3          | 2023-06-29 21:45:29,562 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_3          | 2023-06-29 21:45:29,562 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_3          | 2023-06-29 21:45:29,575 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D93FDE1C1C74,id=c8dd7f08-3522-46ce-9dac-414b9693c03d
datanode_3          | 2023-06-29 21:45:29,577 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_3          | 2023-06-29 21:45:29,578 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_3          | 2023-06-29 21:45:29,578 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_3          | 2023-06-29 21:45:29,578 [c8dd7f08-3522-46ce-9dac-414b9693c03d-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_3          | 2023-06-29 21:45:29,649 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74
datanode_3          | 2023-06-29 21:45:29,653 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74.
datanode_3          | 2023-06-29 21:45:34,764 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO impl.FollowerState: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5202956569ns, electionTimeout:5194ms
datanode_3          | 2023-06-29 21:45:34,770 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO impl.RoleInfo: c8dd7f08-3522-46ce-9dac-414b9693c03d: shutdown c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState
datanode_3          | 2023-06-29 21:45:34,780 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_3          | 2023-06-29 21:45:34,798 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_3          | 2023-06-29 21:45:34,799 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-FollowerState] INFO impl.RoleInfo: c8dd7f08-3522-46ce-9dac-414b9693c03d: start c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1
datanode_3          | 2023-06-29 21:45:34,859 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.LeaderElection: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:45:34,871 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.LeaderElection: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_3          | 2023-06-29 21:45:34,896 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.LeaderElection: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:45:34,896 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.LeaderElection: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_3          | 2023-06-29 21:45:34,898 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.RoleInfo: c8dd7f08-3522-46ce-9dac-414b9693c03d: shutdown c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1
datanode_3          | 2023-06-29 21:45:34,899 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_3          | 2023-06-29 21:45:34,901 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-D93FDE1C1C74 with new leaderId: c8dd7f08-3522-46ce-9dac-414b9693c03d
datanode_3          | 2023-06-29 21:45:34,907 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: change Leader from null to c8dd7f08-3522-46ce-9dac-414b9693c03d at term 1 for becomeLeader, leader elected after 6392ms
datanode_3          | 2023-06-29 21:45:34,988 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_3          | 2023-06-29 21:45:35,035 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:45:35,038 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_3          | 2023-06-29 21:45:35,074 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_3          | 2023-06-29 21:45:35,076 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_3          | 2023-06-29 21:45:35,078 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_3          | 2023-06-29 21:45:35,150 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_3          | 2023-06-29 21:45:35,160 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_3          | 2023-06-29 21:45:35,181 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO impl.RoleInfo: c8dd7f08-3522-46ce-9dac-414b9693c03d: start c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderStateImpl
datanode_3          | 2023-06-29 21:45:35,318 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker: Starting segment from index:0
datanode_3          | 2023-06-29 21:45:35,635 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-LeaderElection1] INFO server.RaftServer$Division: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74: set configuration 0: peers:[c8dd7f08-3522-46ce-9dac-414b9693c03d|rpc:172.24.0.10:9856|admin:172.24.0.10:9857|client:172.24.0.10:9858|dataStream:172.24.0.10:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_3          | 2023-06-29 21:45:35,809 [c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: c8dd7f08-3522-46ce-9dac-414b9693c03d@group-D93FDE1C1C74-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8719d141-45c6-439f-90fe-d93fde1c1c74/current/log_inprogress_0
datanode_3          | 2023-06-29 21:46:22,827 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:47:22,828 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:48:22,828 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:49:22,829 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:50:22,830 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:51:22,830 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_3          | 2023-06-29 21:52:22,831 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:45:33,235 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:45:33,236 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,255 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,784 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3ea36174-d662-4e6c-bd1c-d614f11906cb: Detected pause in JVM or host machine approximately 0.234s with 0.523s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=92ms
datanode_2          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=431ms
datanode_2          | 2023-06-29 21:45:33,796 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:33,796 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:33,796 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:33,798 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:33,798 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:33,799 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: start as a follower, conf=-1: peers:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:33,803 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:45:33,803 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState
datanode_2          | 2023-06-29 21:45:33,819 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FF44D979937E,id=3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:33,819 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:33,819 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:45:33,819 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:45:33,819 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:45:33,828 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:33,830 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:33,834 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e
datanode_2          | 2023-06-29 21:45:33,835 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e.
datanode_2          | 2023-06-29 21:45:33,835 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 3ea36174-d662-4e6c-bd1c-d614f11906cb: addNew group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-B9087FDFF180:java.util.concurrent.CompletableFuture@161a1576[Not completed]
datanode_2          | 2023-06-29 21:45:33,837 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb: new RaftServerImpl for group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_2          | 2023-06-29 21:45:33,839 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_2          | 2023-06-29 21:45:33,840 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_2          | 2023-06-29 21:45:33,840 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-29 21:45:34,049 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-29 21:45:34,049 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-29 21:45:34,049 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-29 21:45:34,050 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-29 21:45:34,051 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 does not exist. Creating ...
datanode_5          | 2023-06-29 21:45:34,053 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/in_use.lock acquired by nodename 7@5d2a01c1f503
datanode_5          | 2023-06-29 21:45:34,056 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 has been successfully formatted.
datanode_5          | 2023-06-29 21:45:34,058 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO ratis.ContainerStateMachine: group-B9087FDFF180: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-29 21:45:34,063 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-29 21:45:34,065 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-29 21:45:34,065 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:34,065 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-29 21:45:34,065 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-29 21:45:34,066 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:34,071 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-29 21:45:34,071 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-29 21:45:34,071 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:34,071 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180
datanode_5          | 2023-06-29 21:45:34,073 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-29 21:45:34,073 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:45:34,074 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:34,074 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-29 21:45:34,074 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-29 21:45:34,074 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-29 21:45:34,075 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-29 21:45:34,075 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-29 21:45:34,076 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:34,078 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:34,667 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-5ffc81fa-0a37-43e0-be73-58283f7df8c3: Detected pause in JVM or host machine approximately 0.583s with 0.572s GC time.
datanode_5          | GC pool 'ParNew' had collection(s): count=1 time=53ms
datanode_5          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=519ms
datanode_5          | 2023-06-29 21:45:34,690 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:34,691 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:34,691 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:34,693 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-29 21:45:34,695 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_2          | 2023-06-29 21:45:33,842 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_2          | 2023-06-29 21:45:33,848 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_2          | 2023-06-29 21:45:33,850 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:45:33,850 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:45:33,851 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_2          | 2023-06-29 21:45:33,851 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_2          | 2023-06-29 21:45:33,851 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_2          | 2023-06-29 21:45:33,851 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_2          | 2023-06-29 21:45:33,852 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 does not exist. Creating ...
datanode_2          | 2023-06-29 21:45:33,854 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/in_use.lock acquired by nodename 6@c97024849072
datanode_2          | 2023-06-29 21:45:33,857 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 has been successfully formatted.
datanode_2          | 2023-06-29 21:45:33,857 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO ratis.ContainerStateMachine: group-B9087FDFF180: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_2          | 2023-06-29 21:45:33,857 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_2          | 2023-06-29 21:45:33,858 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_2          | 2023-06-29 21:45:33,858 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,873 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_2          | 2023-06-29 21:45:33,875 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_2          | 2023-06-29 21:45:33,876 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,881 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_2          | 2023-06-29 21:45:33,881 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_2          | 2023-06-29 21:45:33,882 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,883 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180
datanode_2          | 2023-06-29 21:45:33,883 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_2          | 2023-06-29 21:45:33,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:33,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_2          | 2023-06-29 21:45:33,884 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_2          | 2023-06-29 21:45:33,885 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_2          | 2023-06-29 21:45:33,885 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:33,885 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_2          | 2023-06-29 21:45:33,886 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_2          | 2023-06-29 21:45:33,889 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:33,949 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:33,951 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_2          | 2023-06-29 21:45:33,951 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:33,951 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:33,951 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_2          | 2023-06-29 21:45:33,952 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:33,953 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_2          | 2023-06-29 21:45:33,953 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState
datanode_2          | 2023-06-29 21:45:33,954 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B9087FDFF180,id=3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:33,955 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_2          | 2023-06-29 21:45:33,955 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_2          | 2023-06-29 21:45:33,955 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_2          | 2023-06-29 21:45:33,955 [3ea36174-d662-4e6c-bd1c-d614f11906cb-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_2          | 2023-06-29 21:45:33,956 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:33,956 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180
datanode_2          | 2023-06-29 21:45:33,983 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:34,058 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO impl.FollowerState: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5217793411ns, electionTimeout:5190ms
datanode_2          | 2023-06-29 21:45:34,062 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState
datanode_2          | 2023-06-29 21:45:34,062 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:45:34,093 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-29 21:45:34,103 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1
datanode_2          | 2023-06-29 21:45:34,149 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:34,828 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-3ea36174-d662-4e6c-bd1c-d614f11906cb: Detected pause in JVM or host machine approximately 0.543s with 0.672s GC time.
datanode_2          | GC pool 'ParNew' had collection(s): count=1 time=672ms
datanode_2          | 2023-06-29 21:45:35,006 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_2          | 2023-06-29 21:45:35,024 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_2          | 2023-06-29 21:45:35,024 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:35,082 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:35,119 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: receive requestVote(PRE_VOTE, 1fef6398-5e0e-4531-8b14-70177bcfa866, group-EECA45441CF9, 0, (t:0, i:0))
datanode_2          | 2023-06-29 21:45:35,159 [grpc-default-executor-1] INFO impl.VoteContext: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-CANDIDATE: reject PRE_VOTE from 1fef6398-5e0e-4531-8b14-70177bcfa866: our priority 1 > candidate's priority 0
datanode_2          | 2023-06-29 21:45:35,167 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9 replies to PRE_VOTE vote request: 1fef6398-5e0e-4531-8b14-70177bcfa866<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:FAIL-t0. Peer's state: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9:t0, leader=null, voted=, raftlog=Memoized:3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:35,379 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:45:35,384 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t0
datanode_2          | 2023-06-29 21:45:35,386 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_2          | 2023-06-29 21:45:35,396 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:35,428 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:35,460 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:35,496 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:45:35,496 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection:   Response 0: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t1
datanode_2          | 2023-06-29 21:45:35,496 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1 ELECTION round 0: result PASSED
datanode_2          | 2023-06-29 21:45:35,496 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1
datanode_2          | 2023-06-29 21:45:35,497 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:45:35,497 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EECA45441CF9 with new leaderId: 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:35,499 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: change Leader from null to 3ea36174-d662-4e6c-bd1c-d614f11906cb at term 1 for becomeLeader, leader elected after 8191ms
datanode_2          | 2023-06-29 21:45:35,573 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:45:35,662 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:35,671 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:45:35,733 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:45:35,739 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:45:35,740 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:45:35,797 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180.
datanode_2          | 2023-06-29 21:45:35,811 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:35,826 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:45:35,913 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:45:35,917 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:35,918 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:45:35,927 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-29 21:45:35,936 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:45:35,936 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:45:35,936 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-06-29 21:45:35,940 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-06-29 21:45:35,941 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:45:35,941 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:45:35,948 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_2          | 2023-06-29 21:45:35,949 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_2          | 2023-06-29 21:45:35,949 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_2          | 2023-06-29 21:45:35,949 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_2          | 2023-06-29 21:45:35,949 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_2          | 2023-06-29 21:45:35,950 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_2          | 2023-06-29 21:45:35,953 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_2          | 2023-06-29 21:45:35,953 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_2          | 2023-06-29 21:45:35,953 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_2          | 2023-06-29 21:45:35,954 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_2          | 2023-06-29 21:45:35,956 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderStateImpl
datanode_2          | 2023-06-29 21:45:36,007 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:45:36,123 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-LeaderElection1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:36,612 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-EECA45441CF9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/current/log_inprogress_0
datanode_2          | 2023-06-29 21:45:38,873 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO impl.FollowerState: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5069344510ns, electionTimeout:5042ms
datanode_2          | 2023-06-29 21:45:38,874 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState
datanode_2          | 2023-06-29 21:45:38,875 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:45:38,875 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-29 21:45:38,875 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2
datanode_2          | 2023-06-29 21:45:38,887 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:38,887 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_2          | 2023-06-29 21:45:38,892 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:38,893 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_2          | 2023-06-29 21:45:38,893 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2
datanode_2          | 2023-06-29 21:45:38,893 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_2          | 2023-06-29 21:45:38,893 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FF44D979937E with new leaderId: 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_2          | 2023-06-29 21:45:38,900 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: change Leader from null to 3ea36174-d662-4e6c-bd1c-d614f11906cb at term 1 for becomeLeader, leader elected after 5730ms
datanode_2          | 2023-06-29 21:45:38,903 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_2          | 2023-06-29 21:45:38,911 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:38,911 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_2          | 2023-06-29 21:45:38,917 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_2          | 2023-06-29 21:45:38,919 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_2          | 2023-06-29 21:45:38,920 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_2          | 2023-06-29 21:45:38,920 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_2          | 2023-06-29 21:45:38,921 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_2          | 2023-06-29 21:45:38,922 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderStateImpl
datanode_4          | 2023-06-29 21:45:26,403 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-29 21:45:26,403 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:26,403 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-29 21:45:26,404 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-29 21:45:26,429 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: ConfigurationManager, init=-1: peers:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-29 21:45:26,434 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-29 21:45:26,447 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-29 21:45:26,450 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-29 21:45:26,503 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:26,517 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-29 21:45:26,539 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-29 21:45:26,544 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-29 21:45:26,626 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-29 21:45:26,763 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-29 21:45:26,778 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-29 21:45:26,781 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-29 21:45:26,783 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-29 21:45:26,791 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-29 21:45:26,792 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-29 21:45:26,832 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8a001e4a-d312-430e-b38d-510e705e518c does not exist. Creating ...
datanode_4          | 2023-06-29 21:45:26,860 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8a001e4a-d312-430e-b38d-510e705e518c/in_use.lock acquired by nodename 7@79b5ac674f34
datanode_4          | 2023-06-29 21:45:26,885 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8a001e4a-d312-430e-b38d-510e705e518c has been successfully formatted.
datanode_4          | 2023-06-29 21:45:26,942 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO ratis.ContainerStateMachine: group-510E705E518C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-29 21:45:26,952 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-29 21:45:27,009 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-29 21:45:27,009 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:27,014 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-29 21:45:27,023 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-29 21:45:27,026 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:27,073 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-29 21:45:27,080 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-29 21:45:27,080 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:27,124 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8a001e4a-d312-430e-b38d-510e705e518c
datanode_4          | 2023-06-29 21:45:27,130 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-29 21:45:27,132 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-29 21:45:27,138 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:27,139 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-29 21:45:27,141 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-29 21:45:27,148 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-29 21:45:27,149 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-29 21:45:27,152 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-29 21:45:27,199 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:34,697 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:34,697 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-29 21:45:34,697 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState
datanode_5          | 2023-06-29 21:45:34,698 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B9087FDFF180,id=5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:45:34,703 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:34,708 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-29 21:45:34,708 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-29 21:45:34,708 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-29 21:45:34,714 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-29 21:45:34,750 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-29 21:45:34,770 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: receive requestVote(PRE_VOTE, 1fef6398-5e0e-4531-8b14-70177bcfa866, group-EECA45441CF9, 0, (t:0, i:0))
datanode_5          | 2023-06-29 21:45:34,790 [grpc-default-executor-0] INFO impl.VoteContext: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FOLLOWER: accept PRE_VOTE from 1fef6398-5e0e-4531-8b14-70177bcfa866: our priority 0 <= candidate's priority 0
datanode_5          | 2023-06-29 21:45:34,809 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9 replies to PRE_VOTE vote request: 1fef6398-5e0e-4531-8b14-70177bcfa866<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t0. Peer's state: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9:t0, leader=null, voted=, raftlog=Memoized:5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:35,353 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: receive requestVote(PRE_VOTE, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-EECA45441CF9, 0, (t:0, i:0))
datanode_5          | 2023-06-29 21:45:35,354 [grpc-default-executor-0] INFO impl.VoteContext: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FOLLOWER: accept PRE_VOTE from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 0 <= candidate's priority 1
datanode_5          | 2023-06-29 21:45:35,355 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9 replies to PRE_VOTE vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t0. Peer's state: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9:t0, leader=null, voted=, raftlog=Memoized:5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:35,463 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: receive requestVote(ELECTION, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-EECA45441CF9, 1, (t:0, i:0))
datanode_5          | 2023-06-29 21:45:35,464 [grpc-default-executor-0] INFO impl.VoteContext: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FOLLOWER: accept ELECTION from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 0 <= candidate's priority 1
datanode_5          | 2023-06-29 21:45:35,465 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_5          | 2023-06-29 21:45:35,465 [grpc-default-executor-0] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: shutdown 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState
datanode_5          | 2023-06-29 21:45:35,466 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState] INFO impl.FollowerState: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState was interrupted
datanode_5          | 2023-06-29 21:45:35,466 [grpc-default-executor-0] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-FollowerState
datanode_5          | 2023-06-29 21:45:35,474 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9 replies to ELECTION vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t1. Peer's state: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9:t1, leader=null, voted=3ea36174-d662-4e6c-bd1c-d614f11906cb, raftlog=Memoized:5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:36,305 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EECA45441CF9 with new leaderId: 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_5          | 2023-06-29 21:45:36,305 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-server-thread1] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: change Leader from null to 3ea36174-d662-4e6c-bd1c-d614f11906cb at term 1 for appendEntries, leader elected after 4351ms
datanode_5          | 2023-06-29 21:45:36,417 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-server-thread2] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:36,460 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-server-thread2] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-29 21:45:36,688 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-EECA45441CF9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/current/log_inprogress_0
datanode_5          | 2023-06-29 21:45:39,056 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: receive requestVote(PRE_VOTE, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-B9087FDFF180, 0, (t:0, i:0))
datanode_5          | 2023-06-29 21:45:39,057 [grpc-default-executor-0] INFO impl.VoteContext: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FOLLOWER: reject PRE_VOTE from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 1 > candidate's priority 0
datanode_5          | 2023-06-29 21:45:39,057 [grpc-default-executor-0] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180 replies to PRE_VOTE vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:FAIL-t0. Peer's state: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180:t0, leader=null, voted=, raftlog=Memoized:5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:39,812 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO impl.FollowerState: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5114491684ns, electionTimeout:5061ms
datanode_5          | 2023-06-29 21:45:39,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: shutdown 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState
datanode_5          | 2023-06-29 21:45:39,813 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-29 21:45:39,817 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-29 21:45:39,817 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-FollowerState] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1
datanode_5          | 2023-06-29 21:45:39,818 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:39,828 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-29 21:45:39,828 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-29 21:45:39,829 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for 1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_5          | 2023-06-29 21:45:39,832 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_5          | 2023-06-29 21:45:40,248 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1: PRE_VOTE PASSED received 1 response(s) and 0 exception(s):
datanode_5          | 2023-06-29 21:45:40,252 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection:   Response 0: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:OK-t0
datanode_5          | 2023-06-29 21:45:40,252 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1 PRE_VOTE round 0: result PASSED
datanode_5          | 2023-06-29 21:45:40,262 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:40,279 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-29 21:45:40,279 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-29 21:45:40,299 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:44:30,719 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 9b217bc73ed2/172.24.0.14
om_1                | STARTUP_MSG:   args = [--init]
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
om_1                | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-29 21:44:30,843 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:44:38,260 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:44:40,800 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:44:41,368 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.14:9862
om_1                | 2023-06-29 21:44:41,371 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:44:41,381 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:44:41,735 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:44:43,055 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863]
om_1                | 2023-06-29 21:44:46,807 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
om_1                | 2023-06-29 21:44:48,809 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
om_1                | 2023-06-29 21:44:50,810 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
om_1                | 2023-06-29 21:44:52,812 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
om_1                | 2023-06-29 21:44:54,814 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
om_1                | 2023-06-29 21:44:56,816 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 6.
om_1                | 2023-06-29 21:44:58,817 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 7.
om_1                | 2023-06-29 21:45:00,819 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 8.
om_1                | 2023-06-29 21:45:02,821 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 9.
om_1                | 2023-06-29 21:45:04,823 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 10.
om_1                | 2023-06-29 21:45:06,824 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 11.
om_1                | 2023-06-29 21:45:08,826 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 12.
om_1                | 2023-06-29 21:45:10,828 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 13.
om_1                | 2023-06-29 21:45:12,829 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 14.
om_1                | 2023-06-29 21:45:14,832 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 9b217bc73ed2/172.24.0.14 to scm:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 15.
om_1                | 2023-06-29 21:45:16,935 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1cd49d86-1570-4113-bfd8-4673b0280c86 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 16.
om_1                | 2023-06-29 21:45:18,939 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1cd49d86-1570-4113-bfd8-4673b0280c86 is not the leader. Could not determine the leader node.
om_1                | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om_1                | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om_1                | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14229)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
om_1                | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
om_1                | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om_1                | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
om_1                | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
om_1                | , while invoking $Proxy32.send over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 17.
om_1                | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-2aa45680-139b-4449-b07e-596b1957bdef;layoutVersion=6
om_1                | 2023-06-29 21:45:21,269 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om_1                | /************************************************************
om_1                | SHUTDOWN_MSG: Shutting down OzoneManager at 9b217bc73ed2/172.24.0.14
om_1                | ************************************************************/
om_1                | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om_1                | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om_1                | 2023-06-29 21:45:28,549 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om_1                | /************************************************************
om_1                | STARTUP_MSG: Starting OzoneManager
om_1                | STARTUP_MSG:   host = 9b217bc73ed2/172.24.0.14
om_1                | STARTUP_MSG:   args = []
om_1                | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
datanode_4          | 2023-06-29 21:45:27,207 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:27,264 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-29 21:45:27,265 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-29 21:45:27,267 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-29 21:45:27,289 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-29 21:45:27,290 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-29 21:45:27,298 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: start as a follower, conf=-1: peers:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:27,298 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-29 21:45:27,302 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState
datanode_4          | 2023-06-29 21:45:27,314 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-29 21:45:27,315 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-29 21:45:27,319 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-510E705E518C,id=1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_4          | 2023-06-29 21:45:27,323 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-29 21:45:27,324 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-29 21:45:27,325 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-29 21:45:27,328 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-29 21:45:27,432 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=8a001e4a-d312-430e-b38d-510e705e518c
datanode_4          | 2023-06-29 21:45:27,433 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8a001e4a-d312-430e-b38d-510e705e518c.
datanode_4          | 2023-06-29 21:45:27,433 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 1fef6398-5e0e-4531-8b14-70177bcfa866: addNew group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] returns group-EECA45441CF9:java.util.concurrent.CompletableFuture@8f91b49[Not completed]
datanode_4          | 2023-06-29 21:45:27,452 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866: new RaftServerImpl for group-EECA45441CF9:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-29 21:45:27,465 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-29 21:45:27,475 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-29 21:45:27,477 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-29 21:45:27,477 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:27,478 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-29 21:45:27,487 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-29 21:45:27,487 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-29 21:45:27,489 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-29 21:45:27,491 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-29 21:45:27,504 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-29 21:45:27,509 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:27,509 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_2          | 2023-06-29 21:45:38,925 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:45:38,929 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/caf603ee-435e-47a0-8b37-ff44d979937e/current/log_inprogress_0
datanode_2          | 2023-06-29 21:45:38,935 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E-LeaderElection2] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-FF44D979937E: set configuration 0: peers:[3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:39,043 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO impl.FollowerState: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5089800507ns, electionTimeout:5059ms
datanode_2          | 2023-06-29 21:45:39,043 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState
datanode_2          | 2023-06-29 21:45:39,043 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_2          | 2023-06-29 21:45:39,044 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_2          | 2023-06-29 21:45:39,044 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3
datanode_2          | 2023-06-29 21:45:39,045 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:39,053 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_2          | 2023-06-29 21:45:39,053 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_2          | 2023-06-29 21:45:39,086 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3: PRE_VOTE REJECTED received 1 response(s) and 0 exception(s):
datanode_2          | 2023-06-29 21:45:39,098 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.LeaderElection:   Response 0: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:FAIL-t0
datanode_2          | 2023-06-29 21:45:39,099 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.LeaderElection: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3 PRE_VOTE round 0: result REJECTED
datanode_2          | 2023-06-29 21:45:39,099 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_2          | 2023-06-29 21:45:39,099 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3
datanode_2          | 2023-06-29 21:45:39,100 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-LeaderElection3] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState
datanode_2          | 2023-06-29 21:45:40,229 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: receive requestVote(PRE_VOTE, 5ffc81fa-0a37-43e0-be73-58283f7df8c3, group-B9087FDFF180, 0, (t:0, i:0))
datanode_2          | 2023-06-29 21:45:40,230 [grpc-default-executor-1] INFO impl.VoteContext: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FOLLOWER: accept PRE_VOTE from 5ffc81fa-0a37-43e0-be73-58283f7df8c3: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:45:40,232 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180 replies to PRE_VOTE vote request: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:OK-t0. Peer's state: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180:t0, leader=null, voted=, raftlog=Memoized:3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:40,286 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: receive requestVote(ELECTION, 5ffc81fa-0a37-43e0-be73-58283f7df8c3, group-B9087FDFF180, 1, (t:0, i:0))
datanode_2          | 2023-06-29 21:45:40,286 [grpc-default-executor-1] INFO impl.VoteContext: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FOLLOWER: accept ELECTION from 5ffc81fa-0a37-43e0-be73-58283f7df8c3: our priority 0 <= candidate's priority 1
datanode_2          | 2023-06-29 21:45:40,287 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_2          | 2023-06-29 21:45:40,287 [grpc-default-executor-1] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: shutdown 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState
datanode_2          | 2023-06-29 21:45:40,287 [grpc-default-executor-1] INFO impl.RoleInfo: 3ea36174-d662-4e6c-bd1c-d614f11906cb: start 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState
datanode_2          | 2023-06-29 21:45:40,287 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState] INFO impl.FollowerState: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-FollowerState was interrupted
datanode_2          | 2023-06-29 21:45:40,292 [grpc-default-executor-1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180 replies to ELECTION vote request: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:OK-t1. Peer's state: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180:t1, leader=null, voted=5ffc81fa-0a37-43e0-be73-58283f7df8c3, raftlog=Memoized:3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:40,688 [3ea36174-d662-4e6c-bd1c-d614f11906cb-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B9087FDFF180 with new leaderId: 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_2          | 2023-06-29 21:45:40,688 [3ea36174-d662-4e6c-bd1c-d614f11906cb-server-thread1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: change Leader from null to 5ffc81fa-0a37-43e0-be73-58283f7df8c3 at term 1 for appendEntries, leader elected after 6845ms
datanode_2          | 2023-06-29 21:45:40,693 [3ea36174-d662-4e6c-bd1c-d614f11906cb-server-thread1] INFO server.RaftServer$Division: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_2          | 2023-06-29 21:45:40,694 [3ea36174-d662-4e6c-bd1c-d614f11906cb-server-thread1] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker: Starting segment from index:0
datanode_2          | 2023-06-29 21:45:40,699 [3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3ea36174-d662-4e6c-bd1c-d614f11906cb@group-B9087FDFF180-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/current/log_inprogress_0
datanode_2          | 2023-06-29 21:46:22,542 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:47:22,543 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:48:22,543 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:49:22,544 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:50:22,544 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:51:22,545 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_2          | 2023-06-29 21:52:22,545 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-29 21:45:27,515 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-29 21:45:27,517 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-29 21:45:27,517 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-29 21:45:27,534 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-29 21:45:27,549 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-29 21:45:27,549 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-29 21:45:27,549 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-29 21:45:27,549 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-29 21:45:27,549 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-29 21:45:27,558 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 does not exist. Creating ...
datanode_4          | 2023-06-29 21:45:27,565 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/in_use.lock acquired by nodename 7@79b5ac674f34
datanode_4          | 2023-06-29 21:45:27,577 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 has been successfully formatted.
datanode_4          | 2023-06-29 21:45:27,600 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO ratis.ContainerStateMachine: group-EECA45441CF9: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-29 21:45:27,600 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-29 21:45:27,617 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-29 21:45:27,621 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:27,622 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-29 21:45:27,623 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-29 21:45:27,623 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:27,642 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-29 21:45:27,644 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-29 21:45:27,645 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:27,645 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
datanode_4          | 2023-06-29 21:45:27,651 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_4          | 2023-06-29 21:45:27,651 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_4          | 2023-06-29 21:45:27,651 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:27,652 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_4          | 2023-06-29 21:45:27,653 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-29 21:45:27,661 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_4          | 2023-06-29 21:45:27,666 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-29 21:45:27,667 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_4          | 2023-06-29 21:45:27,675 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:27,688 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:29,152 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1fef6398-5e0e-4531-8b14-70177bcfa866: Detected pause in JVM or host machine approximately 1.118s with 1.451s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=110ms
datanode_4          | GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1341ms
datanode_4          | 2023-06-29 21:45:29,160 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_4          | 2023-06-29 21:45:29,161 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_4          | 2023-06-29 21:45:29,162 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_4          | 2023-06-29 21:45:29,163 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_4          | 2023-06-29 21:45:29,163 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar
om_1                | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
om_1                | STARTUP_MSG:   java = 11.0.19
recon_1             | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 2023-06-29 21:44:28,050 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1             | /************************************************************
recon_1             | STARTUP_MSG: Starting ReconServer
recon_1             | STARTUP_MSG:   host = d379c9c1f103/172.24.0.2
recon_1             | STARTUP_MSG:   args = []
recon_1             | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-5.1.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.3.27.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/joda-time-2.10.6.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.41.2.2.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.3.27.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/spring-core-5.3.27.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.13.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.3.27.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/guice-5.1.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/guice-servlet-5.1.0.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar
recon_1             | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
recon_1             | STARTUP_MSG:   java = 11.0.19
om_1                | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | ************************************************************/
om_1                | 2023-06-29 21:45:28,608 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:45:33,213 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om_1                | 2023-06-29 21:45:34,483 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om_1                | 2023-06-29 21:45:34,770 [main] INFO ha.OMHANodeDetails: Configuration does not have ozone.om.address set. Falling back to the default OM address om/172.24.0.14:9862
om_1                | 2023-06-29 21:45:34,770 [main] INFO ha.OMHANodeDetails: OM Service ID is not set. Setting it to the default ID: omServiceIdDefault
om_1                | 2023-06-29 21:45:34,770 [main] INFO ha.OMHANodeDetails: OM Node ID is not set. Setting it to the default ID: om1
om_1                | 2023-06-29 21:45:34,975 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:35,322 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = QUOTA (version = 6), software layout = QUOTA (version = 6)
om_1                | 2023-06-29 21:45:36,270 [main] INFO reflections.Reflections: Reflections took 643 ms to scan 1 urls, producing 139 keys and 399 values [using 2 cores]
om_1                | 2023-06-29 21:45:36,358 [main] INFO upgrade.OMLayoutVersionManager: Skipping Upgrade Action QuotaRepairUpgradeAction since it has been finalized.
om_1                | 2023-06-29 21:45:36,557 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:37,379 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863]
om_1                | 2023-06-29 21:45:37,532 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 1 nodes: [nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9863]
om_1                | 2023-06-29 21:45:38,837 [main] INFO om.OzoneManager: OM start with adminUsers: [hadoop]
om_1                | 2023-06-29 21:45:38,927 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:39,380 [main] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
om_1                | 2023-06-29 21:45:40,425 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om_1                | 2023-06-29 21:45:40,560 [main] INFO om.OmSnapshotManager: Ozone filesystem snapshot feature is enabled.
om_1                | 2023-06-29 21:45:40,572 [main] WARN server.ServerUtils: ozone.om.snapshot.diff.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:40,835 [main] INFO utils.NativeLibraryLoader: Loading Library: ozone_rocksdb_tools
om_1                | 2023-06-29 21:45:40,842 [main] INFO snapshot.SnapshotDiffManager: Shutting down executorService: 'SstDumpToolExecutor'
om_1                | 2023-06-29 21:45:41,190 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om_1                | 2023-06-29 21:45:41,372 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:45:41,372 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om_1                | 2023-06-29 21:45:41,417 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om_1                | 2023-06-29 21:45:41,436 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om_1                | 2023-06-29 21:45:41,503 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omServiceIdDefault and peers: om:9872
om_1                | 2023-06-29 21:45:41,531 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
datanode_4          | 2023-06-29 21:45:29,164 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:29,165 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_4          | 2023-06-29 21:45:29,166 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState
datanode_4          | 2023-06-29 21:45:29,180 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-EECA45441CF9,id=1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_4          | 2023-06-29 21:45:29,193 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-29 21:45:29,194 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-29 21:45:29,194 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_4          | 2023-06-29 21:45:29,194 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-29 21:45:29,204 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-29 21:45:29,228 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
datanode_4          | 2023-06-29 21:45:29,244 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-29 21:45:32,442 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1fef6398-5e0e-4531-8b14-70177bcfa866: Detected pause in JVM or host machine approximately 0.274s with 0.282s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=282ms
datanode_4          | 2023-06-29 21:45:32,462 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO impl.FollowerState: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5159898907ns, electionTimeout:5139ms
datanode_4          | 2023-06-29 21:45:32,462 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState
datanode_4          | 2023-06-29 21:45:32,474 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-06-29 21:45:32,537 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-06-29 21:45:32,537 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-FollowerState] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1
datanode_4          | 2023-06-29 21:45:32,560 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:32,563 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
datanode_4          | 2023-06-29 21:45:32,574 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:32,574 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
datanode_4          | 2023-06-29 21:45:32,577 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1
datanode_4          | 2023-06-29 21:45:32,578 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_4          | 2023-06-29 21:45:32,582 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-510E705E518C with new leaderId: 1fef6398-5e0e-4531-8b14-70177bcfa866
datanode_4          | 2023-06-29 21:45:32,584 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: change Leader from null to 1fef6398-5e0e-4531-8b14-70177bcfa866 at term 1 for becomeLeader, leader elected after 6075ms
datanode_4          | 2023-06-29 21:45:32,665 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_4          | 2023-06-29 21:45:32,768 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-29 21:45:32,773 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_4          | 2023-06-29 21:45:32,821 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_4          | 2023-06-29 21:45:32,821 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_4          | 2023-06-29 21:45:32,833 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_4          | 2023-06-29 21:45:32,979 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_4          | 2023-06-29 21:45:32,989 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_4          | 2023-06-29 21:45:32,991 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderStateImpl
datanode_4          | 2023-06-29 21:45:33,194 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-29 21:45:33,403 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-LeaderElection1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C: set configuration 0: peers:[1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:33,500 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-510E705E518C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8a001e4a-d312-430e-b38d-510e705e518c/current/log_inprogress_0
datanode_4          | 2023-06-29 21:45:34,386 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO impl.FollowerState: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5219990206ns, electionTimeout:5142ms
datanode_4          | 2023-06-29 21:45:34,387 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState
datanode_4          | 2023-06-29 21:45:34,387 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_4          | 2023-06-29 21:45:34,388 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_4          | 2023-06-29 21:45:34,388 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2
datanode_4          | 2023-06-29 21:45:34,411 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:34,447 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_4          | 2023-06-29 21:45:34,450 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_4          | 2023-06-29 21:45:34,448 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 3ea36174-d662-4e6c-bd1c-d614f11906cb
datanode_4          | 2023-06-29 21:45:34,496 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_4          | 2023-06-29 21:45:35,250 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2: PRE_VOTE REJECTED received 2 response(s) and 0 exception(s):
datanode_4          | 2023-06-29 21:45:35,251 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.LeaderElection:   Response 0: 1fef6398-5e0e-4531-8b14-70177bcfa866<-5ffc81fa-0a37-43e0-be73-58283f7df8c3#0:OK-t0
datanode_4          | 2023-06-29 21:45:35,252 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.LeaderElection:   Response 1: 1fef6398-5e0e-4531-8b14-70177bcfa866<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:FAIL-t0
datanode_4          | 2023-06-29 21:45:35,252 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.LeaderElection: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2 PRE_VOTE round 0: result REJECTED
datanode_4          | 2023-06-29 21:45:35,254 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: changes role from CANDIDATE to FOLLOWER at term 0 for REJECTED
datanode_4          | 2023-06-29 21:45:35,254 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2
datanode_4          | 2023-06-29 21:45:35,254 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-LeaderElection2] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState
datanode_4          | 2023-06-29 21:45:35,297 [grpc-default-executor-0] INFO server.RaftServer: 1fef6398-5e0e-4531-8b14-70177bcfa866: addNew group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] returns group-B9087FDFF180:java.util.concurrent.CompletableFuture@7cb2ea67[Not completed]
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866: new RaftServerImpl for group-B9087FDFF180:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_4          | 2023-06-29 21:45:35,299 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_4          | 2023-06-29 21:45:35,300 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_4          | 2023-06-29 21:45:35,334 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_4          | 2023-06-29 21:45:35,334 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_4          | 2023-06-29 21:45:35,338 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_4          | 2023-06-29 21:45:35,338 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_4          | 2023-06-29 21:45:35,339 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_4          | 2023-06-29 21:45:35,340 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_4          | 2023-06-29 21:45:35,341 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_4          | 2023-06-29 21:45:35,358 [grpc-default-executor-0] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: receive requestVote(PRE_VOTE, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-EECA45441CF9, 0, (t:0, i:0))
datanode_4          | 2023-06-29 21:45:35,369 [grpc-default-executor-0] INFO impl.VoteContext: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FOLLOWER: accept PRE_VOTE from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-29 21:45:35,370 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_4          | 2023-06-29 21:45:35,384 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_4          | 2023-06-29 21:45:35,384 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_4          | 2023-06-29 21:45:35,384 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_4          | 2023-06-29 21:45:35,385 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_4          | 2023-06-29 21:45:35,385 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_4          | 2023-06-29 21:45:35,385 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 does not exist. Creating ...
datanode_4          | 2023-06-29 21:45:35,409 [grpc-default-executor-0] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9 replies to PRE_VOTE vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-1fef6398-5e0e-4531-8b14-70177bcfa866#0:OK-t0. Peer's state: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9:t0, leader=null, voted=, raftlog=Memoized:1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:35,414 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/in_use.lock acquired by nodename 7@79b5ac674f34
datanode_4          | 2023-06-29 21:45:35,428 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180 has been successfully formatted.
datanode_4          | 2023-06-29 21:45:35,436 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO ratis.ContainerStateMachine: group-B9087FDFF180: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_4          | 2023-06-29 21:45:35,437 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-29 21:45:35,438 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_4          | 2023-06-29 21:45:35,439 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:35,439 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-29 21:45:35,439 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_4          | 2023-06-29 21:45:35,442 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:35,466 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_4          | 2023-06-29 21:45:35,475 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-29 21:45:35,477 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:35,478 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180
datanode_4          | 2023-06-29 21:45:35,478 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-29 21:45:40,299 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection:   Response 0: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-3ea36174-d662-4e6c-bd1c-d614f11906cb#0:OK-t1
datanode_5          | 2023-06-29 21:45:40,300 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1 ELECTION round 0: result PASSED
datanode_5          | 2023-06-29 21:45:40,300 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: shutdown 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1
datanode_5          | 2023-06-29 21:45:40,301 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-29 21:45:40,301 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B9087FDFF180 with new leaderId: 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:45:40,303 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: change Leader from null to 5ffc81fa-0a37-43e0-be73-58283f7df8c3 at term 1 for becomeLeader, leader elected after 6252ms
datanode_5          | 2023-06-29 21:45:40,312 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
datanode_5          | 2023-06-29 21:45:40,339 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:45:40,344 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-29 21:45:40,364 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-29 21:45:40,364 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
datanode_5          | 2023-06-29 21:45:40,370 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-29 21:45:40,404 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:45:40,411 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
datanode_5          | 2023-06-29 21:45:40,468 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-29 21:45:40,468 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:40,468 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-29 21:45:40,478 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-29 21:45:40,479 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-29 21:45:40,479 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:45:40,483 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-29 21:45:40,483 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-29 21:45:40,483 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-29 21:45:40,483 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-29 21:45:40,506 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
datanode_5          | 2023-06-29 21:45:40,509 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:40,509 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
datanode_5          | 2023-06-29 21:45:40,512 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
datanode_5          | 2023-06-29 21:45:40,512 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
datanode_5          | 2023-06-29 21:45:40,513 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:45:40,513 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.element-limit = 8 (default)
datanode_5          | 2023-06-29 21:45:40,513 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.install_snapshot.request.timeout = 3000ms (default)
datanode_5          | 2023-06-29 21:45:40,513 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-29 21:45:40,513 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_5          | 2023-06-29 21:45:40,524 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderStateImpl
datanode_5          | 2023-06-29 21:45:40,527 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-29 21:45:40,529 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/current/log_inprogress_0
om_1                | 2023-06-29 21:45:41,659 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om_1                | 2023-06-29 21:45:41,675 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:45:41,677 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:45:41,679 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om_1                | 2023-06-29 21:45:41,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:45:41,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om_1                | 2023-06-29 21:45:41,682 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om_1                | 2023-06-29 21:45:41,683 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om_1                | 2023-06-29 21:45:41,688 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:45:41,691 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om_1                | 2023-06-29 21:45:41,692 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:45:41,713 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om_1                | 2023-06-29 21:45:41,720 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om_1                | 2023-06-29 21:45:41,721 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om_1                | 2023-06-29 21:45:42,389 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:45:42,396 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om_1                | 2023-06-29 21:45:42,398 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om_1                | 2023-06-29 21:45:42,399 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:45:42,400 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:45:42,405 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:45:42,450 [main] INFO server.RaftServer: om1: addNew group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] returns group-C5BA1605619E:java.util.concurrent.CompletableFuture@2da16d31[Not completed]
om_1                | 2023-06-29 21:45:42,451 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om_1                | 2023-06-29 21:45:42,472 [main] INFO om.OzoneManager: Creating RPC Server
om_1                | 2023-06-29 21:45:42,499 [om1-groupManagement] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-C5BA1605619E:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om_1                | 2023-06-29 21:45:42,507 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om_1                | 2023-06-29 21:45:42,514 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om_1                | 2023-06-29 21:45:42,515 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om_1                | 2023-06-29 21:45:42,515 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om_1                | 2023-06-29 21:45:42,515 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om_1                | 2023-06-29 21:45:42,515 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om_1                | 2023-06-29 21:45:42,534 [om1-groupManagement] INFO server.RaftServer$Division: om1@group-C5BA1605619E: ConfigurationManager, init=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om_1                | 2023-06-29 21:45:42,535 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om_1                | 2023-06-29 21:45:42,557 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om_1                | 2023-06-29 21:45:42,558 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om_1                | 2023-06-29 21:45:42,640 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om_1                | 2023-06-29 21:45:42,651 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
om_1                | 2023-06-29 21:45:42,667 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om_1                | 2023-06-29 21:45:42,669 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om_1                | 2023-06-29 21:45:42,879 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
om_1                | 2023-06-29 21:45:43,168 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om_1                | 2023-06-29 21:45:43,183 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om_1                | 2023-06-29 21:45:43,194 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om_1                | 2023-06-29 21:45:43,223 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om_1                | 2023-06-29 21:45:43,229 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om_1                | 2023-06-29 21:45:43,230 [om1-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om_1                | 2023-06-29 21:45:44,412 [main] INFO reflections.Reflections: Reflections took 1558 ms to scan 8 urls, producing 24 keys and 643 values [using 2 cores]
om_1                | 2023-06-29 21:45:45,101 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om_1                | 2023-06-29 21:45:45,115 [main] INFO ipc.Server: Listener at om:9862
om_1                | 2023-06-29 21:45:45,124 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om_1                | 2023-06-29 21:45:46,765 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:45:46,800 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om_1                | 2023-06-29 21:45:46,800 [main] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om_1                | 2023-06-29 21:45:46,921 [main] INFO om.OzoneManager: OzoneManager RPC server is listening at om/172.24.0.14:9862
om_1                | 2023-06-29 21:45:46,922 [main] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om_1                | 2023-06-29 21:45:46,932 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e does not exist. Creating ...
om_1                | 2023-06-29 21:45:46,942 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/in_use.lock acquired by nodename 7@9b217bc73ed2
recon_1             | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.ha.raft.server.retrycache.expirytime=300s, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.sql.db.auto.commit=true, ozone.recon.sql.db.conn.idle.max.age=3600s, ozone.recon.sql.db.conn.idle.test=SELECT 1, ozone.recon.sql.db.conn.idle.test.period=60s, ozone.recon.sql.db.conn.max.active=5, ozone.recon.sql.db.conn.max.age=1800s, ozone.recon.sql.db.conn.timeout=30000ms, ozone.recon.sql.db.driver=org.apache.derby.jdbc.EmbeddedDriver, ozone.recon.sql.db.jdbc.url=jdbc:derby:/data/metadata/recon/ozone_recon_derby.db, ozone.recon.sql.db.jooq.dialect=DERBY, ozone.recon.task.containercounttask.interval=60s, ozone.recon.task.missingcontainer.interval=300s, ozone.recon.task.pipelinesync.interval=300s, ozone.recon.task.safemode.wait.threshold=300s, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | ************************************************************/
recon_1             | 2023-06-29 21:44:28,129 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-29 21:44:32,453 [main] INFO reflections.Reflections: Reflections took 566 ms to scan 1 urls, producing 20 keys and 75 values 
datanode_5          | 2023-06-29 21:45:40,537 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180-LeaderElection1] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-B9087FDFF180: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:55,964 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: new RaftServerImpl for group-FECD1B5DDF7E:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
datanode_5          | 2023-06-29 21:45:55,964 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
datanode_5          | 2023-06-29 21:45:55,965 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
datanode_5          | 2023-06-29 21:45:55,966 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
datanode_5          | 2023-06-29 21:45:55,967 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:55,968 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
datanode_5          | 2023-06-29 21:45:55,968 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
datanode_5          | 2023-06-29 21:45:55,969 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: ConfigurationManager, init=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
datanode_5          | 2023-06-29 21:45:55,969 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
datanode_5          | 2023-06-29 21:45:55,969 [PipelineCommandHandlerThread-0] INFO server.RaftServer: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: addNew group-FECD1B5DDF7E:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER] returns group-FECD1B5DDF7E:java.util.concurrent.CompletableFuture@66ac551c[Not completed]
datanode_5          | 2023-06-29 21:45:55,970 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
datanode_5          | 2023-06-29 21:45:55,972 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
datanode_5          | 2023-06-29 21:45:55,972 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
datanode_5          | 2023-06-29 21:45:55,972 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
datanode_5          | 2023-06-29 21:45:55,972 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
datanode_5          | 2023-06-29 21:45:55,972 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
datanode_5          | 2023-06-29 21:45:55,973 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
datanode_5          | 2023-06-29 21:45:55,973 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
datanode_5          | 2023-06-29 21:45:55,977 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
datanode_5          | 2023-06-29 21:45:55,977 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
datanode_5          | 2023-06-29 21:45:55,978 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
datanode_5          | 2023-06-29 21:45:55,978 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
datanode_5          | 2023-06-29 21:45:55,979 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
datanode_5          | 2023-06-29 21:45:55,979 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e does not exist. Creating ...
datanode_5          | 2023-06-29 21:45:55,983 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e/in_use.lock acquired by nodename 7@5d2a01c1f503
datanode_5          | 2023-06-29 21:45:55,986 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e has been successfully formatted.
datanode_5          | 2023-06-29 21:45:55,996 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO ratis.ContainerStateMachine: group-FECD1B5DDF7E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
datanode_5          | 2023-06-29 21:45:55,996 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_5          | 2023-06-29 21:45:55,996 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
datanode_5          | 2023-06-29 21:45:55,996 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:55,996 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_5          | 2023-06-29 21:45:55,997 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
datanode_5          | 2023-06-29 21:45:55,997 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:55,997 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
datanode_5          | 2023-06-29 21:45:55,999 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_5          | 2023-06-29 21:45:55,999 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:56,000 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: new 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e
datanode_5          | 2023-06-29 21:45:56,000 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
datanode_5          | 2023-06-29 21:45:56,000 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:45:56,001 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:56,001 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
datanode_5          | 2023-06-29 21:45:56,001 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_5          | 2023-06-29 21:45:56,001 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
datanode_5          | 2023-06-29 21:45:56,002 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_5          | 2023-06-29 21:45:56,002 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
datanode_5          | 2023-06-29 21:45:56,002 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
datanode_5          | 2023-06-29 21:45:56,012 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_5          | 2023-06-29 21:45:56,296 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:56,296 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
datanode_5          | 2023-06-29 21:45:56,296 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:56,296 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-29 21:45:56,296 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
datanode_5          | 2023-06-29 21:45:56,297 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:45:56,303 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: changes role from      null to FOLLOWER at term 0 for startAsFollower
datanode_5          | 2023-06-29 21:45:56,303 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState
datanode_5          | 2023-06-29 21:45:56,304 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FECD1B5DDF7E,id=5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:45:56,304 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_5          | 2023-06-29 21:45:56,304 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_5          | 2023-06-29 21:45:56,304 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
datanode_5          | 2023-06-29 21:45:56,304 [5ffc81fa-0a37-43e0-be73-58283f7df8c3-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_5          | 2023-06-29 21:45:56,305 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
datanode_5          | 2023-06-29 21:45:56,305 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
datanode_5          | 2023-06-29 21:45:56,344 [PipelineCommandHandlerThread-0] INFO ratis.XceiverServerRatis: Created group PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e
datanode_5          | 2023-06-29 21:45:56,345 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e.
datanode_5          | 2023-06-29 21:46:01,428 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO impl.FollowerState: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5124847532ns, electionTimeout:5123ms
datanode_5          | 2023-06-29 21:46:01,429 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: shutdown 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState
datanode_5          | 2023-06-29 21:46:01,429 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
datanode_5          | 2023-06-29 21:46:01,430 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
datanode_5          | 2023-06-29 21:46:01,430 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-FollowerState] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2
datanode_5          | 2023-06-29 21:46:01,436 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:46:01,437 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2 PRE_VOTE round 0: result PASSED (term=0)
datanode_5          | 2023-06-29 21:46:01,438 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:46:01,439 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.LeaderElection: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2 ELECTION round 0: result PASSED (term=1)
datanode_5          | 2023-06-29 21:46:01,439 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: shutdown 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2
datanode_5          | 2023-06-29 21:46:01,439 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
datanode_5          | 2023-06-29 21:46:01,439 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-FECD1B5DDF7E with new leaderId: 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_5          | 2023-06-29 21:46:01,441 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: change Leader from null to 5ffc81fa-0a37-43e0-be73-58283f7df8c3 at term 1 for becomeLeader, leader elected after 5467ms
datanode_5          | 2023-06-29 21:46:01,442 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:45:46,965 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e has been successfully formatted.
datanode_4          | 2023-06-29 21:45:35,478 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:46:01,442 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:46:01,442 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
datanode_5          | 2023-06-29 21:46:01,443 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
datanode_5          | 2023-06-29 21:46:01,443 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 2023-06-29 21:44:37,791 [main] INFO reflections.Reflections: Reflections took 629 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 2023-06-29 21:44:38,351 [main] INFO recon.ReconServer: Initializing Recon server...
datanode_5          | 2023-06-29 21:46:01,443 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
datanode_5          | 2023-06-29 21:46:01,443 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
datanode_5          | 2023-06-29 21:46:01,443 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 2023-06-29 21:44:41,849 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:44:49,600 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1             | WARNING: An illegal reflective access operation has occurred
datanode_5          | 2023-06-29 21:46:01,444 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO impl.RoleInfo: 5ffc81fa-0a37-43e0-be73-58283f7df8c3: start 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderStateImpl
datanode_5          | 2023-06-29 21:46:01,445 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker: Starting segment from index:0
datanode_5          | 2023-06-29 21:46:01,446 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e/current/log_inprogress_0
datanode_5          | 2023-06-29 21:46:01,468 [5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E-LeaderElection2] INFO server.RaftServer$Division: 5ffc81fa-0a37-43e0-be73-58283f7df8c3@group-FECD1B5DDF7E: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
datanode_5          | 2023-06-29 21:46:22,616 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | WARNING: Illegal reflective access by org.jooq.tools.reflect.Reflect (file:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar) to constructor java.lang.invoke.MethodHandles$Lookup(java.lang.Class)
recon_1             | WARNING: Please consider reporting this to the maintainers of org.jooq.tools.reflect.Reflect
recon_1             | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1             | WARNING: All illegal access operations will be denied in a future release
recon_1             | 2023-06-29 21:44:51,810 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1             | 2023-06-29 21:44:51,844 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.034 seconds to initialized 0 records to KEY_CONTAINER table
datanode_4          | 2023-06-29 21:45:35,478 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
datanode_4          | 2023-06-29 21:45:35,479 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
recon_1             | 2023-06-29 21:44:52,275 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1             | 2023-06-29 21:44:52,459 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
datanode_4          | 2023-06-29 21:45:35,480 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
datanode_4          | 2023-06-29 21:45:35,482 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
recon_1             | 2023-06-29 21:44:52,460 [main] INFO recon.ReconServer: Creating Recon Schema.
datanode_5          | 2023-06-29 21:47:22,617 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:48:22,617 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:49:22,618 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:50:22,618 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:51:22,619 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_5          | 2023-06-29 21:52:22,619 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 2023-06-29 21:44:55,933 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1             | 2023-06-29 21:44:56,029 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1             | 2023-06-29 21:44:56,115 [main] INFO util.log: Logging initialized @39277ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:44:56,738 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-29 21:44:56,833 [main] INFO http.HttpRequestLog: Http request log for http.requests.recon is not defined
recon_1             | 2023-06-29 21:44:56,916 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:44:56,918 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1             | 2023-06-29 21:44:56,918 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:44:56,918 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:44:57,321 [main] INFO http.BaseHttpServer: HTTP server of recon uses base directory /data/metadata/webserver
recon_1             | 2023-06-29 21:44:57,338 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1             | 2023-06-29 21:44:58,762 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1             | 2023-06-29 21:44:58,812 [main] INFO tasks.ReconTaskControllerImpl: Registered task OmTableInsightTask with controller.
recon_1             | 2023-06-29 21:44:58,871 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1             | 2023-06-29 21:44:59,161 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:44:59,175 [main] INFO ozone.OmUtils: No OzoneManager ServiceID configured.
om_1                | 2023-06-29 21:45:46,976 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om_1                | 2023-06-29 21:45:46,996 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
recon_1             | 2023-06-29 21:45:04,663 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:46,996 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
datanode_4          | 2023-06-29 21:45:35,487 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm_1               | 2023-06-29 21:44:41,991 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
om_1                | 2023-06-29 21:45:47,000 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
datanode_4          | 2023-06-29 21:45:35,493 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | /************************************************************
scm_1               | STARTUP_MSG: Starting StorageContainerManager
recon_1             | 2023-06-29 21:45:05,558 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om_1                | 2023-06-29 21:45:47,002 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | STARTUP_MSG:   host = 68c14f390c32/172.24.0.11
recon_1             | 2023-06-29 21:45:05,694 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 2023-06-29 21:45:05,703 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
datanode_4          | 2023-06-29 21:45:35,494 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om_1                | 2023-06-29 21:45:47,006 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | STARTUP_MSG:   args = [--init]
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 2023-06-29 21:45:06,091 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_4          | 2023-06-29 21:45:35,502 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om_1                | 2023-06-29 21:45:47,014 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
recon_1             | 2023-06-29 21:45:06,395 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
datanode_4          | 2023-06-29 21:45:35,524 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-29 21:45:47,016 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
datanode_4          | 2023-06-29 21:45:35,533 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-29 21:45:47,017 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | STARTUP_MSG:   java = 11.0.19
recon_1             | 2023-06-29 21:45:06,468 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-29 21:45:06,540 [main] INFO node.SCMNodeManager: Entering startup safe mode.
om_1                | 2023-06-29 21:45:47,035 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-C5BA1605619E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e
datanode_4          | 2023-06-29 21:45:35,539 [grpc-default-executor-0] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: receive requestVote(ELECTION, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-EECA45441CF9, 1, (t:0, i:0))
om_1                | 2023-06-29 21:45:47,035 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
scm_1               | ************************************************************/
scm_1               | 2023-06-29 21:44:42,232 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om_1                | 2023-06-29 21:45:47,037 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om_1                | 2023-06-29 21:45:47,044 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om_1                | 2023-06-29 21:45:47,046 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
datanode_4          | 2023-06-29 21:45:35,568 [grpc-default-executor-0] INFO impl.VoteContext: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FOLLOWER: accept ELECTION from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-29 21:45:35,568 [grpc-default-executor-0] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:3ea36174-d662-4e6c-bd1c-d614f11906cb
recon_1             | 2023-06-29 21:45:06,549 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1             | 2023-06-29 21:45:06,560 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-29 21:45:07,104 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om_1                | 2023-06-29 21:45:47,048 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om_1                | 2023-06-29 21:45:47,056 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:45:47,059 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
datanode_4          | 2023-06-29 21:45:35,568 [grpc-default-executor-0] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState
om_1                | 2023-06-29 21:45:47,060 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:44:43,520 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm_1               | 2023-06-29 21:44:46,357 [main] INFO reflections.Reflections: Reflections took 2210 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 2023-06-29 21:45:07,150 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
datanode_4          | 2023-06-29 21:45:35,568 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState] INFO impl.FollowerState: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState was interrupted
om_1                | 2023-06-29 21:45:47,088 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
datanode_4          | 2023-06-29 21:45:35,568 [grpc-default-executor-0] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-FollowerState
om_1                | 2023-06-29 21:45:47,089 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
s3g_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | 2023-06-29 21:45:07,221 [main] INFO ipc.Server: Listener at 0.0.0.0:9891
recon_1             | 2023-06-29 21:45:07,238 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
om_1                | 2023-06-29 21:45:47,121 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om_1                | 2023-06-29 21:45:47,124 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om_1                | 2023-06-29 21:45:47,124 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om_1                | 2023-06-29 21:45:47,143 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:45:47,143 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
s3g_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1               | 2023-06-29 21:44:30,084 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
datanode_4          | 2023-06-29 21:45:35,570 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1             | 2023-06-29 21:45:07,330 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om_1                | 2023-06-29 21:45:47,147 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: start as a follower, conf=-1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:35,581 [grpc-default-executor-0] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9 replies to ELECTION vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-1fef6398-5e0e-4531-8b14-70177bcfa866#0:OK-t1. Peer's state: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9:t1, leader=null, voted=3ea36174-d662-4e6c-bd1c-d614f11906cb, raftlog=Memoized:1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1               | 2023-06-29 21:44:30,095 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:44:48,122 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm_1               | 2023-06-29 21:44:48,380 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1             | 2023-06-29 21:45:07,441 [main] INFO recon.ReconServer: Initializing support of Recon Features...
datanode_4          | 2023-06-29 21:45:35,593 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1             | 2023-06-29 21:45:07,443 [main] INFO recon.ReconServer: Recon server initialized successfully!
om_1                | 2023-06-29 21:45:47,147 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:45:47,154 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-FollowerState
datanode_4          | 2023-06-29 21:45:35,595 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1             | 2023-06-29 21:45:07,444 [main] INFO recon.ReconServer: Starting Recon server
scm_1               | 2023-06-29 21:44:51,300 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
datanode_4          | 2023-06-29 21:45:35,600 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: start as a follower, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:45:47,164 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-C5BA1605619E,id=om1
scm_1               | 2023-06-29 21:44:52,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
s3g_1               | 2023-06-29 21:44:30,326 [main] INFO util.log: Logging initialized @11313ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1             | 2023-06-29 21:45:07,512 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
datanode_4          | 2023-06-29 21:45:35,609 [PipelineCommandHandlerThread-0] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9.
scm_1               | 2023-06-29 21:44:52,647 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
s3g_1               | 2023-06-29 21:44:31,427 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1             | 2023-06-29 21:45:07,524 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-06-29 21:45:35,615 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: changes role from      null to FOLLOWER at term 0 for startAsFollower
s3g_1               | 2023-06-29 21:44:31,590 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
scm_1               | 2023-06-29 21:44:52,671 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm_1               | 2023-06-29 21:44:52,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
om_1                | 2023-06-29 21:45:47,166 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
datanode_4          | 2023-06-29 21:45:35,616 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState
scm_1               | 2023-06-29 21:44:52,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm_1               | 2023-06-29 21:44:52,680 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om_1                | 2023-06-29 21:45:47,166 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
datanode_4          | 2023-06-29 21:45:35,620 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B9087FDFF180,id=1fef6398-5e0e-4531-8b14-70177bcfa866
s3g_1               | 2023-06-29 21:44:31,669 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:45:07,524 [main] INFO impl.MetricsSystemImpl: Recon metrics system started
scm_1               | 2023-06-29 21:44:52,703 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
datanode_4          | 2023-06-29 21:45:35,620 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
s3g_1               | 2023-06-29 21:44:31,701 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
recon_1             | 2023-06-29 21:45:07,764 [main] INFO http.HttpServer2: Jetty bound to port 9888
scm_1               | 2023-06-29 21:44:52,732 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:44:52,733 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
datanode_4          | 2023-06-29 21:45:35,623 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
datanode_4          | 2023-06-29 21:45:35,624 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1             | 2023-06-29 21:45:07,766 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-29 21:44:52,733 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:44:52,923 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
datanode_4          | 2023-06-29 21:45:35,624 [1fef6398-5e0e-4531-8b14-70177bcfa866-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
datanode_4          | 2023-06-29 21:45:35,629 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1             | 2023-06-29 21:45:07,822 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:44:53,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:44:53,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
s3g_1               | 2023-06-29 21:44:31,713 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1             | 2023-06-29 21:45:07,822 [main] INFO server.session: No SessionScavenger set, using defaults
datanode_4          | 2023-06-29 21:45:35,630 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:44:56,152 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om_1                | 2023-06-29 21:45:47,166 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
s3g_1               | 2023-06-29 21:44:31,714 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1             | 2023-06-29 21:45:07,823 [main] INFO server.session: node0 Scavenging every 600000ms
datanode_4          | 2023-06-29 21:45:36,447 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-EECA45441CF9 with new leaderId: 3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:44:56,209 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm_1               | 2023-06-29 21:44:56,209 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
s3g_1               | 2023-06-29 21:44:32,023 [main] INFO http.BaseHttpServer: HTTP server of s3gateway uses base directory /opt/hadoop/ozone_s3g_tmp_base_dir15590505455235115386
recon_1             | 2023-06-29 21:45:07,837 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@a319a2e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:45:47,167 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
scm_1               | 2023-06-29 21:44:56,209 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
datanode_4          | 2023-06-29 21:45:36,448 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: change Leader from null to 3ea36174-d662-4e6c-bd1c-d614f11906cb at term 1 for appendEntries, leader elected after 8938ms
s3g_1               | 2023-06-29 21:44:32,696 [main] INFO s3.Gateway: STARTUP_MSG: 
recon_1             | 2023-06-29 21:45:07,837 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2082e0e4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
om_1                | 2023-06-29 21:45:47,168 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:44:56,209 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:44:56,319 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:44:56,441 [main] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: addNew group-596B1957BDEF:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER] returns group-596B1957BDEF:java.util.concurrent.CompletableFuture@2f2bff16[Not completed]
recon_1             | 2023-06-29 21:45:09,701 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@71f5dd50{recon,/,file:///data/metadata/webserver/jetty-0_0_0_0-9888-ozone-recon-1_4_0-SNAPSHOT_jar-_-any-11899803807187293301/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.4.0-SNAPSHOT.jar!/webapps/recon}
recon_1             | 2023-06-29 21:45:09,709 [main] INFO server.AbstractConnector: Started ServerConnector@531292ed{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
scm_1               | 2023-06-29 21:44:56,788 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86: new RaftServerImpl for group-596B1957BDEF:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm_1               | 2023-06-29 21:44:56,807 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
datanode_4          | 2023-06-29 21:45:36,455 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:0|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-29 21:45:09,709 [main] INFO server.Server: Started @52872ms
recon_1             | 2023-06-29 21:45:09,711 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:44:56,829 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm_1               | 2023-06-29 21:44:56,829 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm_1               | 2023-06-29 21:44:56,829 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm_1               | 2023-06-29 21:44:56,829 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm_1               | 2023-06-29 21:44:56,829 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
s3g_1               | /************************************************************
datanode_4          | 2023-06-29 21:45:36,460 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-29 21:45:36,466 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-EECA45441CF9-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/9ebf2f12-ce42-45a9-9d0f-eeca45441cf9/current/log_inprogress_0
s3g_1               | STARTUP_MSG: Starting Gateway
datanode_4          | 2023-06-29 21:45:39,083 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: receive requestVote(PRE_VOTE, 3ea36174-d662-4e6c-bd1c-d614f11906cb, group-B9087FDFF180, 0, (t:0, i:0))
om_1                | 2023-06-29 21:45:47,168 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:44:56,945 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: ConfigurationManager, init=-1: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 2023-06-29 21:45:09,711 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1               | STARTUP_MSG:   host = 559053ef04f9/172.24.0.8
s3g_1               | STARTUP_MSG:   args = []
scm_1               | 2023-06-29 21:44:56,945 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm_1               | 2023-06-29 21:44:57,033 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 2023-06-29 21:45:09,713 [main] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
s3g_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
om_1                | 2023-06-29 21:45:47,177 [main] INFO server.RaftServer: om1: start RPC server
scm_1               | 2023-06-29 21:44:57,034 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm_1               | 2023-06-29 21:44:57,360 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
s3g_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.51.1.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.54.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.51.1.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/weld-servlet-shaded-3.1.9.Final.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.51.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/cdi-api-2.0.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.51.1.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar
s3g_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
s3g_1               | STARTUP_MSG:   java = 11.0.19
s3g_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.du.refresh.period=1h, hdds.datanode.handler.count=1, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.basedir=/opt/hadoop/ozone_s3g_tmp_base_dir15590505455235115386, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.client.rpc.timeout=15m, ozone.om.client.trash.core.pool.size=5, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.group.rights=ALL, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.port=8981, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.init.default.layout.version=-1, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.finalization.ratis.based.timeout=30s, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.user.rights=ALL, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
om_1                | 2023-06-29 21:45:47,310 [main] INFO server.GrpcService: om1: GrpcService started, listening on 9872
scm_1               | 2023-06-29 21:44:57,417 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 2023-06-29 21:45:09,713 [main] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
s3g_1               | ************************************************************/
s3g_1               | 2023-06-29 21:44:32,769 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
scm_1               | 2023-06-29 21:44:57,506 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
recon_1             | 2023-06-29 21:45:09,718 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
s3g_1               | 2023-06-29 21:44:32,865 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1               | 2023-06-29 21:44:33,572 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om_1                | 2023-06-29 21:45:47,322 [main] INFO om.OzoneManager: Version File has different layout version (6) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm_1               | 2023-06-29 21:44:57,578 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
s3g_1               | 2023-06-29 21:44:34,714 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
datanode_4          | 2023-06-29 21:45:39,083 [grpc-default-executor-1] INFO impl.VoteContext: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FOLLOWER: accept PRE_VOTE from 3ea36174-d662-4e6c-bd1c-d614f11906cb: our priority 0 <= candidate's priority 0
om_1                | 2023-06-29 21:45:47,327 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
scm_1               | 2023-06-29 21:44:57,969 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
s3g_1               | 2023-06-29 21:44:34,714 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1               | 2023-06-29 21:44:35,072 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-S3G: Started
om_1                | 2023-06-29 21:45:47,441 [main] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm_1               | 2023-06-29 21:44:58,072 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
s3g_1               | 2023-06-29 21:44:35,117 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1               | 2023-06-29 21:44:35,118 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
om_1                | 2023-06-29 21:45:47,442 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:44:59,739 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
datanode_4          | 2023-06-29 21:45:39,084 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180 replies to PRE_VOTE vote request: 3ea36174-d662-4e6c-bd1c-d614f11906cb<-1fef6398-5e0e-4531-8b14-70177bcfa866#0:OK-t0. Peer's state: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180:t0, leader=null, voted=, raftlog=Memoized:1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1               | 2023-06-29 21:44:35,358 [main] INFO server.session: DefaultSessionIdManager workerName=node0
om_1                | 2023-06-29 21:45:47,492 [main] INFO util.log: Logging initialized @25180ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:44:59,920 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1             | 2023-06-29 21:45:09,721 [main] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
datanode_4          | 2023-06-29 21:45:40,219 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: receive requestVote(PRE_VOTE, 5ffc81fa-0a37-43e0-be73-58283f7df8c3, group-B9087FDFF180, 0, (t:0, i:0))
s3g_1               | 2023-06-29 21:44:35,374 [main] INFO server.session: No SessionScavenger set, using defaults
om_1                | 2023-06-29 21:45:47,697 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-29 21:44:59,920 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 2023-06-29 21:45:09,721 [main] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
datanode_4          | 2023-06-29 21:45:40,219 [grpc-default-executor-1] INFO impl.VoteContext: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FOLLOWER: accept PRE_VOTE from 5ffc81fa-0a37-43e0-be73-58283f7df8c3: our priority 0 <= candidate's priority 1
s3g_1               | 2023-06-29 21:44:35,385 [main] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-29 21:45:47,716 [main] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm_1               | 2023-06-29 21:44:59,924 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1             | 2023-06-29 21:45:09,721 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
datanode_4          | 2023-06-29 21:45:40,219 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180 replies to PRE_VOTE vote request: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-1fef6398-5e0e-4531-8b14-70177bcfa866#0:OK-t0. Peer's state: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180:t0, leader=null, voted=, raftlog=Memoized:1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
datanode_4          | 2023-06-29 21:45:40,300 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: receive requestVote(ELECTION, 5ffc81fa-0a37-43e0-be73-58283f7df8c3, group-B9087FDFF180, 1, (t:0, i:0))
om_1                | 2023-06-29 21:45:47,745 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1             | 2023-06-29 21:45:09,722 [main] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
datanode_4          | 2023-06-29 21:45:40,301 [grpc-default-executor-1] INFO impl.VoteContext: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FOLLOWER: accept ELECTION from 5ffc81fa-0a37-43e0-be73-58283f7df8c3: our priority 0 <= candidate's priority 1
datanode_4          | 2023-06-29 21:45:40,302 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_4          | 2023-06-29 21:45:40,302 [grpc-default-executor-1] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: shutdown 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState
recon_1             | 2023-06-29 21:45:09,722 [main] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1             | 2023-06-29 21:45:11,850 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d379c9c1f103/172.24.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 1.
recon_1             | 2023-06-29 21:45:13,852 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From d379c9c1f103/172.24.0.2 to scm:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 2.
recon_1             | 2023-06-29 21:45:16,769 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1cd49d86-1570-4113-bfd8-4673b0280c86 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
datanode_4          | 2023-06-29 21:45:40,302 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState] INFO impl.FollowerState: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState was interrupted
datanode_4          | 2023-06-29 21:45:40,306 [grpc-default-executor-1] INFO impl.RoleInfo: 1fef6398-5e0e-4531-8b14-70177bcfa866: start 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-FollowerState
datanode_4          | 2023-06-29 21:45:40,326 [grpc-default-executor-1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180 replies to ELECTION vote request: 5ffc81fa-0a37-43e0-be73-58283f7df8c3<-1fef6398-5e0e-4531-8b14-70177bcfa866#0:OK-t1. Peer's state: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180:t1, leader=null, voted=5ffc81fa-0a37-43e0-be73-58283f7df8c3, raftlog=Memoized:1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
scm_1               | 2023-06-29 21:44:59,924 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
s3g_1               | 2023-06-29 21:44:35,518 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@d816dde{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1               | 2023-06-29 21:44:35,520 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2a7b6f69{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
s3g_1               | 2023-06-29 21:44:40,237 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-S3G: Detected pause in JVM or host machine approximately 0.155s with 0.176s GC time.
s3g_1               | GC pool 'ParNew' had collection(s): count=1 time=176ms
s3g_1               | 2023-06-29 21:45:00,258 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@45f1413c{s3gateway,/,file:///opt/hadoop/ozone_s3g_tmp_base_dir15590505455235115386/jetty-0_0_0_0-9878-ozone-s3gateway-1_4_0-SNAPSHOT_jar-_-any-16735580903349226941/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.4.0-SNAPSHOT.jar!/webapps/s3gateway}
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
s3g_1               | 2023-06-29 21:45:00,394 [main] INFO server.AbstractConnector: Started ServerConnector@32c0915e{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1               | 2023-06-29 21:45:00,395 [main] INFO server.Server: Started @41381ms
s3g_1               | 2023-06-29 21:45:00,420 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om_1                | 2023-06-29 21:45:47,749 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
datanode_4          | 2023-06-29 21:45:40,644 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-B9087FDFF180 with new leaderId: 5ffc81fa-0a37-43e0-be73-58283f7df8c3
datanode_4          | 2023-06-29 21:45:40,644 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: change Leader from null to 5ffc81fa-0a37-43e0-be73-58283f7df8c3 at term 1 for appendEntries, leader elected after 5305ms
datanode_4          | 2023-06-29 21:45:40,644 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO server.RaftServer$Division: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180: set configuration 0: peers:[5ffc81fa-0a37-43e0-be73-58283f7df8c3|rpc:172.24.0.13:9856|admin:172.24.0.13:9857|client:172.24.0.13:9858|dataStream:172.24.0.13:9858|priority:1|startupRole:FOLLOWER, 1fef6398-5e0e-4531-8b14-70177bcfa866|rpc:172.24.0.12:9856|admin:172.24.0.12:9857|client:172.24.0.12:9858|dataStream:172.24.0.12:9858|priority:0|startupRole:FOLLOWER, 3ea36174-d662-4e6c-bd1c-d614f11906cb|rpc:172.24.0.15:9856|admin:172.24.0.15:9857|client:172.24.0.15:9858|dataStream:172.24.0.15:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
s3g_1               | 2023-06-29 21:45:00,420 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
datanode_4          | 2023-06-29 21:45:40,645 [1fef6398-5e0e-4531-8b14-70177bcfa866-server-thread1] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker: Starting segment from index:0
datanode_4          | 2023-06-29 21:45:40,646 [1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1fef6398-5e0e-4531-8b14-70177bcfa866@group-B9087FDFF180-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/dfd3fd7b-c7da-4f01-8c26-b9087fdff180/current/log_inprogress_0
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 3.
recon_1             | 2023-06-29 21:45:18,773 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1cd49d86-1570-4113-bfd8-4673b0280c86 is not the leader. Could not determine the leader node.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
s3g_1               | 2023-06-29 21:45:00,434 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
datanode_4          | 2023-06-29 21:46:19,332 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-1fef6398-5e0e-4531-8b14-70177bcfa866: Detected pause in JVM or host machine approximately 0.298s with 0.347s GC time.
datanode_4          | GC pool 'ParNew' had collection(s): count=1 time=347ms
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
datanode_4          | 2023-06-29 21:46:22,562 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-29 21:47:22,563 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-29 21:48:22,564 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-29 21:44:59,929 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:44:59,952 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef does not exist. Creating ...
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 4.
recon_1             | 2023-06-29 21:45:20,776 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:1cd49d86-1570-4113-bfd8-4673b0280c86 is not the leader. Could not determine the leader node.
scm_1               | 2023-06-29 21:45:00,101 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/in_use.lock acquired by nodename 14@68c14f390c32
scm_1               | 2023-06-29 21:45:00,183 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef has been successfully formatted.
recon_1             | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1             | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:249)
om_1                | 2023-06-29 21:45:47,752 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om_1                | 2023-06-29 21:45:47,752 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om_1                | 2023-06-29 21:45:47,856 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager uses base directory /data/metadata/webserver
scm_1               | 2023-06-29 21:45:00,300 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
datanode_4          | 2023-06-29 21:49:22,564 [BlockDeletingService#2] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-06-29 21:45:47,865 [main] INFO http.HttpServer2: Jetty bound to port 9874
om_1                | 2023-06-29 21:45:47,867 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
recon_1             | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:199)
om_1                | 2023-06-29 21:45:47,953 [main] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1             | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java)
datanode_4          | 2023-06-29 21:50:22,565 [BlockDeletingService#1] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
datanode_4          | 2023-06-29 21:51:22,565 [BlockDeletingService#3] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
om_1                | 2023-06-29 21:45:47,953 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:45:00,528 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:45:00,528 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:45:00,537 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:484)
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:595)
om_1                | 2023-06-29 21:45:48,002 [main] INFO server.session: node0 Scavenging every 660000ms
om_1                | 2023-06-29 21:45:48,046 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@47b67eac{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om_1                | 2023-06-29 21:45:48,049 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@206769f8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
datanode_4          | 2023-06-29 21:52:22,566 [BlockDeletingService#0] INFO interfaces.ContainerDeletionChoosingPolicyTemplate: Chosen 0/5000 blocks from 0 candidate containers.
recon_1             | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:573)
scm_1               | 2023-06-29 21:45:00,584 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:45:00,628 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1227)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1094)
scm_1               | 2023-06-29 21:45:00,764 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:45:00,765 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-29 21:45:00,797 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:45:00,896 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-29 21:45:00,924 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:45:48,356 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5c0f82f{ozoneManager,/,file:///data/metadata/webserver/jetty-0_0_0_0-9874-ozone-manager-1_4_0-SNAPSHOT_jar-_-any-2615007568055658763/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.4.0-SNAPSHOT.jar!/webapps/ozoneManager}
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | , while invoking $Proxy43.submitRequest over nodeId=scmNodeId,nodeAddress=scm/172.24.0.11:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms. Current retry count: 5.
recon_1             | 2023-06-29 21:45:24,171 [main] INFO scm.ReconStorageContainerManagerFacade: Obtained 1 pipelines from SCM.
om_1                | 2023-06-29 21:45:48,377 [main] INFO server.AbstractConnector: Started ServerConnector@46a04668{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om_1                | 2023-06-29 21:45:48,380 [main] INFO server.Server: Started @26067ms
recon_1             | 2023-06-29 21:45:24,172 [main] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
om_1                | 2023-06-29 21:45:48,391 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:45:00,944 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
recon_1             | 2023-06-29 21:45:24,182 [main] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=8a001e4a-d312-430e-b38d-510e705e518c from SCM.
om_1                | 2023-06-29 21:45:48,391 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:45:00,952 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
recon_1             | 2023-06-29 21:45:25,021 [main] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
om_1                | 2023-06-29 21:45:48,393 [main] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
scm_1               | 2023-06-29 21:45:00,964 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
recon_1             | 2023-06-29 21:45:25,031 [main] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
om_1                | 2023-06-29 21:45:48,399 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:45:00,987 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:45:00,996 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om_1                | 2023-06-29 21:45:48,480 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm_1               | 2023-06-29 21:45:00,998 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
recon_1             | 2023-06-29 21:45:25,045 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om_1                | 2023-06-29 21:45:48,524 [main] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
scm_1               | 2023-06-29 21:45:00,999 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
recon_1             | 2023-06-29 21:45:25,210 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
om_1                | 2023-06-29 21:45:48,777 [main] INFO om.GrpcOzoneManagerServer: GrpcOzoneManagerServer is started using port 8981
scm_1               | 2023-06-29 21:45:01,172 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:58842 / 172.24.0.7:58842
om_1                | 2023-06-29 21:45:52,339 [om1@group-C5BA1605619E-FollowerState] INFO impl.FollowerState: om1@group-C5BA1605619E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5185933006ns, electionTimeout:5170ms
scm_1               | 2023-06-29 21:45:01,225 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:45154 / 172.24.0.12:45154
om_1                | 2023-06-29 21:45:52,341 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-FollowerState
om_1                | 2023-06-29 21:45:52,341 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891: skipped Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:42286 / 172.24.0.13:42286
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:56600 / 172.24.0.13:56600
scm_1               | 2023-06-29 21:45:01,471 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:42640 / 172.24.0.15:42640
om_1                | 2023-06-29 21:45:52,347 [om1@group-C5BA1605619E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-29 21:45:01,502 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
recon_1             | 2023-06-29 21:45:25,420 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:56592 / 172.24.0.13:56592
om_1                | 2023-06-29 21:45:52,347 [om1@group-C5BA1605619E-FollowerState] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderElection1
scm_1               | 2023-06-29 21:45:01,568 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1             | 2023-06-29 21:45:25,421 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:56954 / 172.24.0.10:56954
om_1                | 2023-06-29 21:45:52,353 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-29 21:45:25,421 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891: skipped Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:45170 / 172.24.0.12:45170
scm_1               | 2023-06-29 21:45:01,714 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:45:52,354 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
recon_1             | 2023-06-29 21:45:25,421 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891: skipped Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:42626 / 172.24.0.15:42626
scm_1               | 2023-06-29 21:45:01,715 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om_1                | 2023-06-29 21:45:52,356 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 2023-06-29 21:45:25,421 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891: skipped Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:35816 / 172.24.0.10:35816
scm_1               | 2023-06-29 21:45:01,756 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: start as a follower, conf=-1: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:45:52,356 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.LeaderElection: om1@group-C5BA1605619E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1             | 2023-06-29 21:45:26,637 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:38620 / 172.24.0.15:38620: output error
scm_1               | 2023-06-29 21:45:01,801 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from      null to FOLLOWER at term 0 for startAsFollower
om_1                | 2023-06-29 21:45:52,357 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-C5BA1605619E-LeaderElection1
recon_1             | 2023-06-29 21:45:26,639 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:56964 / 172.24.0.10:56964: output error
scm_1               | 2023-06-29 21:45:01,816 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState
om_1                | 2023-06-29 21:45:52,357 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1             | 2023-06-29 21:45:26,640 [IPC Server handler 12 on default port 9891] WARN ipc.Server: IPC Server handler 12 on default port 9891, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:58846 / 172.24.0.7:58846: output error
recon_1             | 2023-06-29 21:45:26,642 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:40514 / 172.24.0.7:40514: output error
om_1                | 2023-06-29 21:45:52,364 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 9718ms
scm_1               | 2023-06-29 21:45:01,872 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
recon_1             | 2023-06-29 21:45:26,643 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:35550 / 172.24.0.12:35550: output error
om_1                | 2023-06-29 21:45:52,370 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:45:01,881 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1             | 2023-06-29 21:45:26,645 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
om_1                | 2023-06-29 21:45:52,375 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:45:01,896 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-596B1957BDEF,id=1cd49d86-1570-4113-bfd8-4673b0280c86
recon_1             | java.nio.channels.ClosedChannelException
om_1                | 2023-06-29 21:45:52,376 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:45:01,922 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om_1                | 2023-06-29 21:45:52,380 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:45:01,941 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-06-29 21:45:52,381 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:45:01,971 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
om_1                | 2023-06-29 21:45:52,382 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om_1                | 2023-06-29 21:45:52,388 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
om_1                | 2023-06-29 21:45:52,389 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-06-29 21:45:01,972 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om_1                | 2023-06-29 21:45:52,391 [om1@group-C5BA1605619E-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-C5BA1605619E-LeaderStateImpl
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-06-29 21:45:02,040 [main] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: start RPC server
om_1                | 2023-06-29 21:45:52,408 [om1@group-C5BA1605619E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-06-29 21:45:02,741 [main] INFO server.GrpcService: 1cd49d86-1570-4113-bfd8-4673b0280c86: GrpcService started, listening on 9894
om_1                | 2023-06-29 21:45:52,433 [om1@group-C5BA1605619E-LeaderElection1] INFO server.RaftServer$Division: om1@group-C5BA1605619E: set configuration 0: peers:[om1|rpc:om:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-06-29 21:45:02,838 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1cd49d86-1570-4113-bfd8-4673b0280c86: Started
om_1                | 2023-06-29 21:45:52,531 [om1@group-C5BA1605619E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-C5BA1605619E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/bf265839-605b-3f16-9796-c5ba1605619e/current/log_inprogress_0
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-29 21:45:06,925 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.FollowerState: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5109551325ns, electionTimeout:5043ms
om_1                | 2023-06-29 21:45:52,648 [om1@group-C5BA1605619E-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-29 21:45:06,929 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState
om_1                | [id: "om1"
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
om_1                | address: "om:9872"
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-29 21:45:06,930 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om_1                | startupRole: FOLLOWER
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-29 21:45:06,937 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
om_1                | ]
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-29 21:45:06,938 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1
om_1                | 2023-06-29 21:45:54,658 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:volvhw1a for user:hadoop
om_1                | 2023-06-29 21:45:59,794 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: default of layout LEGACY in volume: volvhw1a
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:06,944 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 0 for -1: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om_1                | 2023-06-29 21:46:05,040 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ratis of layout LEGACY in volume: volvhw1a
recon_1             | 2023-06-29 21:45:26,648 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
scm_1               | 2023-06-29 21:45:06,945 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 PRE_VOTE round 0: result PASSED (term=0)
om_1                | 2023-06-29 21:46:09,977 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: ecbucket of layout LEGACY in volume: volvhw1a
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-06-29 21:45:06,948 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:06,948 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 ELECTION round 0: result PASSED (term=1)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om_1                | 2023-06-29 21:46:10,648 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Received GET request to obtain DB checkpoint snapshot
scm_1               | 2023-06-29 21:45:06,948 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om_1                | 2023-06-29 21:46:10,685 [qtp1966802439-54] INFO db.RDBCheckpointManager: Created checkpoint in rocksDB at /data/metadata/db.checkpoints/om.db_checkpoint_1688075170652 in 32 milliseconds
scm_1               | 2023-06-29 21:45:06,949 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
om_1                | 2023-06-29 21:46:10,721 [qtp1966802439-54] INFO db.RDBCheckpointUtils: Waited for 33 milliseconds for checkpoint directory /data/metadata/db.checkpoints/om.db_checkpoint_1688075170652 availability.
scm_1               | 2023-06-29 21:45:06,949 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: change Leader from null to 1cd49d86-1570-4113-bfd8-4673b0280c86 at term 1 for becomeLeader, leader elected after 9598ms
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-06-29 21:45:06,957 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om_1                | 2023-06-29 21:46:11,026 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 303 milliseconds
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-06-29 21:45:06,960 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om_1                | 2023-06-29 21:46:11,026 [qtp1966802439-54] INFO utils.DBCheckpointServlet: Excluded SST [] from the latest checkpoint.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-06-29 21:45:06,961 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om_1                | 2023-06-29 21:46:11,026 [qtp1966802439-54] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688075170652
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-06-29 21:45:06,969 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-06-29 21:45:06,969 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-06-29 21:45:06,970 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-29 21:45:07,086 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-29 21:45:07,088 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-06-29 21:45:07,089 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderStateImpl
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-06-29 21:45:07,145 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: Starting segment from index:0
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-29 21:45:07,215 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: set configuration 0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-29 21:45:07,341 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/log_inprogress_0
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-29 21:45:08,786 [main] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: close
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:08,786 [main] INFO server.GrpcService: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown server GrpcServerProtocolService now
recon_1             | 2023-06-29 21:45:26,648 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
scm_1               | 2023-06-29 21:45:08,787 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: shutdown
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-06-29 21:45:08,788 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-596B1957BDEF,id=1cd49d86-1570-4113-bfd8-4673b0280c86
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 2023-06-29 21:45:08,788 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderStateImpl
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-06-29 21:45:08,797 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO impl.PendingRequests: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-PendingRequests: sendNotLeaderResponses
scm_1               | 2023-06-29 21:45:08,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO impl.StateMachineUpdater: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater: Took a snapshot at index 0
scm_1               | 2023-06-29 21:45:08,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO impl.StateMachineUpdater: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm_1               | 2023-06-29 21:45:08,818 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO impl.StateMachineUpdater: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater: set stopIndex = 0
scm_1               | 2023-06-29 21:45:08,821 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: closes. applyIndex: 0
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-06-29 21:45:08,826 [main] INFO server.GrpcService: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown server GrpcServerProtocolService successfully
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-06-29 21:45:09,359 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker close()
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-06-29 21:45:09,360 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1cd49d86-1570-4113-bfd8-4673b0280c86: Stopped
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-06-29 21:45:09,361 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-06-29 21:45:09,364 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-2aa45680-139b-4449-b07e-596b1957bdef; layoutVersion=7; scmId=1cd49d86-1570-4113-bfd8-4673b0280c86
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-06-29 21:45:09,369 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | /************************************************************
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | SHUTDOWN_MSG: Shutting down StorageContainerManager at 68c14f390c32/172.24.0.11
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | ************************************************************/
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-29 21:45:11,210 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | /************************************************************
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
recon_1             | 2023-06-29 21:45:26,648 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1             | java.nio.channels.ClosedChannelException
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | STARTUP_MSG: Starting StorageContainerManager
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | STARTUP_MSG:   host = 68c14f390c32/172.24.0.11
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | STARTUP_MSG:   args = []
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | STARTUP_MSG:   version = 1.4.0-SNAPSHOT
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/hdds-rocks-native-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/commons-net-3.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.15.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-managed-rocksdb-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.51.1.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.5.1.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/commons-text-1.10.0.jar:/opt/hadoop/share/ozone/lib/snakeyaml-2.0.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/zstd-jni-1.5.2-5.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/hamcrest-2.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.5.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.5.1.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.5.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.4.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/awaitility-4.2.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.33.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/ozone/lib/rocksdb-checkpoint-differ-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.7.3.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/guava-32.0.0-jre.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.5.1.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.5.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.6.21.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.51.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.86.Final.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-2.8.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.5.1.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.5.1.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.4.0-SNAPSHOT.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/commons-fileupload-1.5.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | STARTUP_MSG:   build = https://github.com/apache/ozone/1f4c917f3d0f28a19c9b0103d2d5f77566622cf9 ; compiled by 'runner' on 2023-06-29T20:40Z
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | STARTUP_MSG:   java = 11.0.19
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | STARTUP_MSG:   conf = {dfs.container.chunk.write.sync=false, dfs.container.ipc=9859, dfs.container.ipc.random.port=false, dfs.container.ratis.admin.port=9857, dfs.container.ratis.datastream.enabled=false, dfs.container.ratis.datastream.port=9855, dfs.container.ratis.datastream.random.port=false, dfs.container.ratis.enabled=false, dfs.container.ratis.ipc=9858, dfs.container.ratis.ipc.random.port=false, dfs.container.ratis.leader.pending.bytes.limit=1GB, dfs.container.ratis.log.appender.queue.byte-limit=32MB, dfs.container.ratis.log.appender.queue.num-elements=1, dfs.container.ratis.log.purge.gap=1000000, dfs.container.ratis.log.queue.byte-limit=4GB, dfs.container.ratis.log.queue.num-elements=1024, dfs.container.ratis.num.container.op.executors=10, dfs.container.ratis.num.write.chunk.threads.per.volume=10, dfs.container.ratis.replication.level=MAJORITY, dfs.container.ratis.rpc.type=GRPC, dfs.container.ratis.segment.preallocated.size=16KB, dfs.container.ratis.segment.size=1MB, dfs.container.ratis.server.port=9856, dfs.container.ratis.statemachine.max.pending.apply-transactions=10000, dfs.container.ratis.statemachinedata.sync.retries=-1, dfs.container.ratis.statemachinedata.sync.timeout=10s, dfs.ratis.leader.election.minimum.timeout.duration=5s, dfs.ratis.server.retry-cache.timeout.duration=600000ms, dfs.ratis.snapshot.threshold=10000, hadoop.hdds.db.rocksdb.WAL_size_limit_MB=0MB, hadoop.hdds.db.rocksdb.WAL_ttl_seconds=1200, hadoop.hdds.db.rocksdb.logging.enabled=false, hadoop.hdds.db.rocksdb.logging.level=INFO, hadoop.hdds.db.rocksdb.writeoption.sync=false, hdds.block.token.enabled=false, hdds.block.token.expiry.time=1d, hdds.command.status.report.interval=30s, hdds.container.action.max.limit=20, hdds.container.balancer.balancing.iteration.interval=70m, hdds.container.balancer.datanodes.involved.max.percentage.per.iteration=20, hdds.container.balancer.iterations=10, hdds.container.balancer.move.networkTopology.enable=false, hdds.container.balancer.move.replication.timeout=50m, hdds.container.balancer.move.timeout=65m, hdds.container.balancer.size.entering.target.max=26GB, hdds.container.balancer.size.leaving.source.max=26GB, hdds.container.balancer.size.moved.max.per.iteration=500GB, hdds.container.balancer.trigger.du.before.move.enable=false, hdds.container.balancer.utilization.threshold=10, hdds.container.checksum.verification.enabled=true, hdds.container.close.threshold=0.9f, hdds.container.replication.compression=NO_COMPRESSION, hdds.container.report.interval=60m, hdds.container.scrub.data.scan.interval=7d, hdds.container.scrub.dev.data.scan.enabled=true, hdds.container.scrub.dev.metadata.scan.enabled=true, hdds.container.scrub.enabled=false, hdds.container.scrub.metadata.scan.interval=3h, hdds.container.scrub.min.gap=15m, hdds.container.scrub.on.demand.volume.bytes.per.second=5242880, hdds.container.scrub.volume.bytes.per.second=5242880, hdds.container.token.enabled=false, hdds.crl.status.report.interval=60000ms, hdds.datanode.block.delete.queue.limit=5, hdds.datanode.block.delete.threads.max=5, hdds.datanode.block.deleting.limit.per.interval=5000, hdds.datanode.block.deleting.service.interval=60s, hdds.datanode.chunk.data.validation.check=false, hdds.datanode.client.bind.host=0.0.0.0, hdds.datanode.client.port=9864, hdds.datanode.command.queue.limit=5000, hdds.datanode.container.delete.threads.max=2, hdds.datanode.container.schema.v3.enabled=true, hdds.datanode.container.schema.v3.key.separator=|, hdds.datanode.df.refresh.period=5m, hdds.datanode.dir=/data/hdds, hdds.datanode.disk.check.file.size=100B, hdds.datanode.disk.check.io.failures.tolerated=1, hdds.datanode.disk.check.io.test.count=3, hdds.datanode.disk.check.min.gap=10m, hdds.datanode.disk.check.timeout=10m, hdds.datanode.du.refresh.period=1h, hdds.datanode.failed.data.volumes.tolerated=-1, hdds.datanode.failed.db.volumes.tolerated=-1, hdds.datanode.failed.metadata.volumes.tolerated=-1, hdds.datanode.handler.count=1, hdds.datanode.hdds.datanode.check.empty.container.dir.on.delete=false, hdds.datanode.http-address=0.0.0.0:9882, hdds.datanode.http-bind-host=0.0.0.0, hdds.datanode.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.datanode.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.datanode.http.auth.type=simple, hdds.datanode.http.enabled=true, hdds.datanode.https-address=0.0.0.0:9883, hdds.datanode.https-bind-host=0.0.0.0, hdds.datanode.metadata.rocksdb.cache.size=64MB, hdds.datanode.periodic.disk.check.interval.minutes=60, hdds.datanode.ratis.server.request.timeout=2m, hdds.datanode.read.chunk.threads.per.volume=10, hdds.datanode.recovering.container.scrubbing.service.interval=1m, hdds.datanode.replication.outofservice.limit.factor=2.0, hdds.datanode.replication.port=9886, hdds.datanode.replication.streams.limit=10, hdds.datanode.rocksdb.auto-compaction-small-sst-file=true, hdds.datanode.rocksdb.auto-compaction-small-sst-file-num-threshold=512, hdds.datanode.rocksdb.auto-compaction-small-sst-file-size-threshold=1MB, hdds.datanode.rocksdb.delete-obsolete-files-period=1h, hdds.datanode.rocksdb.log.level=INFO, hdds.datanode.rocksdb.log.max-file-num=64, hdds.datanode.rocksdb.log.max-file-size=32MB, hdds.datanode.rocksdb.max-open-files=1024, hdds.datanode.storage.utilization.critical.threshold=0.95, hdds.datanode.storage.utilization.warning.threshold=0.75, hdds.datanode.volume.min.free.space=5GB, hdds.datanode.wait.on.all.followers=false, hdds.db.profile=DISK, hdds.grpc.tls.enabled=false, hdds.grpc.tls.provider=OPENSSL, hdds.heartbeat.interval=30s, hdds.key.dir.name=keys, hdds.key.len=2048, hdds.node.report.interval=60000ms, hdds.pipeline.action.max.limit=20, hdds.pipeline.report.interval=60000ms, hdds.priv.key.file.name=private.pem, hdds.profiler.endpoint.enabled=false, hdds.prometheus.endpoint.enabled=true, hdds.public.key.file.name=public.pem, hdds.ratis.client.exponential.backoff.base.sleep=4s, hdds.ratis.client.exponential.backoff.max.sleep=40s, hdds.ratis.client.multilinear.random.retry.policy=5s, 5, 10s, 5, 15s, 5, 20s, 5, 25s, 5, 60s, 10, hdds.ratis.client.request.watch.timeout=3m, hdds.ratis.client.request.write.timeout=5m, hdds.ratis.client.retry.policy=org.apache.hadoop.hdds.ratis.retrypolicy.RequestTypeDependentRetryPolicyCreator, hdds.ratis.client.retrylimited.max.retries=180, hdds.ratis.client.retrylimited.retry.interval=1s, hdds.ratis.raft.client.async.outstanding-requests.max=32, hdds.ratis.raft.client.rpc.request.timeout=60s, hdds.ratis.raft.client.rpc.watch.request.timeout=180s, hdds.ratis.raft.grpc.flow.control.window=5MB, hdds.ratis.raft.grpc.message.size.max=32MB, hdds.ratis.raft.server.datastream.client.pool.size=10, hdds.ratis.raft.server.datastream.request.threads=20, hdds.ratis.raft.server.delete.ratis.log.directory=true, hdds.ratis.raft.server.leaderelection.pre-vote=true, hdds.ratis.raft.server.notification.no-leader.timeout=300s, hdds.ratis.raft.server.rpc.request.timeout=60s, hdds.ratis.raft.server.rpc.slowness.timeout=300s, hdds.ratis.raft.server.watch.timeout=180s, hdds.ratis.raft.server.write.element-limit=1024, hdds.ratis.server.num.snapshots.retained=5, hdds.recon.heartbeat.interval=60s, hdds.rest.http-address=0.0.0.0:9880, hdds.rest.netty.high.watermark=65535, hdds.rest.netty.low.watermark=32768, hdds.rest.rest-csrf.enabled=false, hdds.scm.block.deleting.service.interval=60s, hdds.scm.block.deletion.per-interval.max=100000, hdds.scm.ec.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, hdds.scm.http.auth.kerberos.principal=HTTP/_HOST@REALM, hdds.scm.http.auth.type=simple, hdds.scm.init.default.layout.version=-1, hdds.scm.kerberos.keytab.file=/etc/security/keytabs/SCM.keytab, hdds.scm.kerberos.principal=SCM/_HOST@REALM, hdds.scm.pipeline.choose.policy.impl=org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy, hdds.scm.replication.container.inflight.deletion.limit=0, hdds.scm.replication.container.inflight.replication.limit=0, hdds.scm.replication.datanode.delete.container.limit=40, hdds.scm.replication.datanode.reconstruction.weight=3, hdds.scm.replication.datanode.replication.limit=20, hdds.scm.replication.enable.legacy=false, hdds.scm.replication.event.timeout=10m, hdds.scm.replication.event.timeout.datanode.offset=30s, hdds.scm.replication.inflight.limit.factor=0.75, hdds.scm.replication.maintenance.remaining.redundancy=1, hdds.scm.replication.maintenance.replica.minimum=2, hdds.scm.replication.over.replicated.interval=30s, hdds.scm.replication.push=true, hdds.scm.replication.thread.interval=300s, hdds.scm.replication.under.replicated.interval=30s, hdds.scm.safemode.atleast.one.node.reported.pipeline.pct=0.90, hdds.scm.safemode.enabled=true, hdds.scm.safemode.healthy.pipeline.pct=0.10, hdds.scm.safemode.min.datanode=3, hdds.scm.safemode.pipeline-availability.check=true, hdds.scm.safemode.pipeline.creation=true, hdds.scm.safemode.threshold.pct=0.99, hdds.scm.unknown-container.action=WARN, hdds.scm.wait.time.after.safemode.exit=5m, hdds.scmclient.failover.max.retry=15, hdds.scmclient.failover.retry.interval=2s, hdds.scmclient.max.retry.timeout=30s, hdds.scmclient.rpc.timeout=15m, hdds.secret.key.algorithm=HmacSHA256, hdds.secret.key.expiry.duration=7d, hdds.secret.key.file.name=secret_keys.json, hdds.secret.key.rotate.check.duration=10m, hdds.secret.key.rotate.duration=1d, hdds.security.client.datanode.container.protocol.acl=*, hdds.security.client.scm.block.protocol.acl=*, hdds.security.client.scm.certificate.protocol.acl=*, hdds.security.client.scm.container.protocol.acl=*, hdds.security.client.scm.secretkey.datanode.protocol.acl=*, hdds.security.client.scm.secretkey.om.protocol.acl=*, hdds.security.client.scm.secretkey.scm.protocol.acl=*, hdds.tracing.enabled=false, hdds.x509.ca.rotation.check.interval=P1D, hdds.x509.ca.rotation.time-of-day=02:00:00, hdds.x509.default.duration=P365D, hdds.x509.dir.name=certs, hdds.x509.file.name=certificate.crt, hdds.x509.max.duration=P1865D, hdds.x509.renew.grace.duration=P28D, hdds.x509.signature.algorithm=SHA256withRSA, ozone.UnsafeByteOperations.enabled=true, ozone.acl.authorizer.class=org.apache.hadoop.ozone.security.acl.OzoneAccessAuthorizer, ozone.acl.enabled=false, ozone.block.deleting.container.limit.per.interval=10, ozone.block.deleting.limit.per.task=1000, ozone.block.deleting.service.interval=1m, ozone.block.deleting.service.timeout=300000ms, ozone.block.deleting.service.workers=10, ozone.chunk.read.buffer.default.size=64KB, ozone.client.bucket.replication.config.refresh.time.ms=30000, ozone.client.bytes.per.checksum=1MB, ozone.client.checksum.combine.mode=COMPOSITE_CRC, ozone.client.checksum.type=CRC32, ozone.client.connection.timeout=5000ms, ozone.client.datastream.buffer.flush.size=16MB, ozone.client.datastream.min.packet.size=1MB, ozone.client.datastream.pipeline.mode=true, ozone.client.datastream.window.size=64MB, ozone.client.ec.grpc.retries.enabled=true, ozone.client.ec.grpc.retries.max=3, ozone.client.ec.reconstruct.stripe.read.pool.limit=30, ozone.client.ec.stripe.queue.size=2, ozone.client.exclude.nodes.expiry.time=600000, ozone.client.failover.max.attempts=500, ozone.client.fs.default.bucket.layout=FILE_SYSTEM_OPTIMIZED, ozone.client.key.latest.version.location=true, ozone.client.key.provider.cache.expiry=10d, ozone.client.list.cache=1000, ozone.client.list.trash.keys.max=1000, ozone.client.max.ec.stripe.write.retries=10, ozone.client.max.retries=5, ozone.client.read.timeout=30s, ozone.client.retry.interval=0, ozone.client.socket.timeout=5000ms, ozone.client.stream.buffer.flush.delay=true, ozone.client.stream.buffer.flush.size=16MB, ozone.client.stream.buffer.increment=0B, ozone.client.stream.buffer.max.size=32MB, ozone.client.stream.buffer.size=4MB, ozone.client.verify.checksum=true, ozone.client.wait.between.retries.millis=2000, ozone.container.cache.lock.stripes=1024, ozone.container.cache.size=1024, ozone.default.bucket.layout=LEGACY, ozone.directory.deleting.service.interval=1m, ozone.filesystem.snapshot.enabled=true, ozone.freon.http-address=0.0.0.0:9884, ozone.freon.http-bind-host=0.0.0.0, ozone.freon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.freon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.freon.http.auth.type=simple, ozone.freon.http.enabled=true, ozone.freon.https-address=0.0.0.0:9885, ozone.freon.https-bind-host=0.0.0.0, ozone.fs.datastream.auto.threshold=4MB, ozone.fs.datastream.enabled=false, ozone.fs.hsync.enabled=false, ozone.fs.iterate.batch-size=100, ozone.fs.listing.page.size=1024, ozone.fs.listing.page.size.max=5000, ozone.http.policy=HTTP_ONLY, ozone.https.client.keystore.resource=ssl-client.xml, ozone.https.client.need-auth=false, ozone.https.server.keystore.resource=ssl-server.xml, ozone.key.deleting.limit.per.task=20000, ozone.key.preallocation.max.blocks=64, ozone.manager.db.checkpoint.transfer.bandwidthPerSec=0, ozone.manager.delegation.remover.scan.interval=3600000, ozone.manager.delegation.token.max-lifetime=7d, ozone.manager.delegation.token.renew-interval=1d, ozone.metadata.dirs=/data/metadata, ozone.metadata.dirs.permissions=750, ozone.metastore.rocksdb.cf.write.buffer.size=128MB, ozone.metastore.rocksdb.statistics=OFF, ozone.network.flexible.fqdn.resolution.enabled=false, ozone.network.jvm.address.cache.enabled=true, ozone.network.topology.aware.read=true, ozone.om.address=om, ozone.om.admin.protocol.max.retries=20, ozone.om.admin.protocol.wait.between.retries=1000, ozone.om.container.location.cache.size=100000, ozone.om.container.location.cache.ttl=360m, ozone.om.db.dirs.permissions=750, ozone.om.delta.update.data.size.max.limit=1024MB, ozone.om.enable.filesystem.paths=false, ozone.om.enable.ofs.shared.tmp.dir=false, ozone.om.fs.snapshot.max.limit=1000, ozone.om.grpc.bossgroup.size=8, ozone.om.grpc.maximum.response.length=134217728, ozone.om.grpc.read.thread.num=32, ozone.om.grpc.workergroup.size=32, ozone.om.handler.count.key=100, ozone.om.http-address=om:9874, ozone.om.http-bind-host=0.0.0.0, ozone.om.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.om.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.om.http.auth.type=simple, ozone.om.http.enabled=true, ozone.om.https-address=0.0.0.0:9875, ozone.om.https-bind-host=0.0.0.0, ozone.om.kerberos.keytab.file=/etc/security/keytabs/OM.keytab, ozone.om.kerberos.principal=OM/_HOST@REALM, ozone.om.key.path.lock.enabled=false, ozone.om.keyname.character.check.enabled=false, ozone.om.leader.election.minimum.timeout.duration=5s, ozone.om.lock.fair=false, ozone.om.multitenancy.enabled=false, ozone.om.multitenancy.ranger.sync.interval=10m, ozone.om.multitenancy.ranger.sync.timeout=10s, ozone.om.namespace.s3.strict=true, ozone.om.open.key.cleanup.limit.per.task=1000, ozone.om.open.key.cleanup.service.interval=24h, ozone.om.open.key.cleanup.service.timeout=300s, ozone.om.open.key.expire.threshold=7d, ozone.om.ratis.enable=true, ozone.om.ratis.log.appender.queue.byte-limit=32MB, ozone.om.ratis.log.appender.queue.num-elements=1024, ozone.om.ratis.log.purge.gap=1000000, ozone.om.ratis.log.purge.preservation.log.num=0, ozone.om.ratis.log.purge.upto.snapshot.index=true, ozone.om.ratis.minimum.timeout=5s, ozone.om.ratis.port=9872, ozone.om.ratis.rpc.type=GRPC, ozone.om.ratis.segment.preallocated.size=4MB, ozone.om.ratis.segment.size=4MB, ozone.om.ratis.server.failure.timeout.duration=120s, ozone.om.ratis.server.leaderelection.pre-vote=true, ozone.om.ratis.server.request.timeout=3s, ozone.om.ratis.server.retry.cache.timeout=600000ms, ozone.om.save.metrics.interval=5m, ozone.om.security.admin.protocol.acl=*, ozone.om.security.client.protocol.acl=*, ozone.om.snapshot.cache.max.size=10, ozone.om.snapshot.compaction.dag.max.time.allowed=30d, ozone.om.snapshot.compaction.dag.prune.daemon.run.interval=3600s, ozone.om.snapshot.db.max.open.files=100, ozone.om.snapshot.diff.cleanup.service.run.internal=1m, ozone.om.snapshot.diff.cleanup.service.timeout=5m, ozone.om.snapshot.diff.job.default.wait.time=1m, ozone.om.snapshot.diff.job.report.persistent.time=7d, ozone.om.snapshot.diff.max.allowed.keys.changed.per.job=10000000, ozone.om.snapshot.diff.max.jobs.purge.per.task=100, ozone.om.snapshot.diff.max.page.size=1000, ozone.om.snapshot.diff.thread.pool.size=10, ozone.om.snapshot.force.full.diff=false, ozone.om.snapshot.provider.connection.timeout=5000s, ozone.om.snapshot.provider.request.timeout=300000ms, ozone.om.snapshot.provider.socket.timeout=5000s, ozone.om.snapshot.sst_dumptool.buffer.size=8KB, ozone.om.snapshot.sst_dumptool.pool.size=1, ozone.om.transport.class=org.apache.hadoop.ozone.om.protocolPB.GrpcOmTransportFactory, ozone.om.unflushed.transaction.max.count=10000, ozone.om.upgrade.quota.recalculate.enabled=true, ozone.om.user.max.volume=1024, ozone.om.volume.listall.allowed=true, ozone.path.deleting.limit.per.task=10000, ozone.recon.address=recon:9891, ozone.recon.containerkey.flush.db.max.threshold=150000, ozone.recon.db.dir=/data/metadata/recon, ozone.recon.db.dirs.permissions=750, ozone.recon.heatmap.enable=false, ozone.recon.http-address=0.0.0.0:9888, ozone.recon.http-bind-host=0.0.0.0, ozone.recon.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.recon.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.recon.http.auth.type=simple, ozone.recon.http.enabled=true, ozone.recon.https-address=0.0.0.0:9889, ozone.recon.https-bind-host=0.0.0.0, ozone.recon.nssummary.flush.db.max.threshold=150000, ozone.recon.om.connection.request.timeout=5000, ozone.recon.om.connection.timeout=5s, ozone.recon.om.snapshot.task.flush.param=false, ozone.recon.om.snapshot.task.initial.delay=1m, ozone.recon.om.snapshot.task.interval.delay=10m, ozone.recon.om.socket.timeout=5s, ozone.recon.scm.connection.request.timeout=5s, ozone.recon.scm.connection.timeout=5s, ozone.recon.scm.container.threshold=100, ozone.recon.scm.snapshot.enabled=true, ozone.recon.scm.snapshot.task.initial.delay=1m, ozone.recon.scm.snapshot.task.interval.delay=24h, ozone.recon.security.client.datanode.container.protocol.acl=*, ozone.recon.task.thread.count=1, ozone.replication=3, ozone.replication.allowed-configs=^((STANDALONE|RATIS)/(ONE|THREE))|(EC/(3-2|6-3|10-4)-(512|1024|2048|4096)k)$, ozone.rest.client.http.connection.max=100, ozone.rest.client.http.connection.per-route.max=20, ozone.s3g.client.buffer.size=4KB, ozone.s3g.default.bucket.layout=OBJECT_STORE, ozone.s3g.http-address=0.0.0.0:9878, ozone.s3g.http-bind-host=0.0.0.0, ozone.s3g.http.auth.kerberos.keytab=/etc/security/keytabs/HTTP.keytab, ozone.s3g.http.auth.kerberos.principal=HTTP/_HOST@REALM, ozone.s3g.http.auth.type=simple, ozone.s3g.http.enabled=true, ozone.s3g.kerberos.keytab.file=/etc/security/keytabs/s3g.keytab, ozone.s3g.kerberos.principal=s3g/_HOST@REALM, ozone.s3g.volume.name=s3v, ozone.scm.block.client.address=scm, ozone.scm.block.client.bind.host=0.0.0.0, ozone.scm.block.client.port=9863, ozone.scm.block.deletion.max.retry=4096, ozone.scm.block.size=256MB, ozone.scm.ca.list.retry.interval=10s, ozone.scm.chunk.size=4MB, ozone.scm.client.address=scm, ozone.scm.client.bind.host=0.0.0.0, ozone.scm.client.port=9860, ozone.scm.close.container.wait.duration=150s, ozone.scm.container.layout=FILE_PER_BLOCK, ozone.scm.container.lock.stripes=512, ozone.scm.container.placement.ec.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter, ozone.scm.container.placement.impl=org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware, ozone.scm.container.size=1GB, ozone.scm.datanode.admin.monitor.interval=30s, ozone.scm.datanode.disallow.same.peers=false, ozone.scm.datanode.id.dir=/data, ozone.scm.datanode.pipeline.limit=2, ozone.scm.datanode.port=9861, ozone.scm.datanode.ratis.volume.free-space.min=10MB, ozone.scm.db.dirs.permissions=750, ozone.scm.dead.node.interval=10m, ozone.scm.ec.pipeline.minimum=5, ozone.scm.ec.pipeline.per.volume.factor=1, ozone.scm.event.ContainerReport.thread.pool.size=10, ozone.scm.expired.container.replica.op.scrub.interval=5m, ozone.scm.grpc.port=9895, ozone.scm.ha.dbtransactionbuffer.flush.interval=600s, ozone.scm.ha.grpc.deadline.interval=30m, ozone.scm.ha.ratis.leader.election.timeout=5s, ozone.scm.ha.ratis.leader.ready.check.interval=2s, ozone.scm.ha.ratis.leader.ready.wait.timeout=60s, ozone.scm.ha.ratis.log.appender.queue.byte-limit=32MB, ozone.scm.ha.ratis.log.appender.queue.num-elements=1024, ozone.scm.ha.ratis.log.purge.enabled=false, ozone.scm.ha.ratis.log.purge.gap=1000000, ozone.scm.ha.ratis.request.timeout=30s, ozone.scm.ha.ratis.rpc.type=GRPC, ozone.scm.ha.ratis.segment.preallocated.size=4MB, ozone.scm.ha.ratis.segment.size=4MB, ozone.scm.ha.ratis.server.failure.timeout.duration=120s, ozone.scm.ha.ratis.server.leaderelection.pre-vote=true, ozone.scm.ha.ratis.server.retry.cache.timeout=60s, ozone.scm.ha.ratis.server.snapshot.creation.gap=1024, ozone.scm.ha.ratis.snapshot.threshold=1000, ozone.scm.handler.count.key=100, ozone.scm.heartbeat.log.warn.interval.count=10, ozone.scm.heartbeat.rpc-retry-count=15, ozone.scm.heartbeat.rpc-retry-interval=1s, ozone.scm.heartbeat.rpc-timeout=5s, ozone.scm.heartbeat.thread.interval=3s, ozone.scm.http-address=0.0.0.0:9876, ozone.scm.http-bind-host=0.0.0.0, ozone.scm.http.enabled=true, ozone.scm.https-address=0.0.0.0:9877, ozone.scm.https-bind-host=0.0.0.0, ozone.scm.info.wait.duration=10m, ozone.scm.keyvalue.container.deletion-choosing.policy=org.apache.hadoop.ozone.container.common.impl.TopNOrderedContainerDeletionChoosingPolicy, ozone.scm.names=scm, ozone.scm.network.topology.schema.file=network-topology-default.xml, ozone.scm.pipeline.allocated.timeout=5m, ozone.scm.pipeline.creation.auto.factor.one=true, ozone.scm.pipeline.creation.interval=120s, ozone.scm.pipeline.destroy.timeout=66s, ozone.scm.pipeline.leader-choose.policy=org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy, ozone.scm.pipeline.owner.container.count=1, ozone.scm.pipeline.per.metadata.disk=2, ozone.scm.pipeline.scrub.interval=5m, ozone.scm.ratis.pipeline.limit=0, ozone.scm.ratis.port=9894, ozone.scm.security.handler.count.key=2, ozone.scm.security.service.bind.host=0.0.0.0, ozone.scm.security.service.port=9961, ozone.scm.sequence.id.batch.size=1000, ozone.scm.skip.bootstrap.validation=false, ozone.scm.stale.node.interval=5m, ozone.scm.update.client.crl.check.interval=600s, ozone.scm.update.service.port=9893, ozone.security.enabled=false, ozone.security.http.kerberos.enabled=false, ozone.server.default.replication=3, ozone.server.default.replication.type=RATIS, ozone.service.shutdown.timeout=60s, ozone.snapshot.deleting.limit.per.task=10, ozone.snapshot.deleting.service.interval=30s, ozone.snapshot.deleting.service.timeout=300s, ozone.snapshot.filtering.limit.per.task=2, ozone.snapshot.filtering.service.interval=1m, ozone.snapshot.key.deleting.limit.per.task=20000, ozone.sst.filtering.service.timeout=300000ms, ozone.trace.enabled=false, recon.om.delta.update.limit=2000, recon.om.delta.update.loop.limit=10, recon.om.snapshot.task.interval.delay=1m, scm.container.client.idle.threshold=10s, scm.container.client.max.size=256}
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | ************************************************************/
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:11,219 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1             | 2023-06-29 21:45:26,648 [IPC Server handler 12 on default port 9891] INFO ipc.Server: IPC Server handler 12 on default port 9891 caught an exception
scm_1               | 2023-06-29 21:45:11,271 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | java.nio.channels.ClosedChannelException
scm_1               | 2023-06-29 21:45:11,431 [main] INFO reflections.Reflections: Reflections took 124 ms to scan 3 urls, producing 132 keys and 287 values 
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1             | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 2023-06-29 21:45:11,557 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1             | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 2023-06-29 21:45:11,570 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
recon_1             | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 2023-06-29 21:45:12,353 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 2023-06-29 21:45:12,542 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1             | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 2023-06-29 21:45:12,871 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.4.0-SNAPSHOT.jar!/network-topology-default.xml]
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 2023-06-29 21:45:12,873 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1             | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 2023-06-29 21:45:12,957 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 2023-06-29 21:45:13,129 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:1cd49d86-1570-4113-bfd8-4673b0280c86
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 2023-06-29 21:45:13,238 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1             | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 2023-06-29 21:45:13,245 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 2023-06-29 21:45:13,247 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
recon_1             | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 2023-06-29 21:45:13,248 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1             | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 2023-06-29 21:45:13,248 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1             | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 2023-06-29 21:45:13,248 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1             | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 2023-06-29 21:45:13,249 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1             | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:13,250 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1             | 2023-06-29 21:45:27,037 [IPC Server handler 20 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1fef6398-5e0e-4531-8b14-70177bcfa866
recon_1             | 2023-06-29 21:45:27,064 [IPC Server handler 20 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 1fef6398-5e0e-4531-8b14-70177bcfa866{ip: 172.24.0.12, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:13,252 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1             | 2023-06-29 21:45:27,179 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=8a001e4a-d312-430e-b38d-510e705e518c reported by 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12)
scm_1               | 2023-06-29 21:45:13,252 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1             | 2023-06-29 21:45:27,210 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 1fef6398-5e0e-4531-8b14-70177bcfa866 to Node DB.
scm_1               | 2023-06-29 21:45:13,253 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm_1               | 2023-06-29 21:45:13,263 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1             | 2023-06-29 21:45:27,645 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,267 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm_1               | 2023-06-29 21:45:13,269 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1             | 2023-06-29 21:45:27,818 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 9ebf2f12-ce42-45a9-9d0f-eeca45441cf9, Nodes: 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.101Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-29 21:45:13,449 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1             | 2023-06-29 21:45:27,837 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12)
scm_1               | 2023-06-29 21:45:13,454 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1             | 2023-06-29 21:45:28,280 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:45:13,454 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1             | 2023-06-29 21:45:28,288 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3ea36174-d662-4e6c-bd1c-d614f11906cb{ip: 172.24.0.15, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:13,455 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1             | 2023-06-29 21:45:28,303 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3ea36174-d662-4e6c-bd1c-d614f11906cb to Node DB.
scm_1               | 2023-06-29 21:45:13,455 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 2023-06-29 21:45:28,319 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,458 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1             | 2023-06-29 21:45:28,730 [IPC Server handler 18 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/6922a681-9151-4f6a-8181-db1e5c68da07
scm_1               | 2023-06-29 21:45:13,461 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: found a subdirectory /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef
recon_1             | 2023-06-29 21:45:28,732 [IPC Server handler 18 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 6922a681-9151-4f6a-8181-db1e5c68da07{ip: 172.24.0.7, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:13,470 [main] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: addNew group-596B1957BDEF:[] returns group-596B1957BDEF:java.util.concurrent.CompletableFuture@6003220a[Not completed]
scm_1               | 2023-06-29 21:45:13,507 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86: new RaftServerImpl for group-596B1957BDEF:[] with SCMStateMachine:uninitialized
recon_1             | 2023-06-29 21:45:28,744 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 6922a681-9151-4f6a-8181-db1e5c68da07 to Node DB.
scm_1               | 2023-06-29 21:45:13,514 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
recon_1             | 2023-06-29 21:45:28,745 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,515 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
recon_1             | 2023-06-29 21:45:28,752 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: c1ab9456-2d2a-44b4-88b9-30fa5cff24d8, Nodes: 6922a681-9151-4f6a-8181-db1e5c68da07(xcompat_datanode_1.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.283Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-29 21:45:13,515 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
recon_1             | 2023-06-29 21:45:28,756 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8 reported by 6922a681-9151-4f6a-8181-db1e5c68da07(xcompat_datanode_1.xcompat_default/172.24.0.7)
scm_1               | 2023-06-29 21:45:13,516 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1             | 2023-06-29 21:45:29,131 [IPC Server handler 22 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c8dd7f08-3522-46ce-9dac-414b9693c03d
scm_1               | 2023-06-29 21:45:13,516 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1             | 2023-06-29 21:45:29,136 [IPC Server handler 22 on default port 9891] INFO node.SCMNodeManager: Registered Data node : c8dd7f08-3522-46ce-9dac-414b9693c03d{ip: 172.24.0.10, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:13,516 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
recon_1             | 2023-06-29 21:45:29,158 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,523 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
recon_1             | 2023-06-29 21:45:29,169 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node c8dd7f08-3522-46ce-9dac-414b9693c03d to Node DB.
scm_1               | 2023-06-29 21:45:13,523 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
recon_1             | 2023-06-29 21:45:29,204 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 8719d141-45c6-439f-90fe-d93fde1c1c74, Nodes: c8dd7f08-3522-46ce-9dac-414b9693c03d(xcompat_datanode_3.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.724Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-29 21:45:13,528 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
recon_1             | 2023-06-29 21:45:29,209 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74 reported by c8dd7f08-3522-46ce-9dac-414b9693c03d(xcompat_datanode_3.xcompat_default/172.24.0.10)
scm_1               | 2023-06-29 21:45:13,534 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
recon_1             | 2023-06-29 21:45:32,277 [IPC Server handler 7 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5ffc81fa-0a37-43e0-be73-58283f7df8c3
scm_1               | 2023-06-29 21:45:13,551 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
recon_1             | 2023-06-29 21:45:32,277 [IPC Server handler 7 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 5ffc81fa-0a37-43e0-be73-58283f7df8c3{ip: 172.24.0.13, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:13,555 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.timeout = 10s (default)
recon_1             | 2023-06-29 21:45:32,278 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)
scm_1               | 2023-06-29 21:45:13,560 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
recon_1             | 2023-06-29 21:45:32,278 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 5ffc81fa-0a37-43e0-be73-58283f7df8c3 to Node DB.
scm_1               | 2023-06-29 21:45:13,560 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1             | 2023-06-29 21:45:32,603 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12)
scm_1               | 2023-06-29 21:45:13,579 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.read.option = DEFAULT (default)
recon_1             | 2023-06-29 21:45:33,237 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,685 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1             | 2023-06-29 21:45:33,243 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: caf603ee-435e-47a0-8b37-ff44d979937e, Nodes: 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3ea36174-d662-4e6c-bd1c-d614f11906cb, CreationTimestamp2023-06-29T21:45:24.206Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-29 21:45:13,687 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm_1               | 2023-06-29 21:45:13,689 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1             | 2023-06-29 21:45:33,244 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,689 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm_1               | 2023-06-29 21:45:13,689 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1             | 2023-06-29 21:45:33,245 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,690 [1cd49d86-1570-4113-bfd8-4673b0280c86-groupManagement] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm_1               | 2023-06-29 21:45:13,692 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1             | 2023-06-29 21:45:33,872 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,692 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1             | 2023-06-29 21:45:33,877 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: dfd3fd7b-c7da-4f01-8c26-b9087fdff180, Nodes: 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.239Z[UTC]] to Recon pipeline metadata.
scm_1               | 2023-06-29 21:45:13,692 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
recon_1             | 2023-06-29 21:45:33,880 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,722 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7), software layout = HADOOP_PRC_PORTS_IN_DATANODEDETAILS (version = 7)
recon_1             | 2023-06-29 21:45:33,881 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
recon_1             | 2023-06-29 21:45:34,066 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)
scm_1               | 2023-06-29 21:45:13,760 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
recon_1             | 2023-06-29 21:45:34,066 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)
scm_1               | 2023-06-29 21:45:13,762 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
recon_1             | 2023-06-29 21:45:35,454 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12)
scm_1               | 2023-06-29 21:45:13,768 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
recon_1             | 2023-06-29 21:45:35,455 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12)
scm_1               | 2023-06-29 21:45:13,770 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1             | 2023-06-29 21:45:35,528 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,850 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1             | 2023-06-29 21:45:35,528 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
scm_1               | 2023-06-29 21:45:13,873 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackAware
recon_1             | 2023-06-29 21:45:38,902 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)
recon_1             | 2023-06-29 21:45:40,310 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 reported by 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)
recon_1             | 2023-06-29 21:45:56,309 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Unknown pipeline PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e. Trying to get from SCM.
scm_1               | 2023-06-29 21:45:13,876 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 2023-06-29 21:45:56,330 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Adding new pipeline Pipeline[ Id: 8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e, Nodes: 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.256Z[UTC]] to Recon pipeline metadata.
recon_1             | 2023-06-29 21:45:56,331 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e reported by 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)
scm_1               | 2023-06-29 21:45:13,887 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1             | 2023-06-29 21:46:09,722 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1             | 2023-06-29 21:46:09,724 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1             | 2023-06-29 21:46:11,099 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688075169724
scm_1               | 2023-06-29 21:45:13,916 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
recon_1             | 2023-06-29 21:46:11,112 [pool-27-thread-1] INFO helpers.OmKeyInfo: OmKeyInfo.getCodec ignorePipeline = true
recon_1             | 2023-06-29 21:46:11,898 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688075169724.
scm_1               | 2023-06-29 21:45:13,917 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
recon_1             | 2023-06-29 21:46:12,055 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1             | 2023-06-29 21:46:12,590 [pool-28-thread-1] INFO tasks.OmTableInsightTask: Completed a 'reprocess' run of OmTableInsightTask.
scm_1               | 2023-06-29 21:45:13,925 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
recon_1             | 2023-06-29 21:46:12,601 [pool-51-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1             | 2023-06-29 21:46:12,602 [pool-51-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
scm_1               | 2023-06-29 21:45:13,925 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1             | 2023-06-29 21:46:12,603 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1             | 2023-06-29 21:46:12,604 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
scm_1               | 2023-06-29 21:45:13,929 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
recon_1             | 2023-06-29 21:46:12,605 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1             | 2023-06-29 21:46:12,611 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
scm_1               | 2023-06-29 21:45:13,935 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
recon_1             | 2023-06-29 21:46:12,611 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.007 seconds to process 0 keys.
recon_1             | 2023-06-29 21:46:12,641 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
scm_1               | 2023-06-29 21:45:13,943 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
recon_1             | 2023-06-29 21:46:12,644 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1             | 2023-06-29 21:46:19,168 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from xcompat_datanode_5.xcompat_default.
scm_1               | 2023-06-29 21:45:13,947 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
recon_1             | 2023-06-29 21:46:19,253 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1             | 2023-06-29 21:46:25,229 [main] INFO scm.ReconScmTask: Registered ContainerSizeCountTask task 
recon_1             | 2023-06-29 21:46:25,229 [main] INFO scm.ReconScmTask: Starting ContainerSizeCountTask Thread.
recon_1             | 2023-06-29 21:46:25,240 [main] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1             | 2023-06-29 21:46:25,243 [main] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
scm_1               | 2023-06-29 21:45:13,992 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:45:13,992 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm_1               | 2023-06-29 21:45:14,013 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm_1               | 2023-06-29 21:45:14,099 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm_1               | 2023-06-29 21:45:14,120 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm_1               | 2023-06-29 21:45:14,124 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
recon_1             | 2023-06-29 21:46:25,257 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
scm_1               | 2023-06-29 21:45:14,141 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm_1               | 2023-06-29 21:45:14,149 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:14,155 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:45:14,206 [main] INFO server.StorageContainerManager: SCM start with adminUsers: [hadoop]
scm_1               | 2023-06-29 21:45:14,865 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:45:14,910 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1             | 2023-06-29 21:46:25,269 [main] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
scm_1               | 2023-06-29 21:45:14,941 [main] INFO ipc.Server: Listener at 0.0.0.0:9861
scm_1               | 2023-06-29 21:45:14,944 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
recon_1             | 2023-06-29 21:46:25,271 [main] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
scm_1               | 2023-06-29 21:45:14,981 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1             | 2023-06-29 21:46:25,274 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 30 milliseconds.
scm_1               | 2023-06-29 21:45:14,986 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:45:14,987 [main] INFO ipc.Server: Listener at 0.0.0.0:9863
scm_1               | 2023-06-29 21:45:14,987 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm_1               | 2023-06-29 21:45:15,083 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm_1               | 2023-06-29 21:45:15,108 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm_1               | 2023-06-29 21:45:15,110 [main] INFO ipc.Server: Listener at 0.0.0.0:9860
scm_1               | 2023-06-29 21:45:15,115 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm_1               | 2023-06-29 21:45:15,245 [main] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm_1               | 2023-06-29 21:45:15,252 [main] INFO server.StorageContainerManager: 
scm_1               | Container Balancer status:
scm_1               | Key                            Value
scm_1               | Running                        false
scm_1               | Container Balancer Configuration values:
scm_1               | Key                                                Value
scm_1               | Threshold                                          10
scm_1               | Max Datanodes to Involve per Iteration(percent)    20
scm_1               | Max Size to Move per Iteration                     500GB
scm_1               | Max Size Entering Target per Iteration             26GB
scm_1               | Max Size Leaving Source per Iteration              26GB
scm_1               | 
scm_1               | 2023-06-29 21:45:15,253 [main] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm_1               | 2023-06-29 21:45:15,259 [main] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:45:15,269 [main] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm_1               | 2023-06-29 21:45:15,277 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/in_use.lock acquired by nodename 8@68c14f390c32
scm_1               | 2023-06-29 21:45:15,281 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=1cd49d86-1570-4113-bfd8-4673b0280c86} from /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/raft-meta
scm_1               | 2023-06-29 21:45:15,328 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: set configuration 0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:15,334 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1             | 2023-06-29 21:46:25,307 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-Recon: Started
recon_1             | 2023-06-29 21:46:25,445 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 158 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:46:25,490 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 45 milliseconds for processing 1 containers.
recon_1             | 2023-06-29 21:46:34,511 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_5.xcompat_default.
recon_1             | 2023-06-29 21:46:34,515 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 not found. Cannot add container #2
recon_1             | 2023-06-29 21:46:34,516 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-29 21:46:34,616 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_4.xcompat_default.
recon_1             | 2023-06-29 21:46:34,641 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconContainerManager: Pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 not found. Cannot add container #2
recon_1             | 2023-06-29 21:46:34,641 [FixedThreadPoolWithAffinityExecutor-8-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-29 21:46:34,752 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:46:34,757 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconContainerManager: Pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 not found. Cannot add container #2
recon_1             | 2023-06-29 21:46:34,757 [FixedThreadPoolWithAffinityExecutor-1-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-29 21:46:35,274 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_1.xcompat_default.
recon_1             | 2023-06-29 21:46:35,287 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 not found. Cannot add container #2
recon_1             | 2023-06-29 21:46:35,287 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-29 21:46:35,292 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from xcompat_datanode_3.xcompat_default.
recon_1             | 2023-06-29 21:46:35,298 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconContainerManager: Pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 not found. Cannot add container #2
recon_1             | 2023-06-29 21:46:35,298 [FixedThreadPoolWithAffinityExecutor-0-0] WARN scm.ReconIncrementalContainerReportHandler: Container 2 not found!
recon_1             | 2023-06-29 21:47:25,278 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Deleted 0 records from "CONTAINER_COUNT_BY_SIZE"
recon_1             | 2023-06-29 21:47:25,304 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:47:25,304 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 25
recon_1             | 2023-06-29 21:47:33,411 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #3 got from xcompat_datanode_2.xcompat_default.
recon_1             | 2023-06-29 21:47:33,435 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #3 to Recon.
recon_1             | 2023-06-29 21:48:25,314 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:48:25,314 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 7
recon_1             | 2023-06-29 21:49:25,314 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:49:25,314 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:50:25,314 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:50:25,315 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:51:25,304 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1             | 2023-06-29 21:51:25,310 [PipelineSyncTask] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=ae529a68-5529-4a6c-a45c-78d75bc088e2 from SCM.
recon_1             | 2023-06-29 21:51:25,315 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Completed a 'process' run of ContainerSizeCountTask.
recon_1             | 2023-06-29 21:51:25,315 [ContainerSizeCountTask] INFO tasks.ContainerSizeCountTask: Elapsed Time in milliseconds for Process() execution: 0
recon_1             | 2023-06-29 21:51:25,331 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 42 milliseconds.
recon_1             | 2023-06-29 21:51:25,495 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 1 milliseconds to process 0 existing database records.
recon_1             | 2023-06-29 21:51:25,498 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 3 milliseconds for processing 2 containers.
scm_1               | 2023-06-29 21:45:15,347 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm_1               | 2023-06-29 21:45:15,348 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:45:15,351 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm_1               | 2023-06-29 21:45:15,353 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm_1               | 2023-06-29 21:45:15,356 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:45:15,366 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm_1               | 2023-06-29 21:45:15,368 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm_1               | 2023-06-29 21:45:15,368 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:45:15,378 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef
scm_1               | 2023-06-29 21:45:15,379 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:45:15,380 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:45:15,382 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm_1               | 2023-06-29 21:45:15,383 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm_1               | 2023-06-29 21:45:15,383 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm_1               | 2023-06-29 21:45:15,385 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm_1               | 2023-06-29 21:45:15,386 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm_1               | 2023-06-29 21:45:15,387 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm_1               | 2023-06-29 21:45:15,403 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm_1               | 2023-06-29 21:45:15,403 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm_1               | 2023-06-29 21:45:15,573 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm_1               | 2023-06-29 21:45:15,574 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm_1               | 2023-06-29 21:45:15,575 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm_1               | 2023-06-29 21:45:15,597 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: set configuration 0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:15,598 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/log_inprogress_0
scm_1               | 2023-06-29 21:45:15,599 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm_1               | 2023-06-29 21:45:15,681 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: start as a follower, conf=0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:15,681 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm_1               | 2023-06-29 21:45:15,683 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState
scm_1               | 2023-06-29 21:45:15,684 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-596B1957BDEF,id=1cd49d86-1570-4113-bfd8-4673b0280c86
scm_1               | 2023-06-29 21:45:15,686 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm_1               | 2023-06-29 21:45:15,686 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm_1               | 2023-06-29 21:45:15,687 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm_1               | 2023-06-29 21:45:15,687 [1cd49d86-1570-4113-bfd8-4673b0280c86-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm_1               | 2023-06-29 21:45:15,690 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm_1               | 2023-06-29 21:45:15,691 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm_1               | 2023-06-29 21:45:15,691 [main] INFO server.RaftServer: 1cd49d86-1570-4113-bfd8-4673b0280c86: start RPC server
scm_1               | 2023-06-29 21:45:15,732 [main] INFO server.GrpcService: 1cd49d86-1570-4113-bfd8-4673b0280c86: GrpcService started, listening on 9894
scm_1               | 2023-06-29 21:45:15,744 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-1cd49d86-1570-4113-bfd8-4673b0280c86: Started
scm_1               | 2023-06-29 21:45:15,745 [main] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm_1               | 2023-06-29 21:45:15,746 [main] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm_1               | 2023-06-29 21:45:15,748 [main] INFO SCMHATransactionMonitor: Starting SCMHATransactionMonitor Service.
scm_1               | 2023-06-29 21:45:15,748 [main] INFO ha.SCMServiceManager: Registering service SCMHATransactionMonitor.
scm_1               | 2023-06-29 21:45:15,749 [main] INFO SCMHATransactionMonitor: SCMHATransactionMonitor Service is already running, skip start.
scm_1               | 2023-06-29 21:45:15,944 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm_1               | 2023-06-29 21:45:15,979 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm_1               | 2023-06-29 21:45:15,979 [main] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm_1               | 2023-06-29 21:45:16,180 [main] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm_1               | 2023-06-29 21:45:16,180 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:45:16,189 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm_1               | 2023-06-29 21:45:16,234 [main] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:45:16,235 [main] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm_1               | 2023-06-29 21:45:16,238 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm_1               | 2023-06-29 21:45:16,239 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:45:16,382 [main] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm_1               | 2023-06-29 21:45:16,382 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm_1               | 2023-06-29 21:45:16,446 [main] INFO util.log: Logging initialized @6445ms to org.eclipse.jetty.util.log.Slf4jLog
scm_1               | 2023-06-29 21:45:16,689 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm_1               | 2023-06-29 21:45:16,719 [main] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm_1               | 2023-06-29 21:45:16,734 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm_1               | 2023-06-29 21:45:16,736 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm_1               | 2023-06-29 21:45:16,738 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm_1               | 2023-06-29 21:45:16,738 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm_1               | 2023-06-29 21:45:16,832 [main] INFO http.BaseHttpServer: HTTP server of scm uses base directory /data/metadata/webserver
scm_1               | 2023-06-29 21:45:16,836 [main] INFO http.HttpServer2: Jetty bound to port 9876
scm_1               | 2023-06-29 21:45:16,840 [main] INFO server.Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.19+7-LTS
scm_1               | 2023-06-29 21:45:16,919 [main] INFO server.session: DefaultSessionIdManager workerName=node0
scm_1               | 2023-06-29 21:45:16,919 [main] INFO server.session: No SessionScavenger set, using defaults
scm_1               | 2023-06-29 21:45:16,921 [main] INFO server.session: node0 Scavenging every 660000ms
scm_1               | 2023-06-29 21:45:16,942 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4c0d7878{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm_1               | 2023-06-29 21:45:16,943 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f50b08a{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/static,AVAILABLE}
scm_1               | 2023-06-29 21:45:17,061 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@2d2c794d{scm,/,file:///data/metadata/webserver/jetty-0_0_0_0-9876-hdds-server-scm-1_4_0-SNAPSHOT_jar-_-any-5308500411727083895/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.4.0-SNAPSHOT.jar!/webapps/scm}
scm_1               | 2023-06-29 21:45:17,069 [main] INFO server.AbstractConnector: Started ServerConnector@33ea437a{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm_1               | 2023-06-29 21:45:17,069 [main] INFO server.Server: Started @7070ms
scm_1               | 2023-06-29 21:45:17,073 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm_1               | 2023-06-29 21:45:17,073 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm_1               | 2023-06-29 21:45:17,075 [main] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm_1               | 2023-06-29 21:45:20,793 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.FollowerState: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5110349810ns, electionTimeout:5101ms
scm_1               | 2023-06-29 21:45:20,794 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState
scm_1               | 2023-06-29 21:45:20,794 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm_1               | 2023-06-29 21:45:20,797 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = true (default)
scm_1               | 2023-06-29 21:45:20,797 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-FollowerState] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1
scm_1               | 2023-06-29 21:45:20,801 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 PRE_VOTE round 0: submit vote requests at term 1 for 0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:20,802 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 PRE_VOTE round 0: result PASSED (term=1)
scm_1               | 2023-06-29 21:45:20,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:20,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.LeaderElection: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm_1               | 2023-06-29 21:45:20,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: shutdown 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1
scm_1               | 2023-06-29 21:45:20,813 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm_1               | 2023-06-29 21:45:20,814 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm_1               | 2023-06-29 21:45:20,814 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm_1               | 2023-06-29 21:45:20,816 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: change Leader from null to 1cd49d86-1570-4113-bfd8-4673b0280c86 at term 2 for becomeLeader, leader elected after 7262ms
scm_1               | 2023-06-29 21:45:20,823 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm_1               | 2023-06-29 21:45:20,826 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:45:20,827 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm_1               | 2023-06-29 21:45:20,831 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm_1               | 2023-06-29 21:45:20,831 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm_1               | 2023-06-29 21:45:20,832 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm_1               | 2023-06-29 21:45:20,837 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm_1               | 2023-06-29 21:45:20,838 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm_1               | 2023-06-29 21:45:20,839 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO impl.RoleInfo: 1cd49d86-1570-4113-bfd8-4673b0280c86: start 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderStateImpl
scm_1               | 2023-06-29 21:45:20,844 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm_1               | 2023-06-29 21:45:20,848 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/log_inprogress_0 to /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/log_0-0
scm_1               | 2023-06-29 21:45:20,864 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/2aa45680-139b-4449-b07e-596b1957bdef/current/log_inprogress_1
scm_1               | 2023-06-29 21:45:20,868 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-LeaderElection1] INFO server.RaftServer$Division: 1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF: set configuration 1: peers:[1cd49d86-1570-4113-bfd8-4673b0280c86|rpc:68c14f390c32:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm_1               | 2023-06-29 21:45:20,875 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm_1               | 2023-06-29 21:45:20,876 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm_1               | 2023-06-29 21:45:20,878 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:20,878 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm_1               | 2023-06-29 21:45:20,878 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm_1               | 2023-06-29 21:45:20,878 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm_1               | 2023-06-29 21:45:20,884 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm_1               | 2023-06-29 21:45:20,884 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm_1               | 2023-06-29 21:45:21,175 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_1.xcompat_default:50706 / 172.24.0.7:50706: output error
scm_1               | 2023-06-29 21:45:21,178 [IPC Server handler 46 on default port 9861] WARN ipc.Server: IPC Server handler 46 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_5.xcompat_default:47166 / 172.24.0.13:47166: output error
scm_1               | 2023-06-29 21:45:21,178 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_2.xcompat_default:46298 / 172.24.0.15:46298: output error
scm_1               | 2023-06-29 21:45:21,203 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_3.xcompat_default:36396 / 172.24.0.10:36396: output error
scm_1               | 2023-06-29 21:45:21,186 [IPC Server handler 5 on default port 9861] WARN ipc.Server: IPC Server handler 5 on default port 9861, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from xcompat_datanode_4.xcompat_default:49348 / 172.24.0.12:49348: output error
scm_1               | 2023-06-29 21:45:21,181 [IPC Server handler 46 on default port 9861] INFO ipc.Server: IPC Server handler 46 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:21,180 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:21,208 [IPC Server handler 5 on default port 9861] INFO ipc.Server: IPC Server handler 5 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:21,208 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:21,207 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm_1               | java.nio.channels.ClosedChannelException
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm_1               | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm_1               | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3743)
scm_1               | 	at org.apache.hadoop.ipc.Server.access$1800(Server.java:148)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1722)
scm_1               | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1792)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2919)
scm_1               | 	at org.apache.hadoop.ipc.Server$Connection.access$400(Server.java:1864)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1176)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:954)
scm_1               | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:940)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1111)
scm_1               | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1017)
scm_1               | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm_1               | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm_1               | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)
scm_1               | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:3048)
scm_1               | 2023-06-29 21:45:23,259 [IPC Server handler 85 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/5ffc81fa-0a37-43e0-be73-58283f7df8c3
scm_1               | 2023-06-29 21:45:23,274 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/1fef6398-5e0e-4531-8b14-70177bcfa866
scm_1               | 2023-06-29 21:45:23,276 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 1fef6398-5e0e-4531-8b14-70177bcfa866{ip: 172.24.0.12, host: xcompat_datanode_4.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:23,284 [IPC Server handler 85 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 5ffc81fa-0a37-43e0-be73-58283f7df8c3{ip: 172.24.0.13, host: xcompat_datanode_5.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:23,294 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:45:23,310 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:45:23,311 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:45:23,325 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm_1               | 2023-06-29 21:45:23,325 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:23,332 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:23,354 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8a001e4a-d312-430e-b38d-510e705e518c to datanode:1fef6398-5e0e-4531-8b14-70177bcfa866
scm_1               | 2023-06-29 21:45:23,956 [IPC Server handler 51 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:45:23,957 [IPC Server handler 51 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3ea36174-d662-4e6c-bd1c-d614f11906cb{ip: 172.24.0.15, host: xcompat_datanode_2.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:23,957 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm_1               | 2023-06-29 21:45:23,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:45:23,980 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm_1               | 2023-06-29 21:45:23,981 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm_1               | 2023-06-29 21:45:23,981 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:24,004 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:24,018 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,093 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 8a001e4a-d312-430e-b38d-510e705e518c, Nodes: 1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:23.353023Z[UTC]]
scm_1               | 2023-06-29 21:45:24,101 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 to datanode:5ffc81fa-0a37-43e0-be73-58283f7df8c3
scm_1               | 2023-06-29 21:45:24,137 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 to datanode:3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:45:24,138 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 to datanode:1fef6398-5e0e-4531-8b14-70177bcfa866
scm_1               | 2023-06-29 21:45:24,196 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,205 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 9ebf2f12-ce42-45a9-9d0f-eeca45441cf9, Nodes: 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.101312Z[UTC]]
scm_1               | 2023-06-29 21:45:24,206 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e to datanode:3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:45:24,224 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,238 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: caf603ee-435e-47a0-8b37-ff44d979937e, Nodes: 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.206549Z[UTC]]
scm_1               | 2023-06-29 21:45:24,239 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 to datanode:3ea36174-d662-4e6c-bd1c-d614f11906cb
scm_1               | 2023-06-29 21:45:24,245 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 to datanode:5ffc81fa-0a37-43e0-be73-58283f7df8c3
scm_1               | 2023-06-29 21:45:24,246 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 to datanode:1fef6398-5e0e-4531-8b14-70177bcfa866
scm_1               | 2023-06-29 21:45:24,250 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,255 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180 contains same datanodes as previous pipelines: PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9 nodeIds: 3ea36174-d662-4e6c-bd1c-d614f11906cb, 5ffc81fa-0a37-43e0-be73-58283f7df8c3, 1fef6398-5e0e-4531-8b14-70177bcfa866
scm_1               | 2023-06-29 21:45:24,255 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: dfd3fd7b-c7da-4f01-8c26-b9087fdff180, Nodes: 3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.239366Z[UTC]]
scm_1               | 2023-06-29 21:45:24,256 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e to datanode:5ffc81fa-0a37-43e0-be73-58283f7df8c3
scm_1               | 2023-06-29 21:45:24,258 [IPC Server handler 85 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/6922a681-9151-4f6a-8181-db1e5c68da07
scm_1               | 2023-06-29 21:45:24,265 [IPC Server handler 85 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 6922a681-9151-4f6a-8181-db1e5c68da07{ip: 172.24.0.7, host: xcompat_datanode_1.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:24,266 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:24,276 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,277 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e, Nodes: 5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.256450Z[UTC]]
scm_1               | 2023-06-29 21:45:24,277 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
scm_1               | 2023-06-29 21:45:24,283 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8 to datanode:6922a681-9151-4f6a-8181-db1e5c68da07
scm_1               | 2023-06-29 21:45:24,289 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,290 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: c1ab9456-2d2a-44b4-88b9-30fa5cff24d8, Nodes: 6922a681-9151-4f6a-8181-db1e5c68da07(xcompat_datanode_1.xcompat_default/172.24.0.7), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.283327Z[UTC]]
scm_1               | 2023-06-29 21:45:24,303 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 1. Excluded 3.
scm_1               | 2023-06-29 21:45:24,708 [IPC Server handler 12 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/c8dd7f08-3522-46ce-9dac-414b9693c03d
scm_1               | 2023-06-29 21:45:24,710 [IPC Server handler 12 on default port 9861] INFO node.SCMNodeManager: Registered Data node : c8dd7f08-3522-46ce-9dac-414b9693c03d{ip: 172.24.0.10, host: xcompat_datanode_3.xcompat_default, ports: [HTTP=9882, CLIENT_RPC=9864, REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm_1               | 2023-06-29 21:45:24,711 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm_1               | 2023-06-29 21:45:24,724 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74 to datanode:c8dd7f08-3522-46ce-9dac-414b9693c03d
scm_1               | 2023-06-29 21:45:24,732 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:24,732 [RatisPipelineUtilsThread - 0] INFO pipeline.BackgroundPipelineCreator: Created new pipeline Pipeline[ Id: 8719d141-45c6-439f-90fe-d93fde1c1c74, Nodes: c8dd7f08-3522-46ce-9dac-414b9693c03d(xcompat_datanode_3.xcompat_default/172.24.0.10), ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:45:24.724146Z[UTC]]
scm_1               | 2023-06-29 21:45:24,733 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-29 21:45:27,113 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:27,114 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=8a001e4a-d312-430e-b38d-510e705e518c
scm_1               | 2023-06-29 21:45:27,116 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:27,631 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:28,936 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:28,937 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=c1ab9456-2d2a-44b4-88b9-30fa5cff24d8
scm_1               | 2023-06-29 21:45:28,937 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:29,313 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:29,313 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=8719d141-45c6-439f-90fe-d93fde1c1c74
scm_1               | 2023-06-29 21:45:29,319 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:32,625 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:33,255 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:33,259 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=caf603ee-435e-47a0-8b37-ff44d979937e
scm_1               | 2023-06-29 21:45:33,261 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:33,879 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:34,475 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:34,960 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:35,470 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:35,532 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:35,552 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm_1               | 2023-06-29 21:45:35,554 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=9ebf2f12-ce42-45a9-9d0f-eeca45441cf9
scm_1               | 2023-06-29 21:45:35,554 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:45:35,559 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm_1               | 2023-06-29 21:45:35,560 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm_1               | 2023-06-29 21:45:35,687 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm_1               | 2023-06-29 21:45:35,687 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO SCMHATransactionMonitor: Service SCMHATransactionMonitor transitions to RUNNING.
scm_1               | 2023-06-29 21:45:40,326 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=dfd3fd7b-c7da-4f01-8c26-b9087fdff180
scm_1               | 2023-06-29 21:45:56,356 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineReportHandler: Opened pipeline PipelineID=8c250d18-b3dc-4c89-b2a8-fecd1b5ddf7e
scm_1               | 2023-06-29 21:46:15,883 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm_1               | 2023-06-29 21:46:15,951 [1cd49d86-1570-4113-bfd8-4673b0280c86@group-596B1957BDEF-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm_1               | 2023-06-29 21:46:15,970 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm_1               | 2023-06-29 21:46:32,203 [IPC Server handler 2 on default port 9863] INFO pipeline.WritableECContainerProvider: Created and opened new pipeline Pipeline[ Id: ae529a68-5529-4a6c-a45c-78d75bc088e2, Nodes: c8dd7f08-3522-46ce-9dac-414b9693c03d(xcompat_datanode_3.xcompat_default/172.24.0.10)3ea36174-d662-4e6c-bd1c-d614f11906cb(xcompat_datanode_2.xcompat_default/172.24.0.15)6922a681-9151-4f6a-8181-db1e5c68da07(xcompat_datanode_1.xcompat_default/172.24.0.7)5ffc81fa-0a37-43e0-be73-58283f7df8c3(xcompat_datanode_5.xcompat_default/172.24.0.13)1fef6398-5e0e-4531-8b14-70177bcfa866(xcompat_datanode_4.xcompat_default/172.24.0.12), ReplicationConfig: EC{rs-3-2-1024k}, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:46:32.172325Z[UTC]]
scm_1               | 2023-06-29 21:47:24,745 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-29 21:48:47,447 [IPC Server handler 88 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for delTxnId, change lastId from 0 to 1000.
scm_1               | 2023-06-29 21:49:24,746 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
scm_1               | 2023-06-29 21:50:14,125 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 300000ms after safemode exit
scm_1               | 2023-06-29 21:51:24,748 [RatisPipelineUtilsThread - 0] WARN pipeline.PipelinePlacementPolicy: Pipeline creation failed due to no sufficient healthy datanodes. Required 3. Found 2. Excluded 3.
