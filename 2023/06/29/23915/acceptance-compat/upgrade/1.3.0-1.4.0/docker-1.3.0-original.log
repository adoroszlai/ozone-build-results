Attaching to ha_dn4_1, ha_om1_1, ha_recon_1, ha_scm3_1, ha_dn5_1, ha_dn2_1, ha_scm2_1, ha_scm1_1, ha_om2_1, ha_dn3_1, ha_s3g_1, ha_dn1_1, ha_om3_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-06-29 21:08:00,589 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 0626a64bf417/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | ************************************************************/
dn1_1    | 2023-06-29 21:08:00,616 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-06-29 21:08:01,163 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-06-29 21:08:01,889 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-06-29 21:08:03,264 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-06-29 21:08:03,264 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-06-29 21:08:04,354 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:0626a64bf417 ip:10.9.0.17
dn1_1    | 2023-06-29 21:08:06,405 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-06-29 21:08:07,740 [main] INFO reflections.Reflections: Reflections took 1073 ms to scan 2 urls, producing 92 keys and 204 values 
dn1_1    | 2023-06-29 21:08:08,788 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-06-29 21:08:09,855 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn1_1    | 2023-06-29 21:08:09,972 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-06-29 21:08:09,992 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-06-29 21:08:10,000 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-06-29 21:08:10,075 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-06-29 21:08:10,334 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-29 21:08:10,369 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn1_1    | 2023-06-29 21:08:10,384 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-06-29 21:08:10,384 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-06-29 21:08:10,434 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-06-29 21:08:10,750 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-06-29 21:08:10,755 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-06-29 21:08:21,499 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-06-29 21:08:22,252 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-29 21:08:22,726 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-06-29 21:08:23,273 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-29 21:08:23,280 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-06-29 21:08:23,291 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-29 21:08:23,296 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-06-29 21:08:23,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-06-29 21:08:23,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-06-29 21:08:23,301 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-06-29 21:08:23,302 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:08:23,316 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-06-29 21:08:23,322 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-29 21:08:23,400 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-29 21:08:23,428 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-06-29 21:08:23,428 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-06-29 21:08:26,052 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-06-29 21:08:26,069 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-06-29 21:08:26,096 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-06-29 21:08:26,104 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:08:26,105 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-29 21:08:26,114 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-29 21:08:26,442 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-06-29 21:08:27,254 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-06-29 21:08:27,376 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-06-29 21:08:27,586 [main] INFO util.log: Logging initialized @37979ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-06-29 21:08:28,648 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-06-29 21:08:28,716 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-06-29 21:08:28,878 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-06-29 21:08:28,911 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-06-29 21:08:28,928 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-29 21:08:28,931 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-06-29 21:08:29,979 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-06-29 21:08:30,012 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-06-29 21:08:30,337 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-06-29 21:08:30,342 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-06-29 21:08:30,344 [main] INFO server.session: node0 Scavenging every 600000ms
dn1_1    | 2023-06-29 21:08:30,508 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b9ac754{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-06-29 21:08:30,516 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-06-29 21:08:32,654 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1d2fb82{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-14270545476581042875/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn1_1    | 2023-06-29 21:08:32,736 [main] INFO server.AbstractConnector: Started ServerConnector@7302ff13{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-06-29 21:08:32,736 [main] INFO server.Server: Started @43129ms
dn1_1    | 2023-06-29 21:08:32,772 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-06-29 21:08:32,772 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-06-29 21:08:32,778 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-06-29 21:08:32,812 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-06-29 21:08:32,939 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4738e19b] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-06-29 21:08:33,832 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-06-29 21:08:34,099 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-06-29 21:08:36,387 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:36,388 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:36,389 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:36,399 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:37,388 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:37,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:37,389 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:37,401 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:38,389 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:38,390 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:38,390 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:38,402 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:39,390 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:39,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:39,406 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:40,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:40,393 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:40,407 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:41,393 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:41,394 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:41,407 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:42,394 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:42,395 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:42,408 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:43,395 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:43,408 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:43,434 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 0626a64bf417/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:58634 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:58634 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-06-29 21:08:44,396 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:44,409 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:45,397 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:45,410 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:46,398 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:46,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:47,399 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:47,404 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-06-29 21:08:03,522 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 226b7e0ffc1d/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | ************************************************************/
dn2_1    | 2023-06-29 21:08:03,595 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-06-29 21:08:03,917 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-06-29 21:08:04,653 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-06-29 21:08:05,653 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-06-29 21:08:05,653 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-06-29 21:08:06,730 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:226b7e0ffc1d ip:10.9.0.18
dn2_1    | 2023-06-29 21:08:08,797 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-06-29 21:08:10,282 [main] INFO reflections.Reflections: Reflections took 1216 ms to scan 2 urls, producing 92 keys and 204 values 
dn2_1    | 2023-06-29 21:08:11,288 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-06-29 21:08:12,460 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn2_1    | 2023-06-29 21:08:12,531 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-06-29 21:08:12,533 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-06-29 21:08:12,555 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-06-29 21:08:12,698 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-06-29 21:08:12,785 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-29 21:08:12,862 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn2_1    | 2023-06-29 21:08:12,864 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-06-29 21:08:12,864 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-06-29 21:08:12,865 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-06-29 21:08:13,046 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-06-29 21:08:13,046 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-06-29 21:08:22,676 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-06-29 21:08:23,136 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-29 21:08:23,518 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-06-29 21:08:24,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-29 21:08:24,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-06-29 21:08:24,250 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-29 21:08:24,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-06-29 21:08:24,251 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-06-29 21:08:24,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-06-29 21:08:24,266 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-06-29 21:08:24,267 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-29 21:08:24,279 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-06-29 21:08:24,280 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-29 21:08:24,554 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-29 21:08:24,650 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-06-29 21:08:24,664 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-06-29 21:08:26,954 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-29 21:08:26,979 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-06-29 21:08:27,024 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-06-29 21:08:27,024 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-29 21:08:27,024 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-29 21:08:27,064 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-29 21:08:27,293 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-06-29 21:08:28,127 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-06-29 21:08:28,273 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-06-29 21:08:28,752 [main] INFO util.log: Logging initialized @35601ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-06-29 21:08:30,241 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-06-29 21:08:30,342 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-06-29 21:08:30,411 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-29 21:08:30,433 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-06-29 21:08:30,449 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-29 21:08:30,464 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn2_1    | 2023-06-29 21:08:31,079 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | java.net.SocketTimeoutException: Call From 0626a64bf417/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:54444 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:54444 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-06-29 21:08:47,411 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:48,299 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-e8df2eed-9784-486d-b808-485e57974990/container.db to cache
dn1_1    | 2023-06-29 21:08:48,300 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-e8df2eed-9784-486d-b808-485e57974990/container.db for volume DS-e8df2eed-9784-486d-b808-485e57974990
dn1_1    | 2023-06-29 21:08:48,301 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-06-29 21:08:48,304 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn1_1    | 2023-06-29 21:08:48,406 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:48,412 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:48,521 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn1_1    | 2023-06-29 21:08:48,556 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start RPC server
dn1_1    | 2023-06-29 21:08:48,563 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: GrpcService started, listening on 9858
dn1_1    | 2023-06-29 21:08:48,564 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: GrpcService started, listening on 9856
dn1_1    | 2023-06-29 21:08:48,565 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: GrpcService started, listening on 9857
dn1_1    | 2023-06-29 21:08:48,577 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 is started using port 9858 for RATIS
dn1_1    | 2023-06-29 21:08:48,577 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-06-29 21:08:48,577 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-06-29 21:08:48,578 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-0a9e83e4-93dc-44cc-b7f2-64f77839ab35: Started
dn1_1    | 2023-06-29 21:08:49,406 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:49,413 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:50,407 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:50,408 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 0626a64bf417/10.9.0.17 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-29 21:08:50,414 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-06-29 21:08:02,491 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = 3920a78a163b/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | 2023-06-29 21:08:50,414 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 0626a64bf417/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-29 21:08:51,006 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 2023-06-29 21:08:31,109 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-06-29 21:08:31,416 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-29 21:08:31,416 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-06-29 21:08:31,426 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-06-29 21:08:31,487 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-29 21:08:31,572 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4525e9e8{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-06-29 21:08:34,305 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@8c43966{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-7927297672627710910/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn2_1    | 2023-06-29 21:08:34,342 [main] INFO server.AbstractConnector: Started ServerConnector@3e26482{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-06-29 21:08:34,342 [main] INFO server.Server: Started @41228ms
dn2_1    | 2023-06-29 21:08:34,370 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-06-29 21:08:34,370 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-29 21:08:34,376 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-06-29 21:08:34,391 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-06-29 21:08:34,459 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2c7080a8] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-06-29 21:08:34,925 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-06-29 21:08:35,227 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-06-29 21:08:37,705 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:37,707 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:37,707 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:37,714 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:38,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:38,711 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:38,714 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:39,710 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:39,712 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:39,715 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:40,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:40,713 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:40,715 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:41,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:41,713 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:41,716 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:42,713 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:42,714 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:42,718 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:42,738 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 226b7e0ffc1d/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59656 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:59656 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-29 21:08:43,713 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:43,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:44,714 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:44,720 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:45,715 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:45,721 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:46,716 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:46,721 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:47,717 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:47,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:47,725 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 226b7e0ffc1d/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:56214 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-29 21:08:51,410 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:51,417 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:52,411 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:52,418 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:53,412 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:53,418 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:54,413 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:54,419 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:55,414 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:55,420 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:56,415 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:56,424 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:57,416 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:57,425 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:58,417 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:58,425 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:59,418 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:08:59,426 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:00,420 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:00,435 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:01,420 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:01,483 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:02,482 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | ************************************************************/
dn3_1    | 2023-06-29 21:08:02,526 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-29 21:08:02,997 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-06-29 21:08:03,882 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-06-29 21:08:04,889 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-06-29 21:08:04,899 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-06-29 21:08:06,085 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:3920a78a163b ip:10.9.0.19
dn3_1    | 2023-06-29 21:08:08,075 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-06-29 21:08:09,338 [main] INFO reflections.Reflections: Reflections took 1070 ms to scan 2 urls, producing 92 keys and 204 values 
dn3_1    | 2023-06-29 21:08:10,208 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-06-29 21:08:11,186 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn3_1    | 2023-06-29 21:08:11,429 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-06-29 21:08:11,431 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-06-29 21:08:11,432 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-06-29 21:08:11,643 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-06-29 21:08:11,784 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-29 21:08:11,793 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn3_1    | 2023-06-29 21:08:11,793 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-06-29 21:08:11,798 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-06-29 21:08:11,798 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-06-29 21:08:11,993 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-06-29 21:08:12,009 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-06-29 21:08:21,661 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-06-29 21:08:22,272 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-29 21:08:22,632 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-06-29 21:08:23,105 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-29 21:08:23,144 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-06-29 21:08:23,144 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-29 21:08:23,145 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-06-29 21:08:23,146 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-06-29 21:08:23,146 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-06-29 21:08:23,147 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-06-29 21:08:23,152 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-29 21:08:23,152 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-06-29 21:08:23,156 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-29 21:08:23,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-29 21:08:23,246 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-06-29 21:08:23,273 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-06-29 21:08:25,367 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-06-29 21:08:25,390 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-06-29 21:08:25,398 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-06-29 21:08:25,399 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-29 21:08:25,412 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-29 21:08:25,414 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-29 21:08:25,550 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-06-29 21:08:26,548 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-06-29 21:08:26,648 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-06-29 21:08:26,795 [main] INFO util.log: Logging initialized @35314ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-06-29 21:08:28,547 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-06-29 21:08:28,576 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-06-29 21:08:28,677 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-06-29 21:08:28,692 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-06-29 21:08:28,693 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:56214 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-29 21:08:48,207 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-5c5dcfac-6874-4e67-9b98-f6a9ae7fc10c/container.db to cache
dn2_1    | 2023-06-29 21:08:48,208 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-5c5dcfac-6874-4e67-9b98-f6a9ae7fc10c/container.db for volume DS-5c5dcfac-6874-4e67-9b98-f6a9ae7fc10c
dn2_1    | 2023-06-29 21:08:48,210 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-06-29 21:08:48,213 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn2_1    | 2023-06-29 21:08:48,425 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 653c0db6-d9db-4099-8201-751dcc40d458
dn2_1    | 2023-06-29 21:08:48,567 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 653c0db6-d9db-4099-8201-751dcc40d458: start RPC server
dn2_1    | 2023-06-29 21:08:48,578 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 653c0db6-d9db-4099-8201-751dcc40d458: GrpcService started, listening on 9858
dn2_1    | 2023-06-29 21:08:48,581 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 653c0db6-d9db-4099-8201-751dcc40d458: GrpcService started, listening on 9856
dn2_1    | 2023-06-29 21:08:48,581 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 653c0db6-d9db-4099-8201-751dcc40d458: GrpcService started, listening on 9857
dn2_1    | 2023-06-29 21:08:48,594 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 653c0db6-d9db-4099-8201-751dcc40d458 is started using port 9858 for RATIS
dn2_1    | 2023-06-29 21:08:48,594 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 653c0db6-d9db-4099-8201-751dcc40d458 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-06-29 21:08:48,594 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 653c0db6-d9db-4099-8201-751dcc40d458 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-06-29 21:08:48,596 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-653c0db6-d9db-4099-8201-751dcc40d458: Started
dn2_1    | 2023-06-29 21:08:48,718 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:48,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:49,720 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:49,723 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:50,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:50,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:51,721 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:51,722 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 226b7e0ffc1d/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 2023-06-29 21:09:02,484 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:03,484 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:03,485 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:04,486 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:05,486 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:06,488 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:07,489 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:08,490 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:09,491 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:10,492 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:11,493 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:12,495 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:12,821 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-06-29 21:09:13,496 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:14,499 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:15,500 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:16,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:17,508 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:18,509 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:19,510 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:20,511 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:21,009 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-29 21:09:21,513 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:22,359 [Command processor thread] INFO server.RaftServer: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: addNew group-BF02ED3673EC:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-BF02ED3673EC:java.util.concurrent.CompletableFuture@d2ac48a[Not completed]
dn1_1    | 2023-06-29 21:09:22,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:22,568 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: new RaftServerImpl for group-BF02ED3673EC:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-29 21:09:22,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-29 21:09:22,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-29 21:09:22,608 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-29 21:09:22,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:22,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-29 21:09:22,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-29 21:09:22,656 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-29 21:09:22,674 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-29 21:09:22,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-29 21:09:22,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-29 21:09:22,793 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:22,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-29 21:09:22,826 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-29 21:09:23,166 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-29 21:09:23,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-29 21:09:23,172 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-29 21:09:23,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-29 21:09:23,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-29 21:09:23,188 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/c2cded09-6bff-4911-8b65-bf02ed3673ec does not exist. Creating ...
dn1_1    | 2023-06-29 21:09:23,229 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/c2cded09-6bff-4911-8b65-bf02ed3673ec/in_use.lock acquired by nodename 7@0626a64bf417
dn1_1    | 2023-06-29 21:09:23,281 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/c2cded09-6bff-4911-8b65-bf02ed3673ec has been successfully formatted.
dn1_1    | 2023-06-29 21:09:23,380 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-BF02ED3673EC: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-29 21:09:23,381 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-29 21:09:23,435 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-29 21:09:23,450 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:09:23,491 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-29 21:09:23,492 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-29 21:09:23,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:23,587 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:23,660 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-29 21:09:23,660 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-29 21:09:23,749 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/c2cded09-6bff-4911-8b65-bf02ed3673ec
dn1_1    | 2023-06-29 21:09:23,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-29 21:09:23,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:09:23,777 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:23,786 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-29 21:09:23,823 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-29 21:09:23,824 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-29 21:09:23,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-29 21:09:23,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-29 21:09:23,929 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:23,948 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:23,954 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:23,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-29 21:09:24,028 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-29 21:09:24,029 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-29 21:09:24,030 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:24,030 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-29 21:09:24,048 [pool-22-thread-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState
dn1_1    | 2023-06-29 21:09:24,100 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:24,110 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:24,111 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-BF02ED3673EC,id=0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn1_1    | 2023-06-29 21:09:24,122 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-29 21:09:24,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-29 21:09:24,136 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-29 21:09:24,147 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-29 21:09:24,334 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=c2cded09-6bff-4911-8b65-bf02ed3673ec
dn1_1    | 2023-06-29 21:09:24,341 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=c2cded09-6bff-4911-8b65-bf02ed3673ec.
dn1_1    | 2023-06-29 21:09:24,347 [Command processor thread] INFO server.RaftServer: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: addNew group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-886978236035:java.util.concurrent.CompletableFuture@677ad878[Not completed]
dn1_1    | 2023-06-29 21:09:24,397 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: new RaftServerImpl for group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-29 21:09:24,419 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-29 21:09:24,419 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-29 21:09:24,419 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-29 21:09:24,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:24,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-29 21:09:24,424 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-29 21:09:24,424 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-29 21:09:24,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-29 21:09:24,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-29 21:09:24,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-29 21:09:24,432 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:24,434 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-29 21:09:24,441 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-29 21:09:24,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-29 21:09:24,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-29 21:09:24,465 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-29 21:09:24,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-29 21:09:24,468 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-29 21:09:24,469 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 does not exist. Creating ...
dn1_1    | 2023-06-29 21:09:24,477 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/in_use.lock acquired by nodename 7@0626a64bf417
dn1_1    | 2023-06-29 21:09:24,483 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 has been successfully formatted.
dn1_1    | 2023-06-29 21:09:24,517 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-886978236035: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-29 21:09:24,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-29 21:09:24,518 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-29 21:08:28,702 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-06-29 21:08:29,069 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn3_1    | 2023-06-29 21:08:29,070 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-06-29 21:08:29,373 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-06-29 21:08:29,373 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-06-29 21:08:29,378 [main] INFO server.session: node0 Scavenging every 600000ms
dn3_1    | 2023-06-29 21:08:29,514 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3434a4f0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-06-29 21:08:29,540 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-06-29 21:08:31,998 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5efcab{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-4444787704317573082/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn3_1    | 2023-06-29 21:08:32,088 [main] INFO server.AbstractConnector: Started ServerConnector@3fdcde7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-06-29 21:08:32,089 [main] INFO server.Server: Started @40608ms
dn3_1    | 2023-06-29 21:08:32,112 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-06-29 21:08:32,112 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-06-29 21:08:32,135 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-06-29 21:08:32,155 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-06-29 21:08:32,433 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5980e5fe] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-06-29 21:08:33,203 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-06-29 21:08:33,772 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-06-29 21:08:35,620 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:35,630 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:35,632 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:35,633 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:36,621 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:36,631 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:36,632 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:36,634 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:37,622 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:37,631 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:37,633 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:37,634 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:38,623 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:38,650 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:38,651 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:38,651 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:39,652 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:39,652 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:39,659 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:40,653 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:40,654 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:40,660 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:41,654 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:41,655 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:41,661 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:42,655 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:42,656 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:42,663 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:43,657 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:43,667 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:43,667 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 3920a78a163b/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:59172 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 2023-06-29 21:09:24,524 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:09:24,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-29 21:09:24,527 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-29 21:09:24,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:24,528 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-29 21:09:24,531 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-29 21:09:24,532 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035
dn1_1    | 2023-06-29 21:09:24,539 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-29 21:09:24,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:09:24,540 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:24,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-29 21:09:24,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-29 21:09:24,543 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-29 21:09:24,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-29 21:09:24,544 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-29 21:09:24,551 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:24,564 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:24,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:24,578 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-06-29 21:08:03,871 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 28efb866b4fe/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.3.0
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn5_1    | ************************************************************/
dn5_1    | 2023-06-29 21:08:03,935 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-06-29 21:08:04,321 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-06-29 21:08:05,260 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-29 21:08:06,427 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-06-29 21:08:06,427 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-06-29 21:08:07,486 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:28efb866b4fe ip:10.9.0.21
dn5_1    | 2023-06-29 21:08:09,520 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-06-29 21:08:10,930 [main] INFO reflections.Reflections: Reflections took 1165 ms to scan 2 urls, producing 92 keys and 204 values 
dn5_1    | 2023-06-29 21:08:11,885 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-06-29 21:08:12,966 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn5_1    | 2023-06-29 21:08:13,038 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-06-29 21:08:13,069 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-06-29 21:08:13,079 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-06-29 21:08:13,290 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-06-29 21:08:13,496 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-29 21:08:13,501 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn5_1    | 2023-06-29 21:08:13,504 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-06-29 21:08:13,505 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-06-29 21:08:13,533 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-06-29 21:08:13,695 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-06-29 21:08:13,695 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-06-29 21:08:22,737 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-06-29 21:08:23,240 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-29 21:08:23,610 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-06-29 21:08:24,210 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-29 21:08:24,230 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-06-29 21:08:24,230 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-29 21:08:24,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-06-29 21:08:24,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-06-29 21:08:24,231 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-06-29 21:08:24,231 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-06-29 21:08:24,246 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-29 21:08:24,246 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-06-29 21:08:24,260 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-29 21:08:24,309 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-29 21:08:24,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-06-29 21:08:24,322 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-06-29 21:08:26,878 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-06-29 21:08:26,886 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-29 21:08:26,951 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-29 21:08:26,964 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:08:26,964 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-29 21:08:26,990 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-29 21:08:27,194 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-06-29 21:08:28,281 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-06-29 21:08:28,445 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-29 21:08:28,637 [main] INFO util.log: Logging initialized @35482ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-06-29 21:08:29,824 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-06-29 21:08:29,916 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-06-29 21:08:30,021 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-06-29 21:08:30,034 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-06-29 21:08:30,034 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-06-29 21:08:30,058 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-06-29 21:08:30,454 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-06-29 21:08:30,492 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-06-29 21:08:30,969 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-29 21:08:30,986 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-06-29 21:08:30,995 [main] INFO server.session: node0 Scavenging every 600000ms
dn5_1    | 2023-06-29 21:08:31,093 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@96897c8{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-06-29 21:08:31,110 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5e002356{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-06-29 21:08:33,020 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@dbed7fd{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-12807444209503336263/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn5_1    | 2023-06-29 21:08:33,103 [main] INFO server.AbstractConnector: Started ServerConnector@521441d5{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-06-29 21:08:33,108 [main] INFO server.Server: Started @39953ms
dn5_1    | 2023-06-29 21:08:33,123 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-06-29 21:08:33,123 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-29 21:08:33,134 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-06-29 21:08:33,168 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-06-29 21:08:33,373 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1bea82f5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-06-29 21:08:33,870 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-06-29 21:08:34,139 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn5_1    | 2023-06-29 21:08:36,695 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:36,695 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:36,696 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:36,695 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:37,697 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:37,697 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:37,697 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:37,698 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:38,698 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:38,699 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:38,698 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:38,698 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:39,699 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:39,700 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:39,700 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:40,700 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:40,701 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:40,701 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:41,396 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-29 21:08:04,135 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = 01689bb3b9ab/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.3.0
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | ************************************************************/
dn4_1    | 2023-06-29 21:08:04,183 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-06-29 21:08:04,652 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-06-29 21:08:05,296 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-06-29 21:08:06,459 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-06-29 21:08:06,467 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-06-29 21:08:07,518 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:01689bb3b9ab ip:10.9.0.20
dn4_1    | 2023-06-29 21:08:09,682 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-06-29 21:08:10,976 [main] INFO reflections.Reflections: Reflections took 935 ms to scan 2 urls, producing 92 keys and 204 values 
dn4_1    | 2023-06-29 21:08:12,019 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-06-29 21:08:12,992 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn4_1    | 2023-06-29 21:08:13,105 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-06-29 21:08:13,113 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-06-29 21:08:13,117 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-06-29 21:08:13,282 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-06-29 21:08:13,494 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-29 21:08:13,508 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn4_1    | 2023-06-29 21:08:13,552 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-06-29 21:08:13,556 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-06-29 21:08:13,560 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-06-29 21:08:13,843 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-06-29 21:08:13,846 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-06-29 21:08:22,862 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-06-29 21:08:23,206 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-29 21:08:23,606 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-06-29 21:08:23,998 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-29 21:08:24,009 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-06-29 21:08:24,028 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-29 21:08:24,035 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-06-29 21:08:24,040 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-06-29 21:08:24,048 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-06-29 21:08:24,049 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-06-29 21:08:24,056 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:08:24,061 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-06-29 21:08:24,068 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-29 21:08:24,127 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-29 21:08:24,166 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-29 21:08:24,184 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-29 21:08:26,486 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-29 21:08:26,488 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-29 21:08:26,496 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-29 21:08:26,505 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:08:26,530 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-29 21:08:26,541 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-29 21:08:26,695 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-06-29 21:08:27,509 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-06-29 21:08:27,627 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-06-29 21:08:27,833 [main] INFO util.log: Logging initialized @34347ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-06-29 21:08:28,999 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-06-29 21:08:29,018 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-06-29 21:08:29,045 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-06-29 21:08:29,123 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-06-29 21:08:29,160 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn4_1    | 2023-06-29 21:08:29,164 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-29 21:08:29,793 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-06-29 21:08:29,809 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-06-29 21:08:30,057 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-29 21:08:30,091 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-06-29 21:08:30,093 [main] INFO server.session: node0 Scavenging every 660000ms
dn4_1    | 2023-06-29 21:08:30,184 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79afa369{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-06-29 21:08:30,188 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@79ba0a6f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-06-29 21:08:32,529 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5a4dda2{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-13665720397891440224/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn4_1    | 2023-06-29 21:08:32,604 [main] INFO server.AbstractConnector: Started ServerConnector@4f363abd{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-06-29 21:08:32,604 [main] INFO server.Server: Started @39118ms
dn4_1    | 2023-06-29 21:08:32,606 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-06-29 21:08:32,606 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-06-29 21:08:32,633 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-06-29 21:08:32,664 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-06-29 21:08:32,949 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@71a4b4ac] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-06-29 21:08:33,544 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-06-29 21:08:33,871 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-06-29 21:08:36,294 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:36,301 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:36,301 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:36,313 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:37,295 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:37,302 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:37,302 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:37,314 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:38,296 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:38,303 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:38,305 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:38,314 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:39,297 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:39,306 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:39,316 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:40,298 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:40,307 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:40,317 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:41,299 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:41,307 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:41,317 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:42,300 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:42,308 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:42,318 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:43,301 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:43,320 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:43,347 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 01689bb3b9ab/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:60578 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-06-29 21:08:51,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:51,727 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 226b7e0ffc1d/10.9.0.18 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-06-29 21:08:52,521 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-06-29 21:08:52,725 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:52,728 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:53,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:53,728 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:54,726 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:54,729 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:55,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:55,736 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:56,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:56,738 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:57,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:57,738 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:58,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:58,739 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:59,733 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:08:59,740 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:00,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:00,740 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:01,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:01,741 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:02,735 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:02,743 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:03,736 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:03,750 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:04,737 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:05,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:59172 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-29 21:08:44,657 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:44,667 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:45,659 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-29 21:08:41,701 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:41,702 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:41,702 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:42,702 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:42,703 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:42,703 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:43,705 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:43,706 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:43,775 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 28efb866b4fe/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:48210 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:48210 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-29 21:08:45,668 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:46,659 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:46,669 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:47,660 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:47,669 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:47,671 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From 3920a78a163b/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42168 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:42168 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-29 21:08:48,183 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-ff338cba-7e8a-40f6-8449-852a795c1a22/container.db to cache
dn3_1    | 2023-06-29 21:08:48,183 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-ff338cba-7e8a-40f6-8449-852a795c1a22/container.db for volume DS-ff338cba-7e8a-40f6-8449-852a795c1a22
dn3_1    | 2023-06-29 21:08:48,184 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-06-29 21:08:48,186 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn3_1    | 2023-06-29 21:08:48,353 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 3561b5c6-83b9-4083-8461-2a79b0297944
dn3_1    | 2023-06-29 21:08:48,444 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 3561b5c6-83b9-4083-8461-2a79b0297944: start RPC server
dn3_1    | 2023-06-29 21:08:48,463 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3561b5c6-83b9-4083-8461-2a79b0297944: GrpcService started, listening on 9858
dn3_1    | 2023-06-29 21:08:48,464 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3561b5c6-83b9-4083-8461-2a79b0297944: GrpcService started, listening on 9856
dn3_1    | 2023-06-29 21:08:48,465 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 3561b5c6-83b9-4083-8461-2a79b0297944: GrpcService started, listening on 9857
dn3_1    | 2023-06-29 21:08:48,479 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-3561b5c6-83b9-4083-8461-2a79b0297944: Started
dn3_1    | 2023-06-29 21:08:48,479 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3561b5c6-83b9-4083-8461-2a79b0297944 is started using port 9858 for RATIS
dn3_1    | 2023-06-29 21:08:48,479 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3561b5c6-83b9-4083-8461-2a79b0297944 is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-06-29 21:08:48,479 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 3561b5c6-83b9-4083-8461-2a79b0297944 is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-06-29 21:08:48,661 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:48,670 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:49,662 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:49,663 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 3920a78a163b/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-06-29 21:08:49,670 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:49,671 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From 3920a78a163b/10.9.0.19 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-29 21:08:03,249 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 7c5c5fe9b525/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--init]
om1_1    | STARTUP_MSG:   version = 1.3.0
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-06-29 21:08:03,337 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-29 21:08:12,295 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-29 21:08:16,292 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-29 21:08:16,744 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-29 21:08:16,744 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-29 21:08:16,750 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-29 21:08:18,455 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-29 21:08:22,626 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:24,628 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:26,629 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:28,631 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:30,632 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:32,634 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:34,640 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:36,642 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:38,643 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:40,645 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:42,648 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:44,696 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:67dd125e-a7b5-412f-8128-ba59bd3344fe is not the leader. Could not determine the leader node.
om1_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om1_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om1_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om1_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:46,697 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-29 21:08:48,700 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 7c5c5fe9b525/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018;layoutVersion=3
om1_1    | 2023-06-29 21:08:50,755 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1    | /************************************************************
om1_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 7c5c5fe9b525/10.9.0.11
om1_1    | ************************************************************/
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-29 21:08:55,695 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 7c5c5fe9b525/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--]
om1_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | 2023-06-29 21:09:24,579 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-29 21:09:24,579 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-29 21:09:24,592 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:24,616 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:24,617 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-29 21:09:24,617 [pool-22-thread-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:24,620 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-886978236035,id=0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn1_1    | 2023-06-29 21:09:24,633 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-29 21:09:24,633 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-29 21:09:24,633 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-29 21:09:24,635 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-29 21:09:24,643 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:24,668 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:24,655 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8bd8ea2c-61df-4132-bcca-886978236035
dn1_1    | 2023-06-29 21:09:25,617 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:26,619 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:27,620 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:28,075 [grpc-default-executor-1] INFO server.RaftServer: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: addNew group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER] returns group-1BB47A98493E:java.util.concurrent.CompletableFuture@5695e837[Not completed]
dn1_1    | 2023-06-29 21:09:28,077 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: new RaftServerImpl for group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-29 21:09:28,082 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-29 21:09:28,082 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-29 21:09:28,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-29 21:09:28,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:28,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-29 21:09:28,083 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-29 21:09:28,083 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-29 21:09:28,086 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-29 21:09:28,087 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-29 21:09:28,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-29 21:09:28,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-29 21:09:28,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-29 21:09:28,091 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-29 21:09:28,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-29 21:09:28,092 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-29 21:09:28,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-29 21:09:28,094 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-29 21:09:28,096 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-29 21:09:28,096 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e does not exist. Creating ...
dn1_1    | 2023-06-29 21:09:28,100 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/in_use.lock acquired by nodename 7@0626a64bf417
dn1_1    | 2023-06-29 21:09:28,104 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e has been successfully formatted.
dn1_1    | 2023-06-29 21:09:28,120 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1BB47A98493E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-29 21:09:28,120 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-29 21:09:28,123 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-29 21:09:28,124 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:09:28,124 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-29 21:09:28,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-29 21:09:28,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:28,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-29 21:09:28,127 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-29 21:09:28,128 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e
dn1_1    | 2023-06-29 21:09:28,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-29 21:09:28,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:09:28,129 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:28,130 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-29 21:09:28,131 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-29 21:09:28,131 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-29 21:09:28,141 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-29 21:09:28,143 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-29 21:09:28,145 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-29 21:09:28,152 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:28,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-29 21:09:28,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-29 21:09:28,158 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-29 21:09:28,159 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-29 21:08:02,686 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 27415c8d6e4a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--init]
om2_1    | STARTUP_MSG:   version = 1.3.0
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-06-29 21:08:02,734 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-29 21:08:11,892 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-29 21:08:15,543 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-29 21:08:16,046 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-29 21:08:16,047 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-29 21:08:16,050 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-29 21:08:17,352 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-29 21:08:21,318 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:23,320 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:25,322 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:27,323 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:29,327 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:31,331 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:33,341 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:35,348 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:37,349 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:39,352 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:41,353 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:43,649 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:67dd125e-a7b5-412f-8128-ba59bd3344fe is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:45,652 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-29 21:08:47,653 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 27415c8d6e4a/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018;layoutVersion=3
om2_1    | 2023-06-29 21:08:49,715 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om2_1    | /************************************************************
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 27415c8d6e4a/10.9.0.12
om2_1    | ************************************************************/
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-29 21:08:52,716 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 27415c8d6e4a/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--]
om2_1    | STARTUP_MSG:   version = 1.3.0
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:60578 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-06-29 21:08:44,304 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:44,320 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:45,305 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:45,323 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:46,307 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:46,323 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:47,308 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:47,316 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From 01689bb3b9ab/10.9.0.20 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:53406 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:53406 remote=scm1/10.9.0.14:9861]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-06-29 21:08:47,325 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:48,225 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-b4436b84-866a-4729-afd3-cbcca8cd887d/container.db to cache
dn4_1    | 2023-06-29 21:08:48,233 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-b4436b84-866a-4729-afd3-cbcca8cd887d/container.db for volume DS-b4436b84-866a-4729-afd3-cbcca8cd887d
dn4_1    | 2023-06-29 21:08:48,236 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-06-29 21:08:48,240 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn4_1    | 2023-06-29 21:08:48,310 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:48,326 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:48,439 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:08:48,475 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 95c58c7b-97c6-4852-a177-ebb1478388b9: start RPC server
dn4_1    | 2023-06-29 21:08:48,483 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 95c58c7b-97c6-4852-a177-ebb1478388b9: GrpcService started, listening on 9858
dn4_1    | 2023-06-29 21:08:48,487 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 95c58c7b-97c6-4852-a177-ebb1478388b9: GrpcService started, listening on 9856
dn4_1    | 2023-06-29 21:08:48,488 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 95c58c7b-97c6-4852-a177-ebb1478388b9: GrpcService started, listening on 9857
dn4_1    | 2023-06-29 21:08:48,510 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95c58c7b-97c6-4852-a177-ebb1478388b9 is started using port 9858 for RATIS
dn4_1    | 2023-06-29 21:08:48,512 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95c58c7b-97c6-4852-a177-ebb1478388b9 is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-06-29 21:08:48,512 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 95c58c7b-97c6-4852-a177-ebb1478388b9 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-06-29 21:08:48,513 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-95c58c7b-97c6-4852-a177-ebb1478388b9: Started
dn4_1    | 2023-06-29 21:08:49,310 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:49,343 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:50,365 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-29 21:08:50,665 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:50,673 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:51,666 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:51,674 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:52,367 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-29 21:08:52,666 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:52,674 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:53,667 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:53,675 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:54,668 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:54,676 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:55,669 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:55,677 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:56,674 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:56,681 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:50,311 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:50,312 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 01689bb3b9ab/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-06-29 21:08:50,344 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:50,345 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From 01689bb3b9ab/10.9.0.20 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 2023-06-29 21:08:57,675 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:57,682 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:58,675 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:58,682 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:59,677 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:08:59,683 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:00,677 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:00,684 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:01,679 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:01,685 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:02,680 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:02,685 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:03,681 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:03,686 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:04,682 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:05,683 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:06,684 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:07,686 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:08,687 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:09,688 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:10,690 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:11,691 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:12,692 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:12,765 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-29 21:09:13,693 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:14,694 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:15,694 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:16,702 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:06,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:07,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:08,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:09,743 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:10,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:11,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:12,748 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:12,773 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-29 21:09:13,749 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:14,750 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:15,751 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:16,751 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:17,752 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:18,753 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:19,754 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:20,755 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:21,755 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:22,522 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-06-29 21:09:22,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:23,759 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:23,871 [Command processor thread] INFO server.RaftServer: 653c0db6-d9db-4099-8201-751dcc40d458: addNew group-9AE2FA46D77C:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-9AE2FA46D77C:java.util.concurrent.CompletableFuture@16c4c79e[Not completed]
dn2_1    | 2023-06-29 21:09:24,010 [pool-22-thread-1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458: new RaftServerImpl for group-9AE2FA46D77C:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-29 21:09:24,081 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-29 21:09:24,088 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-29 21:09:24,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-29 21:09:24,090 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-29 21:09:24,098 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-29 21:09:24,099 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-29 21:09:24,198 [pool-22-thread-1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: ConfigurationManager, init=-1: peers:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-29 21:09:24,203 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-29 21:09:24,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-29 21:09:24,314 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-29 21:09:24,413 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-29 21:09:24,472 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-29 21:09:24,478 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-29 21:09:24,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:24,895 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-29 21:09:24,944 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-29 21:09:24,946 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-29 21:09:24,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-29 21:09:24,951 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-29 21:09:24,952 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c does not exist. Creating ...
dn2_1    | 2023-06-29 21:09:25,004 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c/in_use.lock acquired by nodename 7@226b7e0ffc1d
dn2_1    | 2023-06-29 21:09:25,067 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c has been successfully formatted.
dn2_1    | 2023-06-29 21:09:25,148 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-9AE2FA46D77C: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-29 21:09:25,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-29 21:09:25,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-29 21:09:25,286 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-06-29 21:08:51,316 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:51,347 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:52,317 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-06-29 21:08:55,727 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-29 21:09:00,000 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-29 21:09:01,849 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-29 21:09:02,232 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-29 21:09:02,234 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-29 21:09:02,245 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-29 21:09:02,390 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-06-29 21:09:03,970 [main] INFO reflections.Reflections: Reflections took 997 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om1_1    | 2023-06-29 21:09:04,033 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-29 21:09:04,932 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-29 21:09:05,033 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-29 21:09:07,670 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-29 21:09:08,050 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-29 21:09:08,051 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-29 21:09:09,085 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-06-29 21:09:09,426 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om1_1    | 2023-06-29 21:09:09,610 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-29 21:09:09,611 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-06-29 21:09:09,653 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-06-29 21:09:10,340 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-29 21:09:10,416 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-29 21:09:10,755 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-06-29 21:09:10,832 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1    | 2023-06-29 21:09:10,992 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-06-29 21:09:11,284 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-29 21:09:11,292 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-29 21:09:11,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-29 21:09:11,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-29 21:09:11,300 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-06-29 21:09:11,301 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-06-29 21:09:11,301 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-06-29 21:09:11,311 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-29 21:09:11,313 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-06-29 21:09:11,317 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-29 21:09:11,347 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-29 21:09:11,362 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-06-29 21:09:11,372 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-06-29 21:09:12,415 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-06-29 21:09:12,430 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-06-29 21:09:12,455 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-06-29 21:09:12,456 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-29 21:09:12,456 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-29 21:09:12,480 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-29 21:09:12,524 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@c3719e5[Not completed]
om1_1    | 2023-06-29 21:09:12,524 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-06-29 21:09:12,679 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-06-29 21:09:12,682 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-06-29 21:09:12,813 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-06-29 21:09:12,829 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-29 21:09:12,831 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-06-29 21:09:12,831 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn3_1    | 2023-06-29 21:09:17,703 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:18,704 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:19,704 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:20,706 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:21,707 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:22,380 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-29 21:09:22,380 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-29 21:09:22,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:23,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:23,749 [Command processor thread] INFO server.RaftServer: 3561b5c6-83b9-4083-8461-2a79b0297944: addNew group-8E0B4F3286C8:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns group-8E0B4F3286C8:java.util.concurrent.CompletableFuture@44566c1c[Not completed]
dn3_1    | 2023-06-29 21:09:23,917 [pool-22-thread-1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944: new RaftServerImpl for group-8E0B4F3286C8:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-29 21:09:23,918 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-29 21:09:23,922 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-29 21:09:23,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-29 21:09:23,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-29 21:09:23,927 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-29 21:09:23,928 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-29 21:09:23,985 [pool-22-thread-1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: ConfigurationManager, init=-1: peers:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-29 21:09:24,019 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-29 21:08:44,706 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:44,707 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:45,707 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:45,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:46,708 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:46,709 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:47,709 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:47,710 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:47,716 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 28efb866b4fe/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:56460 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:56460 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-06-29 21:08:48,184 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-196f2137-04e2-41d3-ab02-c319a8f913b0/container.db to cache
dn5_1    | 2023-06-29 21:08:48,186 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018/DS-196f2137-04e2-41d3-ab02-c319a8f913b0/container.db for volume DS-196f2137-04e2-41d3-ab02-c319a8f913b0
dn5_1    | 2023-06-29 21:08:48,187 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-06-29 21:08:48,188 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn5_1    | 2023-06-29 21:08:48,387 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis b423f7b1-089b-406b-a3aa-6a650e93a531
dn5_1    | 2023-06-29 21:08:48,513 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: b423f7b1-089b-406b-a3aa-6a650e93a531: start RPC server
dn5_1    | 2023-06-29 21:08:48,521 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b423f7b1-089b-406b-a3aa-6a650e93a531: GrpcService started, listening on 9858
dn5_1    | 2023-06-29 21:08:48,526 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b423f7b1-089b-406b-a3aa-6a650e93a531: GrpcService started, listening on 9856
dn4_1    | 2023-06-29 21:08:52,348 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:53,318 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:53,348 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:54,319 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:54,349 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:55,320 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:55,350 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:56,321 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:56,350 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:57,322 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:57,351 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:58,323 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:58,352 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:59,324 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:08:59,353 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:00,325 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:00,354 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:01,326 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:01,354 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:02,328 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:02,355 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:03,329 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:03,356 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:04,330 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:05,330 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:06,332 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:07,333 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:08,334 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:24,109 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-29 21:09:24,122 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-29 21:09:24,188 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-29 21:09:24,216 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-29 21:09:24,259 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-29 21:09:24,745 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:24,880 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-29 21:09:24,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-29 21:09:24,897 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-29 21:09:24,973 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-29 21:09:25,014 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-29 21:09:25,017 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8 does not exist. Creating ...
dn3_1    | 2023-06-29 21:09:25,096 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8/in_use.lock acquired by nodename 7@3920a78a163b
dn3_1    | 2023-06-29 21:09:25,178 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8 has been successfully formatted.
dn3_1    | 2023-06-29 21:09:25,321 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8E0B4F3286C8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-29 21:09:25,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-29 21:09:25,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-29 21:09:25,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-29 21:09:25,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-29 21:09:25,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-29 21:09:25,568 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-29 21:09:25,602 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-29 21:09:25,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-29 21:09:25,667 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8
dn3_1    | 2023-06-29 21:09:25,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-29 21:09:25,710 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-29 21:09:25,713 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-29 21:09:25,729 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-29 21:09:25,737 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-29 21:09:25,744 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-29 21:09:25,749 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:25,751 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-29 21:09:25,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-29 21:09:25,836 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-29 21:09:25,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-29 21:09:25,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-29 21:09:25,867 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-29 21:09:25,939 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-29 21:09:25,939 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-29 21:09:25,961 [pool-22-thread-1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: start as a follower, conf=-1: peers:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-29 21:09:25,961 [pool-22-thread-1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-29 21:09:25,968 [pool-22-thread-1] INFO impl.RoleInfo: 3561b5c6-83b9-4083-8461-2a79b0297944: start 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState
dn3_1    | 2023-06-29 21:09:26,003 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8E0B4F3286C8,id=3561b5c6-83b9-4083-8461-2a79b0297944
dn3_1    | 2023-06-29 21:09:26,015 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-29 21:09:26,015 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-29 21:09:26,016 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-29 21:09:26,017 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
om2_1    | 2023-06-29 21:08:52,736 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-29 21:08:57,053 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-29 21:08:59,026 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-29 21:08:59,266 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-29 21:08:59,266 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-29 21:08:59,269 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-29 21:08:59,333 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om2_1    | 2023-06-29 21:09:00,786 [main] INFO reflections.Reflections: Reflections took 1192 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om2_1    | 2023-06-29 21:09:00,878 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-29 21:09:01,822 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-29 21:09:02,167 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-29 21:09:05,322 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-29 21:09:05,873 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-29 21:09:05,878 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-29 21:09:06,949 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-06-29 21:09:07,291 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om2_1    | 2023-06-29 21:09:07,420 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-29 21:09:07,427 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | 2023-06-29 21:09:07,462 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om2_1    | 2023-06-29 21:09:08,039 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | 2023-06-29 21:09:08,092 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om2_1    | 2023-06-29 21:09:08,341 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
om2_1    | 2023-06-29 21:09:08,392 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om2_1    | 2023-06-29 21:09:08,521 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-06-29 21:09:08,897 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-29 21:09:08,911 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-29 21:09:08,926 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-29 21:09:08,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om2_1    | 2023-06-29 21:09:08,931 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | 2023-06-29 21:09:08,933 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om2_1    | 2023-06-29 21:09:08,936 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om2_1    | 2023-06-29 21:09:08,951 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-29 21:09:08,957 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om2_1    | 2023-06-29 21:09:08,961 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-29 21:09:09,041 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-29 21:09:09,063 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om2_1    | 2023-06-29 21:09:09,065 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-06-29 21:09:09,959 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-06-29 21:09:09,961 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om2_1    | 2023-06-29 21:09:09,988 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om2_1    | 2023-06-29 21:09:09,988 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-29 21:09:09,990 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-29 21:09:10,007 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-29 21:09:10,028 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@c3719e5[Not completed]
om2_1    | 2023-06-29 21:09:10,029 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-06-29 21:09:10,152 [pool-26-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om2_1    | 2023-06-29 21:09:10,155 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-06-29 21:09:10,191 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-06-29 21:09:10,192 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-06-29 21:09:10,198 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-06-29 21:09:10,198 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-29 21:09:12,831 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-29 21:09:12,831 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-06-29 21:09:12,909 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-06-29 21:09:12,922 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-29 21:09:13,117 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-06-29 21:09:13,124 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-06-29 21:09:13,182 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-06-29 21:09:13,233 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-06-29 21:09:13,243 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-06-29 21:09:14,227 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-06-29 21:09:14,227 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-06-29 21:09:14,232 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-06-29 21:09:14,234 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-06-29 21:09:14,235 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-06-29 21:09:15,956 [main] INFO reflections.Reflections: Reflections took 2960 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om1_1    | 2023-06-29 21:09:16,942 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-06-29 21:09:17,054 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-06-29 21:09:17,734 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-06-29 21:09:17,831 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-06-29 21:09:17,831 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-06-29 21:09:18,044 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-06-29 21:09:18,048 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-06-29 21:09:18,057 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1    | 2023-06-29 21:09:18,085 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@7c5c5fe9b525
om1_1    | 2023-06-29 21:09:18,173 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1    | 2023-06-29 21:09:18,197 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-29 21:09:18,240 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-06-29 21:09:18,242 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-29 21:09:18,244 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-06-29 21:09:18,259 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-29 21:09:18,274 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-29 21:09:18,302 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-06-29 21:09:18,308 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-06-29 21:09:18,343 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-06-29 21:09:18,387 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-06-29 21:09:18,390 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-06-29 21:09:18,393 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-29 21:09:18,396 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-06-29 21:09:18,400 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-29 21:09:18,401 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-06-29 21:09:18,404 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-29 21:09:18,415 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-06-29 21:09:18,450 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-06-29 21:09:18,470 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-06-29 21:09:18,472 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-29 21:09:18,480 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-06-29 21:09:18,514 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-29 21:09:18,518 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-29 21:09:18,532 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-29 21:09:18,533 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1    | 2023-06-29 21:09:18,544 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-29 21:09:18,548 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-29 21:09:18,560 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-29 21:09:18,567 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-06-29 21:09:18,575 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-06-29 21:09:18,575 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-06-29 21:09:18,575 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-06-29 21:09:18,576 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-06-29 21:09:18,592 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-06-29 21:09:18,805 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om1_1    | 2023-06-29 21:09:18,825 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-06-29 21:09:18,826 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-06-29 21:09:19,013 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-06-29 21:09:19,014 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-06-29 21:09:19,076 [Listener at om1/9862] INFO util.log: Logging initialized @27896ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-06-29 21:09:19,498 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-06-29 21:09:19,508 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-06-29 21:09:19,524 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-06-29 21:09:19,528 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-06-29 21:09:19,528 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-29 21:09:19,530 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-06-29 21:09:19,623 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-06-29 21:09:19,632 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1    | 2023-06-29 21:09:19,710 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-06-29 21:09:19,710 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-06-29 21:09:19,715 [Listener at om1/9862] INFO server.session: node0 Scavenging every 660000ms
om1_1    | 2023-06-29 21:09:19,740 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-06-29 21:09:19,743 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-06-29 21:09:20,724 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-14348495096350641303/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om1_1    | 2023-06-29 21:09:20,769 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-06-29 21:09:20,769 [Listener at om1/9862] INFO server.Server: Started @29590ms
om1_1    | 2023-06-29 21:09:20,783 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-06-29 21:09:20,783 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-29 21:09:20,786 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-06-29 21:09:20,786 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-29 21:09:20,787 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-06-29 21:09:20,852 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-06-29 21:09:20,978 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2084e65a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-06-29 21:09:22,093 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
om1_1    | 2023-06-29 21:09:22,141 [grpc-default-executor-0] INFO impl.VoteContext: om1@group-D66704EFC61C-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-29 21:09:22,141 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om2
om1_1    | 2023-06-29 21:09:22,158 [grpc-default-executor-0] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-29 21:09:22,168 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState was interrupted
om1_1    | 2023-06-29 21:09:22,171 [grpc-default-executor-0] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-29 21:09:22,187 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-29 21:09:22,187 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-29 21:09:22,281 [grpc-default-executor-0] INFO server.RaftServer$Division: om1@group-D66704EFC61C replies to ELECTION vote request: om2<-om1#0:OK-t1. Peer's state: om1@group-D66704EFC61C:t1, leader=null, voted=om2, raftlog=Memoized:om1@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:08:48,528 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: b423f7b1-089b-406b-a3aa-6a650e93a531: GrpcService started, listening on 9857
dn5_1    | 2023-06-29 21:08:48,534 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-b423f7b1-089b-406b-a3aa-6a650e93a531: Started
dn5_1    | 2023-06-29 21:08:48,534 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b423f7b1-089b-406b-a3aa-6a650e93a531 is started using port 9858 for RATIS
dn5_1    | 2023-06-29 21:08:48,534 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b423f7b1-089b-406b-a3aa-6a650e93a531 is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-06-29 21:08:48,535 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis b423f7b1-089b-406b-a3aa-6a650e93a531 is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-06-29 21:08:48,710 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:48,710 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:49,400 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-29 21:08:49,711 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:49,711 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:50,712 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:50,713 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 28efb866b4fe/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn3_1    | 2023-06-29 21:09:26,018 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-29 21:09:26,020 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-29 21:09:26,161 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8
dn3_1    | 2023-06-29 21:09:26,168 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8.
dn3_1    | 2023-06-29 21:09:26,751 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:27,752 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:28,753 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:29,753 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:30,754 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:31,163 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO impl.FollowerState: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5195155370ns, electionTimeout:5147ms
dn3_1    | 2023-06-29 21:09:31,165 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO impl.RoleInfo: 3561b5c6-83b9-4083-8461-2a79b0297944: shutdown 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState
dn3_1    | 2023-06-29 21:09:31,165 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-06-29 21:09:31,168 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-29 21:09:31,169 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-FollowerState] INFO impl.RoleInfo: 3561b5c6-83b9-4083-8461-2a79b0297944: start 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1
dn3_1    | 2023-06-29 21:09:31,186 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO impl.LeaderElection: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-29 21:09:31,188 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO impl.LeaderElection: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-06-29 21:09:31,202 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO impl.RoleInfo: 3561b5c6-83b9-4083-8461-2a79b0297944: shutdown 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1
dn3_1    | 2023-06-29 21:09:31,205 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-06-29 21:09:31,206 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8E0B4F3286C8 with new leaderId: 3561b5c6-83b9-4083-8461-2a79b0297944
dn3_1    | 2023-06-29 21:09:31,208 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: change Leader from null to 3561b5c6-83b9-4083-8461-2a79b0297944 at term 1 for becomeLeader, leader elected after 7018ms
dn3_1    | 2023-06-29 21:09:31,231 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-29 21:09:31,252 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-29 21:09:31,264 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-29 21:09:31,291 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-29 21:09:31,291 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-29 21:09:31,302 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-29 21:09:31,343 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-29 21:09:31,347 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-29 21:09:31,359 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO impl.RoleInfo: 3561b5c6-83b9-4083-8461-2a79b0297944: start 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderStateImpl
dn3_1    | 2023-06-29 21:09:31,405 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-29 21:09:31,453 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-LeaderElection1] INFO server.RaftServer$Division: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8: set configuration 0: peers:[3561b5c6-83b9-4083-8461-2a79b0297944|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-29 21:09:31,498 [3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 3561b5c6-83b9-4083-8461-2a79b0297944@group-8E0B4F3286C8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8/current/log_inprogress_0
dn4_1    | 2023-06-29 21:09:09,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:10,335 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:11,336 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:12,337 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:12,807 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-29 21:09:13,339 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:14,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:15,340 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:16,341 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:17,342 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:18,343 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:19,355 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:20,357 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:21,012 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-06-29 21:09:21,358 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:22,198 [Command processor thread] INFO server.RaftServer: 95c58c7b-97c6-4852-a177-ebb1478388b9: addNew group-8A4DAE2A0900:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-8A4DAE2A0900:java.util.concurrent.CompletableFuture@54c54c87[Not completed]
dn4_1    | 2023-06-29 21:09:22,317 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9: new RaftServerImpl for group-8A4DAE2A0900:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-29 21:09:22,318 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-29 21:09:22,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-29 21:09:22,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-29 21:09:22,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:22,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-29 21:09:22,336 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-29 21:09:22,364 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-29 21:09:24,176 [om1-server-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om2 at term 1 for appendEntries, leader elected after 10993ms
dn5_1    | 2023-06-29 21:08:50,713 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:50,717 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 28efb866b4fe/10.9.0.21 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn5_1    | 2023-06-29 21:08:51,403 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-29 21:08:51,718 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:51,723 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:52,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:52,724 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:53,719 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:53,727 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:54,721 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:54,727 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:22,406 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: ConfigurationManager, init=-1: peers:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-29 21:09:22,406 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-29 21:09:22,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-29 21:09:22,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-29 21:09:22,628 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:22,640 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-29 21:09:22,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-29 21:09:22,975 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-29 21:09:22,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-29 21:09:22,984 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-29 21:09:22,986 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-29 21:09:22,987 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-29 21:09:22,987 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/3a976013-1607-4227-9cfe-8a4dae2a0900 does not exist. Creating ...
dn4_1    | 2023-06-29 21:09:23,026 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/3a976013-1607-4227-9cfe-8a4dae2a0900/in_use.lock acquired by nodename 7@01689bb3b9ab
dn4_1    | 2023-06-29 21:09:23,126 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/3a976013-1607-4227-9cfe-8a4dae2a0900 has been successfully formatted.
dn4_1    | 2023-06-29 21:09:23,259 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-8A4DAE2A0900: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-29 21:09:23,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-29 21:09:23,363 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-29 21:09:23,364 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:09:23,364 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:23,377 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-29 21:09:23,381 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-29 21:09:23,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:23,443 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-29 21:09:23,463 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-29 21:09:23,547 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/3a976013-1607-4227-9cfe-8a4dae2a0900
dn4_1    | 2023-06-29 21:09:23,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-29 21:09:23,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:23,552 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:23,559 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-29 21:09:23,579 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-29 21:09:23,584 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-29 21:09:23,585 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-29 21:09:23,588 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-29 21:09:23,716 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:23,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:23,752 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:23,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:23,803 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:23,810 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:23,813 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: start as a follower, conf=-1: peers:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:23,819 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-29 21:09:23,827 [pool-22-thread-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState
dn4_1    | 2023-06-29 21:09:23,893 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:23,894 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:23,923 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-8A4DAE2A0900,id=95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:09:23,925 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:23,942 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-29 21:09:28,176 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:28,177 [pool-22-thread-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-29 21:09:28,179 [pool-22-thread-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState
dn1_1    | 2023-06-29 21:09:28,185 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1BB47A98493E,id=0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn1_1    | 2023-06-29 21:09:28,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-29 21:09:28,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-29 21:09:28,186 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-29 21:09:28,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-29 21:09:28,192 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:28,221 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:28,231 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035.
dn1_1    | 2023-06-29 21:09:28,622 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:29,255 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5207290883ns, electionTimeout:5144ms
dn1_1    | 2023-06-29 21:09:29,255 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState
dn1_1    | 2023-06-29 21:09:29,256 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-29 21:09:29,258 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-29 21:09:29,259 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1
dn1_1    | 2023-06-29 21:09:29,277 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:29,278 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-06-29 21:09:29,278 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1
dn1_1    | 2023-06-29 21:09:29,279 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-06-29 21:09:29,279 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-BF02ED3673EC with new leaderId: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn1_1    | 2023-06-29 21:09:29,280 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: change Leader from null to 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at term 1 for becomeLeader, leader elected after 6495ms
dn1_1    | 2023-06-29 21:09:29,330 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-06-29 21:09:29,355 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:09:29,365 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-29 21:09:29,391 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-06-29 21:09:29,393 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-06-29 21:09:29,399 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-06-29 21:09:29,447 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:09:29,460 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-06-29 21:09:29,474 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderStateImpl
dn1_1    | 2023-06-29 21:09:29,536 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-06-29 21:09:29,661 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:29,676 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5059537047ns, electionTimeout:5005ms
dn1_1    | 2023-06-29 21:09:29,690 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
om1_1    | 2023-06-29 21:09:24,314 [om1-server-thread2] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-29 21:09:24,373 [om1-server-thread2] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om1_1    | 2023-06-29 21:09:25,718 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om1_1    | 2023-06-29 21:09:28,768 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-06-29 21:10:01,506 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om1_1    | 2023-06-29 21:10:04,270 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om1_1    | 2023-06-29 21:10:13,182 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om1_1    | 2023-06-29 21:10:22,370 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-06-29 21:10:41,767 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 Received prepare request with log index 23
om1_1    | 2023-06-29 21:10:41,769 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om1 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om1_1    | 2023-06-29 21:10:41,770 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om1_1    | 2023-06-29 21:10:41,772 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om1_1    | 2023-06-29 21:10:41,772 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om1_1    | 2023-06-29 21:10:41,772 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om1_1    | 2023-06-29 21:10:41,774 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om1_1    | 2023-06-29 21:10:41,779 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om1_1    | 2023-06-29 21:10:41,781 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om1_1    | 2023-06-29 21:10:41,783 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 prepared at log index 23. Returning response txnID: 23
om1_1    |  with log index 23
dn1_1    | 2023-06-29 21:09:29,691 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-29 21:09:29,690 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 1, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:29,711 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-LeaderElection1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:29,711 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 1, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:29,711 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-29 21:09:29,719 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection2
dn1_1    | 2023-06-29 21:09:29,739 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-CANDIDATE: reject ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:29,743 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 1 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:09:29,743 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection2
dn1_1    | 2023-06-29 21:09:29,744 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:29,765 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:29,765 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:29,779 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t1. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t1, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:29,779 [grpc-default-executor-1] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:29,781 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:29,781 [grpc-default-executor-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:29,784 [grpc-default-executor-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:29,784 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:29,795 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t1. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t1, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:29,810 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:29,810 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:30,052 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-BF02ED3673EC-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/c2cded09-6bff-4911-8b65-bf02ed3673ec/current/log_inprogress_0
dn1_1    | 2023-06-29 21:09:30,691 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:31,692 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-29 21:09:33,168 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-06-29 21:09:33,271 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-1BB47A98493E, 1, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:33,271 [grpc-default-executor-1] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FOLLOWER: accept ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 0 <= candidate's priority 1
om2_1    | 2023-06-29 21:09:10,204 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-29 21:09:10,204 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-06-29 21:09:10,252 [pool-26-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-06-29 21:09:10,270 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-29 21:09:10,425 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-06-29 21:09:10,436 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-06-29 21:09:10,668 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om2_1    | 2023-06-29 21:09:10,680 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om2_1    | 2023-06-29 21:09:10,720 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om2_1    | 2023-06-29 21:09:11,717 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-29 21:09:11,739 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om2_1    | 2023-06-29 21:09:11,744 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 2023-06-29 21:09:11,750 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om2_1    | 2023-06-29 21:09:11,760 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om2_1    | 2023-06-29 21:09:13,460 [main] INFO reflections.Reflections: Reflections took 3023 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om2_1    | 2023-06-29 21:09:14,033 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om2_1    | 2023-06-29 21:09:14,089 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om2_1    | 2023-06-29 21:09:14,695 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om2_1    | 2023-06-29 21:09:14,751 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om2_1    | 2023-06-29 21:09:14,751 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om2_1    | 2023-06-29 21:09:14,918 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
om2_1    | 2023-06-29 21:09:14,918 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
om2_1    | 2023-06-29 21:09:14,920 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om2_1    | 2023-06-29 21:09:14,936 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@27415c8d6e4a
om2_1    | 2023-06-29 21:09:15,022 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1    | 2023-06-29 21:09:15,034 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-06-29 21:09:15,072 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-29 21:09:15,073 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-29 21:09:15,074 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om2_1    | 2023-06-29 21:09:15,075 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om2_1    | 2023-06-29 21:09:15,085 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-29 21:09:15,114 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om2_1    | 2023-06-29 21:09:15,115 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om2_1    | 2023-06-29 21:09:15,139 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om2_1    | 2023-06-29 21:09:15,161 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-06-29 21:09:15,175 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om2_1    | 2023-06-29 21:09:15,176 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om2_1    | 2023-06-29 21:09:15,181 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om2_1    | 2023-06-29 21:09:15,186 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om2_1    | 2023-06-29 21:09:15,188 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om2_1    | 2023-06-29 21:09:15,193 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om2_1    | 2023-06-29 21:09:15,195 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om2_1    | 2023-06-29 21:09:15,249 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om2_1    | 2023-06-29 21:09:15,250 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om2_1    | 2023-06-29 21:09:15,251 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om2_1    | 2023-06-29 21:09:15,252 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om2_1    | 2023-06-29 21:09:15,274 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om2_1    | 2023-06-29 21:09:15,276 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-06-29 21:09:15,291 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-29 21:09:15,292 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-29 21:09:31,755 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-29 21:09:33,162 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-29 21:08:55,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:55,728 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:56,722 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:56,729 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:57,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:57,730 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:58,724 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:58,731 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:59,725 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:08:59,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:00,726 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:00,732 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:01,727 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:01,733 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:02,728 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:02,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:03,729 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:03,734 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:04,735 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:05,737 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:06,738 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:07,739 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:08,741 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:09,742 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:10,744 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:11,798 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:12,799 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:12,805 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-29 21:09:13,800 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:14,800 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:15,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:16,802 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:17,803 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:18,804 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:19,805 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:20,805 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:21,405 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-29 21:09:21,807 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:22,650 [Command processor thread] INFO server.RaftServer: b423f7b1-089b-406b-a3aa-6a650e93a531: addNew group-67710B43930B:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-67710B43930B:java.util.concurrent.CompletableFuture@7d968417[Not completed]
dn5_1    | 2023-06-29 21:09:22,713 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531: new RaftServerImpl for group-67710B43930B:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-29 21:09:22,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-29 21:09:22,734 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-29 21:09:22,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-29 21:09:22,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:22,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-29 21:09:22,736 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-29 21:09:22,774 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: ConfigurationManager, init=-1: peers:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-29 21:09:22,778 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-29 21:09:22,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-29 21:09:22,808 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:22,809 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-29 21:09:22,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:22,863 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-29 21:09:22,872 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-29 21:09:23,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-29 21:09:23,200 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-29 21:09:23,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-29 21:09:23,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-29 21:09:23,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-29 21:09:33,272 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:09:33,272 [grpc-default-executor-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState
dn1_1    | 2023-06-29 21:09:33,272 [grpc-default-executor-1] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState
dn1_1    | 2023-06-29 21:09:33,272 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:33,303 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:33,303 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:33,312 [grpc-default-executor-1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:OK-t1. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E:t1, leader=null, voted=95c58c7b-97c6-4852-a177-ebb1478388b9, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:33,600 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1BB47A98493E with new leaderId: 95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:09:33,600 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35-server-thread1] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: change Leader from null to 95c58c7b-97c6-4852-a177-ebb1478388b9 at term 1 for appendEntries, leader elected after 5509ms
dn1_1    | 2023-06-29 21:09:33,730 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35-server-thread2] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:33,740 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35-server-thread2] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-06-29 21:09:33,750 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-1BB47A98493E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/current/log_inprogress_0
dn1_1    | 2023-06-29 21:09:34,857 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 2, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:34,858 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:34,858 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:34,858 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:34,858 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:34,858 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:34,871 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:34,871 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:34,871 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t2. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t2, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:39,981 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 3, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:39,981 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:39,982 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:39,982 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:39,982 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:39,982 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn2_1    | 2023-06-29 21:09:25,297 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-29 21:09:25,316 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-29 21:09:25,323 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-29 21:09:25,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-29 21:09:25,399 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-29 21:09:25,461 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c
dn2_1    | 2023-06-29 21:09:25,495 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-29 21:09:25,496 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-29 21:09:25,499 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-29 21:09:25,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-29 21:09:25,513 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-29 21:09:25,517 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-29 21:09:25,548 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-29 21:09:25,549 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-29 21:09:25,648 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-29 21:09:25,649 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-29 21:09:25,661 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-29 21:09:25,668 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-29 21:09:25,739 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-29 21:09:25,760 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-29 21:09:25,765 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:25,800 [pool-22-thread-1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: start as a follower, conf=-1: peers:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-29 21:09:25,801 [pool-22-thread-1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-29 21:09:39,987 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:39,988 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t3. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t3, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:39,989 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:45,091 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5109149685ns, electionTimeout:5102ms
dn1_1    | 2023-06-29 21:09:45,092 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:45,092 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn1_1    | 2023-06-29 21:09:45,092 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-29 21:09:45,092 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3
dn1_1    | 2023-06-29 21:09:45,094 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3 ELECTION round 0: submit vote requests at term 4 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:45,100 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:45,100 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:45,100 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3-1] INFO server.GrpcServerProtocolClient: Build channel for b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:45,105 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3-2] INFO server.GrpcServerProtocolClient: Build channel for 95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:09:45,174 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 4, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:45,174 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-CANDIDATE: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: already has voted for 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at current term 4
dn1_1    | 2023-06-29 21:09:45,175 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t4. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t4, leader=null, voted=0a9e83e4-93dc-44cc-b7f2-64f77839ab35, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.LeaderElection:   Response 0: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t4
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.LeaderElection:   Response 1: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t4
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3 ELECTION round 0: result REJECTED
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3
dn1_1    | 2023-06-29 21:09:45,240 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection3] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:45,245 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:45,246 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:50,335 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 5, (t:0, i:0))
dn2_1    | 2023-06-29 21:09:25,803 [pool-22-thread-1] INFO impl.RoleInfo: 653c0db6-d9db-4099-8201-751dcc40d458: start 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState
dn2_1    | 2023-06-29 21:09:25,844 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-9AE2FA46D77C,id=653c0db6-d9db-4099-8201-751dcc40d458
dn2_1    | 2023-06-29 21:09:25,849 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-29 21:09:25,859 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-29 21:09:25,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-29 21:09:25,863 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-29 21:09:25,864 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-29 21:09:25,866 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-29 21:09:26,024 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c
dn2_1    | 2023-06-29 21:09:26,025 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c.
dn2_1    | 2023-06-29 21:09:26,799 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:27,800 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:28,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:29,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:30,802 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:30,976 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO impl.FollowerState: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172498836ns, electionTimeout:5111ms
dn2_1    | 2023-06-29 21:09:30,977 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO impl.RoleInfo: 653c0db6-d9db-4099-8201-751dcc40d458: shutdown 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState
dn2_1    | 2023-06-29 21:09:30,978 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-06-29 21:09:30,983 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-06-29 21:09:30,983 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-FollowerState] INFO impl.RoleInfo: 653c0db6-d9db-4099-8201-751dcc40d458: start 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1
dn2_1    | 2023-06-29 21:09:30,992 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO impl.LeaderElection: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-29 21:09:30,993 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO impl.LeaderElection: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-06-29 21:09:30,994 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO impl.RoleInfo: 653c0db6-d9db-4099-8201-751dcc40d458: shutdown 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1
dn2_1    | 2023-06-29 21:09:30,994 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-06-29 21:09:30,996 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-9AE2FA46D77C with new leaderId: 653c0db6-d9db-4099-8201-751dcc40d458
dn2_1    | 2023-06-29 21:09:30,998 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: change Leader from null to 653c0db6-d9db-4099-8201-751dcc40d458 at term 1 for becomeLeader, leader elected after 6583ms
dn2_1    | 2023-06-29 21:09:31,024 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-29 21:09:31,035 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-29 21:09:31,035 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-06-29 21:09:31,040 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-29 21:09:31,043 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-06-29 21:09:31,052 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-29 21:09:31,069 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-29 21:09:31,075 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-29 21:09:31,080 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO impl.RoleInfo: 653c0db6-d9db-4099-8201-751dcc40d458: start 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderStateImpl
dn2_1    | 2023-06-29 21:09:31,115 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-29 21:09:31,268 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-LeaderElection1] INFO server.RaftServer$Division: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C: set configuration 0: peers:[653c0db6-d9db-4099-8201-751dcc40d458|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-29 21:09:31,359 [653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 653c0db6-d9db-4099-8201-751dcc40d458@group-9AE2FA46D77C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c/current/log_inprogress_0
dn2_1    | 2023-06-29 21:09:31,806 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-29 21:09:33,164 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om2_1    | 2023-06-29 21:09:15,293 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-29 21:09:15,329 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-06-29 21:09:15,329 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-29 21:09:15,329 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-29 21:09:15,344 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-06-29 21:09:15,345 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om2_1    | 2023-06-29 21:09:15,345 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om2_1    | 2023-06-29 21:09:15,346 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om2_1    | 2023-06-29 21:09:15,362 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
om2_1    | 2023-06-29 21:09:15,606 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
om2_1    | 2023-06-29 21:09:15,616 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om2_1    | 2023-06-29 21:09:15,626 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
om2_1    | 2023-06-29 21:09:15,829 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om2_1    | 2023-06-29 21:09:15,829 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-06-29 21:09:15,925 [Listener at om2/9862] INFO util.log: Logging initialized @25953ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-06-29 21:09:16,791 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-06-29 21:09:16,840 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om2_1    | 2023-06-29 21:09:16,873 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-06-29 21:09:16,888 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om2_1    | 2023-06-29 21:09:16,888 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-06-29 21:09:16,889 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-06-29 21:09:17,152 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om2_1    | 2023-06-29 21:09:17,166 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-06-29 21:09:17,349 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-06-29 21:09:17,354 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-06-29 21:09:17,375 [Listener at om2/9862] INFO server.session: node0 Scavenging every 660000ms
om2_1    | 2023-06-29 21:09:17,486 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-06-29 21:09:17,503 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-06-29 21:09:18,873 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-5768367337361912056/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om2_1    | 2023-06-29 21:09:18,914 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om2_1    | 2023-06-29 21:09:18,914 [Listener at om2/9862] INFO server.Server: Started @28941ms
om2_1    | 2023-06-29 21:09:18,934 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-06-29 21:09:18,934 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-06-29 21:09:18,936 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om2_1    | 2023-06-29 21:09:18,948 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-06-29 21:09:18,952 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om2_1    | 2023-06-29 21:09:19,243 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om2_1    | 2023-06-29 21:09:19,301 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2084e65a] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-06-29 21:09:20,458 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5165324557ns, electionTimeout:5128ms
om2_1    | 2023-06-29 21:09:20,460 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-29 21:09:20,460 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om2_1    | 2023-06-29 21:09:20,463 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1    | 2023-06-29 21:09:20,463 [om2@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-06-29 21:09:20,488 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-29 21:09:20,578 [om2@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om1
om2_1    | 2023-06-29 21:09:20,598 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om2_1    | 2023-06-29 21:09:20,601 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-29 21:09:20,603 [om2@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om3
dn1_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:50,336 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:50,340 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:50,343 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t5. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t5, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:50,356 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:50,357 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:55,417 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 6, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:55,417 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:55,417 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn1_1    | 2023-06-29 21:09:55,417 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:55,418 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:55,419 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:55,419 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:55,419 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:09:55,425 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t6. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t6, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:55,446 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 6, (t:0, i:0))
dn1_1    | 2023-06-29 21:09:55,447 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:09:55,448 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 6 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:09:55,448 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:55,448 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:09:55,449 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:09:55,458 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:09:55,449 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t6. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t6, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:09:55,470 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:10:00,502 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 7, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:23,944 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-29 21:09:23,945 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-29 21:09:24,103 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=3a976013-1607-4227-9cfe-8a4dae2a0900
dn4_1    | 2023-06-29 21:09:24,104 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=3a976013-1607-4227-9cfe-8a4dae2a0900.
dn4_1    | 2023-06-29 21:09:24,104 [Command processor thread] INFO server.RaftServer: 95c58c7b-97c6-4852-a177-ebb1478388b9: addNew group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-886978236035:java.util.concurrent.CompletableFuture@edf195a[Not completed]
dn4_1    | 2023-06-29 21:09:24,158 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9: new RaftServerImpl for group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-29 21:09:24,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-29 21:09:24,161 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-29 21:09:24,161 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-29 21:09:24,161 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:24,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-29 21:09:24,189 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-29 21:09:24,191 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-29 21:09:24,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-29 21:09:24,194 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-29 21:09:24,195 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-29 21:09:24,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:24,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-29 21:09:24,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-29 21:09:24,197 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-29 21:09:24,235 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-29 21:09:24,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-29 21:09:24,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-29 21:09:24,236 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-29 21:09:24,237 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 does not exist. Creating ...
dn4_1    | 2023-06-29 21:09:24,247 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/in_use.lock acquired by nodename 7@01689bb3b9ab
dn4_1    | 2023-06-29 21:09:24,262 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 has been successfully formatted.
dn4_1    | 2023-06-29 21:09:24,265 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-886978236035: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-29 21:09:24,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-29 21:09:24,265 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-29 21:09:24,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:09:24,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-29 21:09:24,266 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-29 21:09:24,267 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:24,267 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-29 21:09:24,269 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-29 21:09:24,269 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035
dn4_1    | 2023-06-29 21:09:24,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-29 21:09:24,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:24,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:24,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-29 21:09:24,295 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-29 21:09:24,296 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-29 21:09:24,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-29 21:09:24,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-29 21:09:24,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:24,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:24,314 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:24,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:24,315 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:24,320 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:24,320 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:24,320 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-29 21:09:24,328 [pool-22-thread-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:24,332 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-886978236035,id=95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:09:24,334 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:24,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-29 21:09:24,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-29 21:09:24,339 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-29 21:09:24,360 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8bd8ea2c-61df-4132-bcca-886978236035
dn4_1    | 2023-06-29 21:09:24,390 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:24,399 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:24,442 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:25,624 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:26,626 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:27,635 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:27,954 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035.
dn4_1    | 2023-06-29 21:09:27,963 [Command processor thread] INFO server.RaftServer: 95c58c7b-97c6-4852-a177-ebb1478388b9: addNew group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-1BB47A98493E:java.util.concurrent.CompletableFuture@29a968f8[Not completed]
dn4_1    | 2023-06-29 21:09:27,965 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9: new RaftServerImpl for group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-29 21:09:27,965 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 2023-06-29 21:09:27,966 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-29 21:09:27,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-29 21:09:27,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-29 21:09:27,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn4_1    | 2023-06-29 21:09:27,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn4_1    | 2023-06-29 21:09:27,967 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-29 21:09:27,968 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-29 21:09:22,583 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om2_1    | 2023-06-29 21:09:22,593 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om2<-om1#0:OK-t1
om2_1    | 2023-06-29 21:09:22,598 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om2@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om2_1    | 2023-06-29 21:09:22,599 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-LeaderElection1
om2_1    | 2023-06-29 21:09:22,601 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om2_1    | 2023-06-29 21:09:22,601 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om2 at term 1 for becomeLeader, leader elected after 11933ms
om2_1    | 2023-06-29 21:09:22,638 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1    | 2023-06-29 21:09:22,714 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-06-29 21:09:22,715 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-06-29 21:09:22,738 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om2_1    | 2023-06-29 21:09:22,755 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-06-29 21:09:22,755 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om2_1    | 2023-06-29 21:09:22,793 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-06-29 21:09:22,801 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-06-29 21:09:22,918 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-06-29 21:09:22,918 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-29 21:09:22,918 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-06-29 21:09:22,939 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-06-29 21:09:22,944 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-29 21:09:22,952 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-29 21:09:22,952 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-29 21:09:22,952 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-29 21:09:22,954 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-06-29 21:09:22,976 [om2@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-LeaderStateImpl
om2_1    | 2023-06-29 21:09:23,309 [om2@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om2_1    | 2023-06-29 21:09:23,730 [om2@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-29 21:09:24,366 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om2_1    | 2023-06-29 21:09:25,253 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om2_1    | [id: "om1"
om2_1    | address: "om1:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om3"
om2_1    | address: "om3:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | , id: "om2"
om2_1    | address: "om2:9872"
om2_1    | startupRole: FOLLOWER
om2_1    | ]
om2_1    | 2023-06-29 21:09:41,829 [qtp1099892020-47] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om2_1    | 2023-06-29 21:09:41,859 [qtp1099892020-47] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688072981829 in 29 milliseconds
om2_1    | 2023-06-29 21:09:41,913 [qtp1099892020-47] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 43 milliseconds
om2_1    | 2023-06-29 21:09:41,914 [qtp1099892020-47] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1688072981829
om2_1    | 2023-06-29 21:10:01,182 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om2_1    | 2023-06-29 21:10:04,281 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om2_1    | 2023-06-29 21:10:13,175 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om2_1    | 2023-06-29 21:10:22,354 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-06-29 21:10:41,756 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 Received prepare request with log index 23
om2_1    | 2023-06-29 21:10:41,757 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om2 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om2_1    | 2023-06-29 21:10:46,760 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om2_1    | 2023-06-29 21:10:46,761 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om2_1    | 2023-06-29 21:10:46,761 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om2_1    | 2023-06-29 21:10:46,761 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om2_1    | 2023-06-29 21:10:46,763 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om2@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om2_1    | 2023-06-29 21:10:46,765 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om2_1    | 2023-06-29 21:10:46,775 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om2_1    | 2023-06-29 21:10:46,777 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om2 prepared at log index 23. Returning response txnID: 23
om2_1    |  with log index 23
dn5_1    | 2023-06-29 21:09:23,226 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/6b21d635-226c-4f1b-bd6a-67710b43930b does not exist. Creating ...
dn5_1    | 2023-06-29 21:09:23,264 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/6b21d635-226c-4f1b-bd6a-67710b43930b/in_use.lock acquired by nodename 7@28efb866b4fe
dn5_1    | 2023-06-29 21:09:23,328 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/6b21d635-226c-4f1b-bd6a-67710b43930b has been successfully formatted.
dn5_1    | 2023-06-29 21:09:23,346 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-67710B43930B: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-29 21:09:23,415 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-29 21:09:23,440 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-29 21:09:23,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-29 21:09:23,508 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-29 21:09:23,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-29 21:09:23,597 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:23,632 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-29 21:09:23,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-29 21:09:23,691 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/6b21d635-226c-4f1b-bd6a-67710b43930b
dn5_1    | 2023-06-29 21:09:23,697 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-29 21:09:23,698 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-29 21:09:23,702 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:23,708 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-29 21:09:23,711 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-29 21:09:23,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-29 21:09:23,719 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-29 21:09:23,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-29 21:09:23,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:23,765 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:23,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:23,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:23,810 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:23,818 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:23,832 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:23,842 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: start as a follower, conf=-1: peers:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:23,843 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-06-29 21:09:23,844 [pool-22-thread-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState
dn5_1    | 2023-06-29 21:09:23,873 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-67710B43930B,id=b423f7b1-089b-406b-a3aa-6a650e93a531
dn5_1    | 2023-06-29 21:09:23,876 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:23,877 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:23,893 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:23,893 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-06-29 21:09:23,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-29 21:09:23,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-29 21:09:24,068 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=6b21d635-226c-4f1b-bd6a-67710b43930b
dn5_1    | 2023-06-29 21:09:24,071 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=6b21d635-226c-4f1b-bd6a-67710b43930b.
dn5_1    | 2023-06-29 21:09:24,073 [Command processor thread] INFO server.RaftServer: b423f7b1-089b-406b-a3aa-6a650e93a531: addNew group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-886978236035:java.util.concurrent.CompletableFuture@6ea2a655[Not completed]
dn5_1    | 2023-06-29 21:09:24,101 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531: new RaftServerImpl for group-886978236035:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-29 21:09:24,111 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-29 21:09:24,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-29 21:09:24,165 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-29 21:09:24,165 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:24,166 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-29 21:09:24,166 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-29 21:09:24,167 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-29 21:09:24,174 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-29 21:09:24,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-29 21:09:24,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-29 21:09:24,175 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:24,176 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-29 21:09:24,178 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-29 21:09:24,180 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-29 21:09:24,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-29 21:09:24,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-29 21:09:24,190 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-29 21:09:24,191 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-29 21:09:24,202 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 does not exist. Creating ...
dn5_1    | 2023-06-29 21:09:24,239 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/in_use.lock acquired by nodename 7@28efb866b4fe
dn5_1    | 2023-06-29 21:09:24,243 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035 has been successfully formatted.
dn5_1    | 2023-06-29 21:09:24,267 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-886978236035: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-29 21:09:24,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-29 21:09:24,276 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-29 21:09:24,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-29 21:09:24,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-29 21:09:24,277 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-29 21:09:24,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:24,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-29 21:09:24,291 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-29 21:09:24,299 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035
dn5_1    | 2023-06-29 21:09:24,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-29 21:09:24,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-29 21:09:24,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:24,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-29 21:09:24,304 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-29 21:09:24,305 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-29 21:09:24,306 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-29 21:09:24,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-29 21:09:24,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:24,313 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:24,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:24,315 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:24,320 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:24,335 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:24,356 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:24,357 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-06-29 21:09:24,357 [pool-22-thread-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:24,357 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-886978236035,id=b423f7b1-089b-406b-a3aa-6a650e93a531
dn5_1    | 2023-06-29 21:09:24,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:24,359 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-06-29 21:09:24,361 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-29 21:09:24,361 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-29 21:09:24,362 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:24,389 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=8bd8ea2c-61df-4132-bcca-886978236035
dn5_1    | 2023-06-29 21:09:24,390 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:24,823 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:25,843 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:26,846 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:27,847 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:28,137 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035.
dn5_1    | 2023-06-29 21:09:28,147 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531: new RaftServerImpl for group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-29 21:09:28,148 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-29 21:09:28,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-29 21:09:28,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-29 21:09:28,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:28,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-29 21:09:28,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-29 21:09:28,152 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: ConfigurationManager, init=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-29 21:09:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-29 21:09:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-29 21:09:28,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-29 21:09:28,156 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-29 21:09:28,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-29 21:09:28,157 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-29 21:09:28,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-29 21:09:28,158 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-29 21:09:28,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-29 21:09:28,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-29 21:09:28,159 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-29 21:09:28,159 [Command processor thread] INFO server.RaftServer: b423f7b1-089b-406b-a3aa-6a650e93a531: addNew group-1BB47A98493E:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns      null b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn4_1    | 2023-06-29 21:09:27,968 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-29 21:09:27,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-29 21:09:27,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-29 21:09:27,969 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-29 21:09:27,969 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e does not exist. Creating ...
dn4_1    | 2023-06-29 21:09:27,971 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/in_use.lock acquired by nodename 7@01689bb3b9ab
dn4_1    | 2023-06-29 21:09:27,975 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e has been successfully formatted.
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1BB47A98493E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-29 21:09:27,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn4_1    | 2023-06-29 21:09:27,981 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-29 21:09:27,985 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-29 21:09:28,010 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-29 21:09:28,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:28,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-29 21:09:28,011 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:28,012 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:28,012 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-29 21:09:28,012 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:28,013 [pool-22-thread-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn4_1    | 2023-06-29 21:09:28,021 [pool-22-thread-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState
dn4_1    | 2023-06-29 21:09:28,033 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1BB47A98493E,id=95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:09:28,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-29 21:09:28,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn4_1    | 2023-06-29 21:09:28,033 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn4_1    | 2023-06-29 21:09:28,034 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-29 21:09:28,036 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e
dn4_1    | 2023-06-29 21:09:28,088 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:28,177 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:10:00,502 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FOLLOWER: reject ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 1 > candidate's priority 0
dn1_1    | 2023-06-29 21:10:00,502 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn1_1    | 2023-06-29 21:10:00,502 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:10:00,502 [grpc-default-executor-0] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:10:00,503 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState was interrupted
dn1_1    | 2023-06-29 21:10:00,508 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t7. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t7, leader=null, voted=null, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:10:00,511 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:10:00,512 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:10:05,571 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.FollowerState: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5068742465ns, electionTimeout:5059ms
dn1_1    | 2023-06-29 21:10:05,572 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState
dn1_1    | 2023-06-29 21:10:05,572 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn1_1    | 2023-06-29 21:10:05,572 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-29 21:10:05,572 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-FollowerState] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4
dn1_1    | 2023-06-29 21:10:05,578 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4 ELECTION round 0: submit vote requests at term 8 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:10:05,580 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-29 21:10:05,580 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-29 21:10:05,599 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 8, (t:0, i:0))
dn1_1    | 2023-06-29 21:10:05,603 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn1_1    | 2023-06-29 21:10:05,607 [grpc-default-executor-0] INFO impl.VoteContext: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-CANDIDATE: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: already has voted for 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at current term 8
dn1_1    | 2023-06-29 21:10:05,607 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.LeaderElection:   Response 0: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t8
dn1_1    | 2023-06-29 21:10:05,608 [grpc-default-executor-0] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t8. Peer's state: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035:t8, leader=null, voted=0a9e83e4-93dc-44cc-b7f2-64f77839ab35, raftlog=Memoized:0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-29 21:10:05,608 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4 ELECTION round 0: result PASSED
dn1_1    | 2023-06-29 21:10:05,608 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: shutdown 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4
dn1_1    | 2023-06-29 21:10:05,608 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: changes role from CANDIDATE to LEADER at term 8 for changeToLeader
dn1_1    | 2023-06-29 21:10:05,608 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-886978236035 with new leaderId: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn4_1    | 2023-06-29 21:09:28,502 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e.
dn4_1    | 2023-06-29 21:09:28,636 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:29,000 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5172940957ns, electionTimeout:5103ms
dn4_1    | 2023-06-29 21:09:29,000 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState
dn4_1    | 2023-06-29 21:09:29,001 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-06-29 21:09:29,004 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-29 21:09:29,004 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1
dn4_1    | 2023-06-29 21:09:29,018 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:29,020 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-06-29 21:09:29,021 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1
dn4_1    | 2023-06-29 21:09:29,024 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-06-29 21:09:29,024 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-8A4DAE2A0900 with new leaderId: 95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:09:29,024 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: change Leader from null to 95c58c7b-97c6-4852-a177-ebb1478388b9 at term 1 for becomeLeader, leader elected after 6416ms
dn4_1    | 2023-06-29 21:09:29,035 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-06-29 21:09:29,046 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:29,047 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-06-29 21:09:29,058 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-06-29 21:09:29,062 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-06-29 21:09:29,069 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-29 21:09:29,097 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:29,117 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-06-29 21:09:29,136 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderStateImpl
dn4_1    | 2023-06-29 21:09:29,197 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-29 21:09:29,333 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-LeaderElection1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900: set configuration 0: peers:[95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:29,426 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-8A4DAE2A0900-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/3a976013-1607-4227-9cfe-8a4dae2a0900/current/log_inprogress_0
dn4_1    | 2023-06-29 21:09:29,524 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5195760935ns, electionTimeout:5082ms
dn4_1    | 2023-06-29 21:09:29,524 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:29,525 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-06-29 21:09:29,525 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-29 21:09:29,525 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2
dn4_1    | 2023-06-29 21:09:29,547 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:29,572 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:29,572 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:29,572 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn4_1    | 2023-06-29 21:09:29,572 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for b423f7b1-089b-406b-a3aa-6a650e93a531
dn4_1    | 2023-06-29 21:09:29,641 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:29,684 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 1, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:29,686 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-CANDIDATE: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: already has voted for 95c58c7b-97c6-4852-a177-ebb1478388b9 at current term 1
dn4_1    | 2023-06-29 21:09:29,702 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t1. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t1, leader=null, voted=95c58c7b-97c6-4852-a177-ebb1478388b9, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:29,819 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn4_1    | 2023-06-29 21:09:29,819 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.LeaderElection:   Response 0: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t1
dn4_1    | 2023-06-29 21:09:29,820 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.LeaderElection:   Response 1: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t1
dn4_1    | 2023-06-29 21:09:29,820 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2 ELECTION round 0: result REJECTED
dn4_1    | 2023-06-29 21:09:29,821 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn4_1    | 2023-06-29 21:09:29,824 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2
dn4_1    | 2023-06-29 21:09:29,825 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection2] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:29,845 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:29,845 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:30,646 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:31,646 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-29 21:09:33,161 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-29 21:09:33,249 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5228042698ns, electionTimeout:5071ms
dn4_1    | 2023-06-29 21:09:33,250 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState
dn4_1    | 2023-06-29 21:09:33,250 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn4_1    | 2023-06-29 21:09:33,251 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-29 21:09:33,251 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3
dn4_1    | 2023-06-29 21:09:33,253 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:33,260 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:33,261 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:28,160 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e does not exist. Creating ...
dn5_1    | 2023-06-29 21:09:28,161 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/in_use.lock acquired by nodename 7@28efb866b4fe
dn5_1    | 2023-06-29 21:09:28,171 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e has been successfully formatted.
dn5_1    | 2023-06-29 21:09:28,173 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1BB47A98493E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-29 21:09:28,173 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn5_1    | 2023-06-29 21:09:28,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-29 21:09:28,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-29 21:09:28,182 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn5_1    | 2023-06-29 21:09:28,183 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn5_1    | 2023-06-29 21:09:28,184 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:28,196 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn5_1    | 2023-06-29 21:09:28,197 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn5_1    | 2023-06-29 21:09:28,198 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e
dn5_1    | 2023-06-29 21:09:28,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn5_1    | 2023-06-29 21:09:28,198 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn5_1    | 2023-06-29 21:09:28,199 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:28,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn5_1    | 2023-06-29 21:09:28,201 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn5_1    | 2023-06-29 21:09:28,205 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn5_1    | 2023-06-29 21:09:28,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn5_1    | 2023-06-29 21:09:28,206 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-29 21:09:28,207 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn5_1    | 2023-06-29 21:09:28,212 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:28,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn5_1    | 2023-06-29 21:09:28,214 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:28,216 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:28,217 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn5_1    | 2023-06-29 21:09:28,220 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: start as a follower, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:28,221 [pool-22-thread-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn5_1    | 2023-06-29 21:09:28,224 [pool-22-thread-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState
dn5_1    | 2023-06-29 21:09:28,225 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1BB47A98493E,id=b423f7b1-089b-406b-a3aa-6a650e93a531
dn5_1    | 2023-06-29 21:09:28,225 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:28,226 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-29 21:09:28,226 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:28,227 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn5_1    | 2023-06-29 21:09:28,229 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn5_1    | 2023-06-29 21:09:28,229 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn5_1    | 2023-06-29 21:09:28,230 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e
dn5_1    | 2023-06-29 21:09:28,542 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e.
dn5_1    | 2023-06-29 21:09:28,849 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:29,056 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5212455098ns, electionTimeout:5162ms
dn5_1    | 2023-06-29 21:09:29,057 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState
dn5_1    | 2023-06-29 21:09:29,057 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-29 21:10:05,609 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: change Leader from null to 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at term 8 for becomeLeader, leader elected after 41176ms
dn1_1    | 2023-06-29 21:10:05,609 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-06-29 21:10:05,609 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:10:05,610 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-29 21:10:05,612 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-06-29 21:10:05,612 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-06-29 21:10:05,612 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-06-29 21:10:05,613 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-29 21:10:05,613 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-06-29 21:10:05,682 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-06-29 21:10:05,682 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:10:05,683 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-06-29 21:10:05,694 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-06-29 21:10:05,699 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-29 21:10:05,699 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-29 21:10:05,700 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-29 21:10:05,700 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-06-29 21:10:05,712 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn1_1    | 2023-06-29 21:10:05,714 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-29 21:10:05,714 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn1_1    | 2023-06-29 21:10:05,716 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn1_1    | 2023-06-29 21:10:05,716 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-29 21:10:05,716 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-29 21:10:05,719 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-29 21:10:05,719 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn1_1    | 2023-06-29 21:10:05,724 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO impl.RoleInfo: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: start 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderStateImpl
dn1_1    | 2023-06-29 21:10:05,727 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-06-29 21:10:05,729 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/current/log_inprogress_0
dn1_1    | 2023-06-29 21:10:05,762 [0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035-LeaderElection4] INFO server.RaftServer$Division: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35@group-886978236035: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-29 21:08:00,583 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 259b15bd05c4/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--init]
om3_1    | STARTUP_MSG:   version = 1.3.0
dn5_1    | 2023-06-29 21:09:29,060 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:29,060 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1
dn5_1    | 2023-06-29 21:09:29,065 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:29,066 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn5_1    | 2023-06-29 21:09:29,066 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1
dn5_1    | 2023-06-29 21:09:29,067 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-06-29 21:09:29,067 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-67710B43930B with new leaderId: b423f7b1-089b-406b-a3aa-6a650e93a531
dn5_1    | 2023-06-29 21:09:29,067 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: change Leader from null to b423f7b1-089b-406b-a3aa-6a650e93a531 at term 1 for becomeLeader, leader elected after 6218ms
dn5_1    | 2023-06-29 21:09:29,084 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-06-29 21:09:29,088 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-29 21:09:29,095 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-06-29 21:09:29,115 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn5_1    | 2023-06-29 21:09:29,116 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn5_1    | 2023-06-29 21:09:29,122 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn5_1    | 2023-06-29 21:09:29,138 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-29 21:09:29,148 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn5_1    | 2023-06-29 21:09:29,159 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderStateImpl
dn5_1    | 2023-06-29 21:09:29,191 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-29 21:09:29,278 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-LeaderElection1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B: set configuration 0: peers:[b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:29,408 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-67710B43930B-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/6b21d635-226c-4f1b-bd6a-67710b43930b/current/log_inprogress_0
dn5_1    | 2023-06-29 21:09:29,446 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5089514491ns, electionTimeout:5055ms
dn5_1    | 2023-06-29 21:09:29,449 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:29,449 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn5_1    | 2023-06-29 21:09:29,450 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:29,450 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2
dn5_1    | 2023-06-29 21:09:29,492 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:29,563 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn5_1    | 2023-06-29 21:09:29,568 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:29,570 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:29,571 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 95c58c7b-97c6-4852-a177-ebb1478388b9
dn5_1    | 2023-06-29 21:09:29,686 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 1, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:33,278 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-06-29 21:09:33,278 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.LeaderElection:   Response 0: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:OK-t1
dn4_1    | 2023-06-29 21:09:33,279 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3 ELECTION round 0: result PASSED
dn4_1    | 2023-06-29 21:09:33,279 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3
dn4_1    | 2023-06-29 21:09:33,279 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn4_1    | 2023-06-29 21:09:33,279 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1BB47A98493E with new leaderId: 95c58c7b-97c6-4852-a177-ebb1478388b9
dn4_1    | 2023-06-29 21:09:33,280 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: change Leader from null to 95c58c7b-97c6-4852-a177-ebb1478388b9 at term 1 for becomeLeader, leader elected after 5312ms
dn4_1    | 2023-06-29 21:09:33,280 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn4_1    | 2023-06-29 21:09:33,281 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:33,281 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn4_1    | 2023-06-29 21:09:33,281 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-06-29 21:09:33,282 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn4_1    | 2023-06-29 21:09:33,282 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn4_1    | 2023-06-29 21:09:33,282 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn4_1    | 2023-06-29 21:09:33,282 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn4_1    | 2023-06-29 21:09:33,381 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-06-29 21:09:33,381 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:09:33,381 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-06-29 21:09:33,386 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-06-29 21:09:33,392 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-29 21:09:33,394 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-29 21:09:33,394 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-29 21:09:33,394 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-06-29 21:09:33,400 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 2023-06-29 21:09:33,401 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-29 21:09:33,404 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn4_1    | 2023-06-29 21:09:33,404 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn4_1    | 2023-06-29 21:09:33,404 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-29 21:09:33,405 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-29 21:09:33,405 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-29 21:09:33,405 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn4_1    | 2023-06-29 21:09:33,406 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderStateImpl
dn4_1    | 2023-06-29 21:09:33,408 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-29 21:09:33,424 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-LeaderElection3] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:33,428 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-1BB47A98493E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/current/log_inprogress_0
dn4_1    | 2023-06-29 21:09:34,889 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 2, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:34,889 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: accept ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 0 <= candidate's priority 0
dn4_1    | 2023-06-29 21:09:34,892 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn4_1    | 2023-06-29 21:09:34,893 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:34,893 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:34,893 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState was interrupted
dn4_1    | 2023-06-29 21:09:34,895 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:34,895 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:34,900 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t2. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t2, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:39,998 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 3, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:40,001 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: accept ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 0 <= candidate's priority 0
dn4_1    | 2023-06-29 21:09:40,001 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 3 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn4_1    | 2023-06-29 21:09:40,001 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:40,001 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState was interrupted
dn4_1    | 2023-06-29 21:09:40,001 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:40,005 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:40,005 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:40,006 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t3. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t3, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:45,060 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:45,061 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:45,146 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 4, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:45,146 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: accept ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 0 <= candidate's priority 0
dn4_1    | 2023-06-29 21:09:45,147 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 4 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn4_1    | 2023-06-29 21:09:45,147 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:45,147 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:45,147 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState was interrupted
dn4_1    | 2023-06-29 21:09:45,148 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:45,148 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:29,687 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-CANDIDATE: reject ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: already has voted for b423f7b1-089b-406b-a3aa-6a650e93a531 at current term 1
dn5_1    | 2023-06-29 21:09:29,706 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t1. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035:t1, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:29,829 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:29,830 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t1
dn5_1    | 2023-06-29 21:09:29,830 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.LeaderElection:   Response 1: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t1
dn5_1    | 2023-06-29 21:09:29,830 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:29,831 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
dn5_1    | 2023-06-29 21:09:29,831 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2
dn5_1    | 2023-06-29 21:09:29,831 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection2] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:29,839 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:29,841 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:29,850 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:30,850 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:31,851 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-29 21:09:33,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-29 21:09:33,266 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-1BB47A98493E, 1, (t:0, i:0))
dn5_1    | 2023-06-29 21:09:33,266 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FOLLOWER: accept ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 0 <= candidate's priority 1
dn5_1    | 2023-06-29 21:09:33,266 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn5_1    | 2023-06-29 21:09:33,266 [grpc-default-executor-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState
dn5_1    | 2023-06-29 21:09:33,266 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState was interrupted
dn5_1    | 2023-06-29 21:09:33,267 [grpc-default-executor-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState
dn5_1    | 2023-06-29 21:09:33,268 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:33,268 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:33,273 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:OK-t1. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E:t1, leader=null, voted=95c58c7b-97c6-4852-a177-ebb1478388b9, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:33,568 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1BB47A98493E with new leaderId: 95c58c7b-97c6-4852-a177-ebb1478388b9
dn5_1    | 2023-06-29 21:09:33,568 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: change Leader from null to 95c58c7b-97c6-4852-a177-ebb1478388b9 at term 1 for appendEntries, leader elected after 5411ms
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-06-29 21:08:00,630 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-06-29 21:08:10,012 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-06-29 21:08:13,216 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-29 21:08:13,596 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-29 21:08:13,596 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-29 21:08:13,600 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-29 21:08:14,394 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-29 21:08:18,030 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:20,034 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:22,036 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:24,038 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:26,040 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:28,042 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:30,044 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:32,046 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:34,048 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:36,050 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:38,052 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:40,053 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:42,055 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:44,057 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:46,089 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:67dd125e-a7b5-412f-8128-ba59bd3344fe is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:48,091 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-29 21:08:50,093 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 259b15bd05c4/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018;layoutVersion=3
om3_1    | 2023-06-29 21:08:52,180 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1    | /************************************************************
om3_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 259b15bd05c4/10.9.0.13
om3_1    | ************************************************************/
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-29 21:08:57,802 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = 259b15bd05c4/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--]
om3_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-06-29 21:08:57,827 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-06-29 21:09:02,095 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-06-29 21:09:04,102 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-29 21:09:04,432 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-29 21:09:04,432 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-29 21:09:04,445 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-29 21:09:04,558 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1    | 2023-06-29 21:09:05,635 [main] INFO reflections.Reflections: Reflections took 900 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om3_1    | 2023-06-29 21:09:05,709 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-29 21:09:06,907 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-29 21:09:07,236 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-29 21:09:10,002 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-29 21:09:10,857 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-06-29 21:09:10,867 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-06-29 21:09:11,980 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-06-29 21:09:12,150 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om3_1    | 2023-06-29 21:09:12,300 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-29 21:09:12,301 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-06-29 21:09:12,363 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-06-29 21:09:13,005 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-06-29 21:09:13,195 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-29 21:09:13,389 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-06-29 21:09:13,462 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1    | 2023-06-29 21:09:13,725 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-06-29 21:09:14,092 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-29 21:09:14,096 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-29 21:09:14,098 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-29 21:09:14,098 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-29 21:09:14,100 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-06-29 21:09:14,100 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-06-29 21:09:14,100 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-06-29 21:09:14,105 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-29 21:09:14,114 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-06-29 21:09:14,115 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-06-29 21:09:14,143 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-06-29 21:09:14,152 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-06-29 21:09:14,157 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-06-29 21:09:15,039 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-06-29 21:09:15,041 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-06-29 21:09:15,046 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-06-29 21:09:15,046 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-29 21:09:15,047 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-29 21:09:15,076 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-29 21:09:15,096 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@c3719e5[Not completed]
om3_1    | 2023-06-29 21:09:15,096 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-06-29 21:09:15,260 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-06-29 21:09:15,288 [pool-26-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-06-29 21:09:15,311 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-06-29 21:09:15,318 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-06-29 21:09:15,318 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-29 21:09:33,690 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:0|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:33,700 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-29 21:09:33,721 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-1BB47A98493E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/a67dd9f1-63f5-4458-8ea5-1bb47a98493e/current/log_inprogress_0
dn5_1    | 2023-06-29 21:09:34,841 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5010123155ns, electionTimeout:5000ms
dn5_1    | 2023-06-29 21:09:34,842 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:34,842 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn5_1    | 2023-06-29 21:09:34,842 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:34,843 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3
dn5_1    | 2023-06-29 21:09:34,854 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3 ELECTION round 0: submit vote requests at term 2 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:34,882 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:34,883 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:34,884 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:34,884 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t2
dn5_1    | 2023-06-29 21:09:34,887 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:34,897 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 2 for REJECTED
dn5_1    | 2023-06-29 21:09:34,898 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3
dn5_1    | 2023-06-29 21:09:34,898 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection3] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:34,908 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:34,913 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:39,972 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5073628797ns, electionTimeout:5059ms
dn5_1    | 2023-06-29 21:09:39,972 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:39,973 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 2 for changeToCandidate
dn5_1    | 2023-06-29 21:09:39,974 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:39,974 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4
dn5_1    | 2023-06-29 21:09:39,977 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4 ELECTION round 0: submit vote requests at term 3 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:40,011 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:40,011 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:40,011 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:40,011 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t3
dn5_1    | 2023-06-29 21:09:40,013 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:40,017 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 3 for REJECTED
dn5_1    | 2023-06-29 21:09:40,017 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4
dn5_1    | 2023-06-29 21:09:40,017 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection4] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:40,018 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:40,018 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:45,128 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5111118410ns, electionTimeout:5110ms
dn5_1    | 2023-06-29 21:09:45,128 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:45,129 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 3 for changeToCandidate
dn5_1    | 2023-06-29 21:09:45,129 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:45,129 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5
dn5_1    | 2023-06-29 21:09:45,134 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5 ELECTION round 0: submit vote requests at term 4 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:45,135 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:45,142 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5: ELECTION REJECTED received 2 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t4
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.LeaderElection:   Response 1: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t4
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 4 for REJECTED
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5
dn5_1    | 2023-06-29 21:09:45,178 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection5] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:45,179 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:45,182 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:45,206 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: receive requestVote(ELECTION, 0a9e83e4-93dc-44cc-b7f2-64f77839ab35, group-886978236035, 4, (t:0, i:0))
dn5_1    | 2023-06-29 21:09:45,209 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FOLLOWER: reject ELECTION from 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: already has voted for b423f7b1-089b-406b-a3aa-6a650e93a531 at current term 4
dn5_1    | 2023-06-29 21:09:45,209 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035 replies to ELECTION vote request: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t4. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035:t4, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:50,327 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5148514923ns, electionTimeout:5144ms
dn5_1    | 2023-06-29 21:09:50,327 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-06-29 21:08:02,154 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 4429ee788db7/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.3.0
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-06-29 21:08:00,322 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
s3g_1    | 2023-06-29 21:08:00,364 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-06-29 21:08:00,597 [main] INFO util.log: Logging initialized @10795ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-06-29 21:08:02,067 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-06-29 21:08:02,289 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
s3g_1    | 2023-06-29 21:08:02,361 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-06-29 21:08:02,392 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-06-29 21:08:02,392 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
s3g_1    | 2023-06-29 21:08:02,403 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-06-29 21:08:02,862 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
s3g_1    | STARTUP_MSG:   host = e680cd802abb/10.9.0.23
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.3.0
dn4_1    | 2023-06-29 21:09:45,149 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t4. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t4, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:45,231 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, 0a9e83e4-93dc-44cc-b7f2-64f77839ab35, group-886978236035, 4, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:45,231 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: reject ELECTION from 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: already has voted for b423f7b1-089b-406b-a3aa-6a650e93a531 at current term 4
dn4_1    | 2023-06-29 21:09:45,231 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t4. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t4, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:50,256 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:50,257 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:50,333 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 5, (t:0, i:0))
dn4_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: accept ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: our priority 0 <= candidate's priority 0
dn4_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 5 for candidate:b423f7b1-089b-406b-a3aa-6a650e93a531
dn4_1    | 2023-06-29 21:09:50,336 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:50,337 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:50,337 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState was interrupted
dn4_1    | 2023-06-29 21:09:50,353 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t5. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t5, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:50,355 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:50,359 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:55,409 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5055898890ns, electionTimeout:5048ms
dn4_1    | 2023-06-29 21:09:55,409 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:55,410 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn4_1    | 2023-06-29 21:09:55,410 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-29 21:09:55,411 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4
dn4_1    | 2023-06-29 21:09:55,428 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4 ELECTION round 0: submit vote requests at term 6 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:55,429 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:55,441 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:09:55,448 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 6, (t:0, i:0))
om3_1    | 2023-06-29 21:09:15,318 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-29 21:09:15,332 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-29 21:09:15,332 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-06-29 21:09:15,518 [pool-26-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-06-29 21:09:15,561 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-29 21:09:15,664 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-06-29 21:09:15,682 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-06-29 21:09:15,814 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-06-29 21:09:15,875 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-06-29 21:09:15,875 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-06-29 21:09:17,489 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-29 21:09:17,505 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-06-29 21:09:17,519 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-06-29 21:09:17,524 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-06-29 21:09:17,530 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-06-29 21:09:19,004 [main] INFO reflections.Reflections: Reflections took 3397 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om3_1    | 2023-06-29 21:09:19,334 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-06-29 21:09:19,359 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-06-29 21:09:19,802 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-06-29 21:09:19,834 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-06-29 21:09:19,835 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-06-29 21:09:19,980 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-06-29 21:09:19,980 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-06-29 21:09:19,982 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1    | 2023-06-29 21:09:20,001 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@259b15bd05c4
om3_1    | 2023-06-29 21:09:20,041 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1    | 2023-06-29 21:09:20,063 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-06-29 21:09:20,080 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-06-29 21:09:20,081 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-29 21:09:20,084 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-06-29 21:09:20,087 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-06-29 21:09:20,096 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-29 21:09:20,104 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-06-29 21:09:20,110 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-06-29 21:09:20,120 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-06-29 21:09:20,121 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-06-29 21:09:20,121 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-06-29 21:09:20,125 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-29 21:09:20,128 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-06-29 21:09:20,128 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-06-29 21:09:20,129 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-06-29 21:09:20,131 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-06-29 21:09:20,132 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-06-29 21:09:20,157 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-06-29 21:09:20,160 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-06-29 21:09:20,160 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-06-29 21:09:20,161 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-06-29 21:09:20,174 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-29 21:09:20,174 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-29 21:09:20,178 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
s3g_1    | ************************************************************/
s3g_1    | 2023-06-29 21:08:02,917 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-06-29 21:08:03,128 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-06-29 21:08:03,872 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-06-29 21:08:05,319 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-06-29 21:08:05,327 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-06-29 21:08:05,577 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-06-29 21:08:05,596 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1    | 2023-06-29 21:08:05,943 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-06-29 21:08:05,943 [main] INFO server.session: No SessionScavenger set, using defaults
s3g_1    | 2023-06-29 21:08:05,951 [main] INFO server.session: node0 Scavenging every 660000ms
s3g_1    | 2023-06-29 21:08:06,311 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-06-29 21:08:06,349 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
s3g_1    | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1    | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | Jun 29, 2023 9:08:34 PM org.glassfish.jersey.internal.Errors logErrors
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
s3g_1    | 
s3g_1    | 2023-06-29 21:08:34,404 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7c5f29c6{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-12640273118123252316/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
dn4_1    | 2023-06-29 21:09:55,452 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-CANDIDATE: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: already has voted for 95c58c7b-97c6-4852-a177-ebb1478388b9 at current term 6
dn4_1    | 2023-06-29 21:09:55,452 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t6. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t6, leader=null, voted=95c58c7b-97c6-4852-a177-ebb1478388b9, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.LeaderElection:   Response 0: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t6
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4 ELECTION round 0: result REJECTED
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4
dn4_1    | 2023-06-29 21:09:55,463 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection4] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:09:55,474 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:09:55,475 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:10:00,491 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5028420965ns, electionTimeout:5016ms
dn4_1    | 2023-06-29 21:10:00,492 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:10:00,492 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 6 for changeToCandidate
dn4_1    | 2023-06-29 21:10:00,492 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-29 21:10:00,492 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5
dn4_1    | 2023-06-29 21:10:00,498 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5 ELECTION round 0: submit vote requests at term 7 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:10:00,503 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:10:00,504 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.LeaderElection:   Response 0: 95c58c7b-97c6-4852-a177-ebb1478388b9<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t7
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.LeaderElection: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5 ELECTION round 0: result REJECTED
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 7 for REJECTED
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5
dn4_1    | 2023-06-29 21:10:00,512 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-LeaderElection5] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:10:00,526 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:10:00,527 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:10:05,593 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, 0a9e83e4-93dc-44cc-b7f2-64f77839ab35, group-886978236035, 8, (t:0, i:0))
dn4_1    | 2023-06-29 21:10:05,593 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: accept ELECTION from 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-29 21:10:05,593 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 8 for candidate:0a9e83e4-93dc-44cc-b7f2-64f77839ab35
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
recon_1  | ************************************************************/
recon_1  | 2023-06-29 21:08:02,225 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-06-29 21:08:06,441 [main] INFO reflections.Reflections: Reflections took 286 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1  | 2023-06-29 21:08:11,701 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-06-29 21:08:13,904 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-29 21:08:21,872 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-06-29 21:08:23,461 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-06-29 21:08:23,468 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.006 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-06-29 21:08:23,500 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-29 21:08:23,676 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-06-29 21:08:23,688 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-06-29 21:08:26,448 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-06-29 21:08:29,960 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-06-29 21:08:30,112 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-06-29 21:08:30,222 [main] INFO util.log: Logging initialized @37965ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-06-29 21:08:31,422 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-06-29 21:08:31,534 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-06-29 21:08:31,581 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-06-29 21:08:31,641 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-06-29 21:08:31,652 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-06-29 21:08:31,653 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-06-29 21:08:32,969 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-06-29 21:08:34,700 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-06-29 21:08:34,712 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1  | 2023-06-29 21:08:34,720 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-06-29 21:08:34,766 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-06-29 21:08:34,771 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-06-29 21:08:36,550 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-29 21:08:36,801 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-29 21:08:36,909 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1  | 2023-06-29 21:08:36,911 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-06-29 21:08:37,053 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-29 21:08:37,265 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-06-29 21:08:37,386 [main] INFO reflections.Reflections: Reflections took 105 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 2023-06-29 21:08:37,487 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-06-29 21:08:37,534 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-06-29 21:08:37,545 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1  | 2023-06-29 21:08:37,551 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-06-29 21:08:37,606 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-29 21:08:37,638 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-06-29 21:08:37,708 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-06-29 21:08:37,747 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 2023-06-29 21:08:37,836 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-06-29 21:08:37,836 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-06-29 21:08:37,910 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-06-29 21:08:37,941 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-06-29 21:08:37,941 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-06-29 21:08:38,387 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
recon_1  | 2023-06-29 21:08:38,390 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-06-29 21:08:38,469 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-06-29 21:08:38,469 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-06-29 21:08:38,474 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 660000ms
recon_1  | 2023-06-29 21:08:38,499 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-06-29 21:08:38,502 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-06-29 21:08:41,427 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@518b0c12{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-2343889640696331263/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1  | 2023-06-29 21:08:41,440 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-06-29 21:08:41,441 [Listener at 0.0.0.0/9891] INFO server.Server: Started @49184ms
recon_1  | 2023-06-29 21:08:41,447 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-06-29 21:08:41,447 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-06-29 21:08:41,448 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-06-29 21:08:41,449 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-06-29 21:08:41,462 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1  | 2023-06-29 21:08:41,476 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1  | 2023-06-29 21:08:41,477 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-06-29 21:08:41,477 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-29 21:08:41,478 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
recon_1  | 2023-06-29 21:08:41,480 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-06-29 21:08:43,589 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4429ee788db7/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-29 21:08:45,591 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4429ee788db7/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-29 21:08:47,616 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:67dd125e-a7b5-412f-8128-ba59bd3344fe is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-29 21:08:49,618 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4429ee788db7/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-29 21:08:51,620 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 4429ee788db7/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-29 21:08:53,948 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-06-29 21:08:53,949 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1  | 2023-06-29 21:08:53,950 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=6b21d635-226c-4f1b-bd6a-67710b43930b from SCM.
recon_1  | 2023-06-29 21:08:54,195 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]].
recon_1  | 2023-06-29 21:08:54,202 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e from SCM.
recon_1  | 2023-06-29 21:08:54,213 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]].
recon_1  | 2023-06-29 21:08:54,220 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c from SCM.
recon_1  | 2023-06-29 21:08:54,231 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]].
recon_1  | 2023-06-29 21:08:54,234 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=c2cded09-6bff-4911-8b65-bf02ed3673ec from SCM.
recon_1  | 2023-06-29 21:08:54,241 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]].
recon_1  | 2023-06-29 21:08:54,242 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 from SCM.
recon_1  | 2023-06-29 21:08:54,248 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]].
recon_1  | 2023-06-29 21:08:54,249 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8 from SCM.
recon_1  | 2023-06-29 21:08:54,256 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]].
recon_1  | 2023-06-29 21:08:54,264 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=3a976013-1607-4227-9cfe-8a4dae2a0900 from SCM.
recon_1  | 2023-06-29 21:08:54,268 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]].
recon_1  | 2023-06-29 21:08:54,269 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
om3_1    | 2023-06-29 21:09:20,181 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om3_1    | 2023-06-29 21:09:20,183 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-29 21:09:20,189 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-29 21:09:20,190 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-29 21:09:20,191 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-06-29 21:09:20,208 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-06-29 21:09:20,210 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-06-29 21:09:20,210 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-06-29 21:09:20,211 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-06-29 21:09:20,220 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-06-29 21:09:20,362 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-06-29 21:09:20,384 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-06-29 21:09:20,391 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-06-29 21:09:20,569 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-06-29 21:09:20,570 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-06-29 21:09:20,667 [Listener at om3/9862] INFO util.log: Logging initialized @27869ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-06-29 21:09:21,212 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-06-29 21:09:21,242 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-06-29 21:09:21,288 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-06-29 21:09:21,290 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-06-29 21:09:21,290 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-06-29 21:09:21,293 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-06-29 21:09:21,420 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-06-29 21:09:21,428 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-06-29 21:09:21,595 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-06-29 21:09:21,595 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-06-29 21:09:21,608 [Listener at om3/9862] INFO server.session: node0 Scavenging every 600000ms
om3_1    | 2023-06-29 21:09:21,775 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@104a287c{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-06-29 21:09:21,776 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2fbd390{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-06-29 21:09:23,578 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om2, group-D66704EFC61C, 1, (t:0, i:~))
om3_1    | 2023-06-29 21:09:23,687 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: accept ELECTION from om2: our priority 0 <= candidate's priority 0
om3_1    | 2023-06-29 21:09:23,688 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om2
om3_1    | 2023-06-29 21:09:23,695 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-29 21:09:23,695 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState was interrupted
om3_1    | 2023-06-29 21:09:23,695 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-29 21:09:23,761 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-29 21:09:23,775 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-29 21:09:23,972 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om2<-om3#0:OK-t1. Peer's state: om3@group-D66704EFC61C:t1, leader=null, voted=om2, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-29 21:09:24,550 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om2 at term 1 for appendEntries, leader elected after 8735ms
om3_1    | 2023-06-29 21:09:24,582 [om3-server-thread2] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-29 21:09:24,753 [om3-server-thread2] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om3_1    | 2023-06-29 21:09:25,828 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om3_1    | 2023-06-29 21:09:26,341 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@734fbae3{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-11577121196270801274/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om3_1    | 2023-06-29 21:09:26,391 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@4e4894d{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
s3g_1    | 2023-06-29 21:08:34,450 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
s3g_1    | 2023-06-29 21:08:34,450 [main] INFO server.Server: Started @44648ms
s3g_1    | 2023-06-29 21:08:34,461 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
s3g_1    | 2023-06-29 21:08:34,461 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
s3g_1    | 2023-06-29 21:08:34,469 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
s3g_1    | 2023-06-29 21:10:21,174 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-06-29 21:10:21,196 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
s3g_1    | 2023-06-29 21:10:21,202 [qtp384515747-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
s3g_1    | 2023-06-29 21:10:21,202 [qtp384515747-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
s3g_1    | 2023-06-29 21:10:22,326 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/old1-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
s3g_1    | 2023-06-29 21:10:23,179 [qtp384515747-16] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
s3g_1    | 2023-06-29 21:10:23,424 [qtp384515747-16] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-06-29 21:09:50,327 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 4 for changeToCandidate
dn5_1    | 2023-06-29 21:09:50,327 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:50,327 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6
dn5_1    | 2023-06-29 21:09:50,329 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6 ELECTION round 0: submit vote requests at term 5 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:50,330 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:50,330 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:50,345 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:50,345 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t5
dn5_1    | 2023-06-29 21:09:50,345 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:50,346 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 5 for REJECTED
dn5_1    | 2023-06-29 21:09:50,346 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6
dn5_1    | 2023-06-29 21:09:50,346 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection6] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:50,347 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:50,347 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:55,407 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5061404057ns, electionTimeout:5060ms
dn5_1    | 2023-06-29 21:09:55,407 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:55,408 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 5 for changeToCandidate
dn5_1    | 2023-06-29 21:09:55,408 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:09:55,408 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7
dn5_1    | 2023-06-29 21:09:55,413 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7 ELECTION round 0: submit vote requests at term 6 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:55,431 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:55,435 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t6
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 6 for REJECTED
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7
dn5_1    | 2023-06-29 21:09:55,436 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection7] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:09:55,457 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 6, (t:0, i:0))
dn4_1    | 2023-06-29 21:10:05,594 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: shutdown 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:10:05,594 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO impl.FollowerState: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState was interrupted
dn4_1    | 2023-06-29 21:10:05,594 [grpc-default-executor-1] INFO impl.RoleInfo: 95c58c7b-97c6-4852-a177-ebb1478388b9: start 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState
dn4_1    | 2023-06-29 21:10:05,600 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:OK-t8. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t8, leader=null, voted=0a9e83e4-93dc-44cc-b7f2-64f77839ab35, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:10:05,603 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-29 21:10:05,604 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-29 21:10:05,620 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: receive requestVote(ELECTION, b423f7b1-089b-406b-a3aa-6a650e93a531, group-886978236035, 8, (t:0, i:0))
dn4_1    | 2023-06-29 21:10:05,620 [grpc-default-executor-1] INFO impl.VoteContext: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-FOLLOWER: reject ELECTION from b423f7b1-089b-406b-a3aa-6a650e93a531: already has voted for 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at current term 8
dn4_1    | 2023-06-29 21:10:05,620 [grpc-default-executor-1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035 replies to ELECTION vote request: b423f7b1-089b-406b-a3aa-6a650e93a531<-95c58c7b-97c6-4852-a177-ebb1478388b9#0:FAIL-t8. Peer's state: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035:t8, leader=null, voted=0a9e83e4-93dc-44cc-b7f2-64f77839ab35, raftlog=Memoized:95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:10:05,841 [95c58c7b-97c6-4852-a177-ebb1478388b9-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-886978236035 with new leaderId: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn4_1    | 2023-06-29 21:10:05,841 [95c58c7b-97c6-4852-a177-ebb1478388b9-server-thread1] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: change Leader from null to 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at term 8 for appendEntries, leader elected after 41645ms
dn4_1    | 2023-06-29 21:10:05,883 [95c58c7b-97c6-4852-a177-ebb1478388b9-server-thread2] INFO server.RaftServer$Division: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-29 21:10:05,884 [95c58c7b-97c6-4852-a177-ebb1478388b9-server-thread2] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-29 21:10:05,886 [95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 95c58c7b-97c6-4852-a177-ebb1478388b9@group-886978236035-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/current/log_inprogress_0
dn4_1    | 2023-06-29 21:10:09,871 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-95c58c7b-97c6-4852-a177-ebb1478388b9: Detected pause in JVM or host machine (eg GC): pause of approximately 154316296ns.
dn4_1    | GC pool 'ParNew' had collection(s): count=1 time=199ms
om3_1    | 2023-06-29 21:09:26,392 [Listener at om3/9862] INFO server.Server: Started @33594ms
om3_1    | 2023-06-29 21:09:26,411 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-06-29 21:09:26,412 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-06-29 21:09:26,417 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-06-29 21:09:26,417 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-06-29 21:09:26,632 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-06-29 21:09:27,043 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-06-29 21:09:27,108 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@22fb9a2c] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-06-29 21:09:28,825 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-06-29 21:10:01,710 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om3_1    | 2023-06-29 21:10:04,278 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om3_1    | 2023-06-29 21:10:13,187 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om3_1    | 2023-06-29 21:10:22,360 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-06-29 21:10:41,763 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 Received prepare request with log index 23
om3_1    | 2023-06-29 21:10:41,765 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om3 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om3_1    | 2023-06-29 21:10:46,766 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om3_1    | 2023-06-29 21:10:46,767 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om3_1    | 2023-06-29 21:10:46,767 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om3_1    | 2023-06-29 21:10:46,768 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om3_1    | 2023-06-29 21:10:46,772 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om3_1    | 2023-06-29 21:10:46,773 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om3_1    | 2023-06-29 21:10:46,799 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om3_1    | 2023-06-29 21:10:46,958 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 prepared at log index 23. Returning response txnID: 23
om3_1    |  with log index 23
recon_1  | 2023-06-29 21:08:54,270 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-06-29 21:08:54,280 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-06-29 21:08:54,381 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-06-29 21:08:54,575 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1  | 2023-06-29 21:08:54,643 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-06-29 21:08:54,749 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1  | 2023-06-29 21:08:54,764 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-06-29 21:08:54,806 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-06-29 21:08:54,937 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 154 milliseconds.
recon_1  | 2023-06-29 21:08:55,527 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:58634: output error
recon_1  | 2023-06-29 21:08:55,540 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:37260: output error
recon_1  | 2023-06-29 21:08:55,540 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:40362: output error
recon_1  | 2023-06-29 21:08:55,538 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:37246: output error
recon_1  | 2023-06-29 21:08:55,537 [IPC Server handler 3 on default port 9891] WARN ipc.Server: IPC Server handler 3 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:60578: output error
recon_1  | 2023-06-29 21:08:55,537 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:57112: output error
recon_1  | 2023-06-29 21:08:55,537 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:40346: output error
recon_1  | 2023-06-29 21:08:55,536 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:37804: output error
recon_1  | 2023-06-29 21:08:55,536 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:55798: output error
recon_1  | 2023-06-29 21:08:55,532 [IPC Server handler 17 on default port 9891] WARN ipc.Server: IPC Server handler 17 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:59172: output error
recon_1  | 2023-06-29 21:08:55,532 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:59656: output error
recon_1  | 2023-06-29 21:08:55,532 [IPC Server handler 16 on default port 9891] WARN ipc.Server: IPC Server handler 16 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:55786: output error
recon_1  | 2023-06-29 21:08:55,532 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:57108: output error
recon_1  | 2023-06-29 21:08:55,527 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:37806: output error
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-06-29 21:08:07,434 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm1_1   | STARTUP_MSG:   host = 7e4109ce0dc7/10.9.0.14
scm1_1   | STARTUP_MSG:   args = [--init]
scm1_1   | STARTUP_MSG:   version = 1.3.0
dn5_1    | 2023-06-29 21:09:55,457 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FOLLOWER: reject ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: already has voted for b423f7b1-089b-406b-a3aa-6a650e93a531 at current term 6
dn5_1    | 2023-06-29 21:09:55,457 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t6. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035:t6, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:09:55,460 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:09:55,460 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:10:00,507 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: receive requestVote(ELECTION, 95c58c7b-97c6-4852-a177-ebb1478388b9, group-886978236035, 7, (t:0, i:0))
dn5_1    | 2023-06-29 21:10:00,508 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FOLLOWER: accept ELECTION from 95c58c7b-97c6-4852-a177-ebb1478388b9: our priority 0 <= candidate's priority 0
dn5_1    | 2023-06-29 21:10:00,508 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to FOLLOWER at term 7 for candidate:95c58c7b-97c6-4852-a177-ebb1478388b9
dn5_1    | 2023-06-29 21:10:00,508 [grpc-default-executor-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:10:00,508 [grpc-default-executor-1] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:10:00,508 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState was interrupted
dn5_1    | 2023-06-29 21:10:00,514 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:10:00,514 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:10:00,516 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035 replies to ELECTION vote request: 95c58c7b-97c6-4852-a177-ebb1478388b9<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:OK-t7. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035:t7, leader=null, voted=95c58c7b-97c6-4852-a177-ebb1478388b9, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:10:05,581 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.FollowerState: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5066356728ns, electionTimeout:5066ms
dn5_1    | 2023-06-29 21:10:05,581 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:10:05,581 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from  FOLLOWER to CANDIDATE at term 7 for changeToCandidate
dn5_1    | 2023-06-29 21:10:05,581 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn5_1    | 2023-06-29 21:10:05,582 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8
dn5_1    | 2023-06-29 21:10:05,587 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: receive requestVote(ELECTION, 0a9e83e4-93dc-44cc-b7f2-64f77839ab35, group-886978236035, 8, (t:0, i:0))
dn5_1    | 2023-06-29 21:10:05,588 [grpc-default-executor-1] INFO impl.VoteContext: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-CANDIDATE: reject ELECTION from 0a9e83e4-93dc-44cc-b7f2-64f77839ab35: already has voted for b423f7b1-089b-406b-a3aa-6a650e93a531 at current term 8
dn5_1    | 2023-06-29 21:10:05,588 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8 ELECTION round 0: submit vote requests at term 8 for -1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:10:05,588 [grpc-default-executor-1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035 replies to ELECTION vote request: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35<-b423f7b1-089b-406b-a3aa-6a650e93a531#0:FAIL-t8. Peer's state: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035:t8, leader=null, voted=b423f7b1-089b-406b-a3aa-6a650e93a531, raftlog=Memoized:b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:10:05,591 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:10:05,613 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,574 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,564 [IPC Server handler 5 on default port 9891] WARN ipc.Server: IPC Server handler 5 on default port 9891, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:48210: output error
recon_1  | 2023-06-29 21:08:55,575 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,575 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 3 on default port 9891] INFO ipc.Server: IPC Server handler 3 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
dn5_1    | 2023-06-29 21:10:05,613 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8: ELECTION REJECTED received 1 response(s) and 0 exception(s):
dn5_1    | 2023-06-29 21:10:05,613 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.LeaderElection:   Response 0: b423f7b1-089b-406b-a3aa-6a650e93a531<-0a9e83e4-93dc-44cc-b7f2-64f77839ab35#0:FAIL-t8
dn5_1    | 2023-06-29 21:10:05,614 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.LeaderElection: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8 ELECTION round 0: result REJECTED
dn5_1    | 2023-06-29 21:10:05,614 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: changes role from CANDIDATE to FOLLOWER at term 8 for REJECTED
dn5_1    | 2023-06-29 21:10:05,614 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: shutdown b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8
dn5_1    | 2023-06-29 21:10:05,614 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-LeaderElection8] INFO impl.RoleInfo: b423f7b1-089b-406b-a3aa-6a650e93a531: start b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState
dn5_1    | 2023-06-29 21:10:05,617 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 2023-06-29 21:10:05,619 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn5_1    | 2023-06-29 21:10:05,873 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-886978236035 with new leaderId: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35
dn5_1    | 2023-06-29 21:10:05,873 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread1] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: change Leader from null to 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 at term 8 for appendEntries, leader elected after 41697ms
dn5_1    | 2023-06-29 21:10:05,878 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread2] INFO server.RaftServer$Division: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035: set configuration 0: peers:[0a9e83e4-93dc-44cc-b7f2-64f77839ab35|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER, b423f7b1-089b-406b-a3aa-6a650e93a531|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:0|startupRole:FOLLOWER, 95c58c7b-97c6-4852-a177-ebb1478388b9|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn5_1    | 2023-06-29 21:10:05,879 [b423f7b1-089b-406b-a3aa-6a650e93a531-server-thread2] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker: Starting segment from index:0
dn5_1    | 2023-06-29 21:10:05,886 [b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: b423f7b1-089b-406b-a3aa-6a650e93a531@group-886978236035-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/8bd8ea2c-61df-4132-bcca-886978236035/current/log_inprogress_0
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | ************************************************************/
scm1_1   | 2023-06-29 21:08:07,553 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-06-29 21:08:08,487 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-29 21:08:08,977 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-06-29 21:08:09,221 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-06-29 21:08:10,176 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm1_1   | 2023-06-29 21:08:10,182 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-06-29 21:08:12,544 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-06-29 21:08:14,457 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-29 21:08:14,589 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-29 21:08:14,628 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-29 21:08:14,628 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-29 21:08:14,637 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-06-29 21:08:14,638 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-06-29 21:08:14,669 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-06-29 21:08:14,718 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-29 21:08:14,733 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-29 21:08:14,734 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-29 21:08:14,914 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-29 21:08:15,101 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-06-29 21:08:15,195 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-29 21:08:19,277 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-06-29 21:08:19,386 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-06-29 21:08:19,407 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-06-29 21:08:19,420 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-06-29 21:08:19,433 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-29 21:08:19,465 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-29 21:08:19,625 [main] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: addNew group-FDDFC6378018:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] returns group-FDDFC6378018:java.util.concurrent.CompletableFuture@7e242b4d[Not completed]
scm1_1   | 2023-06-29 21:08:19,939 [pool-2-thread-1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe: new RaftServerImpl for group-FDDFC6378018:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm1_1   | 2023-06-29 21:08:19,986 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-06-29 21:08:19,987 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-29 21:08:19,987 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-06-29 21:08:20,005 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-06-29 21:08:20,012 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-29 21:08:20,015 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-06-29 21:08:20,205 [pool-2-thread-1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: ConfigurationManager, init=-1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-06-29 21:08:20,209 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-29 21:08:20,284 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-06-29 21:08:20,296 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-06-29 21:08:20,584 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-06-29 21:08:20,592 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-06-29 21:08:20,593 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-29 21:08:20,935 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-06-29 21:08:23,333 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-29 21:08:23,334 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-29 21:08:23,345 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-29 21:08:23,355 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-29 21:08:23,356 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 16 on default port 9891] INFO ipc.Server: IPC Server handler 16 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 17 on default port 9891] INFO ipc.Server: IPC Server handler 17 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 2023-06-29 21:08:23,404 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 does not exist. Creating ...
scm1_1   | 2023-06-29 21:08:23,625 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/in_use.lock acquired by nodename 13@7e4109ce0dc7
scm1_1   | 2023-06-29 21:08:23,745 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 has been successfully formatted.
scm1_1   | 2023-06-29 21:08:23,809 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-06-29 21:08:23,992 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-29 21:08:23,999 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-29 21:08:24,049 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-29 21:08:24,058 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-29 21:08:24,077 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:24,247 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-29 21:08:24,261 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-29 21:08:24,329 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018
scm1_1   | 2023-06-29 21:08:24,344 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-29 21:08:24,369 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:24,373 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:24,380 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:24,403 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-29 21:08:24,429 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-29 21:08:24,435 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-29 21:08:24,445 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-29 21:08:24,701 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-06-29 21:08:24,702 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-29 21:08:24,702 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-29 21:08:24,702 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-06-29 21:08:24,737 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-29 21:08:24,738 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-29 21:08:24,761 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: start as a follower, conf=-1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:24,772 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-06-29 21:08:24,781 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState
scm1_1   | 2023-06-29 21:08:25,034 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-29 21:08:25,093 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-29 21:08:25,047 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FDDFC6378018,id=67dd125e-a7b5-412f-8128-ba59bd3344fe
scm1_1   | 2023-06-29 21:08:25,105 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-29 21:08:25,124 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-06-29 21:08:25,125 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-29 21:08:25,149 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-29 21:08:25,188 [main] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start RPC server
scm1_1   | 2023-06-29 21:08:25,881 [main] INFO server.GrpcService: 67dd125e-a7b5-412f-8128-ba59bd3344fe: GrpcService started, listening on 9894
scm1_1   | 2023-06-29 21:08:25,956 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-67dd125e-a7b5-412f-8128-ba59bd3344fe: Started
scm1_1   | 2023-06-29 21:08:30,148 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.FollowerState: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5366796658ns, electionTimeout:5036ms
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-29 21:08:30,156 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState
scm1_1   | 2023-06-29 21:08:30,208 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-29 21:08:30,272 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-29 21:08:30,272 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1
scm1_1   | 2023-06-29 21:08:30,421 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.LeaderElection: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:30,423 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.LeaderElection: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-06-29 21:08:30,435 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1
scm1_1   | 2023-06-29 21:08:30,443 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-06-29 21:08:30,458 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: change Leader from null to 67dd125e-a7b5-412f-8128-ba59bd3344fe at term 1 for becomeLeader, leader elected after 9902ms
scm1_1   | 2023-06-29 21:08:30,589 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-29 21:08:30,744 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:30,758 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-29 21:08:30,854 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1_1   | 2023-06-29 21:08:30,864 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-29 21:08:30,867 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-29 21:08:30,981 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:30,999 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-06-29 21:08:31,012 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl
scm1_1   | 2023-06-29 21:08:31,562 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: Starting segment from index:0
scm1_1   | 2023-06-29 21:08:32,263 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:32,790 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0
scm1_1   | 2023-06-29 21:08:34,055 [main] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: close
scm1_1   | 2023-06-29 21:08:34,056 [main] INFO server.GrpcService: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown server GrpcServerProtocolService now
scm1_1   | 2023-06-29 21:08:34,072 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: shutdown
scm1_1   | 2023-06-29 21:08:34,072 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-FDDFC6378018,id=67dd125e-a7b5-412f-8128-ba59bd3344fe
scm1_1   | 2023-06-29 21:08:34,072 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl
scm1_1   | 2023-06-29 21:08:34,184 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO impl.PendingRequests: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-PendingRequests: sendNotLeaderResponses
scm1_1   | 2023-06-29 21:08:34,215 [main] INFO server.GrpcService: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown server GrpcServerProtocolService successfully
scm1_1   | 2023-06-29 21:08:34,224 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO impl.StateMachineUpdater: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater: set stopIndex = 0
scm1_1   | 2023-06-29 21:08:34,234 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO impl.StateMachineUpdater: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater: Took a snapshot at index 0
scm1_1   | 2023-06-29 21:08:34,234 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO impl.StateMachineUpdater: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm1_1   | 2023-06-29 21:08:34,256 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: closes. applyIndex: 0
scm1_1   | 2023-06-29 21:08:34,359 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
scm1_1   | 2023-06-29 21:08:34,404 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker close()
scm1_1   | 2023-06-29 21:08:34,422 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-67dd125e-a7b5-412f-8128-ba59bd3344fe: Stopped
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,566 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 2023-06-29 21:08:34,427 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-29 21:08:34,450 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018; layoutVersion=4; scmId=67dd125e-a7b5-412f-8128-ba59bd3344fe
scm1_1   | 2023-06-29 21:08:34,499 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1_1   | /************************************************************
scm1_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 7e4109ce0dc7/10.9.0.14
scm1_1   | ************************************************************/
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-06-29 21:08:37,984 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | /************************************************************
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm1_1   | STARTUP_MSG:   host = 7e4109ce0dc7/10.9.0.14
scm1_1   | STARTUP_MSG:   args = []
scm1_1   | STARTUP_MSG:   version = 1.3.0
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:08:55,588 [IPC Server handler 5 on default port 9891] INFO ipc.Server: IPC Server handler 5 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | ************************************************************/
scm1_1   | 2023-06-29 21:08:37,996 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-06-29 21:08:38,049 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-29 21:08:38,076 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm1_1   | 2023-06-29 21:08:38,083 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm1_1   | 2023-06-29 21:08:38,116 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
scm1_1   | 2023-06-29 21:08:38,116 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
scm1_1   | 2023-06-29 21:08:38,952 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-29 21:08:39,286 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-29 21:08:39,646 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm1_1   | 2023-06-29 21:08:39,647 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm1_1   | 2023-06-29 21:08:39,754 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm1_1   | 2023-06-29 21:08:39,774 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:67dd125e-a7b5-412f-8128-ba59bd3344fe
scm1_1   | 2023-06-29 21:08:39,882 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm1_1   | 2023-06-29 21:08:39,972 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-29 21:08:39,975 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-29 21:08:39,978 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-29 21:08:39,979 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-29 21:08:39,980 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-06-29 21:08:39,980 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-06-29 21:08:39,981 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-06-29 21:08:39,989 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-29 21:08:39,990 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm1_1   | 2023-06-29 21:08:39,990 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-29 21:08:40,003 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-29 21:08:40,008 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm1_1   | 2023-06-29 21:08:40,009 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-29 21:08:40,398 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm1_1   | 2023-06-29 21:08:40,401 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm1_1   | 2023-06-29 21:08:40,404 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm1_1   | 2023-06-29 21:08:40,405 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-06-29 21:08:40,405 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-29 21:08:40,413 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-29 21:08:40,417 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: found a subdirectory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018
scm1_1   | 2023-06-29 21:08:40,428 [main] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: addNew group-FDDFC6378018:[] returns group-FDDFC6378018:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm1_1   | 2023-06-29 21:08:40,451 [pool-16-thread-1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe: new RaftServerImpl for group-FDDFC6378018:[] with SCMStateMachine:uninitialized
scm1_1   | 2023-06-29 21:08:40,452 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm1_1   | 2023-06-29 21:08:40,455 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-29 21:08:40,455 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm1_1   | 2023-06-29 21:08:40,460 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm1_1   | 2023-06-29 21:08:40,460 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-29 21:08:40,460 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm1_1   | 2023-06-29 21:08:40,469 [pool-16-thread-1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm1_1   | 2023-06-29 21:08:40,470 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm1_1   | 2023-06-29 21:08:40,472 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm1_1   | 2023-06-29 21:08:40,473 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm1_1   | 2023-06-29 21:08:40,503 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-06-29 21:08:40,508 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-06-29 21:08:40,508 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm1_1   | 2023-06-29 21:08:40,730 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-29 21:08:40,730 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm1_1   | 2023-06-29 21:08:40,732 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm1_1   | 2023-06-29 21:08:40,733 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-29 21:08:40,734 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-06-29 21:08:40,735 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm1_1   | 2023-06-29 21:08:40,735 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1_1   | 2023-06-29 21:08:40,735 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm1_1   | 2023-06-29 21:08:40,747 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1_1   | 2023-06-29 21:08:40,981 [main] INFO reflections.Reflections: Reflections took 209 ms to scan 3 urls, producing 112 keys and 252 values 
scm1_1   | 2023-06-29 21:08:41,086 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm1_1   | 2023-06-29 21:08:41,087 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm1_1   | 2023-06-29 21:08:41,090 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1_1   | 2023-06-29 21:08:41,091 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm1_1   | 2023-06-29 21:08:41,156 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm1_1   | 2023-06-29 21:08:41,176 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm1_1   | 2023-06-29 21:08:41,176 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-06-29 21:08:41,188 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm1_1   | 2023-06-29 21:08:41,229 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm1_1   | 2023-06-29 21:08:41,230 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm1_1   | 2023-06-29 21:08:41,238 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm1_1   | 2023-06-29 21:08:41,238 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:41,242 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm1_1   | 2023-06-29 21:08:41,254 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-06-29 21:08:41,260 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm1_1   | 2023-06-29 21:08:41,263 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm1_1   | 2023-06-29 21:08:41,340 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-06-29 21:08:41,368 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1   | 2023-06-29 21:08:41,435 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm1_1   | 2023-06-29 21:08:41,460 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1_1   | 2023-06-29 21:08:41,465 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | 2023-06-29 21:08:41,472 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1_1   | 2023-06-29 21:08:41,477 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:41,478 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1_1   | 2023-06-29 21:08:42,227 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-29 21:08:42,249 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-29 21:08:42,292 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-06-29 21:08:42,407 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-29 21:08:42,414 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-29 21:08:42,419 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1_1   | 2023-06-29 21:08:42,453 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-29 21:08:42,459 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-29 21:08:42,460 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | 2023-06-29 21:08:42,519 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | 2023-06-29 21:08:42,519 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm1_1   | Container Balancer status:
scm1_1   | Key                            Value
scm1_1   | Running                        true
scm1_1   | Container Balancer Configuration values:
scm1_1   | Key                                                Value
scm1_1   | Threshold                                          10
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
scm1_1   | Max Size to Move per Iteration                     500GB
scm1_1   | Max Size Entering Target per Iteration             26GB
scm1_1   | Max Size Leaving Source per Iteration              26GB
scm1_1   | 
scm1_1   | 2023-06-29 21:08:42,519 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-06-29 21:08:42,520 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm1_1   | 2023-06-29 21:08:42,530 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm1_1   | 2023-06-29 21:08:42,533 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm1_1   | 2023-06-29 21:08:42,542 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/in_use.lock acquired by nodename 7@7e4109ce0dc7
scm1_1   | 2023-06-29 21:08:42,546 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=67dd125e-a7b5-412f-8128-ba59bd3344fe} from /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/raft-meta
scm1_1   | 2023-06-29 21:08:42,596 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:42,600 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-06-29 21:08:42,607 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-29 21:08:42,608 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-29 21:08:42,609 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-29 21:08:42,610 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-29 21:08:42,614 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:42,620 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-29 21:08:42,620 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-29 21:08:42,625 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018
scm1_1   | 2023-06-29 21:08:42,626 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-29 21:08:42,626 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:42,627 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:42,628 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-29 21:08:42,629 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-29 21:08:42,631 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-29 21:08:42,631 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-29 21:08:42,640 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-29 21:08:42,655 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-06-29 21:08:42,656 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-29 21:08:42,657 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-29 21:08:42,658 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-06-29 21:08:42,708 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:42,711 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0
scm1_1   | 2023-06-29 21:08:42,723 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:08:42,724 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-29 21:08:42,798 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: start as a follower, conf=0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:42,799 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1_1   | 2023-06-29 21:08:42,802 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState
scm1_1   | 2023-06-29 21:08:42,807 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FDDFC6378018,id=67dd125e-a7b5-412f-8128-ba59bd3344fe
scm1_1   | 2023-06-29 21:08:42,809 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-29 21:08:42,807 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-29 21:08:42,812 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-29 21:08:42,812 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-06-29 21:08:42,812 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-29 21:08:42,813 [67dd125e-a7b5-412f-8128-ba59bd3344fe-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-29 21:08:42,818 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start RPC server
scm1_1   | 2023-06-29 21:08:42,859 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 67dd125e-a7b5-412f-8128-ba59bd3344fe: GrpcService started, listening on 9894
scm1_1   | 2023-06-29 21:08:42,863 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-67dd125e-a7b5-412f-8128-ba59bd3344fe: Started
scm1_1   | 2023-06-29 21:08:42,867 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-29 21:08:42,867 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-06-29 21:08:42,931 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-06-29 21:08:42,942 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-06-29 21:08:42,942 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-06-29 21:08:43,203 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-06-29 21:08:43,204 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm1_1   | 2023-06-29 21:08:43,225 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-06-29 21:08:43,225 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-29 21:08:43,232 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-06-29 21:08:43,235 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-29 21:08:43,235 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm1_1   | 2023-06-29 21:08:43,323 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@723742b2] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm1_1   | 2023-06-29 21:08:43,336 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-06-29 21:08:43,337 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-06-29 21:08:43,426 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @7738ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-06-29 21:08:43,635 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-06-29 21:08:43,643 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-06-29 21:08:43,664 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-06-29 21:08:43,672 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-06-29 21:08:43,673 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-29 21:08:43,674 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-06-29 21:08:43,716 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-06-29 21:08:43,717 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-06-29 21:08:43,741 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1   | 2023-06-29 21:08:43,741 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-06-29 21:08:43,742 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | 2023-06-29 21:08:43,752 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6d6039df{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm1_1   | 2023-06-29 21:08:43,753 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4e4395c{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm1_1   | 2023-06-29 21:08:44,076 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5b322873{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-6731934334202251868/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm1_1   | 2023-06-29 21:08:44,089 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@7e94e331{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-06-29 21:08:44,091 [Listener at 0.0.0.0/9860] INFO server.Server: Started @8403ms
scm1_1   | 2023-06-29 21:08:44,092 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-06-29 21:08:44,093 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-06-29 21:08:44,094 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-06-29 21:08:47,813 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.FollowerState: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5010921388ns, electionTimeout:5000ms
scm1_1   | 2023-06-29 21:08:47,814 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState
scm1_1   | 2023-06-29 21:08:47,814 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
scm1_1   | 2023-06-29 21:08:47,817 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-29 21:08:47,817 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-FollowerState] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1
scm1_1   | 2023-06-29 21:08:47,828 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.LeaderElection: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:47,829 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.LeaderElection: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1 ELECTION round 0: result PASSED (term=2)
scm1_1   | 2023-06-29 21:08:47,829 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: shutdown 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1
scm1_1   | 2023-06-29 21:08:47,829 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
scm1_1   | 2023-06-29 21:08:47,830 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
scm1_1   | 2023-06-29 21:08:47,830 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm1_1   | 2023-06-29 21:08:47,831 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: change Leader from null to 67dd125e-a7b5-412f-8128-ba59bd3344fe at term 2 for becomeLeader, leader elected after 7326ms
scm1_1   | 2023-06-29 21:08:47,836 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-29 21:08:47,839 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:47,840 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-29 21:08:47,844 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
scm1_1   | 2023-06-29 21:08:47,844 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-29 21:08:47,844 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-29 21:08:47,848 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm1_1   | 2023-06-29 21:08:47,849 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-06-29 21:08:47,850 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO impl.RoleInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe: start 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl
scm1_1   | 2023-06-29 21:08:47,856 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm1_1   | 2023-06-29 21:08:47,859 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0 to /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_0-0
scm1_1   | 2023-06-29 21:08:47,872 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderElection1] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:08:47,873 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_1
scm1_1   | 2023-06-29 21:08:47,880 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm1_1   | 2023-06-29 21:08:47,880 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm1_1   | 2023-06-29 21:08:47,887 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:47,887 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm1_1   | 2023-06-29 21:08:47,888 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm1_1   | 2023-06-29 21:08:47,888 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm1_1   | 2023-06-29 21:08:47,891 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-29 21:08:47,891 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1   | 2023-06-29 21:08:48,014 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:53406: output error
scm1_1   | 2023-06-29 21:08:48,023 [IPC Server handler 4 on default port 9861] WARN ipc.Server: IPC Server handler 4 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:42168: output error
scm1_1   | 2023-06-29 21:08:48,023 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#2 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:56214: output error
scm1_1   | 2023-06-29 21:08:48,023 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:54444: output error
scm1_1   | 2023-06-29 21:08:48,032 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm3_1   | Waiting for the service scm2:9894
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | 2023-06-29 21:09:11,281 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | STARTUP_MSG:   host = 82bcbdc527e6/10.9.0.16
scm3_1   | STARTUP_MSG:   args = [--bootstrap]
scm3_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | Waiting for the service scm1:9894
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-06-29 21:08:35,002 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm2_1   | STARTUP_MSG:   host = 13fc168c4bef/10.9.0.15
scm2_1   | STARTUP_MSG:   args = [--bootstrap]
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | ************************************************************/
scm2_1   | 2023-06-29 21:08:35,019 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-06-29 21:08:35,299 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-29 21:08:35,402 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-06-29 21:08:35,413 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-06-29 21:08:35,584 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm2_1   | 2023-06-29 21:08:35,584 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-06-29 21:08:35,929 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm2_1   | 2023-06-29 21:08:38,649 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 13fc168c4bef/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-29 21:08:40,651 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 13fc168c4bef/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-29 21:08:42,655 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 13fc168c4bef/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-29 21:08:44,704 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:67dd125e-a7b5-412f-8128-ba59bd3344fe is not the leader. Could not determine the leader node.
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-29 21:08:46,706 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 13fc168c4bef/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-29 21:08:48,744 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018, SCMID fdfa269f-8b15-4612-aba6-bdafa0a99e33
scm2_1   | 2023-06-29 21:08:48,744 [main] INFO server.StorageContainerManager: Primary SCM Node ID 67dd125e-a7b5-412f-8128-ba59bd3344fe
scm2_1   | 2023-06-29 21:08:48,752 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2_1   | /************************************************************
scm2_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 13fc168c4bef/10.9.0.15
scm2_1   | ************************************************************/
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-06-29 21:08:50,518 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | /************************************************************
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm2_1   | STARTUP_MSG:   host = 13fc168c4bef/10.9.0.15
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | ************************************************************/
scm3_1   | 2023-06-29 21:09:11,333 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | 2023-06-29 21:09:11,588 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-29 21:09:11,696 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3_1   | 2023-06-29 21:09:11,704 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-06-29 21:09:11,876 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-06-29 21:09:11,876 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1   | 2023-06-29 21:09:12,325 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
scm3_1   | 2023-06-29 21:09:13,206 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-a9fefe57-df11-4d0f-aa1d-fddfc6378018, SCMID 663949d4-905f-469d-b15f-059a3d7e4114
scm3_1   | 2023-06-29 21:09:13,206 [main] INFO server.StorageContainerManager: Primary SCM Node ID 67dd125e-a7b5-412f-8128-ba59bd3344fe
scm3_1   | 2023-06-29 21:09:13,239 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm3_1   | /************************************************************
scm3_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 82bcbdc527e6/10.9.0.16
scm3_1   | ************************************************************/
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | 2023-06-29 21:09:18,398 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | STARTUP_MSG:   host = 82bcbdc527e6/10.9.0.16
scm3_1   | STARTUP_MSG:   args = []
scm3_1   | STARTUP_MSG:   version = 1.3.0
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm3_1   | ************************************************************/
scm3_1   | 2023-06-29 21:09:18,436 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm3_1   | 2023-06-29 21:09:18,638 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-29 21:09:18,748 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm3_1   | 2023-06-29 21:09:18,774 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm3_1   | 2023-06-29 21:09:18,910 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm3_1   | 2023-06-29 21:09:18,910 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm3_1   | 2023-06-29 21:09:20,896 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-29 21:09:21,717 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm3_1   | 2023-06-29 21:09:22,817 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm3_1   | 2023-06-29 21:09:22,826 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm3_1   | 2023-06-29 21:09:23,268 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm3_1   | 2023-06-29 21:09:23,399 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:663949d4-905f-469d-b15f-059a3d7e4114
scm3_1   | 2023-06-29 21:09:24,039 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm3_1   | 2023-06-29 21:09:24,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-06-29 21:09:24,714 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-06-29 21:09:24,735 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm3_1   | 2023-06-29 21:09:24,737 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm3_1   | 2023-06-29 21:09:24,754 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm3_1   | 2023-06-29 21:09:24,756 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm3_1   | 2023-06-29 21:09:24,760 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm3_1   | 2023-06-29 21:09:24,767 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-29 21:09:24,792 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm3_1   | 2023-06-29 21:09:24,812 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm3_1   | 2023-06-29 21:09:24,929 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm3_1   | 2023-06-29 21:09:24,979 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm3_1   | 2023-06-29 21:09:24,992 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm3_1   | 2023-06-29 21:09:26,714 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm3_1   | 2023-06-29 21:09:26,746 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm3_1   | 2023-06-29 21:09:26,747 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm3_1   | 2023-06-29 21:09:26,754 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-29 21:09:21,160 [IPC Server handler 34 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0a9e83e4-93dc-44cc-b7f2-64f77839ab35
recon_1  | 2023-06-29 21:09:21,169 [IPC Server handler 36 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95c58c7b-97c6-4852-a177-ebb1478388b9
recon_1  | 2023-06-29 21:09:21,173 [IPC Server handler 36 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:21,182 [IPC Server handler 34 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:21,201 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 95c58c7b-97c6-4852-a177-ebb1478388b9 to Node DB.
recon_1  | 2023-06-29 21:09:21,216 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 0a9e83e4-93dc-44cc-b7f2-64f77839ab35 to Node DB.
recon_1  | 2023-06-29 21:09:21,442 [IPC Server handler 32 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b423f7b1-089b-406b-a3aa-6a650e93a531
recon_1  | 2023-06-29 21:09:21,442 [IPC Server handler 32 on default port 9891] INFO node.SCMNodeManager: Registered Data node : b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:21,442 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node b423f7b1-089b-406b-a3aa-6a650e93a531 to Node DB.
recon_1  | 2023-06-29 21:09:22,465 [IPC Server handler 50 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3561b5c6-83b9-4083-8461-2a79b0297944
recon_1  | 2023-06-29 21:09:22,465 [IPC Server handler 50 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:22,466 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 3561b5c6-83b9-4083-8461-2a79b0297944 to Node DB.
recon_1  | 2023-06-29 21:09:22,570 [IPC Server handler 47 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/653c0db6-d9db-4099-8201-751dcc40d458
recon_1  | 2023-06-29 21:09:22,570 [IPC Server handler 47 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:22,571 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 653c0db6-d9db-4099-8201-751dcc40d458 to Node DB.
recon_1  | 2023-06-29 21:09:23,299 [IPC Server handler 30 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn4_1.ha_net
recon_1  | 2023-06-29 21:09:23,305 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=3a976013-1607-4227-9cfe-8a4dae2a0900 reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:23,306 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:23,510 [IPC Server handler 53 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn5_1.ha_net
recon_1  | 2023-06-29 21:09:23,512 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=6b21d635-226c-4f1b-bd6a-67710b43930b reported by b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:23,516 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b423f7b1-089b-406b-a3aa-6a650e93a531, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:23,515 [IPC Server handler 54 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn1_1.ha_net
recon_1  | 2023-06-29 21:09:23,520 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=c2cded09-6bff-4911-8b65-bf02ed3673ec reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:09:26,765 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-29 21:09:26,786 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-29 21:09:26,819 [main] INFO server.RaftServer: 663949d4-905f-469d-b15f-059a3d7e4114: addNew group-FDDFC6378018:[] returns group-FDDFC6378018:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm3_1   | 2023-06-29 21:09:26,968 [pool-16-thread-1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114: new RaftServerImpl for group-FDDFC6378018:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-06-29 21:09:26,977 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-06-29 21:09:26,990 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm3_1   | 2023-06-29 21:09:26,990 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-06-29 21:09:26,991 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-06-29 21:09:26,991 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-29 21:09:26,991 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-29 21:09:27,025 [pool-16-thread-1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-06-29 21:09:27,033 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-29 21:09:27,062 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-06-29 21:09:27,068 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-06-29 21:09:27,131 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm3_1   | 2023-06-29 21:09:27,144 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1   | 2023-06-29 21:09:27,155 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm3_1   | 2023-06-29 21:09:27,792 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm3_1   | 2023-06-29 21:09:27,806 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-06-29 21:09:27,808 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-06-29 21:09:27,809 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-06-29 21:09:27,823 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm3_1   | 2023-06-29 21:09:27,824 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm3_1   | 2023-06-29 21:09:27,825 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm3_1   | 2023-06-29 21:09:27,825 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm3_1   | 2023-06-29 21:09:27,914 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm3_1   | 2023-06-29 21:09:28,630 [main] INFO reflections.Reflections: Reflections took 552 ms to scan 3 urls, producing 112 keys and 252 values 
scm3_1   | 2023-06-29 21:09:28,885 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm3_1   | 2023-06-29 21:09:28,886 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm3_1   | 2023-06-29 21:09:28,893 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm3_1   | 2023-06-29 21:09:28,895 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1   | 2023-06-29 21:09:28,998 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | 2023-06-29 21:09:29,057 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3_1   | 2023-06-29 21:09:29,063 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-06-29 21:09:29,096 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3_1   | 2023-06-29 21:09:29,214 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3_1   | 2023-06-29 21:09:29,216 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-06-29 21:09:29,248 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3_1   | 2023-06-29 21:09:29,248 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm3_1   | 2023-06-29 21:09:29,251 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm3_1   | 2023-06-29 21:09:29,263 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm3_1   | 2023-06-29 21:09:29,283 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm3_1   | 2023-06-29 21:09:29,320 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm3_1   | 2023-06-29 21:09:29,518 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm3_1   | 2023-06-29 21:09:29,598 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm3_1   | 2023-06-29 21:09:29,738 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm3_1   | 2023-06-29 21:09:29,751 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm3_1   | 2023-06-29 21:09:29,766 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | 2023-06-29 21:09:29,775 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-06-29 21:09:29,782 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:29,787 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-29 21:09:31,515 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-29 21:09:31,543 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | ************************************************************/
scm2_1   | 2023-06-29 21:08:50,560 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm2_1   | 2023-06-29 21:08:50,633 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-29 21:09:23,546 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:24,263 [IPC Server handler 29 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
recon_1  | 2023-06-29 21:09:24,263 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:24,297 [IPC Server handler 24 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
recon_1  | 2023-06-29 21:09:24,298 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:24,488 [IPC Server handler 30 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
recon_1  | 2023-06-29 21:09:24,489 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:25,246 [IPC Server handler 29 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn2_1.ha_net
recon_1  | 2023-06-29 21:09:25,247 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c reported by 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:25,247 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:653c0db6-d9db-4099-8201-751dcc40d458, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:25,353 [IPC Server handler 37 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn3_1.ha_net
recon_1  | 2023-06-29 21:09:25,358 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8 reported by 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:25,359 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3561b5c6-83b9-4083-8461-2a79b0297944, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:27,990 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:27,994 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:28,119 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:28,120 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:28,178 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e reported by b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:28,178 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:31,000 [IPC Server handler 34 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:08:48,033 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:08:48,034 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:08:48,033 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:08:48,059 [IPC Server handler 7 on default port 9861] WARN ipc.Server: IPC Server handler 7 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:56460: output error
scm1_1   | 2023-06-29 21:08:48,059 [IPC Server handler 7 on default port 9861] INFO ipc.Server: IPC Server handler 7 on default port 9861 caught an exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 2023-06-29 21:09:31,213 [IPC Server handler 24 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
recon_1  | 2023-06-29 21:09:33,363 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:33,364 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:09:33,365 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:41,479 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-06-29 21:09:41,481 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
recon_1  | 2023-06-29 21:09:42,021 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1688072981481
recon_1  | 2023-06-29 21:09:42,031 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:09:42,032 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:09:42,129 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1688072981481.
recon_1  | 2023-06-29 21:09:42,171 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
recon_1  | 2023-06-29 21:09:42,184 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
recon_1  | 2023-06-29 21:09:42,189 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
recon_1  | 2023-06-29 21:09:42,546 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
recon_1  | 2023-06-29 21:09:42,547 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
recon_1  | 2023-06-29 21:09:42,548 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-06-29 21:09:42,557 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-06-29 21:09:42,567 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
recon_1  | 2023-06-29 21:09:42,567 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.02 seconds to process 0 keys.
recon_1  | 2023-06-29 21:09:42,609 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
recon_1  | 2023-06-29 21:09:42,609 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
recon_1  | 2023-06-29 21:09:59,088 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:09:59,295 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:10:03,320 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:10:05,633 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 reported by 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-29 21:10:05,635 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]] moved to OPEN state
recon_1  | 2023-06-29 21:10:09,594 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn4_1.ha_net.
scm2_1   | 2023-06-29 21:08:50,680 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
scm2_1   | 2023-06-29 21:08:50,694 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
scm2_1   | 2023-06-29 21:08:50,806 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
scm2_1   | 2023-06-29 21:08:50,806 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-06-29 21:08:52,234 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-29 21:08:52,839 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-29 21:08:53,553 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
scm2_1   | 2023-06-29 21:08:53,584 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
scm2_1   | 2023-06-29 21:08:53,972 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
scm2_1   | 2023-06-29 21:08:54,062 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:fdfa269f-8b15-4612-aba6-bdafa0a99e33
scm2_1   | 2023-06-29 21:08:54,411 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
scm2_1   | 2023-06-29 21:08:54,659 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-29 21:08:54,670 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-06-29 21:08:54,670 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-29 21:08:54,672 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm2_1   | 2023-06-29 21:08:54,672 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm2_1   | 2023-06-29 21:08:54,672 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm2_1   | 2023-06-29 21:08:54,680 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm2_1   | 2023-06-29 21:08:54,682 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-29 21:08:54,697 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
scm2_1   | 2023-06-29 21:08:54,725 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm2_1   | 2023-06-29 21:08:54,789 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm2_1   | 2023-06-29 21:08:54,814 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
scm2_1   | 2023-06-29 21:08:54,826 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm2_1   | 2023-06-29 21:08:56,063 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
scm2_1   | 2023-06-29 21:08:56,066 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
scm2_1   | 2023-06-29 21:08:56,072 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
scm2_1   | 2023-06-29 21:08:56,072 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-06-29 21:08:56,073 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-29 21:08:56,084 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-29 21:08:56,106 [main] INFO server.RaftServer: fdfa269f-8b15-4612-aba6-bdafa0a99e33: addNew group-FDDFC6378018:[] returns group-FDDFC6378018:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm2_1   | 2023-06-29 21:08:56,167 [pool-16-thread-1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33: new RaftServerImpl for group-FDDFC6378018:[] with SCMStateMachine:uninitialized
scm2_1   | 2023-06-29 21:08:56,173 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm2_1   | 2023-06-29 21:08:56,174 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-29 21:08:56,175 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-29 21:08:56,176 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm2_1   | 2023-06-29 21:08:56,177 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-29 21:08:56,178 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-06-29 21:08:56,215 [pool-16-thread-1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-06-29 21:08:56,216 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-29 21:08:56,253 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1   | 2023-06-29 21:08:56,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-29 21:08:56,371 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-06-29 21:08:56,395 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2_1   | 2023-06-29 21:08:56,411 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-29 21:08:57,407 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-29 21:08:57,408 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-06-29 21:08:57,408 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-06-29 21:08:57,410 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-06-29 21:08:57,417 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-29 21:08:57,421 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2_1   | 2023-06-29 21:08:57,426 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-06-29 21:08:57,428 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2_1   | 2023-06-29 21:08:57,519 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:08:49,056 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95c58c7b-97c6-4852-a177-ebb1478388b9
scm3_1   | 2023-06-29 21:09:31,591 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-06-29 21:09:31,691 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-29 21:09:31,702 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-06-29 21:09:31,708 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm3_1   | 2023-06-29 21:09:31,734 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-29 21:09:31,742 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-06-29 21:09:31,743 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm3_1   | 2023-06-29 21:09:31,798 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm3_1   | 2023-06-29 21:09:31,799 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm3_1   | Container Balancer status:
scm3_1   | Key                            Value
scm3_1   | Running                        true
scm3_1   | Container Balancer Configuration values:
scm3_1   | Key                                                Value
scm3_1   | Threshold                                          10
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
scm3_1   | Max Size Leaving Source per Iteration              26GB
scm3_1   | 
scm3_1   | 2023-06-29 21:09:31,799 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-06-29 21:09:31,799 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm3_1   | 2023-06-29 21:09:31,803 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm3_1   | 2023-06-29 21:09:31,809 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm3_1   | 2023-06-29 21:09:31,810 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 does not exist. Creating ...
scm3_1   | 2023-06-29 21:09:31,815 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/in_use.lock acquired by nodename 6@82bcbdc527e6
scm3_1   | 2023-06-29 21:09:31,838 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 has been successfully formatted.
scm3_1   | 2023-06-29 21:09:31,850 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-06-29 21:09:31,859 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-29 21:09:31,859 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-29 21:09:31,863 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-06-29 21:09:31,864 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-06-29 21:09:31,868 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-29 21:09:31,874 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-06-29 21:09:31,875 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-06-29 21:09:31,880 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018
scm3_1   | 2023-06-29 21:09:31,881 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-29 21:09:31,881 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-06-29 21:09:31,882 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-29 21:09:31,882 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-06-29 21:09:31,882 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-06-29 21:09:31,883 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-06-29 21:09:31,883 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-06-29 21:09:31,884 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-06-29 21:09:31,898 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | 2023-06-29 21:09:31,904 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | 2023-06-29 21:09:31,904 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | 2023-06-29 21:09:31,904 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | 2023-06-29 21:09:31,919 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-29 21:09:31,919 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-29 21:09:31,932 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2_1   | 2023-06-29 21:08:58,223 [main] INFO reflections.Reflections: Reflections took 580 ms to scan 3 urls, producing 112 keys and 252 values 
scm2_1   | 2023-06-29 21:08:58,532 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2_1   | 2023-06-29 21:08:58,545 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2_1   | 2023-06-29 21:08:58,557 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2_1   | 2023-06-29 21:08:58,562 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2_1   | 2023-06-29 21:08:58,725 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-06-29 21:08:58,813 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2_1   | 2023-06-29 21:08:58,816 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-29 21:08:58,826 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2_1   | 2023-06-29 21:08:58,885 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1   | 2023-06-29 21:08:58,893 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-29 21:08:58,903 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm2_1   | 2023-06-29 21:08:58,911 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
scm2_1   | 2023-06-29 21:08:58,914 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2_1   | 2023-06-29 21:08:58,918 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm2_1   | 2023-06-29 21:08:58,924 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
scm2_1   | 2023-06-29 21:08:58,931 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
scm2_1   | 2023-06-29 21:08:59,075 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm2_1   | 2023-06-29 21:08:59,139 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm2_1   | 2023-06-29 21:08:59,302 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
scm2_1   | 2023-06-29 21:08:59,343 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm2_1   | 2023-06-29 21:08:59,350 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm2_1   | 2023-06-29 21:08:59,366 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm2_1   | 2023-06-29 21:08:59,369 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:08:59,379 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1   | 2023-06-29 21:09:02,527 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-29 21:09:02,781 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-29 21:09:03,055 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm2_1   | 2023-06-29 21:09:03,278 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-29 21:09:03,302 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-29 21:09:03,343 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 2023-06-29 21:09:03,620 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm2_1   | 2023-06-29 21:09:03,677 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | 2023-06-29 21:09:03,706 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 2023-06-29 21:09:04,045 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm2_1   | 2023-06-29 21:09:04,058 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
scm2_1   | Running                        true
scm2_1   | Container Balancer Configuration values:
scm2_1   | Key                                                Value
scm2_1   | Threshold                                          10
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | Max Size to Move per Iteration                     500GB
scm2_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 
scm2_1   | 2023-06-29 21:09:04,058 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-29 21:09:04,061 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-06-29 21:09:04,078 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-06-29 21:09:04,084 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2_1   | 2023-06-29 21:09:04,120 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 does not exist. Creating ...
scm2_1   | 2023-06-29 21:09:04,188 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/in_use.lock acquired by nodename 6@13fc168c4bef
scm2_1   | 2023-06-29 21:09:04,243 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018 has been successfully formatted.
scm2_1   | 2023-06-29 21:09:04,272 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-06-29 21:09:04,328 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-06-29 21:09:04,328 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-29 21:09:04,341 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-06-29 21:09:04,344 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
recon_1  | 2023-06-29 21:10:09,653 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
recon_1  | 2023-06-29 21:10:17,755 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: New container #2 got from ha_dn1_1.ha_net.
recon_1  | 2023-06-29 21:10:17,787 [FixedThreadPoolWithAffinityExecutor-8-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
recon_1  | 2023-06-29 21:10:42,655 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
recon_1  | 2023-06-29 21:10:42,657 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,657 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,657 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,658 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,658 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,658 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
recon_1  | 2023-06-29 21:10:42,658 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
recon_1  | 2023-06-29 21:10:42,658 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 2 
recon_1  | 2023-06-29 21:10:42,675 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-06-29 21:10:42,702 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 12, SequenceNumber diff: 31, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-29 21:10:42,702 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 31 records
recon_1  | 2023-06-29 21:10:42,705 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-06-29 21:10:42,706 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-06-29 21:10:42,784 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
recon_1  | 2023-06-29 21:10:42,794 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
recon_1  | 2023-06-29 21:10:42,811 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
scm3_1   | 2023-06-29 21:09:31,932 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: changes role from      null to FOLLOWER at term 0 for startInitializing
scm3_1   | 2023-06-29 21:09:31,933 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FDDFC6378018,id=663949d4-905f-469d-b15f-059a3d7e4114
scm3_1   | 2023-06-29 21:09:31,934 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-06-29 21:09:31,935 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm3_1   | 2023-06-29 21:09:31,939 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-06-29 21:09:31,944 [663949d4-905f-469d-b15f-059a3d7e4114-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-06-29 21:09:31,947 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 663949d4-905f-469d-b15f-059a3d7e4114: start RPC server
scm3_1   | 2023-06-29 21:09:32,030 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 663949d4-905f-469d-b15f-059a3d7e4114: GrpcService started, listening on 9894
scm3_1   | 2023-06-29 21:09:32,044 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-663949d4-905f-469d-b15f-059a3d7e4114: Started
scm3_1   | 2023-06-29 21:09:32,057 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
scm3_1   | 2023-06-29 21:09:32,408 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: receive installSnapshot: 67dd125e-a7b5-412f-8128-ba59bd3344fe->663949d4-905f-469d-b15f-059a3d7e4114#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-06-29 21:09:32,413 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm3_1   | 2023-06-29 21:09:32,413 [grpc-default-executor-0] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: change Leader from null to 67dd125e-a7b5-412f-8128-ba59bd3344fe at term 2 for installSnapshot, leader elected after 5282ms
scm3_1   | 2023-06-29 21:09:32,415 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: Received notification to install snapshot at index 0
scm3_1   | 2023-06-29 21:09:32,417 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm3_1   | 2023-06-29 21:09:32,495 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set new configuration index: 19
scm3_1   | configurationEntry {
scm3_1   |   peers {
scm3_1   |     id: "fdfa269f-8b15-4612-aba6-bdafa0a99e33"
scm3_1   |     address: "scm2:9894"
scm3_1   |     startupRole: FOLLOWER
scm3_1   |   }
scm3_1   |   peers {
scm3_1   |     id: "67dd125e-a7b5-412f-8128-ba59bd3344fe"
scm3_1   |     address: "scm1:9894"
scm3_1   |     startupRole: FOLLOWER
scm3_1   |   }
scm3_1   | }
scm3_1   |  from snapshot
scm3_1   | 2023-06-29 21:09:32,504 [grpc-default-executor-0] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 19: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,508 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: reply installSnapshot: 67dd125e-a7b5-412f-8128-ba59bd3344fe<-663949d4-905f-469d-b15f-059a3d7e4114#0:OK-t0,ALREADY_INSTALLED
scm3_1   | 2023-06-29 21:09:32,524 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: 663949d4-905f-469d-b15f-059a3d7e4114: Completed INSTALL_SNAPSHOT, lastRequest: 67dd125e-a7b5-412f-8128-ba59bd3344fe->663949d4-905f-469d-b15f-059a3d7e4114#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-06-29 21:09:32,568 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO impl.RoleInfo: 663949d4-905f-469d-b15f-059a3d7e4114: start 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-FollowerState
scm3_1   | 2023-06-29 21:09:32,574 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1   | 2023-06-29 21:09:32,576 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: inconsistency entries. Reply:67dd125e-a7b5-412f-8128-ba59bd3344fe<-663949d4-905f-469d-b15f-059a3d7e4114#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3_1   | 2023-06-29 21:09:32,577 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread2] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm3_1   | 2023-06-29 21:09:32,577 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread2] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: inconsistency entries. Reply:67dd125e-a7b5-412f-8128-ba59bd3344fe<-663949d4-905f-469d-b15f-059a3d7e4114#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm3_1   | 2023-06-29 21:09:32,589 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,592 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,593 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 17: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-29 21:09:32,593 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 19: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,603 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1   | 2023-06-29 21:09:04,362 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-29 21:09:04,383 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-29 21:09:04,391 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-06-29 21:09:04,422 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018
scm2_1   | 2023-06-29 21:09:04,458 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-06-29 21:09:04,463 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm2_1   | 2023-06-29 21:09:04,466 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-29 21:09:04,469 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm2_1   | 2023-06-29 21:09:04,473 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm2_1   | 2023-06-29 21:09:04,474 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm2_1   | 2023-06-29 21:09:04,474 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm2_1   | 2023-06-29 21:09:04,483 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm2_1   | 2023-06-29 21:09:04,563 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-06-29 21:09:04,566 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm2_1   | 2023-06-29 21:09:04,570 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm2_1   | 2023-06-29 21:09:04,591 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm2_1   | 2023-06-29 21:09:04,634 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-06-29 21:09:04,634 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm2_1   | 2023-06-29 21:09:04,649 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:04,655 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: changes role from      null to FOLLOWER at term 0 for startInitializing
scm2_1   | 2023-06-29 21:09:04,674 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-FDDFC6378018,id=fdfa269f-8b15-4612-aba6-bdafa0a99e33
scm2_1   | 2023-06-29 21:09:04,677 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm2_1   | 2023-06-29 21:09:04,678 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm2_1   | 2023-06-29 21:09:04,679 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm2_1   | 2023-06-29 21:09:04,680 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm2_1   | 2023-06-29 21:09:04,699 [Listener at 0.0.0.0/9860] INFO server.RaftServer: fdfa269f-8b15-4612-aba6-bdafa0a99e33: start RPC server
scm2_1   | 2023-06-29 21:09:04,957 [Listener at 0.0.0.0/9860] INFO server.GrpcService: fdfa269f-8b15-4612-aba6-bdafa0a99e33: GrpcService started, listening on 9894
scm2_1   | 2023-06-29 21:09:05,001 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-fdfa269f-8b15-4612-aba6-bdafa0a99e33: Started
scm2_1   | 2023-06-29 21:09:05,247 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm2_1   | 2023-06-29 21:09:08,945 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: receive installSnapshot: 67dd125e-a7b5-412f-8128-ba59bd3344fe->fdfa269f-8b15-4612-aba6-bdafa0a99e33#0-t2,notify:(t:1, i:0)
scm2_1   | 2023-06-29 21:09:08,976 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
scm2_1   | 2023-06-29 21:09:08,976 [grpc-default-executor-0] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: change Leader from null to 67dd125e-a7b5-412f-8128-ba59bd3344fe at term 2 for installSnapshot, leader elected after 12605ms
scm2_1   | 2023-06-29 21:09:08,981 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: Received notification to install snapshot at index 0
scm2_1   | 2023-06-29 21:09:08,987 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
scm2_1   | 2023-06-29 21:09:09,475 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set new configuration index: 1
scm2_1   | configurationEntry {
scm2_1   |   peers {
scm2_1   |     id: "67dd125e-a7b5-412f-8128-ba59bd3344fe"
scm2_1   |     address: "scm1:9894"
scm2_1   |     startupRole: FOLLOWER
scm2_1   |   }
scm2_1   | }
scm2_1   |  from snapshot
scm2_1   | 2023-06-29 21:09:09,483 [grpc-default-executor-0] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:09,486 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: reply installSnapshot: 67dd125e-a7b5-412f-8128-ba59bd3344fe<-fdfa269f-8b15-4612-aba6-bdafa0a99e33#0:OK-t0,ALREADY_INSTALLED
scm3_1   | 2023-06-29 21:09:32,631 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm3_1   | 2023-06-29 21:09:32,647 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,647 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,647 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 17: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-29 21:09:32,647 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 19: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,738 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0
scm3_1   | 2023-06-29 21:09:32,751 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0 to /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_0-0
scm3_1   | 2023-06-29 21:09:32,774 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_1
scm3_1   | 2023-06-29 21:09:32,797 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:32,798 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-06-29 21:09:32,802 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread1] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 31: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm3_1   | 2023-06-29 21:09:32,805 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-29 21:09:32,805 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1   | 2023-06-29 21:09:32,809 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-29 21:09:32,827 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-06-29 21:09:32,836 [663949d4-905f-469d-b15f-059a3d7e4114-server-thread2] INFO server.RaftServer$Division: 663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018: set configuration 33: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-29 21:09:32,909 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-FDDFC6378018:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm3_1   | 2023-06-29 21:09:32,909 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm3_1   | 2023-06-29 21:09:33,101 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,105 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,105 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm3_1   | 2023-06-29 21:09:33,105 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3_1   | 2023-06-29 21:09:33,128 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,128 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,058 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-29 21:08:49,060 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm1_1   | 2023-06-29 21:08:49,061 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-29 21:08:49,065 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:49,065 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:49,078 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-06-29 21:08:49,084 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=c2cded09-6bff-4911-8b65-bf02ed3673ec to datanode:0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm1_1   | 2023-06-29 21:08:49,085 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-06-29 21:08:49,083 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm1_1   | 2023-06-29 21:08:49,090 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm1_1   | 2023-06-29 21:08:49,294 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]].
scm1_1   | 2023-06-29 21:08:49,297 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,311 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=3a976013-1607-4227-9cfe-8a4dae2a0900 to datanode:95c58c7b-97c6-4852-a177-ebb1478388b9
scm1_1   | 2023-06-29 21:08:49,316 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]].
scm1_1   | 2023-06-29 21:08:49,318 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,439 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b423f7b1-089b-406b-a3aa-6a650e93a531
scm1_1   | 2023-06-29 21:08:49,440 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-29 21:08:49,441 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:49,441 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm1_1   | 2023-06-29 21:08:49,441 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm1_1   | 2023-06-29 21:08:49,441 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-06-29 21:08:49,443 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=6b21d635-226c-4f1b-bd6a-67710b43930b to datanode:b423f7b1-089b-406b-a3aa-6a650e93a531
scm1_1   | 2023-06-29 21:08:49,446 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]].
scm1_1   | 2023-06-29 21:08:49,441 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-06-29 21:08:49,448 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm1_1   | 2023-06-29 21:08:49,448 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:49,448 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,453 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 to datanode:0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm1_1   | 2023-06-29 21:08:49,453 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 to datanode:b423f7b1-089b-406b-a3aa-6a650e93a531
scm1_1   | 2023-06-29 21:08:49,453 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 to datanode:95c58c7b-97c6-4852-a177-ebb1478388b9
scm2_1   | 2023-06-29 21:09:09,538 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: fdfa269f-8b15-4612-aba6-bdafa0a99e33: Completed INSTALL_SNAPSHOT, lastRequest: 67dd125e-a7b5-412f-8128-ba59bd3344fe->fdfa269f-8b15-4612-aba6-bdafa0a99e33#0-t2,notify:(t:1, i:0)
scm2_1   | 2023-06-29 21:09:09,719 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO impl.RoleInfo: fdfa269f-8b15-4612-aba6-bdafa0a99e33: start fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-FollowerState
scm2_1   | 2023-06-29 21:09:09,727 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2_1   | 2023-06-29 21:09:09,743 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: inconsistency entries. Reply:67dd125e-a7b5-412f-8128-ba59bd3344fe<-fdfa269f-8b15-4612-aba6-bdafa0a99e33#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2_1   | 2023-06-29 21:09:09,791 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm2_1   | 2023-06-29 21:09:09,791 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: inconsistency entries. Reply:67dd125e-a7b5-412f-8128-ba59bd3344fe<-fdfa269f-8b15-4612-aba6-bdafa0a99e33#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
scm2_1   | 2023-06-29 21:09:09,867 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 0: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:09,867 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 1: peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:09,876 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1   | 2023-06-29 21:09:09,912 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
scm2_1   | 2023-06-29 21:09:10,393 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0
scm2_1   | 2023-06-29 21:09:10,450 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_0 to /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_0-0
scm2_1   | 2023-06-29 21:09:10,562 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/a9fefe57-df11-4d0f-aa1d-fddfc6378018/current/log_inprogress_1
scm2_1   | 2023-06-29 21:09:10,626 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:10,632 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm2_1   | 2023-06-29 21:09:10,633 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1   | 2023-06-29 21:09:10,634 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm2_1   | 2023-06-29 21:09:10,674 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm2_1   | 2023-06-29 21:09:10,886 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 17: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-06-29 21:09:11,108 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-29 21:09:11,425 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 19: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:11,688 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-FDDFC6378018:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-29 21:09:11,688 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm2_1   | 2023-06-29 21:09:12,078 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,090 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,090 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
scm2_1   | 2023-06-29 21:09:12,090 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
scm3_1   | 2023-06-29 21:09:33,131 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,131 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,137 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,140 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,142 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,142 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,146 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,146 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,150 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]].
scm3_1   | 2023-06-29 21:09:33,150 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,213 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,214 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,215 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,216 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,217 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,240 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm3_1   | 2023-06-29 21:09:33,293 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3_1   | 2023-06-29 21:09:33,293 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm3_1   | 2023-06-29 21:09:33,408 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95c58c7b-97c6-4852-a177-ebb1478388b9
scm3_1   | 2023-06-29 21:09:33,410 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm3_1   | 2023-06-29 21:09:33,423 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:09:33,487 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:09:33,487 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-06-29 21:08:49,456 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]].
scm1_1   | 2023-06-29 21:08:49,456 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,459 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e to datanode:95c58c7b-97c6-4852-a177-ebb1478388b9
scm1_1   | 2023-06-29 21:08:49,465 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e to datanode:0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm1_1   | 2023-06-29 21:08:49,465 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e to datanode:b423f7b1-089b-406b-a3aa-6a650e93a531
scm1_1   | 2023-06-29 21:08:49,473 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]].
scm1_1   | 2023-06-29 21:08:49,474 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:49,474 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=a67dd9f1-63f5-4458-8ea5-1bb47a98493e contains same datanodes as previous pipelines: PipelineID=8bd8ea2c-61df-4132-bcca-886978236035 nodeIds: 95c58c7b-97c6-4852-a177-ebb1478388b9, 0a9e83e4-93dc-44cc-b7f2-64f77839ab35, b423f7b1-089b-406b-a3aa-6a650e93a531
scm1_1   | 2023-06-29 21:08:50,469 [IPC Server handler 11 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3561b5c6-83b9-4083-8461-2a79b0297944
scm1_1   | 2023-06-29 21:08:50,470 [IPC Server handler 11 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-29 21:08:50,471 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:50,472 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8 to datanode:3561b5c6-83b9-4083-8461-2a79b0297944
scm1_1   | 2023-06-29 21:08:50,475 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]].
scm1_1   | 2023-06-29 21:08:50,476 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:08:50,569 [IPC Server handler 12 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/653c0db6-d9db-4099-8201-751dcc40d458
scm1_1   | 2023-06-29 21:08:50,570 [IPC Server handler 12 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-29 21:08:50,571 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
scm1_1   | 2023-06-29 21:08:50,572 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c to datanode:653c0db6-d9db-4099-8201-751dcc40d458
scm1_1   | 2023-06-29 21:08:50,576 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]].
scm1_1   | 2023-06-29 21:08:50,577 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:05,627 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: 67dd125e-a7b5-412f-8128-ba59bd3344fe: Submitting SetConfiguration request to Ratis server with new SCM peers list: [67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-29 21:09:12,246 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,246 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,247 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,247 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,286 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,287 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,312 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,312 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,320 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,321 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,321 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]].
scm2_1   | 2023-06-29 21:09:12,321 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,728 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm2_1   | 2023-06-29 21:09:12,800 [IPC Server handler 6 on default port 9861] WARN ipc.Server: IPC Server handler 6 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:37088: output error
scm2_1   | 2023-06-29 21:09:12,803 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-29 21:09:05,649 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: receive setConfiguration SetConfigurationRequest:client-3A8DB2D44805->67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-06-29 21:09:05,668 [IPC Server handler 0 on default port 9863] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-3A8DB2D44805->67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-06-29 21:09:05,725 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-29 21:09:05,725 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-29 21:09:05,731 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-06-29 21:09:05,740 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-06-29 21:09:05,749 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-29 21:09:05,751 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-29 21:09:05,751 [IPC Server handler 0 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-29 21:09:05,751 [IPC Server handler 0 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-29 21:09:05,764 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1_1   | 2023-06-29 21:09:05,893 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-GrpcLogAppender: send 67dd125e-a7b5-412f-8128-ba59bd3344fe->fdfa269f-8b15-4612-aba6-bdafa0a99e33#0-t2,notify:(t:1, i:0)
scm1_1   | 2023-06-29 21:09:05,904 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for fdfa269f-8b15-4612-aba6-bdafa0a99e33
scm1_1   | 2023-06-29 21:09:09,559 [grpc-default-executor-0] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-InstallSnapshotResponseHandler: received the first reply 67dd125e-a7b5-412f-8128-ba59bd3344fe<-fdfa269f-8b15-4612-aba6-bdafa0a99e33#0:OK-t0,ALREADY_INSTALLED
scm1_1   | 2023-06-29 21:09:09,593 [grpc-default-executor-0] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1   | 2023-06-29 21:09:09,595 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: snapshotIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:09:09,595 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: matchIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:09:09,604 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: nextIndex: setUnconditionally 0 -> 1
scm1_1   | 2023-06-29 21:09:09,604 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33 acknowledged installing snapshot
scm1_1   | 2023-06-29 21:09:09,604 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: nextIndex: updateToMax old=1, new=1, updated? false
scm1_1   | 2023-06-29 21:09:09,833 [grpc-default-executor-1] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: nextIndex: updateUnconditionally 17 -> 0
scm1_1   | 2023-06-29 21:09:09,852 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->fdfa269f-8b15-4612-aba6-bdafa0a99e33: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:09:10,682 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 17: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-06-29 21:09:10,908 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 19: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:09:11,495 [IPC Server handler 0 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: fdfa269f-8b15-4612-aba6-bdafa0a99e33.
scm1_1   | 2023-06-29 21:09:23,640 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:24,077 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:12,807 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:39146: output error
scm2_1   | 2023-06-29 21:09:12,812 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-29 21:09:12,804 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:35998: output error
scm2_1   | 2023-06-29 21:09:12,827 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-29 21:09:12,807 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#8 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:60364: output error
scm2_1   | 2023-06-29 21:09:12,835 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-29 21:09:12,805 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#9 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:49732: output error
scm2_1   | 2023-06-29 21:09:12,839 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
scm2_1   | java.nio.channels.ClosedChannelException
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-29 21:09:12,924 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-06-29 21:09:12,924 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm2_1   | 2023-06-29 21:09:13,742 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-fdfa269f-8b15-4612-aba6-bdafa0a99e33: Detected pause in JVM or host machine (eg GC): pause of approximately 195530848ns.
scm2_1   | GC pool 'ParNew' had collection(s): count=1 time=205ms
scm2_1   | 2023-06-29 21:09:14,377 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm2_1   | 2023-06-29 21:09:14,384 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-29 21:09:14,488 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm2_1   | 2023-06-29 21:09:14,981 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm2_1   | 2023-06-29 21:09:14,983 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm2_1   | 2023-06-29 21:09:14,983 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm2_1   | 2023-06-29 21:09:15,042 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm2_1   | 2023-06-29 21:09:15,253 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@10a907ec] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-06-29 21:09:15,337 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1   | 2023-06-29 21:09:15,349 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm2_1   | 2023-06-29 21:09:15,492 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @26548ms to org.eclipse.jetty.util.log.Slf4jLog
scm2_1   | 2023-06-29 21:09:15,962 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-06-29 21:09:16,036 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-06-29 21:09:16,122 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm2_1   | 2023-06-29 21:09:16,127 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | 2023-06-29 21:09:16,131 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm2_1   | 2023-06-29 21:09:16,142 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-06-29 21:09:16,473 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-06-29 21:09:16,485 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-06-29 21:09:16,854 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm2_1   | 2023-06-29 21:09:16,890 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-06-29 21:09:16,904 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2_1   | 2023-06-29 21:09:16,968 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@650a1aff{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm2_1   | 2023-06-29 21:09:16,971 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@11c7a0b4{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-06-29 21:09:18,252 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@63a84bb6{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-3748202594750361102/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm2_1   | 2023-06-29 21:09:18,317 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@b3857e2{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm2_1   | 2023-06-29 21:09:18,317 [Listener at 0.0.0.0/9860] INFO server.Server: Started @29373ms
scm2_1   | 2023-06-29 21:09:18,347 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-06-29 21:09:18,347 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm2_1   | 2023-06-29 21:09:18,354 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-06-29 21:09:21,058 [IPC Server handler 58 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/95c58c7b-97c6-4852-a177-ebb1478388b9
scm2_1   | 2023-06-29 21:09:21,060 [IPC Server handler 58 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-29 21:09:21,088 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:21,112 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm2_1   | 2023-06-29 21:09:21,120 [IPC Server handler 68 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm2_1   | 2023-06-29 21:09:21,130 [IPC Server handler 68 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-29 21:09:21,131 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:21,141 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1   | 2023-06-29 21:09:21,442 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b423f7b1-089b-406b-a3aa-6a650e93a531
scm2_1   | 2023-06-29 21:09:21,443 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-29 21:09:21,443 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:21,445 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-06-29 21:09:21,445 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-06-29 21:09:21,445 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-06-29 21:09:21,445 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-29 21:09:21,446 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm2_1   | 2023-06-29 21:09:21,447 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:22,460 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3561b5c6-83b9-4083-8461-2a79b0297944
scm2_1   | 2023-06-29 21:09:22,461 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-29 21:09:22,461 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:22,571 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/653c0db6-d9db-4099-8201-751dcc40d458
scm2_1   | 2023-06-29 21:09:22,572 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-29 21:09:22,572 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-29 21:09:23,524 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 3a976013-1607-4227-9cfe-8a4dae2a0900, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.311Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-29 21:09:24,095 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:24,322 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:24,567 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:24,635 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:24,636 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:24,645 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:24,645 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-29 21:09:33,537 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-29 21:09:34,131 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-06-29 21:09:34,135 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-29 21:09:34,137 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
scm3_1   | 2023-06-29 21:09:34,180 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm3_1   | 2023-06-29 21:09:34,185 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm3_1   | 2023-06-29 21:09:34,186 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-29 21:09:34,186 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3_1   | 2023-06-29 21:09:34,326 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7a572ea0] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3_1   | 2023-06-29 21:09:34,354 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm3_1   | 2023-06-29 21:09:34,354 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-06-29 21:09:34,387 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @20501ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-06-29 21:09:34,567 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-06-29 21:09:34,572 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm3_1   | 2023-06-29 21:09:34,580 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-06-29 21:09:34,583 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm3_1   | 2023-06-29 21:09:34,584 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm3_1   | 2023-06-29 21:09:34,584 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm3_1   | 2023-06-29 21:09:34,657 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm3_1   | 2023-06-29 21:09:34,658 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm3_1   | 2023-06-29 21:09:34,687 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-06-29 21:09:34,687 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm3_1   | 2023-06-29 21:09:34,688 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm3_1   | 2023-06-29 21:09:34,702 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6b09ce57{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-06-29 21:09:34,703 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@790ac3e0{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm3_1   | 2023-06-29 21:09:35,183 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@69ba3f4e{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-9228752535864832586/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm3_1   | 2023-06-29 21:09:35,199 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@632241f5{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm3_1   | 2023-06-29 21:09:35,200 [Listener at 0.0.0.0/9860] INFO server.Server: Started @21314ms
scm3_1   | 2023-06-29 21:09:35,207 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm3_1   | 2023-06-29 21:09:35,207 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm3_1   | 2023-06-29 21:09:35,208 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm3_1   | 2023-06-29 21:09:59,099 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/b423f7b1-089b-406b-a3aa-6a650e93a531
scm3_1   | 2023-06-29 21:09:59,102 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:09:59,103 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:09:59,106 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm3_1   | 2023-06-29 21:09:59,107 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm3_1   | 2023-06-29 21:09:59,296 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/0a9e83e4-93dc-44cc-b7f2-64f77839ab35
scm3_1   | 2023-06-29 21:09:59,296 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:09:59,299 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-06-29 21:09:59,299 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm3_1   | 2023-06-29 21:09:59,300 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm3_1   | 2023-06-29 21:09:59,300 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm3_1   | 2023-06-29 21:09:59,300 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:09:59,300 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-29 21:09:24,645 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:25,251 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:25,273 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:25,418 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3561b5c6-83b9-4083-8461-2a79b0297944, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-29 21:09:25,465 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:31,018 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:31,225 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:32,803 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 31: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm2_1   | 2023-06-29 21:09:32,819 [fdfa269f-8b15-4612-aba6-bdafa0a99e33-server-thread1] INFO server.RaftServer$Division: fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018: set configuration 33: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-29 21:09:33,171 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:33,172 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,170 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 6b21d635-226c-4f1b-bd6a-67710b43930b, Nodes: b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:b423f7b1-089b-406b-a3aa-6a650e93a531, CreationTimestamp2023-06-29T21:08:49.443Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:24,170 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,288 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:24,313 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: c2cded09-6bff-4911-8b65-bf02ed3673ec, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.083Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:24,315 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,406 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:24,519 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,523 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,523 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:24,523 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:25,184 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 563d4dea-e8c9-4ba1-a8f1-9ae2fa46d77c, Nodes: 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:653c0db6-d9db-4099-8201-751dcc40d458, CreationTimestamp2023-06-29T21:08:50.572Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:25,235 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:25,254 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:25,325 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: b5d541dd-1a26-4a84-b6f7-8e0b4f3286c8, Nodes: 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:3561b5c6-83b9-4083-8461-2a79b0297944, CreationTimestamp2023-06-29T21:08:50.472Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:25,375 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:25,383 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:28,025 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:28,128 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:28,200 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:31,002 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:31,217 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:32,136 [IPC Server handler 7 on default port 9863] INFO ha.SCMRatisServerImpl: 67dd125e-a7b5-412f-8128-ba59bd3344fe: Submitting SetConfiguration request to Ratis server with new SCM peers list: [fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-29 21:09:32,137 [IPC Server handler 7 on default port 9863] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: receive setConfiguration SetConfigurationRequest:client-3A8DB2D44805->67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-06-29 21:09:32,137 [IPC Server handler 7 on default port 9863] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-3A8DB2D44805->67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
scm1_1   | 2023-06-29 21:09:32,137 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
scm1_1   | 2023-06-29 21:09:32,137 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-29 21:09:59,302 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
scm3_1   | 2023-06-29 21:09:59,303 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:09:59,303 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm3_1   | 2023-06-29 21:09:59,303 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm3_1   | 2023-06-29 21:09:59,303 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm3_1   | 2023-06-29 21:09:59,303 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm3_1   | 2023-06-29 21:09:59,304 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm3_1   | 2023-06-29 21:10:01,014 [IPC Server handler 2 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/653c0db6-d9db-4099-8201-751dcc40d458
scm3_1   | 2023-06-29 21:10:01,014 [IPC Server handler 2 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 653c0db6-d9db-4099-8201-751dcc40d458{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:10:01,014 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:10:01,250 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/3561b5c6-83b9-4083-8461-2a79b0297944
scm3_1   | 2023-06-29 21:10:01,251 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 3561b5c6-83b9-4083-8461-2a79b0297944{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-29 21:10:01,252 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-29 21:10:05,648 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]] moved to OPEN state
scm3_1   | 2023-06-29 21:10:07,120 [663949d4-905f-469d-b15f-059a3d7e4114@group-FDDFC6378018-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm2_1   | 2023-06-29 21:09:33,179 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:33,308 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-29 21:09:33,311 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:33,340 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2_1   | 2023-06-29 21:09:59,096 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:59,097 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm2_1   | 2023-06-29 21:09:59,097 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm2_1   | 2023-06-29 21:09:59,097 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm2_1   | 2023-06-29 21:09:59,098 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 2023-06-29 21:09:59,099 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-29 21:09:59,101 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm2_1   | 2023-06-29 21:10:05,634 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-29 21:10:07,125 [fdfa269f-8b15-4612-aba6-bdafa0a99e33@group-FDDFC6378018-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-06-29 21:09:32,138 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
scm1_1   | 2023-06-29 21:09:32,138 [IPC Server handler 7 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
scm1_1   | 2023-06-29 21:09:32,138 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
scm1_1   | 2023-06-29 21:09:32,138 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm1_1   | 2023-06-29 21:09:32,138 [IPC Server handler 7 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
scm1_1   | 2023-06-29 21:09:32,139 [IPC Server handler 7 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
scm1_1   | 2023-06-29 21:09:32,140 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
scm1_1   | 2023-06-29 21:09:32,141 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-GrpcLogAppender: send 67dd125e-a7b5-412f-8128-ba59bd3344fe->663949d4-905f-469d-b15f-059a3d7e4114#0-t2,notify:(t:1, i:0)
scm1_1   | 2023-06-29 21:09:32,141 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for 663949d4-905f-469d-b15f-059a3d7e4114
scm1_1   | 2023-06-29 21:09:32,522 [grpc-default-executor-0] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-InstallSnapshotResponseHandler: received the first reply 67dd125e-a7b5-412f-8128-ba59bd3344fe<-663949d4-905f-469d-b15f-059a3d7e4114#0:OK-t0,ALREADY_INSTALLED
scm1_1   | 2023-06-29 21:09:32,522 [grpc-default-executor-0] INFO server.GrpcLogAppender: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1   | 2023-06-29 21:09:32,522 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: snapshotIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:09:32,522 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: matchIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-29 21:09:32,523 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: nextIndex: setUnconditionally 0 -> 1
scm1_1   | 2023-06-29 21:09:32,523 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114 acknowledged installing snapshot
scm1_1   | 2023-06-29 21:09:32,523 [grpc-default-executor-0] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: nextIndex: updateToMax old=1, new=1, updated? false
scm1_1   | 2023-06-29 21:09:32,581 [grpc-default-executor-1] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: nextIndex: updateUnconditionally 31 -> 0
scm1_1   | 2023-06-29 21:09:32,586 [grpc-default-executor-1] INFO leader.FollowerInfo: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018->663949d4-905f-469d-b15f-059a3d7e4114: nextIndex: updateUnconditionally 31 -> 0
scm1_1   | 2023-06-29 21:09:32,800 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 31: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | 2023-06-29 21:09:32,809 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-LeaderStateImpl] INFO server.RaftServer$Division: 67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018: set configuration 33: peers:[fdfa269f-8b15-4612-aba6-bdafa0a99e33|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 663949d4-905f-469d-b15f-059a3d7e4114|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, 67dd125e-a7b5-412f-8128-ba59bd3344fe|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-29 21:09:32,826 [IPC Server handler 7 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: 663949d4-905f-469d-b15f-059a3d7e4114.
scm1_1   | 2023-06-29 21:09:33,316 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: a67dd9f1-63f5-4458-8ea5-1bb47a98493e, Nodes: 95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:95c58c7b-97c6-4852-a177-ebb1478388b9, CreationTimestamp2023-06-29T21:08:49.459Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:09:33,324 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm1_1   | 2023-06-29 21:09:33,328 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-06-29 21:09:33,351 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 2023-06-29 21:09:33,351 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-06-29 21:09:33,354 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm1_1   | 2023-06-29 21:09:33,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm1_1   | 2023-06-29 21:09:33,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-06-29 21:09:33,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
scm1_1   | 2023-06-29 21:09:33,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
scm1_1   | 2023-06-29 21:09:33,355 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
scm1_1   | 2023-06-29 21:09:33,358 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
scm1_1   | 2023-06-29 21:09:41,467 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-29 21:09:41,481 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-29 21:10:05,641 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 8bd8ea2c-61df-4132-bcca-886978236035, Nodes: 0a9e83e4-93dc-44cc-b7f2-64f77839ab35{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}b423f7b1-089b-406b-a3aa-6a650e93a531{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}95c58c7b-97c6-4852-a177-ebb1478388b9{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:0a9e83e4-93dc-44cc-b7f2-64f77839ab35, CreationTimestamp2023-06-29T21:08:49.453Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-29 21:10:07,045 [IPC Server handler 16 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1_1   | 2023-06-29 21:10:07,113 [67dd125e-a7b5-412f-8128-ba59bd3344fe@group-FDDFC6378018-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-06-29 21:10:07,142 [IPC Server handler 16 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm1_1   | 2023-06-29 21:10:11,468 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-29 21:10:11,481 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-29 21:10:41,468 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-29 21:10:41,481 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
