Attaching to ha_om3_1, ha_scm3_1, ha_dn2_1, ha_dn4_1, ha_dn1_1, ha_s3g_1, ha_dn3_1, ha_dn5_1, ha_om1_1, ha_om2_1, ha_scm2_1, ha_scm1_1, ha_recon_1
dn1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn1_1    | 2023-06-16 10:47:44,747 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn1_1    | /************************************************************
dn1_1    | STARTUP_MSG: Starting HddsDatanodeService
dn1_1    | STARTUP_MSG:   host = 01f1b3f9281a/10.9.0.17
dn1_1    | STARTUP_MSG:   args = []
dn1_1    | STARTUP_MSG:   version = 1.3.0
dn1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn1_1    | STARTUP_MSG:   java = 11.0.14.1
dn1_1    | ************************************************************/
dn1_1    | 2023-06-16 10:47:44,815 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn1_1    | 2023-06-16 10:47:45,277 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn1_1    | 2023-06-16 10:47:46,030 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn1_1    | 2023-06-16 10:47:47,175 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn1_1    | 2023-06-16 10:47:47,178 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn1_1    | 2023-06-16 10:47:48,156 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:01f1b3f9281a ip:10.9.0.17
dn1_1    | 2023-06-16 10:47:50,075 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn1_1    | 2023-06-16 10:47:51,295 [main] INFO reflections.Reflections: Reflections took 925 ms to scan 2 urls, producing 92 keys and 204 values 
dn1_1    | 2023-06-16 10:47:52,157 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn1_1    | 2023-06-16 10:47:53,318 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn1_1    | 2023-06-16 10:47:53,481 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn1_1    | 2023-06-16 10:47:53,507 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn1_1    | 2023-06-16 10:47:53,511 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn1_1    | 2023-06-16 10:47:53,673 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn1_1    | 2023-06-16 10:47:53,907 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-16 10:47:53,909 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn1_1    | 2023-06-16 10:47:53,935 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn1_1    | 2023-06-16 10:47:53,937 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn1_1    | 2023-06-16 10:47:53,958 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn1_1    | 2023-06-16 10:47:54,136 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn1_1    | 2023-06-16 10:47:54,136 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn1_1    | 2023-06-16 10:48:03,920 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn1_1    | 2023-06-16 10:48:04,412 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn1_1    | 2023-06-16 10:48:04,744 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn1_1    | 2023-06-16 10:48:05,470 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-16 10:48:05,543 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn1_1    | 2023-06-16 10:48:05,557 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn1_1    | 2023-06-16 10:48:05,571 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn1_1    | 2023-06-16 10:48:05,581 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn1_1    | 2023-06-16 10:48:05,587 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn1_1    | 2023-06-16 10:48:05,588 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn1_1    | 2023-06-16 10:48:05,607 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-16 10:48:05,631 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn1_1    | 2023-06-16 10:48:05,639 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn1_1    | 2023-06-16 10:48:05,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn1_1    | 2023-06-16 10:48:05,866 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn1_1    | 2023-06-16 10:48:05,881 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn1_1    | 2023-06-16 10:48:08,594 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn1_1    | 2023-06-16 10:48:08,677 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn1_1    | 2023-06-16 10:48:08,691 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn1_1    | 2023-06-16 10:48:08,692 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-16 10:48:08,719 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-16 10:48:08,744 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-16 10:48:08,892 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn1_1    | 2023-06-16 10:48:10,066 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn1_1    | 2023-06-16 10:48:10,141 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn1_1    | 2023-06-16 10:48:10,260 [main] INFO util.log: Logging initialized @35725ms to org.eclipse.jetty.util.log.Slf4jLog
dn1_1    | 2023-06-16 10:48:10,921 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn1_1    | 2023-06-16 10:48:10,929 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn1_1    | 2023-06-16 10:48:10,978 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn1_1    | 2023-06-16 10:48:11,253 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn1_1    | 2023-06-16 10:48:11,253 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn1_1    | 2023-06-16 10:48:11,253 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-16 10:48:11,479 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-06-16 10:48:11,480 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn1_1    | 2023-06-16 10:48:11,691 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn1_1    | 2023-06-16 10:48:11,691 [main] INFO server.session: No SessionScavenger set, using defaults
dn1_1    | 2023-06-16 10:48:11,693 [main] INFO server.session: node0 Scavenging every 660000ms
dn1_1    | 2023-06-16 10:48:11,782 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3434a4f0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn1_1    | 2023-06-16 10:48:11,788 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn1_1    | 2023-06-16 10:48:13,621 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5efcab{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-680813133460627964/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn1_1    | 2023-06-16 10:48:13,697 [main] INFO server.AbstractConnector: Started ServerConnector@3fdcde7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn1_1    | 2023-06-16 10:48:13,708 [main] INFO server.Server: Started @39173ms
dn1_1    | 2023-06-16 10:48:13,743 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn1_1    | 2023-06-16 10:48:13,743 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn1_1    | 2023-06-16 10:48:13,749 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn1_1    | 2023-06-16 10:48:13,772 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn1_1    | 2023-06-16 10:48:13,918 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@791bb529] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn1_1    | 2023-06-16 10:48:14,578 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn1_1    | 2023-06-16 10:48:15,071 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn1_1    | 2023-06-16 10:48:17,379 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:17,379 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:17,379 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:17,379 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:18,380 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:18,389 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:18,390 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:18,418 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:19,381 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:19,390 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:19,392 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:19,419 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:20,382 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:20,393 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:20,420 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:21,383 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:21,394 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:21,421 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:22,383 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn2_1    | 2023-06-16 10:47:44,629 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn2_1    | /************************************************************
dn2_1    | STARTUP_MSG: Starting HddsDatanodeService
dn2_1    | STARTUP_MSG:   host = 4d99f9962a79/10.9.0.18
dn2_1    | STARTUP_MSG:   args = []
dn2_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn2_1    | STARTUP_MSG:   java = 11.0.14.1
dn2_1    | ************************************************************/
dn2_1    | 2023-06-16 10:47:44,716 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn2_1    | 2023-06-16 10:47:45,220 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn2_1    | 2023-06-16 10:47:45,867 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn2_1    | 2023-06-16 10:47:47,188 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn2_1    | 2023-06-16 10:47:47,194 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn2_1    | 2023-06-16 10:47:48,061 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:4d99f9962a79 ip:10.9.0.18
dn2_1    | 2023-06-16 10:47:49,954 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn2_1    | 2023-06-16 10:47:51,176 [main] INFO reflections.Reflections: Reflections took 990 ms to scan 2 urls, producing 92 keys and 204 values 
dn2_1    | 2023-06-16 10:47:52,111 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn2_1    | 2023-06-16 10:47:53,156 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn2_1    | 2023-06-16 10:47:53,298 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn2_1    | 2023-06-16 10:47:53,300 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn2_1    | 2023-06-16 10:47:53,301 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn2_1    | 2023-06-16 10:47:53,397 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn2_1    | 2023-06-16 10:47:53,522 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-16 10:47:53,531 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn2_1    | 2023-06-16 10:47:53,551 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn2_1    | 2023-06-16 10:47:53,556 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn2_1    | 2023-06-16 10:47:53,577 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn2_1    | 2023-06-16 10:47:53,857 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn2_1    | 2023-06-16 10:47:53,857 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn2_1    | 2023-06-16 10:48:03,691 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn2_1    | 2023-06-16 10:48:04,378 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn2_1    | 2023-06-16 10:48:05,001 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn2_1    | 2023-06-16 10:48:05,861 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-16 10:48:05,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn2_1    | 2023-06-16 10:48:05,862 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn2_1    | 2023-06-16 10:48:05,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn2_1    | 2023-06-16 10:48:05,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn2_1    | 2023-06-16 10:48:05,889 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn2_1    | 2023-06-16 10:48:05,894 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn2_1    | 2023-06-16 10:48:05,899 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:48:05,907 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn2_1    | 2023-06-16 10:48:05,907 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-16 10:48:06,000 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-16 10:48:06,037 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn2_1    | 2023-06-16 10:48:06,037 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn2_1    | 2023-06-16 10:48:08,276 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn2_1    | 2023-06-16 10:48:08,278 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn2_1    | 2023-06-16 10:48:08,295 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn2_1    | 2023-06-16 10:48:08,311 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:48:08,311 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-16 10:48:08,333 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-16 10:48:08,510 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn2_1    | 2023-06-16 10:48:09,611 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn2_1    | 2023-06-16 10:48:09,760 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn2_1    | 2023-06-16 10:48:10,227 [main] INFO util.log: Logging initialized @35953ms to org.eclipse.jetty.util.log.Slf4jLog
dn2_1    | 2023-06-16 10:48:11,429 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn2_1    | 2023-06-16 10:48:11,466 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn2_1    | 2023-06-16 10:48:11,631 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn2_1    | 2023-06-16 10:48:11,637 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn2_1    | 2023-06-16 10:48:11,637 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn1_1    | 2023-06-16 10:48:22,394 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:22,422 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:23,385 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:23,396 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:23,423 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:24,386 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:24,397 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:24,424 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:24,427 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 01f1b3f9281a/10.9.0.17 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:34440 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:34440 remote=recon/10.9.0.22:9891]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-06-16 10:48:25,387 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:25,425 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:26,388 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:26,425 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:27,389 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:27,426 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:28,390 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:28,427 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:29,391 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:29,407 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn1_1    | java.net.SocketTimeoutException: Call From 01f1b3f9281a/10.9.0.17 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:56944 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.17:56944 remote=scm1/10.9.0.14:9861]
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn1_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn1_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn1_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn1_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn1_1    | 2023-06-16 10:48:29,428 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:29,942 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-16 10:48:30,056 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-94fef24a-b5ca-458f-af32-d3b36ad7c4a5/container.db to cache
dn1_1    | 2023-06-16 10:48:30,056 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-94fef24a-b5ca-458f-af32-d3b36ad7c4a5/container.db for volume DS-94fef24a-b5ca-458f-af32-d3b36ad7c4a5
dn1_1    | 2023-06-16 10:48:30,064 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn1_1    | 2023-06-16 10:48:30,067 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn1_1    | 2023-06-16 10:48:30,350 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 13cc61e5-01ec-4df0-9437-cac1ff0f189e
dn1_1    | 2023-06-16 10:48:30,392 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:30,429 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: start RPC server
dn1_1    | 2023-06-16 10:48:30,429 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:30,431 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: GrpcService started, listening on 9858
dn1_1    | 2023-06-16 10:48:30,432 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: GrpcService started, listening on 9856
dn1_1    | 2023-06-16 10:48:30,433 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: GrpcService started, listening on 9857
dn1_1    | 2023-06-16 10:48:30,458 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 13cc61e5-01ec-4df0-9437-cac1ff0f189e is started using port 9858 for RATIS
dn1_1    | 2023-06-16 10:48:30,459 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 13cc61e5-01ec-4df0-9437-cac1ff0f189e is started using port 9857 for RATIS_ADMIN
dn1_1    | 2023-06-16 10:48:30,459 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 13cc61e5-01ec-4df0-9437-cac1ff0f189e is started using port 9856 for RATIS_SERVER
dn1_1    | 2023-06-16 10:48:30,472 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-13cc61e5-01ec-4df0-9437-cac1ff0f189e: Started
dn1_1    | 2023-06-16 10:48:31,420 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:31,422 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 01f1b3f9281a/10.9.0.17 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-16 10:48:31,430 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:31,437 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn1_1    | java.net.ConnectException: Call From 01f1b3f9281a/10.9.0.17 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn1_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn1_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn1_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn1_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 2023-06-16 10:48:11,637 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn2_1    | 2023-06-16 10:48:12,087 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn2_1    | 2023-06-16 10:48:12,100 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn2_1    | 2023-06-16 10:48:12,456 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn2_1    | 2023-06-16 10:48:12,456 [main] INFO server.session: No SessionScavenger set, using defaults
dn2_1    | 2023-06-16 10:48:12,462 [main] INFO server.session: node0 Scavenging every 600000ms
dn2_1    | 2023-06-16 10:48:12,593 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b9ac754{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn2_1    | 2023-06-16 10:48:12,594 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn2_1    | 2023-06-16 10:48:14,603 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1d2fb82{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-18013781363767548687/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn2_1    | 2023-06-16 10:48:14,723 [main] INFO server.AbstractConnector: Started ServerConnector@7302ff13{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn2_1    | 2023-06-16 10:48:14,723 [main] INFO server.Server: Started @40449ms
dn2_1    | 2023-06-16 10:48:14,751 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn2_1    | 2023-06-16 10:48:14,751 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn2_1    | 2023-06-16 10:48:14,759 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn2_1    | 2023-06-16 10:48:14,793 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn2_1    | 2023-06-16 10:48:15,253 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@269bdb1e] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn2_1    | 2023-06-16 10:48:15,528 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn2_1    | 2023-06-16 10:48:15,969 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn2_1    | 2023-06-16 10:48:18,450 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:18,463 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:18,468 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:18,468 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:19,091 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-06-16 10:48:19,451 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:19,464 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:19,468 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:19,469 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:20,465 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:20,469 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:20,470 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:21,466 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn1_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.net.ConnectException: Connection refused
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn1_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn1_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn1_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn1_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn1_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn1_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn1_1    | 	... 12 more
dn1_1    | 2023-06-16 10:48:32,423 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:32,439 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:33,424 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:33,440 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:33,953 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-16 10:48:34,425 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:34,440 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:35,426 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:35,441 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:36,427 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:36,442 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:37,428 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:37,443 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:38,429 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:38,444 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:39,429 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:39,444 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:40,430 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:40,445 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:41,431 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:41,446 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:42,432 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:42,447 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:43,433 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:43,447 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:44,434 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:44,448 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:45,435 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:45,449 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:46,436 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:46,449 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:47,437 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:47,451 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:48,440 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:48,452 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:49,441 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:49,452 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:50,442 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:50,453 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:51,443 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:51,454 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:52,444 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:52,455 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:53,445 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:21,469 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:21,470 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:22,466 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:22,470 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:22,471 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:23,468 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:23,471 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:23,472 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:24,468 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:24,473 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:24,473 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:24,484 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 4d99f9962a79/10.9.0.18 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45900 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:45900 remote=recon/10.9.0.22:9891]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-16 10:48:25,474 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:25,474 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:26,474 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn3_1    | 2023-06-16 10:47:45,101 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn3_1    | /************************************************************
dn3_1    | STARTUP_MSG: Starting HddsDatanodeService
dn3_1    | STARTUP_MSG:   host = d437f4d74d34/10.9.0.19
dn3_1    | STARTUP_MSG:   args = []
dn3_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn3_1    | STARTUP_MSG:   java = 11.0.14.1
dn3_1    | ************************************************************/
dn3_1    | 2023-06-16 10:47:45,157 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn3_1    | 2023-06-16 10:47:45,558 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn3_1    | 2023-06-16 10:47:46,338 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn3_1    | 2023-06-16 10:47:47,547 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn3_1    | 2023-06-16 10:47:47,548 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn3_1    | 2023-06-16 10:47:48,456 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:d437f4d74d34 ip:10.9.0.19
dn3_1    | 2023-06-16 10:47:50,385 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn3_1    | 2023-06-16 10:47:51,741 [main] INFO reflections.Reflections: Reflections took 1079 ms to scan 2 urls, producing 92 keys and 204 values 
dn3_1    | 2023-06-16 10:47:52,796 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn3_1    | 2023-06-16 10:47:53,713 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn3_1    | 2023-06-16 10:47:53,794 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn3_1    | 2023-06-16 10:47:53,820 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn3_1    | 2023-06-16 10:47:53,828 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn3_1    | 2023-06-16 10:47:53,988 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn3_1    | 2023-06-16 10:47:54,070 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-16 10:47:54,124 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn3_1    | 2023-06-16 10:47:54,139 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn3_1    | 2023-06-16 10:47:54,140 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn3_1    | 2023-06-16 10:47:54,146 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn3_1    | 2023-06-16 10:47:54,416 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn3_1    | 2023-06-16 10:47:54,416 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn3_1    | 2023-06-16 10:48:04,103 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn3_1    | 2023-06-16 10:48:04,962 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn3_1    | 2023-06-16 10:48:05,389 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn3_1    | 2023-06-16 10:48:06,056 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-16 10:48:06,076 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn3_1    | 2023-06-16 10:48:06,099 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn3_1    | 2023-06-16 10:48:06,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn3_1    | 2023-06-16 10:48:06,109 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn3_1    | 2023-06-16 10:48:06,114 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn3_1    | 2023-06-16 10:48:06,116 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn3_1    | 2023-06-16 10:48:06,118 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:48:06,135 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn3_1    | 2023-06-16 10:48:06,143 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-16 10:48:06,203 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-16 10:48:06,265 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn3_1    | 2023-06-16 10:48:06,276 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn3_1    | 2023-06-16 10:48:08,841 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn3_1    | 2023-06-16 10:48:08,874 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn3_1    | 2023-06-16 10:48:08,883 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn3_1    | 2023-06-16 10:48:08,903 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:48:08,904 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-16 10:48:08,936 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-16 10:48:09,122 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn3_1    | 2023-06-16 10:48:10,321 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn3_1    | 2023-06-16 10:48:10,420 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn3_1    | 2023-06-16 10:48:10,632 [main] INFO util.log: Logging initialized @36317ms to org.eclipse.jetty.util.log.Slf4jLog
dn3_1    | 2023-06-16 10:48:11,316 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn3_1    | 2023-06-16 10:48:11,353 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn3_1    | 2023-06-16 10:48:11,425 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn3_1    | 2023-06-16 10:48:11,444 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn3_1    | 2023-06-16 10:48:11,485 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn3_1    | 2023-06-16 10:48:11,485 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 2023-06-16 10:48:12,003 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn1_1    | 2023-06-16 10:48:53,456 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:54,456 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:55,457 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:56,458 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:57,459 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:58,463 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:48:59,464 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:00,465 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:00,759 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn1_1    | 2023-06-16 10:49:01,466 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:02,467 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:03,468 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:03,969 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-16 10:49:03,970 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn1_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn1_1    | Caused by: java.util.concurrent.TimeoutException
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn1_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn1_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn1_1    | 	... 1 more
dn1_1    | 2023-06-16 10:49:04,469 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:05,202 [Command processor thread] INFO server.RaftServer: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: addNew group-6E4272A38D3E:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] returns group-6E4272A38D3E:java.util.concurrent.CompletableFuture@40907185[Not completed]
dn3_1    | 2023-06-16 10:48:12,006 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn3_1    | 2023-06-16 10:48:12,328 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn3_1    | 2023-06-16 10:48:12,329 [main] INFO server.session: No SessionScavenger set, using defaults
dn3_1    | 2023-06-16 10:48:12,341 [main] INFO server.session: node0 Scavenging every 660000ms
dn3_1    | 2023-06-16 10:48:12,463 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3434a4f0{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn3_1    | 2023-06-16 10:48:12,464 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@267f9765{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn3_1    | 2023-06-16 10:48:15,186 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@7e5efcab{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-1510070572794541617/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn3_1    | 2023-06-16 10:48:15,281 [main] INFO server.AbstractConnector: Started ServerConnector@3fdcde7a{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn3_1    | 2023-06-16 10:48:15,281 [main] INFO server.Server: Started @40966ms
dn3_1    | 2023-06-16 10:48:15,310 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn3_1    | 2023-06-16 10:48:15,310 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn3_1    | 2023-06-16 10:48:15,312 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn3_1    | 2023-06-16 10:48:15,354 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn3_1    | 2023-06-16 10:48:15,656 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@60a8d837] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn3_1    | 2023-06-16 10:48:16,062 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn3_1    | 2023-06-16 10:48:16,390 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-06-16 10:48:19,065 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:19,065 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:19,068 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:19,081 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:19,722 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:48:20,066 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:20,069 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:20,082 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:21,067 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:21,069 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:21,082 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:21,725 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:48:22,068 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:22,070 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:22,083 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:23,069 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:23,071 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:23,084 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:23,730 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:48:24,070 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:24,072 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:24,084 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:24,088 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From d437f4d74d34/10.9.0.19 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:55690 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:55690 remote=recon/10.9.0.22:9891]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-16 10:48:25,071 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:25,072 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:26,072 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:26,073 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:27,073 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:27,074 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:28,074 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:28,075 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:29,075 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:29,075 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:29,107 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn3_1    | java.net.SocketTimeoutException: Call From d437f4d74d34/10.9.0.19 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:49050 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 2023-06-16 10:48:26,476 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:27,475 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:27,476 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:28,477 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:28,477 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:29,478 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:29,478 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:29,480 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn2_1    | java.net.SocketTimeoutException: Call From 4d99f9962a79/10.9.0.18 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:47300 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.18:47300 remote=scm1/10.9.0.14:9861]
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn2_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn2_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn2_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn2_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn2_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-16 10:48:29,942 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-f91e7747-a07c-4117-9d93-509b760d6ef4/container.db to cache
dn2_1    | 2023-06-16 10:48:29,948 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-f91e7747-a07c-4117-9d93-509b760d6ef4/container.db for volume DS-f91e7747-a07c-4117-9d93-509b760d6ef4
dn2_1    | 2023-06-16 10:48:29,949 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn2_1    | 2023-06-16 10:48:29,956 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn2_1    | 2023-06-16 10:48:30,187 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:48:30,256 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start RPC server
dn2_1    | 2023-06-16 10:48:30,266 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: GrpcService started, listening on 9858
dn2_1    | 2023-06-16 10:48:30,267 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: GrpcService started, listening on 9856
dn2_1    | 2023-06-16 10:48:30,269 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: GrpcService started, listening on 9857
dn2_1    | 2023-06-16 10:48:30,304 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7e6c224f-5cdf-45b7-b597-f8bf583ee016 is started using port 9858 for RATIS
dn2_1    | 2023-06-16 10:48:30,304 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7e6c224f-5cdf-45b7-b597-f8bf583ee016 is started using port 9857 for RATIS_ADMIN
dn2_1    | 2023-06-16 10:48:30,304 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 7e6c224f-5cdf-45b7-b597-f8bf583ee016 is started using port 9856 for RATIS_SERVER
dn2_1    | 2023-06-16 10:48:30,316 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-7e6c224f-5cdf-45b7-b597-f8bf583ee016: Started
dn2_1    | 2023-06-16 10:48:30,479 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:30,480 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:31,096 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-06-16 10:48:31,480 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:31,481 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:32,481 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:32,482 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 4d99f9962a79/10.9.0.18 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-06-16 10:48:32,483 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:32,483 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn2_1    | java.net.ConnectException: Call From 4d99f9962a79/10.9.0.18 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn2_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn2_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn2_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn2_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn2_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.net.ConnectException: Connection refused
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn2_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn2_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn2_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn2_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn2_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn2_1    | 	... 12 more
dn2_1    | 2023-06-16 10:48:33,484 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:33,485 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:34,490 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:34,491 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:35,491 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:35,492 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:36,492 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:36,493 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:37,493 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:37,495 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:38,494 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:38,495 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:39,496 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:39,496 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:40,505 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:40,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:41,506 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.19:49050 remote=scm1/10.9.0.14:9861]
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn3_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn1_1    | 2023-06-16 10:49:05,339 [pool-22-thread-1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: new RaftServerImpl for group-6E4272A38D3E:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn1_1    | 2023-06-16 10:49:05,348 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn1_1    | 2023-06-16 10:49:05,349 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn1_1    | 2023-06-16 10:49:05,350 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn1_1    | 2023-06-16 10:49:05,350 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn1_1    | 2023-06-16 10:49:05,350 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn1_1    | 2023-06-16 10:49:05,351 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn1_1    | 2023-06-16 10:49:05,394 [pool-22-thread-1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: ConfigurationManager, init=-1: peers:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn1_1    | 2023-06-16 10:49:05,430 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn1_1    | 2023-06-16 10:49:05,454 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn1_1    | 2023-06-16 10:49:05,481 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:05,482 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn1_1    | 2023-06-16 10:49:05,533 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn1_1    | 2023-06-16 10:49:05,545 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn1_1    | 2023-06-16 10:49:05,562 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn1_1    | 2023-06-16 10:49:05,928 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn1_1    | 2023-06-16 10:49:05,929 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn1_1    | 2023-06-16 10:49:05,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn1_1    | 2023-06-16 10:49:05,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn1_1    | 2023-06-16 10:49:05,961 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn1_1    | 2023-06-16 10:49:05,961 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e156c71f-4c3d-49f1-9f4c-6e4272a38d3e does not exist. Creating ...
dn1_1    | 2023-06-16 10:49:06,014 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e156c71f-4c3d-49f1-9f4c-6e4272a38d3e/in_use.lock acquired by nodename 6@01f1b3f9281a
dn1_1    | 2023-06-16 10:49:06,074 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e156c71f-4c3d-49f1-9f4c-6e4272a38d3e has been successfully formatted.
dn1_1    | 2023-06-16 10:49:06,194 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-6E4272A38D3E: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn1_1    | 2023-06-16 10:49:06,237 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn1_1    | 2023-06-16 10:49:06,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn1_1    | 2023-06-16 10:49:06,281 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn1_1    | 2023-06-16 10:49:06,307 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn1_1    | 2023-06-16 10:49:06,308 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn1_1    | 2023-06-16 10:49:06,322 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-16 10:49:06,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn1_1    | 2023-06-16 10:49:06,380 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn1_1    | 2023-06-16 10:49:06,427 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e156c71f-4c3d-49f1-9f4c-6e4272a38d3e
dn1_1    | 2023-06-16 10:49:06,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn1_1    | 2023-06-16 10:49:06,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn1_1    | 2023-06-16 10:49:06,428 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn1_1    | 2023-06-16 10:49:06,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn1_1    | 2023-06-16 10:49:06,447 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn1_1    | 2023-06-16 10:49:06,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn1_1    | 2023-06-16 10:49:06,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn1_1    | 2023-06-16 10:49:06,449 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn1_1    | 2023-06-16 10:49:06,491 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:06,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn1_1    | 2023-06-16 10:49:06,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn1_1    | 2023-06-16 10:49:06,532 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn1_1    | 2023-06-16 10:49:06,553 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn1_1    | 2023-06-16 10:49:06,591 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-16 10:49:06,591 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn1_1    | 2023-06-16 10:49:06,612 [pool-22-thread-1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: start as a follower, conf=-1: peers:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-16 10:49:06,621 [pool-22-thread-1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn1_1    | 2023-06-16 10:49:06,641 [pool-22-thread-1] INFO impl.RoleInfo: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: start 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState
dn1_1    | 2023-06-16 10:49:06,642 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn1_1    | 2023-06-16 10:49:06,648 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn1_1    | 2023-06-16 10:49:06,678 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-6E4272A38D3E,id=13cc61e5-01ec-4df0-9437-cac1ff0f189e
dn1_1    | 2023-06-16 10:49:06,684 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn1_1    | 2023-06-16 10:49:06,688 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn1_1    | 2023-06-16 10:49:06,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn1_1    | 2023-06-16 10:49:06,693 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn1_1    | 2023-06-16 10:49:06,795 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e156c71f-4c3d-49f1-9f4c-6e4272a38d3e
dn1_1    | 2023-06-16 10:49:06,800 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e156c71f-4c3d-49f1-9f4c-6e4272a38d3e.
dn1_1    | 2023-06-16 10:49:07,499 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:08,502 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:09,502 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:10,503 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:11,504 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:11,759 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO impl.FollowerState: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5120139284ns, electionTimeout:5108ms
dn1_1    | 2023-06-16 10:49:11,760 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO impl.RoleInfo: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: shutdown 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState
dn1_1    | 2023-06-16 10:49:11,760 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn1_1    | 2023-06-16 10:49:11,763 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn1_1    | 2023-06-16 10:49:11,767 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-FollowerState] INFO impl.RoleInfo: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: start 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1
dn1_1    | 2023-06-16 10:49:11,777 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO impl.LeaderElection: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-16 10:49:11,778 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO impl.LeaderElection: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn1_1    | 2023-06-16 10:49:11,778 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO impl.RoleInfo: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: shutdown 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1
dn1_1    | 2023-06-16 10:49:11,779 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn1_1    | 2023-06-16 10:49:11,779 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-6E4272A38D3E with new leaderId: 13cc61e5-01ec-4df0-9437-cac1ff0f189e
dn1_1    | 2023-06-16 10:49:11,779 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: change Leader from null to 13cc61e5-01ec-4df0-9437-cac1ff0f189e at term 1 for becomeLeader, leader elected after 6248ms
dn1_1    | 2023-06-16 10:49:11,802 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn1_1    | 2023-06-16 10:49:11,816 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-16 10:49:11,819 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn1_1    | 2023-06-16 10:49:11,833 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn1_1    | 2023-06-16 10:49:11,833 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn1_1    | 2023-06-16 10:49:11,834 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn1_1    | 2023-06-16 10:49:11,879 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn1_1    | 2023-06-16 10:49:11,880 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn1_1    | 2023-06-16 10:49:11,897 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO impl.RoleInfo: 13cc61e5-01ec-4df0-9437-cac1ff0f189e: start 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderStateImpl
dn1_1    | 2023-06-16 10:49:11,917 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker: Starting segment from index:0
dn1_1    | 2023-06-16 10:49:12,044 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-LeaderElection1] INFO server.RaftServer$Division: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E: set configuration 0: peers:[13cc61e5-01ec-4df0-9437-cac1ff0f189e|rpc:10.9.0.17:9856|admin:10.9.0.17:9857|client:10.9.0.17:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn1_1    | 2023-06-16 10:49:12,110 [13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 13cc61e5-01ec-4df0-9437-cac1ff0f189e@group-6E4272A38D3E-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e156c71f-4c3d-49f1-9f4c-6e4272a38d3e/current/log_inprogress_0
dn1_1    | 2023-06-16 10:49:12,505 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:13,506 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:14,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn1_1    | 2023-06-16 10:49:16,946 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn3_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn3_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn3_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn3_1    | 2023-06-16 10:48:30,020 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-d25e39f6-af56-4f0b-b29b-d03bf6f4f4f7/container.db to cache
dn3_1    | 2023-06-16 10:48:30,029 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-d25e39f6-af56-4f0b-b29b-d03bf6f4f4f7/container.db for volume DS-d25e39f6-af56-4f0b-b29b-d03bf6f4f4f7
dn3_1    | 2023-06-16 10:48:30,032 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn3_1    | 2023-06-16 10:48:30,037 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn3_1    | 2023-06-16 10:48:30,077 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:30,078 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:30,226 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:48:30,352 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start RPC server
dn3_1    | 2023-06-16 10:48:30,378 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: a27fbf0d-ae65-4185-b735-d902c8c8b71a: GrpcService started, listening on 9858
dn3_1    | 2023-06-16 10:48:30,380 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: a27fbf0d-ae65-4185-b735-d902c8c8b71a: GrpcService started, listening on 9856
dn3_1    | 2023-06-16 10:48:30,381 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: a27fbf0d-ae65-4185-b735-d902c8c8b71a: GrpcService started, listening on 9857
dn3_1    | 2023-06-16 10:48:30,433 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a27fbf0d-ae65-4185-b735-d902c8c8b71a is started using port 9858 for RATIS
dn3_1    | 2023-06-16 10:48:30,433 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a27fbf0d-ae65-4185-b735-d902c8c8b71a is started using port 9857 for RATIS_ADMIN
dn3_1    | 2023-06-16 10:48:30,434 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis a27fbf0d-ae65-4185-b735-d902c8c8b71a is started using port 9856 for RATIS_SERVER
dn3_1    | 2023-06-16 10:48:30,434 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-a27fbf0d-ae65-4185-b735-d902c8c8b71a: Started
dn3_1    | 2023-06-16 10:48:31,078 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:31,078 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:32,080 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:32,080 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:33,081 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:33,081 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:33,082 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From d437f4d74d34/10.9.0.19 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 2023-06-16 10:48:41,507 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:42,507 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:42,508 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:43,508 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:43,509 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:44,509 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:44,510 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:45,510 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:45,511 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:46,511 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:46,512 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:47,512 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:47,513 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:48,514 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:48,515 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:49,515 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:49,516 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:50,528 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:50,529 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:51,534 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:51,539 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:52,539 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:52,540 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:53,540 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:53,541 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:54,541 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:55,542 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:56,543 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-06-16 10:48:33,082 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn3_1    | java.net.ConnectException: Call From d437f4d74d34/10.9.0.19 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn3_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn3_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn3_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn3_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn3_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.net.ConnectException: Connection refused
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn3_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn3_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn3_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn3_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn3_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn3_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn3_1    | 	... 12 more
dn3_1    | 2023-06-16 10:48:33,735 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:48:34,084 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:34,084 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:35,084 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:35,085 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:36,086 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:57,544 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:58,545 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:48:59,546 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:00,546 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:00,769 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-16 10:49:01,547 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:02,548 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:03,100 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn2_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn2_1    | Caused by: java.util.concurrent.TimeoutException
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn2_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn2_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn2_1    | 	... 1 more
dn2_1    | 2023-06-16 10:49:03,549 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:04,402 [Command processor thread] INFO server.RaftServer: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: addNew group-75B0B54839B8:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] returns group-75B0B54839B8:java.util.concurrent.CompletableFuture@3be42190[Not completed]
dn2_1    | 2023-06-16 10:49:04,482 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: new RaftServerImpl for group-75B0B54839B8:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-16 10:49:04,483 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-16 10:49:04,484 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-16 10:49:04,484 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-16 10:49:04,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:04,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-16 10:49:04,488 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-16 10:49:04,498 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: ConfigurationManager, init=-1: peers:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-16 10:49:04,509 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-16 10:49:04,547 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-16 10:49:04,550 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:04,555 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-16 10:49:04,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:04,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-16 10:49:04,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-16 10:49:04,942 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-16 10:49:04,980 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-16 10:49:04,989 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-16 10:49:04,990 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-16 10:49:04,994 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn5_1    | 2023-06-16 10:47:44,071 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn5_1    | /************************************************************
dn5_1    | STARTUP_MSG: Starting HddsDatanodeService
dn5_1    | STARTUP_MSG:   host = 2e2f108eef4e/10.9.0.21
dn5_1    | STARTUP_MSG:   args = []
dn5_1    | STARTUP_MSG:   version = 1.3.0
dn2_1    | 2023-06-16 10:49:04,999 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/e4ca3c21-179d-436f-b702-75b0b54839b8 does not exist. Creating ...
dn2_1    | 2023-06-16 10:49:05,024 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/e4ca3c21-179d-436f-b702-75b0b54839b8/in_use.lock acquired by nodename 7@4d99f9962a79
dn2_1    | 2023-06-16 10:49:05,057 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/e4ca3c21-179d-436f-b702-75b0b54839b8 has been successfully formatted.
dn2_1    | 2023-06-16 10:49:05,123 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-75B0B54839B8: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-16 10:49:05,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-16 10:49:05,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-16 10:49:05,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:49:05,335 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-16 10:49:05,341 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-16 10:49:05,387 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,412 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-16 10:49:05,417 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-16 10:49:05,433 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/e4ca3c21-179d-436f-b702-75b0b54839b8
dn2_1    | 2023-06-16 10:49:05,451 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-16 10:49:05,452 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:05,455 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,456 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-16 10:49:05,461 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-16 10:49:05,462 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-16 10:49:05,466 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-16 10:49:05,475 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-16 10:49:05,529 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,530 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:05,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:05,536 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:05,551 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:05,570 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:05,571 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:05,578 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: start as a follower, conf=-1: peers:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:05,578 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-16 10:49:05,582 [pool-22-thread-1] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState
dn2_1    | 2023-06-16 10:49:05,587 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:05,592 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:05,607 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-75B0B54839B8,id=7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:49:05,620 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:05,637 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-16 10:49:05,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-16 10:49:05,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-16 10:49:05,755 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=e4ca3c21-179d-436f-b702-75b0b54839b8
dn2_1    | 2023-06-16 10:49:05,764 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=e4ca3c21-179d-436f-b702-75b0b54839b8.
dn2_1    | 2023-06-16 10:49:05,767 [Command processor thread] INFO server.RaftServer: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: addNew group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-E3FF46599155:java.util.concurrent.CompletableFuture@2b6aa499[Not completed]
dn4_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn4_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-16 10:47:45,376 [main] INFO ozone.HddsDatanodeService: STARTUP_MSG: 
dn4_1    | /************************************************************
dn4_1    | STARTUP_MSG: Starting HddsDatanodeService
dn4_1    | STARTUP_MSG:   host = ee32ea3a212f/10.9.0.20
dn4_1    | STARTUP_MSG:   args = []
dn4_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | 2023-06-16 10:48:36,086 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:37,086 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:37,087 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:38,087 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:38,088 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:39,088 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:39,089 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:40,089 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:40,090 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:41,090 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:41,091 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:42,091 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:42,092 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:43,092 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:43,093 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:44,093 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:44,094 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:45,094 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:45,094 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:46,095 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:46,096 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:47,096 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:47,097 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:48,097 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:48,098 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:49,098 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:49,099 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:50,099 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn5_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn5_1    | STARTUP_MSG:   java = 11.0.14.1
dn5_1    | ************************************************************/
dn5_1    | 2023-06-16 10:47:44,115 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-06-16 10:47:44,531 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn5_1    | 2023-06-16 10:47:45,297 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn5_1    | 2023-06-16 10:47:46,176 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn5_1    | 2023-06-16 10:47:46,177 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn5_1    | 2023-06-16 10:47:47,049 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:2e2f108eef4e ip:10.9.0.21
dn5_1    | 2023-06-16 10:47:48,737 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn5_1    | 2023-06-16 10:47:50,068 [main] INFO reflections.Reflections: Reflections took 1090 ms to scan 2 urls, producing 92 keys and 204 values 
dn5_1    | 2023-06-16 10:47:51,036 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn5_1    | 2023-06-16 10:47:52,287 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn5_1    | 2023-06-16 10:47:52,382 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn5_1    | 2023-06-16 10:47:52,384 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn5_1    | 2023-06-16 10:47:52,407 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn5_1    | 2023-06-16 10:47:52,570 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn5_1    | 2023-06-16 10:47:52,755 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-16 10:47:52,763 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn5_1    | 2023-06-16 10:47:52,769 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn5_1    | 2023-06-16 10:47:52,778 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn5_1    | 2023-06-16 10:47:52,778 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn5_1    | 2023-06-16 10:47:53,030 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn5_1    | 2023-06-16 10:47:53,033 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn5_1    | 2023-06-16 10:48:02,585 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn5_1    | 2023-06-16 10:48:03,225 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-16 10:48:03,591 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn5_1    | 2023-06-16 10:48:04,429 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-16 10:48:04,464 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn5_1    | 2023-06-16 10:48:04,471 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn5_1    | 2023-06-16 10:48:04,485 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn5_1    | 2023-06-16 10:48:04,486 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-06-16 10:48:04,486 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn5_1    | 2023-06-16 10:48:04,491 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn5_1    | 2023-06-16 10:48:04,496 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-16 10:48:04,502 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn5_1    | 2023-06-16 10:48:04,511 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn5_1    | 2023-06-16 10:48:04,602 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn5_1    | 2023-06-16 10:48:04,651 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn5_1    | 2023-06-16 10:48:04,668 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn5_1    | 2023-06-16 10:48:07,325 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn5_1    | 2023-06-16 10:48:07,351 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-16 10:48:07,369 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-16 10:48:07,373 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn5_1    | 2023-06-16 10:48:07,382 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-16 10:48:07,415 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-16 10:48:07,636 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn5_1    | 2023-06-16 10:48:09,122 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn5_1    | 2023-06-16 10:48:09,295 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn5_1    | 2023-06-16 10:48:09,623 [main] INFO util.log: Logging initialized @36680ms to org.eclipse.jetty.util.log.Slf4jLog
dn5_1    | 2023-06-16 10:48:11,196 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn5_1    | 2023-06-16 10:48:11,238 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn5_1    | 2023-06-16 10:48:11,334 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn5_1    | 2023-06-16 10:48:11,369 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn5_1    | 2023-06-16 10:48:11,388 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn5_1    | 2023-06-16 10:48:11,388 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn5_1    | 2023-06-16 10:48:11,857 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn5_1    | 2023-06-16 10:48:11,863 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn5_1    | 2023-06-16 10:48:12,228 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn5_1    | 2023-06-16 10:48:12,229 [main] INFO server.session: No SessionScavenger set, using defaults
dn5_1    | 2023-06-16 10:48:12,243 [main] INFO server.session: node0 Scavenging every 660000ms
dn5_1    | 2023-06-16 10:48:12,313 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7f97bc14{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn5_1    | 2023-06-16 10:48:12,319 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4f820f42{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn5_1    | 2023-06-16 10:48:15,182 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@250d440{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-1065233593883189332/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn5_1    | 2023-06-16 10:48:15,273 [main] INFO server.AbstractConnector: Started ServerConnector@592ca48c{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn5_1    | 2023-06-16 10:48:15,287 [main] INFO server.Server: Started @42344ms
dn5_1    | 2023-06-16 10:48:15,320 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-06-16 10:48:15,320 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-16 10:48:15,324 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn5_1    | 2023-06-16 10:48:15,344 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn5_1    | 2023-06-16 10:48:15,718 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5bbcf5b5] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn5_1    | 2023-06-16 10:48:16,059 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn5_1    | 2023-06-16 10:48:16,539 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn3_1    | 2023-06-16 10:48:50,100 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:51,102 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:51,103 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:52,103 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:52,104 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:53,105 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:53,107 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:54,108 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:55,109 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:56,109 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:57,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:58,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:48:59,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:00,113 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:00,805 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-16 10:49:01,114 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:02,114 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:03,115 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:03,736 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:49:03,737 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn3_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 2023-06-16 10:48:19,092 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:19,092 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] INFO ipc.Client: Retrying connect to server: recon/10.9.0.22:9891. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:19,093 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:19,110 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:19,720 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-16 10:48:20,092 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:20,093 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:20,111 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:21,093 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:21,095 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:21,112 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:21,726 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn2_1    | 2023-06-16 10:49:05,784 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: new RaftServerImpl for group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-16 10:49:05,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-16 10:49:05,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-16 10:49:05,792 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-16 10:49:05,795 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:05,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-16 10:49:05,797 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-16 10:49:05,798 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-16 10:49:05,799 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-16 10:49:05,803 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-16 10:49:05,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-16 10:49:05,807 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:05,813 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-16 10:49:05,814 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-16 10:49:05,815 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-16 10:49:05,816 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-16 10:49:05,817 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-16 10:49:05,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-16 10:49:05,818 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-16 10:49:05,821 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 does not exist. Creating ...
dn2_1    | 2023-06-16 10:49:05,828 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/in_use.lock acquired by nodename 7@4d99f9962a79
dn2_1    | 2023-06-16 10:49:05,833 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 has been successfully formatted.
dn2_1    | 2023-06-16 10:49:05,848 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-E3FF46599155: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-16 10:49:05,848 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-16 10:49:05,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-16 10:49:05,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:49:05,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-16 10:49:05,854 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-16 10:49:05,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,855 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-16 10:49:05,858 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-16 10:49:05,858 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155
dn2_1    | 2023-06-16 10:49:05,866 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-16 10:49:05,866 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-16 10:49:05,869 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-16 10:49:05,873 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:05,875 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:05,878 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:05,878 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:05,879 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:05,880 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:05,881 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:05,889 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-16 10:49:05,890 [pool-22-thread-1] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState
dn2_1    | 2023-06-16 10:49:05,891 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E3FF46599155,id=7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:49:05,894 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:05,900 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-16 10:49:05,901 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-16 10:49:05,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-16 10:49:05,902 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:05,903 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155
dn2_1    | 2023-06-16 10:49:05,930 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:06,552 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:07,559 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:08,560 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:08,614 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155.
dn2_1    | 2023-06-16 10:49:08,614 [Command processor thread] INFO server.RaftServer: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: addNew group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-42EFE7C8DED3:java.util.concurrent.CompletableFuture@5011d107[Not completed]
dn2_1    | 2023-06-16 10:49:08,624 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: new RaftServerImpl for group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn2_1    | 2023-06-16 10:49:08,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn2_1    | 2023-06-16 10:49:08,624 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn2_1    | 2023-06-16 10:49:08,625 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn2_1    | 2023-06-16 10:49:08,626 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn2_1    | 2023-06-16 10:49:08,627 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn2_1    | 2023-06-16 10:49:08,630 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn2_1    | 2023-06-16 10:49:08,631 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn2_1    | 2023-06-16 10:49:08,631 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn2_1    | 2023-06-16 10:49:08,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn2_1    | 2023-06-16 10:49:08,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn2_1    | 2023-06-16 10:49:08,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn2_1    | 2023-06-16 10:49:08,645 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn2_1    | 2023-06-16 10:49:08,645 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 does not exist. Creating ...
dn2_1    | 2023-06-16 10:49:08,654 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/in_use.lock acquired by nodename 7@4d99f9962a79
dn2_1    | 2023-06-16 10:49:08,661 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 has been successfully formatted.
dn2_1    | 2023-06-16 10:49:08,672 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-42EFE7C8DED3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn2_1    | 2023-06-16 10:49:08,674 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn2_1    | 2023-06-16 10:49:08,675 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn2_1    | 2023-06-16 10:49:08,675 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:49:08,675 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn2_1    | 2023-06-16 10:49:08,676 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn2_1    | 2023-06-16 10:49:08,677 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:08,677 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-16 10:49:08,677 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn2_1    | 2023-06-16 10:49:08,677 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3
dn2_1    | 2023-06-16 10:49:08,678 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn2_1    | 2023-06-16 10:49:08,687 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:08,690 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:08,692 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn2_1    | 2023-06-16 10:49:08,693 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn2_1    | 2023-06-16 10:49:08,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn2_1    | 2023-06-16 10:49:08,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn2_1    | 2023-06-16 10:49:08,694 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn2_1    | 2023-06-16 10:49:08,698 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn2_1    | 2023-06-16 10:49:08,704 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:08,707 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn2_1    | 2023-06-16 10:49:08,707 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:08,707 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:08,707 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn2_1    | 2023-06-16 10:49:08,719 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:08,719 [pool-22-thread-1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn2_1    | 2023-06-16 10:49:08,719 [pool-22-thread-1] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState
dn2_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42EFE7C8DED3,id=7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn2_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn2_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn2_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn2_1    | 2023-06-16 10:49:08,724 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:08,736 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3
dn2_1    | 2023-06-16 10:49:08,762 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:09,006 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3.
dn2_1    | 2023-06-16 10:49:09,561 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:10,279 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: receive requestVote(ELECTION, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, group-E3FF46599155, 1, (t:0, i:0))
dn2_1    | 2023-06-16 10:49:10,281 [grpc-default-executor-0] INFO impl.VoteContext: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FOLLOWER: accept ELECTION from 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: our priority 0 <= candidate's priority 0
dn2_1    | 2023-06-16 10:49:10,281 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn2_1    | 2023-06-16 10:49:10,281 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState
dn2_1    | 2023-06-16 10:49:10,282 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO impl.FollowerState: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState was interrupted
dn2_1    | 2023-06-16 10:49:10,283 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState
dn2_1    | 2023-06-16 10:49:10,285 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-16 10:48:22,096 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:22,096 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:22,119 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:23,097 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:23,097 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:23,120 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:24,097 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:24,098 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:24,121 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:24,136 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 2e2f108eef4e/10.9.0.21 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:59748 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:59748 remote=recon/10.9.0.22:9891]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn2_1    | 2023-06-16 10:49:10,285 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:10,315 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155 replies to ELECTION vote request: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:OK-t1. Peer's state: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155:t1, leader=null, voted=2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, raftlog=Memoized:7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:10,562 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:10,663 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO impl.FollowerState: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5080510353ns, electionTimeout:5069ms
dn2_1    | 2023-06-16 10:49:10,663 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState
dn2_1    | 2023-06-16 10:49:10,663 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn2_1    | 2023-06-16 10:49:10,666 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-06-16 10:49:10,666 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-FollowerState] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1
dn2_1    | 2023-06-16 10:49:10,674 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO impl.LeaderElection: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:10,674 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO impl.LeaderElection: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn2_1    | 2023-06-16 10:49:10,676 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1
dn2_1    | 2023-06-16 10:49:10,678 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn2_1    | 2023-06-16 10:49:10,682 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-75B0B54839B8 with new leaderId: 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:49:10,682 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: change Leader from null to 7e6c224f-5cdf-45b7-b597-f8bf583ee016 at term 1 for becomeLeader, leader elected after 6091ms
dn2_1    | 2023-06-16 10:49:10,694 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-16 10:49:10,698 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:10,698 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-06-16 10:49:10,714 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-16 10:49:10,730 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-06-16 10:49:10,731 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-16 10:49:10,736 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:10,750 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-16 10:49:10,753 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderStateImpl
dn2_1    | 2023-06-16 10:49:10,784 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-16 10:49:10,851 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-LeaderElection1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8: set configuration 0: peers:[7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:11,065 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-75B0B54839B8-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/e4ca3c21-179d-436f-b702-75b0b54839b8/current/log_inprogress_0
dn2_1    | 2023-06-16 10:49:11,562 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:12,563 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:13,564 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-datanode-1.3.0.jar
dn4_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | ************************************************************/
dn4_1    | 2023-06-16 10:47:45,418 [main] INFO ozone.HddsDatanodeService: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 2023-06-16 10:47:45,877 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 2023-06-16 10:47:46,698 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-06-16 10:47:47,809 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
dn4_1    | 2023-06-16 10:47:47,809 [main] INFO impl.MetricsSystemImpl: HddsDatanode metrics system started
dn4_1    | 2023-06-16 10:47:48,861 [main] INFO ozone.HddsDatanodeService: HddsDatanodeService host:ee32ea3a212f ip:10.9.0.20
dn4_1    | 2023-06-16 10:47:50,732 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-06-16 10:47:52,140 [main] INFO reflections.Reflections: Reflections took 1138 ms to scan 2 urls, producing 92 keys and 204 values 
dn4_1    | 2023-06-16 10:47:53,151 [main] INFO statemachine.DatanodeStateMachine: Datanode State Machine Task Thread Pool size 4
dn4_1    | 2023-06-16 10:47:54,201 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/hdds/scmUsed not found
dn4_1    | 2023-06-16 10:47:54,343 [main] INFO volume.HddsVolume: Creating HddsVolume: /data/hdds/hdds of storage type : DISK capacity : 89297309696
dn4_1    | 2023-06-16 10:47:54,350 [main] INFO volume.MutableVolumeSet: Added Volume : /data/hdds/hdds to VolumeSet
dn4_1    | 2023-06-16 10:47:54,370 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/hdds/hdds
dn4_1    | 2023-06-16 10:47:54,473 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/hdds/hdds
dn4_1    | 2023-06-16 10:47:54,693 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-16 10:47:54,694 [main] INFO fs.SaveSpaceUsageToFile: Cached usage info file /data/metadata/ratis/scmUsed not found
dn4_1    | 2023-06-16 10:47:54,735 [main] INFO volume.MutableVolumeSet: Added Volume : /data/metadata/ratis to VolumeSet
dn4_1    | 2023-06-16 10:47:54,736 [main] INFO volume.ThrottledAsyncChecker: Scheduling a check for /data/metadata/ratis
dn4_1    | 2023-06-16 10:47:54,750 [main] INFO volume.StorageVolumeChecker: Scheduled health check for volume /data/metadata/ratis
dn4_1    | 2023-06-16 10:47:54,959 [Thread-5] INFO ozoneimpl.ContainerReader: Finish verifying containers on volume /data/hdds/hdds
dn4_1    | 2023-06-16 10:47:54,961 [main] INFO ozoneimpl.OzoneContainer: Build ContainerSet costs 0s
dn4_1    | 2023-06-16 10:48:04,591 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for DNAudit to [].
dn4_1    | 2023-06-16 10:48:05,442 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-16 10:48:05,911 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-06-16 10:48:06,560 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-16 10:48:06,569 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9857 (custom)
dn4_1    | 2023-06-16 10:48:06,577 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-16 10:48:06,583 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9858 (custom)
dn4_1    | 2023-06-16 10:48:06,583 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn4_1    | 2023-06-16 10:48:06,587 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9856 (custom)
dn4_1    | 2023-06-16 10:48:06,599 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32MB (=33554432) (custom)
dn4_1    | 2023-06-16 10:48:06,603 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-16 10:48:06,625 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 5MB (=5242880) (custom)
dn4_1    | 2023-06-16 10:48:06,631 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn4_1    | 2023-06-16 10:48:06,695 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-16 10:48:06,732 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-16 10:48:06,745 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-16 10:48:09,930 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-16 10:48:09,938 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-16 10:48:09,998 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-16 10:48:10,023 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn4_1    | 2023-06-16 10:48:10,023 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 2023-06-16 10:48:10,065 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn4_1    | 2023-06-16 10:48:10,294 [main] INFO server.XceiverServerGrpc: GrpcServer channel type EpollServerSocketChannel
dn4_1    | 2023-06-16 10:48:11,677 [main] INFO http.BaseHttpServer: Starting Web-server for hddsDatanode at: http://0.0.0.0:9882
dn4_1    | 2023-06-16 10:48:11,721 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
dn4_1    | 2023-06-16 10:48:11,856 [main] INFO util.log: Logging initialized @37514ms to org.eclipse.jetty.util.log.Slf4jLog
dn4_1    | 2023-06-16 10:48:12,689 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
dn4_1    | 2023-06-16 10:48:12,728 [main] INFO http.HttpRequestLog: Http request log for http.requests.hddsDatanode is not defined
dn4_1    | 2023-06-16 10:48:12,773 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
dn4_1    | 2023-06-16 10:48:12,924 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hddsDatanode
dn4_1    | 2023-06-16 10:48:12,952 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn3_1    | Caused by: java.util.concurrent.TimeoutException
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn3_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn3_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn3_1    | 	... 1 more
dn3_1    | 2023-06-16 10:49:04,117 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:05,041 [Command processor thread] INFO server.RaftServer: a27fbf0d-ae65-4185-b735-d902c8c8b71a: addNew group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-E3FF46599155:java.util.concurrent.CompletableFuture@2b31a701[Not completed]
dn3_1    | 2023-06-16 10:49:05,135 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:05,265 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a: new RaftServerImpl for group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-16 10:49:05,271 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-16 10:49:05,272 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-16 10:49:05,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-16 10:49:05,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:05,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-16 10:49:05,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-16 10:49:05,403 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-16 10:49:05,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-16 10:49:05,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-16 10:49:05,449 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-16 10:49:05,514 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:05,535 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-16 10:49:05,538 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-16 10:49:05,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-16 10:49:05,833 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-16 10:49:05,843 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-16 10:49:05,843 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-16 10:49:05,853 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-16 10:49:05,853 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 does not exist. Creating ...
dn3_1    | 2023-06-16 10:49:05,917 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/in_use.lock acquired by nodename 7@d437f4d74d34
dn3_1    | 2023-06-16 10:49:05,970 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 has been successfully formatted.
dn3_1    | 2023-06-16 10:49:06,054 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-E3FF46599155: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-16 10:49:06,073 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-16 10:49:06,125 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-16 10:49:06,140 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:06,171 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:49:06,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-16 10:49:06,177 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-16 10:49:06,179 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:06,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn2_1    | 2023-06-16 10:49:13,724 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: receive requestVote(ELECTION, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, group-42EFE7C8DED3, 1, (t:0, i:0))
dn2_1    | 2023-06-16 10:49:13,726 [grpc-default-executor-0] INFO impl.VoteContext: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FOLLOWER: reject ELECTION from 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: our priority 1 > candidate's priority 0
dn2_1    | 2023-06-16 10:49:13,726 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn2_1    | 2023-06-16 10:49:13,726 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState
dn2_1    | 2023-06-16 10:49:13,727 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState was interrupted
dn2_1    | 2023-06-16 10:49:13,727 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState
dn2_1    | 2023-06-16 10:49:13,728 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:13,728 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:13,730 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3 replies to ELECTION vote request: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:FAIL-t1. Peer's state: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3:t1, leader=null, voted=null, raftlog=Memoized:7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:14,565 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn2_1    | 2023-06-16 10:49:15,409 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:15,410 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:15,443 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: receive requestVote(ELECTION, a27fbf0d-ae65-4185-b735-d902c8c8b71a, group-E3FF46599155, 2, (t:0, i:0))
dn2_1    | 2023-06-16 10:49:15,443 [grpc-default-executor-0] INFO impl.VoteContext: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FOLLOWER: accept ELECTION from a27fbf0d-ae65-4185-b735-d902c8c8b71a: our priority 0 <= candidate's priority 1
dn2_1    | 2023-06-16 10:49:15,444 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn2_1    | 2023-06-16 10:49:15,444 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState
dn2_1    | 2023-06-16 10:49:15,444 [grpc-default-executor-0] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState
dn2_1    | 2023-06-16 10:49:15,444 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO impl.FollowerState: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState was interrupted
dn2_1    | 2023-06-16 10:49:15,451 [grpc-default-executor-0] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155 replies to ELECTION vote request: a27fbf0d-ae65-4185-b735-d902c8c8b71a<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:OK-t2. Peer's state: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155:t2, leader=null, voted=a27fbf0d-ae65-4185-b735-d902c8c8b71a, raftlog=Memoized:7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:15,452 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:15,452 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:15,724 [7e6c224f-5cdf-45b7-b597-f8bf583ee016-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E3FF46599155 with new leaderId: a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn2_1    | 2023-06-16 10:49:15,725 [7e6c224f-5cdf-45b7-b597-f8bf583ee016-server-thread1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: change Leader from null to a27fbf0d-ae65-4185-b735-d902c8c8b71a at term 2 for appendEntries, leader elected after 9916ms
dn2_1    | 2023-06-16 10:49:15,744 [7e6c224f-5cdf-45b7-b597-f8bf583ee016-server-thread1] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:15,744 [7e6c224f-5cdf-45b7-b597-f8bf583ee016-server-thread1] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-16 10:49:15,757 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-E3FF46599155-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/current/log_inprogress_0
dn2_1    | 2023-06-16 10:49:16,933 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn2_1    | 2023-06-16 10:49:18,748 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5021057588ns, electionTimeout:5020ms
dn2_1    | 2023-06-16 10:49:18,748 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState
dn2_1    | 2023-06-16 10:49:18,748 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn2_1    | 2023-06-16 10:49:18,749 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn2_1    | 2023-06-16 10:49:18,749 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-FollowerState] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2
dn2_1    | 2023-06-16 10:49:18,754 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.LeaderElection: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn2_1    | 2023-06-16 10:49:18,761 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn2_1    | 2023-06-16 10:49:18,762 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn2_1    | 2023-06-16 10:49:18,762 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn2_1    | 2023-06-16 10:49:18,769 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn2_1    | 2023-06-16 10:49:18,826 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.LeaderElection: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn2_1    | 2023-06-16 10:49:18,826 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.LeaderElection:   Response 0: 7e6c224f-5cdf-45b7-b597-f8bf583ee016<-a27fbf0d-ae65-4185-b735-d902c8c8b71a#0:OK-t2
dn2_1    | 2023-06-16 10:49:18,830 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.LeaderElection: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2 ELECTION round 0: result PASSED
dn2_1    | 2023-06-16 10:49:18,830 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: shutdown 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2
dn2_1    | 2023-06-16 10:49:18,830 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn2_1    | 2023-06-16 10:49:18,830 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-42EFE7C8DED3 with new leaderId: 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn2_1    | 2023-06-16 10:49:18,830 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: change Leader from null to 7e6c224f-5cdf-45b7-b597-f8bf583ee016 at term 2 for becomeLeader, leader elected after 10199ms
dn2_1    | 2023-06-16 10:49:18,831 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn2_1    | 2023-06-16 10:49:18,831 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:18,831 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn2_1    | 2023-06-16 10:49:18,832 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn2_1    | 2023-06-16 10:49:18,833 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn2_1    | 2023-06-16 10:49:18,833 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn2_1    | 2023-06-16 10:49:18,833 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn2_1    | 2023-06-16 10:49:18,833 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn2_1    | 2023-06-16 10:49:18,848 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-06-16 10:49:18,848 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:49:18,849 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-06-16 10:49:18,875 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-06-16 10:49:18,878 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-16 10:49:18,878 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-16 10:49:18,879 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-16 10:49:18,882 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-06-16 10:49:18,898 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn2_1    | 2023-06-16 10:49:18,898 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn2_1    | 2023-06-16 10:49:18,900 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn2_1    | 2023-06-16 10:49:18,901 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn2_1    | 2023-06-16 10:49:18,901 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn2_1    | 2023-06-16 10:49:18,901 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn2_1    | 2023-06-16 10:49:18,901 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn2_1    | 2023-06-16 10:49:18,901 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn2_1    | 2023-06-16 10:49:18,902 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO impl.RoleInfo: 7e6c224f-5cdf-45b7-b597-f8bf583ee016: start 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderStateImpl
dn2_1    | 2023-06-16 10:49:18,912 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker: Starting segment from index:0
dn2_1    | 2023-06-16 10:49:18,916 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/current/log_inprogress_0
dn2_1    | 2023-06-16 10:49:18,937 [7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3-LeaderElection2] INFO server.RaftServer$Division: 7e6c224f-5cdf-45b7-b597-f8bf583ee016@group-42EFE7C8DED3: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-16 10:47:43,448 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 466d1b15294c/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--init]
om1_1    | STARTUP_MSG:   version = 1.3.0
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-06-16 10:47:43,483 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-16 10:47:52,605 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-16 10:47:55,930 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-16 10:47:56,523 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-16 10:47:56,526 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-16 10:47:56,615 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:47:57,887 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-16 10:48:01,555 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:03,562 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:05,564 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:07,566 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:09,567 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:11,569 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:13,574 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:15,576 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:17,578 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:19,580 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:21,581 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:23,583 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:25,585 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:48:27,587 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 466d1b15294c/10.9.0.11 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-82184b77-007c-43b9-9d2d-b37b80052e22;layoutVersion=3
om1_1    | 2023-06-16 10:48:29,757 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1    | /************************************************************
om1_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 466d1b15294c/10.9.0.11
om1_1    | ************************************************************/
om1_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om1_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om1_1    | 2023-06-16 10:48:32,272 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om1_1    | /************************************************************
om1_1    | STARTUP_MSG: Starting OzoneManager
om1_1    | STARTUP_MSG:   host = 466d1b15294c/10.9.0.11
om1_1    | STARTUP_MSG:   args = [--]
om1_1    | STARTUP_MSG:   version = 1.3.0
dn3_1    | 2023-06-16 10:49:06,255 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-16 10:49:06,273 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155
dn3_1    | 2023-06-16 10:49:06,296 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-16 10:49:06,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:06,300 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:06,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-16 10:49:06,301 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-16 10:49:06,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-16 10:49:06,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-16 10:49:06,309 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-16 10:49:06,388 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:06,390 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:06,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:06,403 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:06,465 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:06,466 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:06,476 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:06,477 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-16 10:49:06,486 [pool-22-thread-1] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState
dn3_1    | 2023-06-16 10:49:06,536 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:06,537 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:06,551 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E3FF46599155,id=a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:49:06,553 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:06,560 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-16 10:49:06,561 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-16 10:49:06,565 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-16 10:49:06,696 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155
dn3_1    | 2023-06-16 10:49:07,173 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:08,181 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:08,716 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155.
dn3_1    | 2023-06-16 10:49:08,718 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a: new RaftServerImpl for group-5F4EC0623377:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-16 10:49:08,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-16 10:49:08,720 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-16 10:49:08,722 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-16 10:49:08,723 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-16 10:49:08,725 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-16 10:49:08,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-16 10:49:08,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-16 10:49:08,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:08,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-16 10:49:08,726 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-16 10:48:12,973 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-16 10:48:13,446 [main] INFO http.HttpServer2: Jetty bound to port 9882
dn4_1    | 2023-06-16 10:48:13,447 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
dn4_1    | 2023-06-16 10:48:13,684 [main] INFO server.session: DefaultSessionIdManager workerName=node0
dn4_1    | 2023-06-16 10:48:13,684 [main] INFO server.session: No SessionScavenger set, using defaults
dn4_1    | 2023-06-16 10:48:13,704 [main] INFO server.session: node0 Scavenging every 600000ms
dn4_1    | 2023-06-16 10:48:13,848 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@3b9ac754{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
dn4_1    | 2023-06-16 10:48:13,874 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@261de205{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/static,AVAILABLE}
dn4_1    | 2023-06-16 10:48:16,315 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@1d2fb82{hddsDatanode,/,file:///tmp/jetty-0_0_0_0-9882-hdds-container-service-1_3_0_jar-_-any-1911083359406957378/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar!/webapps/hddsDatanode}
dn4_1    | 2023-06-16 10:48:16,366 [main] INFO server.AbstractConnector: Started ServerConnector@7302ff13{HTTP/1.1, (http/1.1)}{0.0.0.0:9882}
dn4_1    | 2023-06-16 10:48:16,366 [main] INFO server.Server: Started @42024ms
dn4_1    | 2023-06-16 10:48:16,382 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn4_1    | 2023-06-16 10:48:16,382 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn4_1    | 2023-06-16 10:48:16,403 [main] INFO http.BaseHttpServer: HTTP server of hddsDatanode listening at http://0.0.0.0:9882
dn4_1    | 2023-06-16 10:48:16,460 [Datanode State Machine Daemon Thread] INFO statemachine.DatanodeStateMachine: Ozone container server started.
dn4_1    | 2023-06-16 10:48:16,499 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5a78040d] INFO util.JvmPauseMonitor: Starting JVM pause monitor
dn4_1    | 2023-06-16 10:48:17,031 [Datanode State Machine Task Thread - 0] INFO statemachine.SCMConnectionManager: Adding Recon Server : recon/10.9.0.22:9891
dn4_1    | 2023-06-16 10:48:17,284 [Datanode State Machine Task Thread - 0] INFO datanode.InitDatanodeState: DatanodeDetails is persisted to /data/datanode.id
dn4_1    | 2023-06-16 10:48:19,745 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:19,746 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:19,756 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:20,538 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-06-16 10:48:20,746 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:20,747 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:20,756 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:21,747 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:21,748 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:21,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:22,541 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
om1_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om1_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om1_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | ************************************************************/
om1_1    | 2023-06-16 10:48:32,278 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-16 10:48:35,933 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om1_1    | 2023-06-16 10:48:37,692 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om1_1    | 2023-06-16 10:48:37,985 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om1, RPC Address: om1:9862 and Ratis port: 9872
om1_1    | 2023-06-16 10:48:37,986 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om1: om1
om1_1    | 2023-06-16 10:48:38,006 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:48:38,064 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om1_1    | 2023-06-16 10:48:39,313 [main] INFO reflections.Reflections: Reflections took 1060 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om1_1    | 2023-06-16 10:48:39,338 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:48:40,139 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-16 10:48:40,253 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om1_1    | 2023-06-16 10:48:42,810 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:48:43,515 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-16 10:48:43,517 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-16 10:48:44,360 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om1_1    | 2023-06-16 10:48:44,534 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om1_1    | 2023-06-16 10:48:44,732 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-16 10:48:44,738 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om1_1    | 2023-06-16 10:48:44,779 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om1_1    | 2023-06-16 10:48:45,562 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-16 10:48:45,753 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om1_1    | 2023-06-16 10:48:45,923 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om1:9872, om3:9872, om2:9872
om1_1    | 2023-06-16 10:48:45,962 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om1_1    | 2023-06-16 10:48:46,094 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-06-16 10:48:46,366 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-16 10:48:46,385 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-16 10:48:46,385 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-16 10:48:46,386 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-16 10:48:46,386 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om1_1    | 2023-06-16 10:48:46,386 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om1_1    | 2023-06-16 10:48:46,387 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om1_1    | 2023-06-16 10:48:46,399 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-16 10:48:46,401 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-06-16 10:48:46,403 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-16 10:48:46,451 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-16 10:48:46,490 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om1_1    | 2023-06-16 10:48:46,499 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om1_1    | 2023-06-16 10:48:47,382 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om1_1    | 2023-06-16 10:48:47,393 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om1_1    | 2023-06-16 10:48:47,407 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-06-16 10:48:47,410 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-16 10:48:47,410 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-16 10:48:47,417 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-16 10:48:47,458 [main] INFO server.RaftServer: om1: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@65c17e38[Not completed]
om1_1    | 2023-06-16 10:48:47,460 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om1_1    | 2023-06-16 10:48:47,571 [main] INFO om.OzoneManager: Creating RPC Server
om1_1    | 2023-06-16 10:48:47,588 [pool-26-thread-1] INFO server.RaftServer$Division: om1: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om1_1    | 2023-06-16 10:48:47,708 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-06-16 10:48:47,711 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-16 10:48:47,711 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-06-16 10:48:47,711 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-16 10:48:47,713 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-16 10:48:47,714 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-06-16 10:48:47,734 [pool-26-thread-1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-06-16 10:48:47,771 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om1_1    | 2023-06-16 10:48:47,808 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-06-16 10:48:47,855 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-06-16 10:48:47,972 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om1_1    | 2023-06-16 10:48:48,014 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om1_1    | 2023-06-16 10:48:48,057 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-16 10:47:43,751 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = a9029c5fc9fa/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--init]
om3_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-06-16 10:47:43,816 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-06-16 10:47:52,790 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-06-16 10:47:56,579 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-16 10:47:57,145 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-16 10:47:57,162 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-16 10:47:57,169 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-16 10:47:58,795 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-16 10:48:02,449 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:04,451 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:06,453 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:08,454 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:10,457 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:12,459 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:14,464 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:16,466 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:18,480 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:20,481 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:22,483 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:25,195 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:80f154b6-d6d9-4d00-b482-76c3516d18ec is not the leader. Could not determine the leader node.
om3_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om3_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om3_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
om3_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om3_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om3_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om3_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om3_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om3_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om3_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om3_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om3_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:27,196 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | 2023-06-16 10:48:29,198 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From a9029c5fc9fa/10.9.0.13 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om3_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-82184b77-007c-43b9-9d2d-b37b80052e22;layoutVersion=3
om3_1    | 2023-06-16 10:48:31,239 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om3_1    | /************************************************************
om3_1    | SHUTDOWN_MSG: Shutting down OzoneManager at a9029c5fc9fa/10.9.0.13
om3_1    | ************************************************************/
om3_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om3_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om3_1    | 2023-06-16 10:48:35,510 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
om3_1    | /************************************************************
om3_1    | STARTUP_MSG: Starting OzoneManager
om3_1    | STARTUP_MSG:   host = a9029c5fc9fa/10.9.0.13
om3_1    | STARTUP_MSG:   args = [--]
om3_1    | STARTUP_MSG:   version = 1.3.0
om3_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om3_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om3_1    | STARTUP_MSG:   java = 11.0.14.1
om3_1    | ************************************************************/
om3_1    | 2023-06-16 10:48:35,542 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om3_1    | 2023-06-16 10:48:39,535 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om3_1    | 2023-06-16 10:48:40,831 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om3_1    | 2023-06-16 10:48:41,115 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om3, RPC Address: om3:9862 and Ratis port: 9872
om3_1    | 2023-06-16 10:48:41,115 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om3: om3
om3_1    | 2023-06-16 10:48:41,121 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-06-16 10:48:22,542 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-06-16 10:48:22,748 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:22,748 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:22,757 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:23,749 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:23,749 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:23,758 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:23,776 [EndpointStateMachine task thread for recon/10.9.0.22:9891 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to Recon server at recon:9891 for past 0 seconds.
dn4_1    | java.net.SocketTimeoutException: Call From ee32ea3a212f/10.9.0.20 to recon:9891 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:39678 remote=recon/10.9.0.22:9891]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy39.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.20:39678 remote=recon/10.9.0.22:9891]
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn3_1    | 2023-06-16 10:49:08,727 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-16 10:49:08,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-16 10:49:08,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-16 10:49:08,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-16 10:49:08,728 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-16 10:49:08,730 [Command processor thread] INFO server.RaftServer: a27fbf0d-ae65-4185-b735-d902c8c8b71a: addNew group-5F4EC0623377:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER] returns      null a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377:t0, leader=null, voted=null, raftlog=UNINITIALIZED, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null NEW
dn3_1    | 2023-06-16 10:49:08,733 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/78655ac1-fdd1-422f-9722-5f4ec0623377 does not exist. Creating ...
dn3_1    | 2023-06-16 10:49:08,735 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/78655ac1-fdd1-422f-9722-5f4ec0623377/in_use.lock acquired by nodename 7@d437f4d74d34
dn3_1    | 2023-06-16 10:49:08,739 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/78655ac1-fdd1-422f-9722-5f4ec0623377 has been successfully formatted.
dn3_1    | 2023-06-16 10:49:08,741 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-5F4EC0623377: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-16 10:49:08,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-16 10:49:08,741 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-16 10:49:08,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:49:08,742 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-16 10:49:08,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-16 10:49:08,755 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,759 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/78655ac1-fdd1-422f-9722-5f4ec0623377
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,760 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-16 10:49:08,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-16 10:49:08,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-16 10:49:08,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-16 10:49:08,761 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-16 10:49:08,762 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:08,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:08,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:08,767 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:08,767 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:08,804 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:08,805 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-16 10:49:08,805 [pool-22-thread-1] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState
dn3_1    | 2023-06-16 10:49:08,807 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-5F4EC0623377,id=a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:49:08,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:08,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-16 10:49:08,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-16 10:49:08,808 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-16 10:49:08,808 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:08,818 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=78655ac1-fdd1-422f-9722-5f4ec0623377
dn3_1    | 2023-06-16 10:49:08,819 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=78655ac1-fdd1-422f-9722-5f4ec0623377.
dn3_1    | 2023-06-16 10:49:08,819 [Command processor thread] INFO server.RaftServer: a27fbf0d-ae65-4185-b735-d902c8c8b71a: addNew group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-42EFE7C8DED3:java.util.concurrent.CompletableFuture@6ac56f9e[Not completed]
dn3_1    | 2023-06-16 10:49:08,821 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a: new RaftServerImpl for group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn3_1    | 2023-06-16 10:49:08,828 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn3_1    | 2023-06-16 10:49:08,830 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn3_1    | 2023-06-16 10:49:08,830 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn3_1    | 2023-06-16 10:49:08,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn3_1    | 2023-06-16 10:49:08,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn3_1    | 2023-06-16 10:49:08,831 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn3_1    | 2023-06-16 10:49:08,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn3_1    | 2023-06-16 10:49:08,839 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn3_1    | 2023-06-16 10:49:08,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-16 10:49:08,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn3_1    | 2023-06-16 10:49:08,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn3_1    | 2023-06-16 10:49:08,840 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn3_1    | 2023-06-16 10:49:08,841 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn3_1    | 2023-06-16 10:49:08,841 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 does not exist. Creating ...
dn3_1    | 2023-06-16 10:49:08,831 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:08,847 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/in_use.lock acquired by nodename 7@d437f4d74d34
dn3_1    | 2023-06-16 10:49:08,849 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 has been successfully formatted.
dn3_1    | 2023-06-16 10:49:08,850 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-42EFE7C8DED3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn3_1    | 2023-06-16 10:49:08,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
dn3_1    | 2023-06-16 10:49:08,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn3_1    | 2023-06-16 10:49:08,850 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:49:08,860 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn3_1    | 2023-06-16 10:49:08,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn3_1    | 2023-06-16 10:49:08,862 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,865 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn3_1    | 2023-06-16 10:49:08,883 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn3_1    | 2023-06-16 10:49:08,884 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn3_1    | 2023-06-16 10:49:08,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn3_1    | 2023-06-16 10:49:08,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn3_1    | 2023-06-16 10:49:08,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn3_1    | 2023-06-16 10:49:08,885 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn3_1    | 2023-06-16 10:49:08,888 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:08,891 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn3_1    | 2023-06-16 10:49:08,896 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:08,900 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:08,900 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn3_1    | 2023-06-16 10:49:08,901 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:08,907 [pool-22-thread-1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-16 10:49:08,908 [pool-22-thread-1] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState
dn3_1    | 2023-06-16 10:49:08,922 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42EFE7C8DED3,id=a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:49:08,923 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn3_1    | 2023-06-16 10:49:08,923 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:08,930 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:08,923 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
dn3_1    | 2023-06-16 10:49:08,931 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
dn3_1    | 2023-06-16 10:49:08,931 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn3_1    | 2023-06-16 10:49:08,932 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3
dn3_1    | 2023-06-16 10:49:09,090 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3.
om1_1    | 2023-06-16 10:48:48,827 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-06-16 10:48:48,827 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om1_1    | 2023-06-16 10:48:48,829 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om1_1    | 2023-06-16 10:48:48,843 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om1_1    | 2023-06-16 10:48:48,849 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-06-16 10:48:50,301 [main] INFO reflections.Reflections: Reflections took 2337 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om1_1    | 2023-06-16 10:48:50,894 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-06-16 10:48:50,962 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-06-16 10:48:51,872 [Listener at om1/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-06-16 10:48:51,910 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-06-16 10:48:51,910 [Listener at om1/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om1_1    | 2023-06-16 10:48:52,031 [Listener at om1/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om1/10.9.0.11:9862
om1_1    | 2023-06-16 10:48:52,031 [Listener at om1/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om1 at port 9872
om1_1    | 2023-06-16 10:48:52,047 [om1-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om1_1    | 2023-06-16 10:48:52,065 [om1-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 7@466d1b15294c
om1_1    | 2023-06-16 10:48:52,164 [om1-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om1_1    | 2023-06-16 10:48:52,172 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-16 10:48:52,210 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-06-16 10:48:52,213 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-16 10:48:52,214 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-06-16 10:48:52,222 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-16 10:48:52,225 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-16 10:48:52,247 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-06-16 10:48:52,247 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-06-16 10:48:52,298 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om1@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om1_1    | 2023-06-16 10:48:52,318 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om1_1    | 2023-06-16 10:48:52,319 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om1_1    | 2023-06-16 10:48:52,321 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om1_1    | 2023-06-16 10:48:52,323 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om1_1    | 2023-06-16 10:48:52,324 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-16 10:48:52,329 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-06-16 10:48:52,330 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-16 10:48:52,330 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om1_1    | 2023-06-16 10:48:52,355 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om1_1    | 2023-06-16 10:48:52,371 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-06-16 10:48:52,372 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-16 10:48:52,379 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om1_1    | 2023-06-16 10:48:52,388 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-16 10:48:52,400 [om1-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-16 10:48:52,417 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:48:52,417 [om1-impl-thread1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
om1_1    | 2023-06-16 10:48:52,419 [om1-impl-thread1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-16 10:48:52,440 [om1-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om1
om1_1    | 2023-06-16 10:48:52,454 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-16 10:48:52,464 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-16 10:48:52,465 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-06-16 10:48:52,466 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om1_1    | 2023-06-16 10:48:52,470 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om1_1    | 2023-06-16 10:48:52,471 [om1-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om1_1    | 2023-06-16 10:48:52,491 [Listener at om1/9862] INFO server.RaftServer: om1: start RPC server
om1_1    | 2023-06-16 10:48:52,694 [Listener at om1/9862] INFO server.GrpcService: om1: GrpcService started, listening on 9872
om3_1    | 2023-06-16 10:48:41,191 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
om3_1    | 2023-06-16 10:48:42,377 [main] INFO reflections.Reflections: Reflections took 1038 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
om3_1    | 2023-06-16 10:48:42,410 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-16 10:48:43,283 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-16 10:48:43,454 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om3_1    | 2023-06-16 10:48:45,821 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om3_1    | 2023-06-16 10:48:46,239 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-06-16 10:48:46,244 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om3_1    | 2023-06-16 10:48:47,162 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om3_1    | 2023-06-16 10:48:47,335 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
om3_1    | 2023-06-16 10:48:47,423 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-16 10:48:47,427 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
om3_1    | 2023-06-16 10:48:47,466 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
om3_1    | 2023-06-16 10:48:48,206 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om3_1    | 2023-06-16 10:48:48,246 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
om3_1    | 2023-06-16 10:48:48,410 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om3:9872, om1:9872, om2:9872
om3_1    | 2023-06-16 10:48:48,457 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
om3_1    | 2023-06-16 10:48:48,590 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om3_1    | 2023-06-16 10:48:48,801 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-16 10:48:48,818 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-16 10:48:48,819 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om3_1    | 2023-06-16 10:48:48,820 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
om3_1    | 2023-06-16 10:48:48,820 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om3_1    | 2023-06-16 10:48:48,820 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
om3_1    | 2023-06-16 10:48:48,823 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
om3_1    | 2023-06-16 10:48:48,826 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-16 10:48:48,828 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om3_1    | 2023-06-16 10:48:48,830 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om3_1    | 2023-06-16 10:48:48,859 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om3_1    | 2023-06-16 10:48:48,885 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
om3_1    | 2023-06-16 10:48:48,885 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om3_1    | 2023-06-16 10:48:49,538 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om3_1    | 2023-06-16 10:48:49,545 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
om3_1    | 2023-06-16 10:48:49,547 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om3_1    | 2023-06-16 10:48:49,548 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om3_1    | 2023-06-16 10:48:49,550 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-16 10:48:49,557 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-16 10:48:49,578 [main] INFO server.RaftServer: om3: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@65c17e38[Not completed]
om3_1    | 2023-06-16 10:48:49,578 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om3_1    | 2023-06-16 10:48:49,668 [main] INFO om.OzoneManager: Creating RPC Server
om3_1    | 2023-06-16 10:48:49,669 [pool-26-thread-1] INFO server.RaftServer$Division: om3: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
om3_1    | 2023-06-16 10:48:49,791 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om3_1    | 2023-06-16 10:48:49,808 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om3_1    | 2023-06-16 10:48:49,808 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om3_1    | 2023-06-16 10:48:49,808 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om1_1    | 2023-06-16 10:48:52,704 [Listener at om1/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om1_1    | 2023-06-16 10:48:52,726 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om1: Started
om1_1    | 2023-06-16 10:48:52,884 [Listener at om1/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om1_1    | 2023-06-16 10:48:52,885 [Listener at om1/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om1_1    | 2023-06-16 10:48:52,995 [Listener at om1/9862] INFO util.log: Logging initialized @22486ms to org.eclipse.jetty.util.log.Slf4jLog
om1_1    | 2023-06-16 10:48:53,492 [Listener at om1/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om1_1    | 2023-06-16 10:48:53,536 [Listener at om1/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om1_1    | 2023-06-16 10:48:53,585 [Listener at om1/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-06-16 10:48:53,587 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-06-16 10:48:53,600 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-06-16 10:48:53,607 [Listener at om1/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-16 10:48:53,802 [Listener at om1/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-06-16 10:48:53,805 [Listener at om1/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om1_1    | 2023-06-16 10:48:53,996 [Listener at om1/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-06-16 10:48:53,996 [Listener at om1/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-06-16 10:48:53,997 [Listener at om1/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1    | 2023-06-16 10:48:54,073 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@64dc86c6{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-06-16 10:48:54,075 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@166b11e{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-06-16 10:48:55,133 [Listener at om1/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@223967ea{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-16364116393993397680/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om1_1    | 2023-06-16 10:48:55,184 [Listener at om1/9862] INFO server.AbstractConnector: Started ServerConnector@1de5cc88{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-06-16 10:48:55,185 [Listener at om1/9862] INFO server.Server: Started @24676ms
om1_1    | 2023-06-16 10:48:55,187 [Listener at om1/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-06-16 10:48:55,187 [Listener at om1/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-16 10:48:55,198 [Listener at om1/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-06-16 10:48:55,210 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om1_1    | 2023-06-16 10:48:55,199 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-16 10:48:55,369 [Listener at om1/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om1_1    | 2023-06-16 10:48:55,463 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1b8fa2fa] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-06-16 10:48:57,494 [om1@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om1@group-D66704EFC61C-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5075672589ns, electionTimeout:5028ms
om1_1    | 2023-06-16 10:48:57,496 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-FollowerState
om1_1    | 2023-06-16 10:48:57,496 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1    | 2023-06-16 10:48:57,499 [om1@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1    | 2023-06-16 10:48:57,499 [om1@group-D66704EFC61C-FollowerState] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-06-16 10:48:57,519 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:48:57,607 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-16 10:48:57,612 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om1_1    | 2023-06-16 10:48:57,615 [om1@group-D66704EFC61C-LeaderElection1-2] INFO server.GrpcServerProtocolClient: Build channel for om2
om1_1    | 2023-06-16 10:48:57,659 [om1@group-D66704EFC61C-LeaderElection1-1] INFO server.GrpcServerProtocolClient: Build channel for om3
om1_1    | 2023-06-16 10:49:00,187 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1: ELECTION PASSED received 1 response(s) and 0 exception(s):
om1_1    | 2023-06-16 10:49:00,195 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection:   Response 0: om1<-om3#0:OK-t1
om1_1    | 2023-06-16 10:49:00,195 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.LeaderElection: om1@group-D66704EFC61C-LeaderElection1 ELECTION round 0: result PASSED
om1_1    | 2023-06-16 10:49:00,201 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: shutdown om1@group-D66704EFC61C-LeaderElection1
om1_1    | 2023-06-16 10:49:00,201 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om1_1    | 2023-06-16 10:49:00,201 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: change Leader from null to om1 at term 1 for becomeLeader, leader elected after 12229ms
om1_1    | 2023-06-16 10:49:00,222 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om1_1    | 2023-06-16 10:49:00,242 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-06-16 10:49:00,246 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
dn5_1    | 2023-06-16 10:48:25,099 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:25,121 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:26,101 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:26,122 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:27,101 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:27,123 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:28,102 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:28,124 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:29,103 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:29,105 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm1:9861 for past 0 seconds.
dn5_1    | java.net.SocketTimeoutException: Call From 2e2f108eef4e/10.9.0.21 to scm1:9861 failed on socket timeout exception: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51836 remote=scm1/10.9.0.14:9861]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:848)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.SocketTimeoutException: 5000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.9.0.21:51836 remote=scm1/10.9.0.14:9861]
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn5_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn5_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn5_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn5_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn5_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn5_1    | 2023-06-16 10:48:29,124 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:29,999 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-5a4a1cfb-ab10-44fc-b119-480860c4071d/container.db to cache
dn5_1    | 2023-06-16 10:48:29,999 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-5a4a1cfb-ab10-44fc-b119-480860c4071d/container.db for volume DS-5a4a1cfb-ab10-44fc-b119-480860c4071d
dn5_1    | 2023-06-16 10:48:30,004 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn5_1    | 2023-06-16 10:48:30,014 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn5_1    | 2023-06-16 10:48:30,104 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:30,126 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:30,274 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 74bc4d26-95cc-4798-833d-04dc94ff925a
dn5_1    | 2023-06-16 10:48:30,435 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 74bc4d26-95cc-4798-833d-04dc94ff925a: start RPC server
dn5_1    | 2023-06-16 10:48:30,443 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 74bc4d26-95cc-4798-833d-04dc94ff925a: GrpcService started, listening on 9858
dn5_1    | 2023-06-16 10:48:30,444 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 74bc4d26-95cc-4798-833d-04dc94ff925a: GrpcService started, listening on 9856
dn5_1    | 2023-06-16 10:48:30,445 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 74bc4d26-95cc-4798-833d-04dc94ff925a: GrpcService started, listening on 9857
dn5_1    | 2023-06-16 10:48:30,468 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74bc4d26-95cc-4798-833d-04dc94ff925a is started using port 9858 for RATIS
dn5_1    | 2023-06-16 10:48:30,469 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74bc4d26-95cc-4798-833d-04dc94ff925a is started using port 9857 for RATIS_ADMIN
dn5_1    | 2023-06-16 10:48:30,469 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 74bc4d26-95cc-4798-833d-04dc94ff925a is started using port 9856 for RATIS_SERVER
dn5_1    | 2023-06-16 10:48:30,472 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-74bc4d26-95cc-4798-833d-04dc94ff925a: Started
dn5_1    | 2023-06-16 10:48:31,105 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:31,127 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:32,106 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:32,128 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:33,106 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:33,107 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 2e2f108eef4e/10.9.0.21 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
recon_1  | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 2023-06-16 10:47:39,199 [main] INFO recon.ReconServer: STARTUP_MSG: 
recon_1  | /************************************************************
recon_1  | STARTUP_MSG: Starting ReconServer
recon_1  | STARTUP_MSG:   host = 668b8d626d36/10.9.0.22
recon_1  | STARTUP_MSG:   args = []
recon_1  | STARTUP_MSG:   version = 1.3.0
recon_1  | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/sqlite-jdbc-3.25.2.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/guice-4.0.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jooq-3.11.10.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/jersey-entity-filtering-2.34.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/spring-beans-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/derby-10.14.2.0.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/spring-core-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/jooq-codegen-3.11.10.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/spring-tx-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/guice-servlet-4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/guice-bridge-2.5.0.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/aopalliance-1.0.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-tools-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-media-json-jackson-2.34.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/guice-multibindings-4.0.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/bonecp-0.8.0.RELEASE.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/guice-assistedinject-4.0.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jooq-meta-3.11.10.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/ozone-reconcodegen-1.3.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-2.34.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/spring-jdbc-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/spring-jcl-5.2.20.RELEASE.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar
recon_1  | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
recon_1  | STARTUP_MSG:   java = 11.0.14.1
recon_1  | ************************************************************/
recon_1  | 2023-06-16 10:47:39,241 [main] INFO recon.ReconServer: registered UNIX signal handlers for [TERM, HUP, INT]
recon_1  | WARNING: An illegal reflective access operation has occurred
recon_1  | WARNING: Illegal reflective access by com.google.inject.internal.cglib.core.$ReflectUtils$2 (file:/opt/hadoop/share/ozone/lib/guice-4.0.jar) to method java.lang.ClassLoader.defineClass(java.lang.String,byte[],int,int,java.security.ProtectionDomain)
recon_1  | WARNING: Please consider reporting this to the maintainers of com.google.inject.internal.cglib.core.$ReflectUtils$2
recon_1  | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
recon_1  | WARNING: All illegal access operations will be denied in a future release
recon_1  | 2023-06-16 10:47:44,033 [main] INFO reflections.Reflections: Reflections took 635 ms to scan 1 urls, producing 16 keys and 49 values 
recon_1  | 2023-06-16 10:47:49,247 [main] INFO recon.ReconServer: Initializing Recon server...
recon_1  | 2023-06-16 10:47:51,436 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-16 10:48:00,957 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-06-16 10:48:02,958 [main] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
recon_1  | 2023-06-16 10:48:02,959 [main] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
recon_1  | 2023-06-16 10:48:03,004 [main] INFO persistence.DefaultDataSourceProvider: JDBC Url for Recon : jdbc:derby:/data/metadata/recon/ozone_recon_derby.db 
recon_1  | 2023-06-16 10:48:03,263 [main] INFO codegen.SqlDbUtils: Created derby database at jdbc:derby:/data/metadata/recon/ozone_recon_derby.db.
recon_1  | 2023-06-16 10:48:03,264 [main] INFO recon.ReconServer: Creating Recon Schema.
recon_1  | 2023-06-16 10:48:06,741 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-06-16 10:48:10,556 [main] INFO http.BaseHttpServer: Starting Web-server for recon at: http://0.0.0.0:9888
recon_1  | 2023-06-16 10:48:10,705 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
recon_1  | 2023-06-16 10:48:10,805 [main] INFO util.log: Logging initialized @40527ms to org.eclipse.jetty.util.log.Slf4jLog
recon_1  | 2023-06-16 10:48:11,553 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
recon_1  | 2023-06-16 10:48:11,589 [main] WARN http.HttpRequestLog: Jetty request log can only be enabled using Log4j
recon_1  | 2023-06-16 10:48:11,644 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
recon_1  | 2023-06-16 10:48:11,668 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context recon
recon_1  | 2023-06-16 10:48:11,674 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
recon_1  | 2023-06-16 10:48:11,675 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
recon_1  | 2023-06-16 10:48:12,878 [main] INFO tasks.ReconTaskControllerImpl: Registered task ContainerKeyMapperTask with controller.
recon_1  | 2023-06-16 10:48:14,172 [main] INFO tasks.ReconTaskControllerImpl: Registered task FileSizeCountTask with controller.
recon_1  | 2023-06-16 10:48:14,272 [main] INFO tasks.ReconTaskControllerImpl: Registered task TableCountTask with controller.
recon_1  | 2023-06-16 10:48:14,343 [main] INFO tasks.ReconTaskControllerImpl: Registered task NSSummaryTask with controller.
recon_1  | 2023-06-16 10:48:14,453 [main] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
recon_1  | 2023-06-16 10:48:14,454 [main] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
recon_1  | 2023-06-16 10:48:16,793 [main] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:48:17,440 [main] WARN recon.ReconUtils: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:48:17,722 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1  | 2023-06-16 10:48:17,724 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-06-16 10:48:18,000 [main] WARN db.DBStoreBuilder: ozone.recon.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:48:18,266 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
recon_1  | 2023-06-16 10:48:18,374 [main] INFO reflections.Reflections: Reflections took 101 ms to scan 3 urls, producing 112 keys and 252 values 
recon_1  | 2023-06-16 10:48:18,466 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
recon_1  | 2023-06-16 10:48:18,485 [main] INFO node.SCMNodeManager: Entering startup safe mode.
recon_1  | 2023-06-16 10:48:18,493 [main] INFO scm.ReconNodeManager: Loaded 0 nodes from node DB.
recon_1  | 2023-06-16 10:48:18,498 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
recon_1  | 2023-06-16 10:48:18,558 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-16 10:48:18,597 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-06-16 10:48:18,677 [Socket Reader #1 for port 9891] INFO ipc.Server: Starting Socket Reader #1 for port 9891
recon_1  | 2023-06-16 10:48:18,711 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
recon_1  | 2023-06-16 10:48:18,880 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Recon server initialized successfully!
recon_1  | 2023-06-16 10:48:18,883 [Listener at 0.0.0.0/9891] INFO recon.ReconServer: Starting Recon server
recon_1  | 2023-06-16 10:48:18,997 [Listener at 0.0.0.0/9891] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
recon_1  | 2023-06-16 10:48:19,016 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
recon_1  | 2023-06-16 10:48:19,017 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Recon metrics system started
recon_1  | 2023-06-16 10:48:19,460 [Listener at 0.0.0.0/9891] INFO http.HttpServer2: Jetty bound to port 9888
dn3_1    | 2023-06-16 10:49:09,182 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:10,183 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:10,295 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: receive requestVote(ELECTION, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, group-E3FF46599155, 1, (t:0, i:0))
dn3_1    | 2023-06-16 10:49:10,299 [grpc-default-executor-0] INFO impl.VoteContext: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FOLLOWER: reject ELECTION from 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: our priority 1 > candidate's priority 0
dn3_1    | 2023-06-16 10:49:10,301 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn3_1    | 2023-06-16 10:49:10,303 [grpc-default-executor-0] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState
dn3_1    | 2023-06-16 10:49:10,303 [grpc-default-executor-0] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState
dn3_1    | 2023-06-16 10:49:10,304 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:10,304 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:10,304 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO impl.FollowerState: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState was interrupted
dn3_1    | 2023-06-16 10:49:10,320 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155 replies to ELECTION vote request: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-a27fbf0d-ae65-4185-b735-d902c8c8b71a#0:FAIL-t1. Peer's state: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155:t1, leader=null, voted=null, raftlog=Memoized:a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:11,183 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:12,184 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:13,185 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:13,723 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: receive requestVote(ELECTION, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, group-42EFE7C8DED3, 1, (t:0, i:0))
dn3_1    | 2023-06-16 10:49:13,723 [grpc-default-executor-0] INFO impl.VoteContext: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FOLLOWER: accept ELECTION from 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: our priority 0 <= candidate's priority 0
dn3_1    | 2023-06-16 10:49:13,724 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn3_1    | 2023-06-16 10:49:13,725 [grpc-default-executor-0] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState
dn3_1    | 2023-06-16 10:49:13,725 [grpc-default-executor-0] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState
dn3_1    | 2023-06-16 10:49:13,725 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState was interrupted
dn3_1    | 2023-06-16 10:49:13,740 [grpc-default-executor-0] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3 replies to ELECTION vote request: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-a27fbf0d-ae65-4185-b735-d902c8c8b71a#0:OK-t1. Peer's state: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3:t1, leader=null, voted=2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, raftlog=Memoized:a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:13,753 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:13,754 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:13,907 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO impl.FollowerState: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5102095714ns, electionTimeout:5061ms
dn3_1    | 2023-06-16 10:49:13,908 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState
dn3_1    | 2023-06-16 10:49:13,909 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
dn3_1    | 2023-06-16 10:49:13,911 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-16 10:49:13,911 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-FollowerState] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1
dn3_1    | 2023-06-16 10:49:13,915 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO impl.LeaderElection: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:13,916 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO impl.LeaderElection: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn3_1    | 2023-06-16 10:49:13,916 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1
dn3_1    | 2023-06-16 10:49:13,917 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn3_1    | 2023-06-16 10:49:13,917 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-5F4EC0623377 with new leaderId: a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:49:13,917 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: change Leader from null to a27fbf0d-ae65-4185-b735-d902c8c8b71a at term 1 for becomeLeader, leader elected after 5191ms
dn3_1    | 2023-06-16 10:49:13,926 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-16 10:49:13,933 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:13,937 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-16 10:49:13,942 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-16 10:49:13,942 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-16 10:49:13,943 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-16 10:49:13,953 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:13,955 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-16 10:49:13,956 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderStateImpl
dn3_1    | 2023-06-16 10:49:13,988 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-16 10:49:14,066 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-LeaderElection1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:14,192 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn3_1    | 2023-06-16 10:49:14,294 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-5F4EC0623377-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/78655ac1-fdd1-422f-9722-5f4ec0623377/current/log_inprogress_0
dn3_1    | 2023-06-16 10:49:15,395 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO impl.FollowerState: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5091588852ns, electionTimeout:5090ms
dn3_1    | 2023-06-16 10:49:15,395 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState
dn3_1    | 2023-06-16 10:49:15,395 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
dn3_1    | 2023-06-16 10:49:15,396 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn3_1    | 2023-06-16 10:49:15,396 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-FollowerState] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2
dn3_1    | 2023-06-16 10:49:15,403 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2 ELECTION round 0: submit vote requests at term 2 for -1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:15,407 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn3_1    | 2023-06-16 10:49:15,411 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:15,418 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:15,422 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn3_1    | 2023-06-16 10:49:15,469 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2: ELECTION PASSED received 1 response(s) and 0 exception(s):
dn3_1    | 2023-06-16 10:49:15,469 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection:   Response 0: a27fbf0d-ae65-4185-b735-d902c8c8b71a<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:OK-t2
dn3_1    | 2023-06-16 10:49:15,469 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2 ELECTION round 0: result PASSED
dn3_1    | 2023-06-16 10:49:15,469 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2
dn3_1    | 2023-06-16 10:49:15,470 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
dn3_1    | 2023-06-16 10:49:15,470 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E3FF46599155 with new leaderId: a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn3_1    | 2023-06-16 10:49:15,470 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: change Leader from null to a27fbf0d-ae65-4185-b735-d902c8c8b71a at term 2 for becomeLeader, leader elected after 9977ms
dn3_1    | 2023-06-16 10:49:15,470 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn3_1    | 2023-06-16 10:49:15,472 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:15,473 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn3_1    | 2023-06-16 10:49:15,473 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn3_1    | 2023-06-16 10:49:15,473 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
dn3_1    | 2023-06-16 10:49:15,473 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
dn3_1    | 2023-06-16 10:49:15,474 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn3_1    | 2023-06-16 10:49:15,474 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
dn3_1    | 2023-06-16 10:49:15,514 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn3_1    | 2023-06-16 10:49:15,523 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:49:15,526 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-06-16 10:49:15,540 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-06-16 10:49:15,540 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-16 10:49:15,541 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-16 10:49:15,541 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-16 10:49:15,541 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-06-16 10:49:15,551 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
dn4_1    | 	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:133)
dn4_1    | 	at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
dn4_1    | 	at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at java.base/java.io.FilterInputStream.read(FilterInputStream.java:83)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:563)
dn4_1    | 	at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
dn4_1    | 	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134)
dn4_1    | 2023-06-16 10:48:24,750 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:24,751 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm1/10.9.0.14:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:24,759 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:25,751 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:25,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:26,752 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:26,760 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:27,753 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:27,761 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:28,754 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:28,762 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:29,755 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:29,763 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:29,921 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO utils.DatanodeStoreCache: Added db /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-b87e5871-4d73-4681-a147-5a63525e439d/container.db to cache
dn4_1    | 2023-06-16 10:48:29,928 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO volume.HddsVolume: SchemaV3 db is created and loaded at /data/hdds/hdds/CID-82184b77-007c-43b9-9d2d-b37b80052e22/DS-b87e5871-4d73-4681-a147-5a63525e439d/container.db for volume DS-b87e5871-4d73-4681-a147-5a63525e439d
dn4_1    | 2023-06-16 10:48:29,931 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Attempting to start container services.
dn4_1    | 2023-06-16 10:48:29,939 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Background container scanner has been disabled.
dn4_1    | 2023-06-16 10:48:30,151 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: Starting XceiverServerRatis 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
dn4_1    | 2023-06-16 10:48:30,240 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.RaftServer: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start RPC server
dn4_1    | 2023-06-16 10:48:30,276 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: GrpcService started, listening on 9858
dn4_1    | 2023-06-16 10:48:30,299 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: GrpcService started, listening on 9856
dn4_1    | 2023-06-16 10:48:30,305 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO server.GrpcService: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: GrpcService started, listening on 9857
dn4_1    | 2023-06-16 10:48:30,336 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: Started
dn4_1    | 2023-06-16 10:48:30,340 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8 is started using port 9858 for RATIS
dn4_1    | 2023-06-16 10:48:30,340 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8 is started using port 9857 for RATIS_ADMIN
dn4_1    | 2023-06-16 10:48:30,340 [EndpointStateMachine task thread for scm1/10.9.0.14:9861 - 0 ] INFO ratis.XceiverServerRatis: XceiverServerRatis 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8 is started using port 9856 for RATIS_SERVER
dn4_1    | 2023-06-16 10:48:30,756 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:30,763 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn5_1    | 2023-06-16 10:48:33,129 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:33,130 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn5_1    | java.net.ConnectException: Call From 2e2f108eef4e/10.9.0.21 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn5_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn5_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn5_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 2023-06-16 10:48:19,461 [Listener at 0.0.0.0/9891] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
recon_1  | 2023-06-16 10:48:19,560 [Listener at 0.0.0.0/9891] INFO server.session: DefaultSessionIdManager workerName=node0
recon_1  | 2023-06-16 10:48:19,560 [Listener at 0.0.0.0/9891] INFO server.session: No SessionScavenger set, using defaults
recon_1  | 2023-06-16 10:48:19,561 [Listener at 0.0.0.0/9891] INFO server.session: node0 Scavenging every 600000ms
recon_1  | 2023-06-16 10:48:19,591 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4a216eb4{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
recon_1  | 2023-06-16 10:48:19,592 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@689faf79{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/static,AVAILABLE}
recon_1  | 2023-06-16 10:48:22,562 [Listener at 0.0.0.0/9891] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@74ab8610{recon,/,file:///tmp/jetty-0_0_0_0-9888-ozone-recon-1_3_0_jar-_-any-11491665946163223881/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-recon-1.3.0.jar!/webapps/recon}
recon_1  | 2023-06-16 10:48:22,572 [Listener at 0.0.0.0/9891] INFO server.AbstractConnector: Started ServerConnector@6aa7e176{HTTP/1.1, (http/1.1)}{0.0.0.0:9888}
recon_1  | 2023-06-16 10:48:22,573 [Listener at 0.0.0.0/9891] INFO server.Server: Started @52295ms
recon_1  | 2023-06-16 10:48:22,576 [Listener at 0.0.0.0/9891] INFO impl.MetricsSinkAdapter: Sink prometheus started
recon_1  | 2023-06-16 10:48:22,577 [Listener at 0.0.0.0/9891] INFO impl.MetricsSystemImpl: Registered sink prometheus
recon_1  | 2023-06-16 10:48:22,578 [Listener at 0.0.0.0/9891] INFO http.BaseHttpServer: HTTP server of recon listening at http://0.0.0.0:9888
recon_1  | 2023-06-16 10:48:22,579 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Starting Ozone Manager Service Provider.
recon_1  | 2023-06-16 10:48:22,598 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmDeltaRequest task 
recon_1  | 2023-06-16 10:48:22,608 [Listener at 0.0.0.0/9891] INFO impl.OzoneManagerServiceProviderImpl: Registered OmSnapshotRequest task 
recon_1  | 2023-06-16 10:48:22,608 [Listener at 0.0.0.0/9891] INFO recovery.ReconOmMetadataManagerImpl: Starting ReconOMMetadataManagerImpl
recon_1  | 2023-06-16 10:48:22,609 [Listener at 0.0.0.0/9891] WARN recon.ReconUtils: ozone.recon.om.db.dir is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:48:22,609 [Listener at 0.0.0.0/9891] INFO tasks.ReconTaskControllerImpl: Starting Recon Task Controller.
om3_1    | 2023-06-16 10:48:49,809 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om3_1    | 2023-06-16 10:48:49,813 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om3_1    | 2023-06-16 10:48:49,872 [pool-26-thread-1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om3_1    | 2023-06-16 10:48:49,879 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om3_1    | 2023-06-16 10:48:49,942 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om3_1    | 2023-06-16 10:48:49,942 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om3_1    | 2023-06-16 10:48:50,093 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
om3_1    | 2023-06-16 10:48:50,128 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
om3_1    | 2023-06-16 10:48:50,146 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om3_1    | 2023-06-16 10:48:50,905 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om3_1    | 2023-06-16 10:48:50,947 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
om3_1    | 2023-06-16 10:48:50,953 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om3_1    | 2023-06-16 10:48:50,954 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
om3_1    | 2023-06-16 10:48:50,954 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om3_1    | 2023-06-16 10:48:53,147 [main] INFO reflections.Reflections: Reflections took 3173 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om3_1    | 2023-06-16 10:48:53,691 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om3_1    | 2023-06-16 10:48:53,752 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om3_1    | 2023-06-16 10:48:54,470 [Listener at om3/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om3_1    | 2023-06-16 10:48:54,556 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om3_1    | 2023-06-16 10:48:54,556 [Listener at om3/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
om3_1    | 2023-06-16 10:48:54,685 [Listener at om3/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om3/10.9.0.13:9862
om3_1    | 2023-06-16 10:48:54,685 [Listener at om3/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om3 at port 9872
om3_1    | 2023-06-16 10:48:54,697 [om3-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
om3_1    | 2023-06-16 10:48:54,710 [om3-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@a9029c5fc9fa
om3_1    | 2023-06-16 10:48:54,796 [om3-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om3_1    | 2023-06-16 10:48:54,808 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om3_1    | 2023-06-16 10:48:54,869 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om3_1    | 2023-06-16 10:48:54,879 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om3_1    | 2023-06-16 10:48:54,895 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om3_1    | 2023-06-16 10:48:54,901 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om3_1    | 2023-06-16 10:48:54,919 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-16 10:48:54,959 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om3_1    | 2023-06-16 10:48:54,975 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om3_1    | 2023-06-16 10:48:55,017 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om3@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
om3_1    | 2023-06-16 10:48:55,023 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
om3_1    | 2023-06-16 10:48:55,027 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
om3_1    | 2023-06-16 10:48:55,030 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
om3_1    | 2023-06-16 10:48:55,055 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
om3_1    | 2023-06-16 10:48:55,056 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om3_1    | 2023-06-16 10:48:55,074 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om3_1    | 2023-06-16 10:48:55,075 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om3_1    | 2023-06-16 10:48:55,076 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
om3_1    | 2023-06-16 10:48:55,127 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
om3_1    | 2023-06-16 10:48:55,138 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om3_1    | 2023-06-16 10:48:55,138 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om3_1    | 2023-06-16 10:48:55,141 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
om3_1    | 2023-06-16 10:48:55,171 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-16 10:48:55,176 [om3-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om3_1    | 2023-06-16 10:48:55,200 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-16 10:48:55,200 [om3-impl-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
dn3_1    | 2023-06-16 10:49:15,552 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn3_1    | 2023-06-16 10:49:15,552 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1 (custom)
dn3_1    | 2023-06-16 10:49:15,564 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
dn3_1    | 2023-06-16 10:49:15,564 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 60s (custom)
dn3_1    | 2023-06-16 10:49:15,564 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn3_1    | 2023-06-16 10:49:15,565 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn3_1    | 2023-06-16 10:49:15,565 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
dn3_1    | 2023-06-16 10:49:15,566 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderStateImpl
dn3_1    | 2023-06-16 10:49:15,566 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-16 10:49:15,568 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/current/log_inprogress_0
dn3_1    | 2023-06-16 10:49:15,605 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155-LeaderElection2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-E3FF46599155: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:16,917 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn3_1    | 2023-06-16 10:49:18,801 [grpc-default-executor-2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: receive requestVote(ELECTION, 7e6c224f-5cdf-45b7-b597-f8bf583ee016, group-42EFE7C8DED3, 2, (t:0, i:0))
dn3_1    | 2023-06-16 10:49:18,802 [grpc-default-executor-2] INFO impl.VoteContext: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FOLLOWER: accept ELECTION from 7e6c224f-5cdf-45b7-b597-f8bf583ee016: our priority 0 <= candidate's priority 1
dn3_1    | 2023-06-16 10:49:18,802 [grpc-default-executor-2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn3_1    | 2023-06-16 10:49:18,802 [grpc-default-executor-2] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: shutdown a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState
dn3_1    | 2023-06-16 10:49:18,802 [grpc-default-executor-2] INFO impl.RoleInfo: a27fbf0d-ae65-4185-b735-d902c8c8b71a: start a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState
dn3_1    | 2023-06-16 10:49:18,802 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState was interrupted
dn3_1    | 2023-06-16 10:49:18,809 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn3_1    | 2023-06-16 10:49:18,809 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn3_1    | 2023-06-16 10:49:18,818 [grpc-default-executor-2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3 replies to ELECTION vote request: 7e6c224f-5cdf-45b7-b597-f8bf583ee016<-a27fbf0d-ae65-4185-b735-d902c8c8b71a#0:OK-t2. Peer's state: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3:t2, leader=null, voted=7e6c224f-5cdf-45b7-b597-f8bf583ee016, raftlog=Memoized:a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:19,044 [a27fbf0d-ae65-4185-b735-d902c8c8b71a-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-42EFE7C8DED3 with new leaderId: 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn3_1    | 2023-06-16 10:49:19,047 [a27fbf0d-ae65-4185-b735-d902c8c8b71a-server-thread1] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: change Leader from null to 7e6c224f-5cdf-45b7-b597-f8bf583ee016 at term 2 for appendEntries, leader elected after 10213ms
dn3_1    | 2023-06-16 10:49:19,163 [a27fbf0d-ae65-4185-b735-d902c8c8b71a-server-thread2] INFO server.RaftServer$Division: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn3_1    | 2023-06-16 10:49:19,171 [a27fbf0d-ae65-4185-b735-d902c8c8b71a-server-thread2] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker: Starting segment from index:0
dn3_1    | 2023-06-16 10:49:19,178 [a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: a27fbf0d-ae65-4185-b735-d902c8c8b71a@group-42EFE7C8DED3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/current/log_inprogress_0
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn5_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn5_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn5_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.net.ConnectException: Connection refused
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn5_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn5_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn5_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn5_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn5_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn5_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn5_1    | 	... 12 more
dn5_1    | 2023-06-16 10:48:33,732 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn5_1    | Caused by: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-16 10:48:34,118 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:34,143 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:35,119 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:35,146 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:36,120 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:36,147 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:37,121 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:37,148 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:38,122 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:38,149 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:39,123 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:39,149 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om3_1    | 2023-06-16 10:48:55,204 [om3-impl-thread1] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-16 10:48:55,230 [om3-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om3
om3_1    | 2023-06-16 10:48:55,242 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-16 10:48:55,257 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-16 10:48:55,258 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om3_1    | 2023-06-16 10:48:55,261 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
om3_1    | 2023-06-16 10:48:55,265 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
om3_1    | 2023-06-16 10:48:55,266 [om3-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
om3_1    | 2023-06-16 10:48:55,305 [Listener at om3/9862] INFO server.RaftServer: om3: start RPC server
om3_1    | 2023-06-16 10:48:55,614 [Listener at om3/9862] INFO server.GrpcService: om3: GrpcService started, listening on 9872
om3_1    | 2023-06-16 10:48:55,618 [Listener at om3/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
om3_1    | 2023-06-16 10:48:55,633 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om3: Started
om3_1    | 2023-06-16 10:48:55,800 [Listener at om3/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
om3_1    | 2023-06-16 10:48:55,800 [Listener at om3/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om3_1    | 2023-06-16 10:48:55,920 [Listener at om3/9862] INFO util.log: Logging initialized @24315ms to org.eclipse.jetty.util.log.Slf4jLog
om3_1    | 2023-06-16 10:48:56,274 [Listener at om3/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om3_1    | 2023-06-16 10:48:56,292 [Listener at om3/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
om3_1    | 2023-06-16 10:48:56,303 [Listener at om3/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om3_1    | 2023-06-16 10:48:56,369 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om3_1    | 2023-06-16 10:48:56,369 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om3_1    | 2023-06-16 10:48:56,370 [Listener at om3/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om3_1    | 2023-06-16 10:48:56,552 [Listener at om3/9862] INFO http.HttpServer2: Jetty bound to port 9874
om3_1    | 2023-06-16 10:48:56,553 [Listener at om3/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om3_1    | 2023-06-16 10:48:56,658 [Listener at om3/9862] INFO server.session: DefaultSessionIdManager workerName=node0
om3_1    | 2023-06-16 10:48:56,660 [Listener at om3/9862] INFO server.session: No SessionScavenger set, using defaults
om3_1    | 2023-06-16 10:48:56,665 [Listener at om3/9862] INFO server.session: node0 Scavenging every 660000ms
om3_1    | 2023-06-16 10:48:56,713 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@4357524b{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om3_1    | 2023-06-16 10:48:56,714 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@1e191150{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om3_1    | 2023-06-16 10:48:58,118 [Listener at om3/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@40faff12{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-3552788773613860639/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om3_1    | 2023-06-16 10:48:58,164 [Listener at om3/9862] INFO server.AbstractConnector: Started ServerConnector@1e120628{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om3_1    | 2023-06-16 10:48:58,165 [Listener at om3/9862] INFO server.Server: Started @26559ms
om3_1    | 2023-06-16 10:48:58,175 [Listener at om3/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
om3_1    | 2023-06-16 10:48:58,175 [Listener at om3/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om3_1    | 2023-06-16 10:48:58,182 [Listener at om3/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om3_1    | 2023-06-16 10:48:58,183 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om3_1    | 2023-06-16 10:48:58,184 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
om3_1    | 2023-06-16 10:48:58,461 [Listener at om3/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
om3_1    | 2023-06-16 10:48:58,680 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@22fb9a2c] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om3_1    | 2023-06-16 10:48:59,839 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 1, (t:0, i:~))
om3_1    | 2023-06-16 10:48:59,863 [grpc-default-executor-0] INFO impl.VoteContext: om3@group-D66704EFC61C-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om3_1    | 2023-06-16 10:48:59,866 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om1
om3_1    | 2023-06-16 10:48:59,868 [grpc-default-executor-0] INFO impl.RoleInfo: om3: shutdown om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-16 10:48:59,881 [om3@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om3@group-D66704EFC61C-FollowerState was interrupted
om3_1    | 2023-06-16 10:48:59,882 [grpc-default-executor-0] INFO impl.RoleInfo: om3: start om3@group-D66704EFC61C-FollowerState
om3_1    | 2023-06-16 10:48:59,884 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om3_1    | 2023-06-16 10:48:59,898 [om3@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om3_1    | 2023-06-16 10:48:59,989 [grpc-default-executor-0] INFO server.RaftServer$Division: om3@group-D66704EFC61C replies to ELECTION vote request: om1<-om3#0:OK-t1. Peer's state: om3@group-D66704EFC61C:t1, leader=null, voted=om1, raftlog=Memoized:om3@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-16 10:49:01,053 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: change Leader from null to om1 at term 1 for appendEntries, leader elected after 10959ms
recon_1  | 2023-06-16 10:48:22,612 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Recon ScmDatanodeProtocol RPC server is listening at /0.0.0.0:9891
recon_1  | 2023-06-16 10:48:24,671 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 668b8d626d36/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-16 10:48:26,676 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 668b8d626d36/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-16 10:48:28,706 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:80f154b6-d6d9-4d00-b482-76c3516d18ec is not the leader. Could not determine the leader node.
recon_1  | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
recon_1  | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
recon_1  | 	at org.apache.hadoop.hdds.scm.protocol.StorageContainerLocationProtocolServerSideTranslatorPB.submitRequest(StorageContainerLocationProtocolServerSideTranslatorPB.java:193)
recon_1  | 	at org.apache.hadoop.hdds.protocol.proto.StorageContainerLocationProtocolProtos$StorageContainerLocationProtocolService$2.callBlockingMethod(StorageContainerLocationProtocolProtos.java:62732)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
recon_1  | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
recon_1  | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | , while invoking $Proxy43.submitRequest over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9860 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-16 10:48:30,707 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 668b8d626d36/10.9.0.22 to scm2:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9860 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-16 10:48:32,709 [Listener at 0.0.0.0/9891] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 668b8d626d36/10.9.0.22 to scm3:9860 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy43.submitRequest over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9860 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
recon_1  | 2023-06-16 10:48:35,135 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: Obtained 7 pipelines from SCM.
recon_1  | 2023-06-16 10:48:35,137 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Recon has 0 pipelines in house.
recon_1  | 2023-06-16 10:48:35,144 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 from SCM.
recon_1  | 2023-06-16 10:48:35,241 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]].
recon_1  | 2023-06-16 10:48:35,291 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=78655ac1-fdd1-422f-9722-5f4ec0623377 from SCM.
recon_1  | 2023-06-16 10:48:35,293 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]].
recon_1  | 2023-06-16 10:48:35,293 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1 from SCM.
recon_1  | 2023-06-16 10:48:35,303 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]].
recon_1  | 2023-06-16 10:48:35,304 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=e156c71f-4c3d-49f1-9f4c-6e4272a38d3e from SCM.
recon_1  | 2023-06-16 10:48:35,306 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]].
dn4_1    | 2023-06-16 10:48:31,757 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:31,764 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:32,549 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	... 1 more
dn4_1    | 2023-06-16 10:48:32,758 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:32,768 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:33,759 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:33,760 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm2:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From ee32ea3a212f/10.9.0.20 to scm2:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
dn4_1    | 2023-06-16 10:48:33,769 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:33,769 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] WARN statemachine.EndpointStateMachine: Unable to communicate to SCM server at scm3:9861 for past 0 seconds.
dn4_1    | java.net.ConnectException: Call From ee32ea3a212f/10.9.0.20 to scm3:9861 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
dn4_1    | 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
dn4_1    | 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
dn4_1    | 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
recon_1  | 2023-06-16 10:48:35,315 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=e4ca3c21-179d-436f-b702-75b0b54839b8 from SCM.
recon_1  | 2023-06-16 10:48:35,316 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]].
recon_1  | 2023-06-16 10:48:35,323 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 from SCM.
recon_1  | 2023-06-16 10:48:35,324 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]].
recon_1  | 2023-06-16 10:48:35,325 [Listener at 0.0.0.0/9891] INFO scm.ReconPipelineManager: Adding new pipeline PipelineID=21e502a4-0e73-4435-b1e3-1e7ec9f35ce6 from SCM.
recon_1  | 2023-06-16 10:48:35,335 [Listener at 0.0.0.0/9891] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]].
recon_1  | 2023-06-16 10:48:35,338 [Listener at 0.0.0.0/9891] INFO scm.ReconStorageContainerManagerFacade: SCM DB initialized
recon_1  | 2023-06-16 10:48:35,339 [Listener at 0.0.0.0/9891] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9891
recon_1  | 2023-06-16 10:48:35,397 [IPC Server listener on 9891] INFO ipc.Server: IPC Server listener on 9891: starting
recon_1  | 2023-06-16 10:48:35,341 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
recon_1  | 2023-06-16 10:48:35,848 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered PipelineSyncTask task 
recon_1  | 2023-06-16 10:48:35,854 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting PipelineSyncTask Thread.
recon_1  | 2023-06-16 10:48:35,917 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
recon_1  | 2023-06-16 10:48:35,954 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Registered ContainerHealthTask task 
recon_1  | 2023-06-16 10:48:35,961 [Listener at 0.0.0.0/9891] INFO scm.ReconScmTask: Starting ContainerHealthTask Thread.
recon_1  | 2023-06-16 10:48:36,014 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 152 milliseconds.
recon_1  | 2023-06-16 10:48:36,649 [IPC Server handler 0 on default port 9891] WARN ipc.Server: IPC Server handler 0 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:45900: output error
recon_1  | 2023-06-16 10:48:36,651 [IPC Server handler 18 on default port 9891] WARN ipc.Server: IPC Server handler 18 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:34440: output error
recon_1  | 2023-06-16 10:48:36,673 [IPC Server handler 18 on default port 9891] INFO ipc.Server: IPC Server handler 18 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,659 [IPC Server handler 15 on default port 9891] WARN ipc.Server: IPC Server handler 15 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:33518: output error
recon_1  | 2023-06-16 10:48:36,658 [IPC Server handler 8 on default port 9891] WARN ipc.Server: IPC Server handler 8 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:45920: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 14 on default port 9891] WARN ipc.Server: IPC Server handler 14 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:43954: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 13 on default port 9891] WARN ipc.Server: IPC Server handler 13 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:39678: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 11 on default port 9891] WARN ipc.Server: IPC Server handler 11 on default port 9891, call Call#6 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:45864: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 10 on default port 9891] WARN ipc.Server: IPC Server handler 10 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:45850: output error
recon_1  | 2023-06-16 10:48:36,686 [IPC Server handler 10 on default port 9891] INFO ipc.Server: IPC Server handler 10 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 9 on default port 9891] WARN ipc.Server: IPC Server handler 9 on default port 9891, call Call#0 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:59748: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 7 on default port 9891] WARN ipc.Server: IPC Server handler 7 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:47994: output error
recon_1  | 2023-06-16 10:48:36,652 [IPC Server handler 6 on default port 9891] WARN ipc.Server: IPC Server handler 6 on default port 9891, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.19:55690: output error
recon_1  | 2023-06-16 10:48:36,687 [IPC Server handler 6 on default port 9891] INFO ipc.Server: IPC Server handler 6 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,651 [IPC Server handler 2 on default port 9891] WARN ipc.Server: IPC Server handler 2 on default port 9891, call Call#5 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.20:48008: output error
recon_1  | 2023-06-16 10:48:36,651 [IPC Server handler 4 on default port 9891] WARN ipc.Server: IPC Server handler 4 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.17:33512: output error
om3_1    | 2023-06-16 10:49:01,061 [om3-server-thread1] INFO server.RaftServer$Division: om3@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om3_1    | 2023-06-16 10:49:01,071 [om3-server-thread1] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om3_1    | 2023-06-16 10:49:01,547 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om3_1    | 2023-06-16 10:49:04,359 [om3@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om3_1    | [id: "om1"
om3_1    | address: "om1:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om3"
om3_1    | address: "om3:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | , id: "om2"
om3_1    | address: "om2:9872"
om3_1    | startupRole: FOLLOWER
om3_1    | ]
om3_1    | 2023-06-16 10:49:46,297 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om3_1    | 2023-06-16 10:49:48,666 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om3_1    | 2023-06-16 10:49:57,094 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om3_1    | 2023-06-16 10:50:06,468 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om3_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om3_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om3_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om3_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om3_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om3_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om3_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om3_1    | 2023-06-16 10:50:26,815 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 Received prepare request with log index 23
om3_1    | 2023-06-16 10:50:26,817 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om3 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om3_1    | 2023-06-16 10:50:31,819 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
om3_1    | 2023-06-16 10:50:31,820 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om3_1    | 2023-06-16 10:50:31,820 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
om3_1    | 2023-06-16 10:50:31,820 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
om3_1    | 2023-06-16 10:50:31,823 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om3@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
om3_1    | 2023-06-16 10:50:31,824 [om3@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om3@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
om3_1    | 2023-06-16 10:50:31,827 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
om3_1    | 2023-06-16 10:50:31,829 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om3 prepared at log index 23. Returning response txnID: 23
om3_1    |  with log index 23
dn5_1    | 2023-06-16 10:48:40,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:40,150 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:41,124 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:41,154 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:42,125 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:42,155 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:43,126 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:43,155 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:44,127 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:44,156 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:45,128 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:45,157 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:46,129 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:46,158 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:47,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:47,158 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:48,130 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:48,159 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:49,132 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:49,160 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:50,133 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:50,161 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:51,134 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:51,162 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:52,139 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:52,163 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:53,139 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:53,164 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-16 10:48:36,687 [IPC Server handler 4 on default port 9891] INFO ipc.Server: IPC Server handler 4 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,651 [IPC Server handler 1 on default port 9891] WARN ipc.Server: IPC Server handler 1 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.18:45912: output error
recon_1  | 2023-06-16 10:48:36,688 [IPC Server handler 2 on default port 9891] INFO ipc.Server: IPC Server handler 2 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,687 [IPC Server handler 9 on default port 9891] INFO ipc.Server: IPC Server handler 9 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,687 [IPC Server handler 7 on default port 9891] INFO ipc.Server: IPC Server handler 7 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,686 [IPC Server handler 8 on default port 9891] INFO ipc.Server: IPC Server handler 8 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,686 [IPC Server handler 13 on default port 9891] INFO ipc.Server: IPC Server handler 13 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,686 [IPC Server handler 14 on default port 9891] INFO ipc.Server: IPC Server handler 14 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om1_1    | 2023-06-16 10:49:00,274 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om1_1    | 2023-06-16 10:49:00,278 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om1_1    | 2023-06-16 10:49:00,295 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
om1_1    | 2023-06-16 10:49:00,334 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om1_1    | 2023-06-16 10:49:00,362 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om1_1    | 2023-06-16 10:49:00,530 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-06-16 10:49:00,530 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-16 10:49:00,530 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-06-16 10:49:00,533 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-06-16 10:49:00,539 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-16 10:49:00,539 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-06-16 10:49:00,539 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-16 10:49:00,539 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-06-16 10:49:00,544 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om1_1    | 2023-06-16 10:49:00,544 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-16 10:49:00,544 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-06-16 10:49:00,546 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-06-16 10:49:00,554 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
om1_1    | 2023-06-16 10:49:00,555 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om1_1    | 2023-06-16 10:49:00,555 [om1@group-D66704EFC61C-LeaderElection1] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om1_1    | 2023-06-16 10:49:00,555 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om1_1    | 2023-06-16 10:49:00,578 [om1@group-D66704EFC61C-LeaderElection1] INFO impl.RoleInfo: om1: start om1@group-D66704EFC61C-LeaderStateImpl
om1_1    | 2023-06-16 10:49:00,652 [om1@group-D66704EFC61C-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
om1_1    | 2023-06-16 10:49:00,817 [om1@group-D66704EFC61C-LeaderElection1] INFO server.RaftServer$Division: om1@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:49:01,316 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om1_1    | 2023-06-16 10:49:01,714 [om1@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
om1_1    | [id: "om1"
om1_1    | address: "om1:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om3"
om1_1    | address: "om3:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | , id: "om2"
om1_1    | address: "om2:9872"
om1_1    | startupRole: FOLLOWER
om1_1    | ]
om1_1    | 2023-06-16 10:49:23,284 [qtp1040023210-48] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om1_1    | 2023-06-16 10:49:23,315 [qtp1040023210-48] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912563287 in 27 milliseconds
om1_1    | 2023-06-16 10:49:23,437 [qtp1040023210-48] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 121 milliseconds
om1_1    | 2023-06-16 10:49:23,438 [qtp1040023210-48] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912563287
om1_1    | 2023-06-16 10:49:45,615 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om1_1    | 2023-06-16 10:49:48,677 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om1_1    | 2023-06-16 10:49:57,100 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om1_1    | 2023-06-16 10:50:06,457 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om1_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
om1_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | 2023-06-16 10:50:26,809 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 Received prepare request with log index 23
om1_1    | 2023-06-16 10:50:26,814 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: om1 waiting for index 23 to flush to OM DB and index 24 to flush to Ratis state machine.
om1_1    | 2023-06-16 10:50:26,819 [OM StateMachine ApplyTransaction Thread - 0] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:24)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,686 [IPC Server handler 11 on default port 9891] INFO ipc.Server: IPC Server handler 11 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,691 [IPC Server handler 15 on default port 9891] INFO ipc.Server: IPC Server handler 15 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,672 [IPC Server handler 19 on default port 9891] WARN ipc.Server: IPC Server handler 19 on default port 9891, call Call#4 Retry#0 org.apache.hadoop.ozone.protocol.ReconDatanodeProtocol.submitRequest from 10.9.0.21:43944: output error
recon_1  | 2023-06-16 10:48:36,721 [IPC Server handler 19 on default port 9891] INFO ipc.Server: IPC Server handler 19 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,672 [IPC Server handler 0 on default port 9891] INFO ipc.Server: IPC Server handler 0 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
recon_1  | 2023-06-16 10:48:36,688 [IPC Server handler 1 on default port 9891] INFO ipc.Server: IPC Server handler 1 on default port 9891 caught an exception
recon_1  | java.nio.channels.ClosedChannelException
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
recon_1  | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
dn5_1    | 2023-06-16 10:48:54,172 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,819 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 24 -> 24
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
dn5_1    | 2023-06-16 10:48:55,173 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:56,174 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:57,175 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)
dn4_1    | 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 2023-06-16 10:47:40,877 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
scm2_1   | Waiting for the service scm1:9894
recon_1  | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:235)
dn4_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:122)
dn4_1    | 	at com.sun.proxy.$Proxy38.submitRequest(Unknown Source)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.submitRequest(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:117)
dn4_1    | 	at org.apache.hadoop.ozone.protocolPB.StorageContainerDatanodeProtocolClientSideTranslatorPB.getVersion(StorageContainerDatanodeProtocolClientSideTranslatorPB.java:133)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:69)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.endpoint.VersionEndpointTask.call(VersionEndpointTask.java:40)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | Caused by: java.net.ConnectException: Connection refused
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
dn4_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm2_1   | 2023-06-16 10:48:10,379 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm2_1   | /************************************************************
dn4_1    | 	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
dn4_1    | 	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)
dn4_1    | 	at org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
dn5_1    | 2023-06-16 10:48:58,177 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:48:59,177 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:00,178 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:00,804 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn5_1    | 2023-06-16 10:49:01,179 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | STARTUP_MSG:   host = b1adc14b735f/10.9.0.15
scm2_1   | STARTUP_MSG:   args = [--bootstrap]
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
dn5_1    | 2023-06-16 10:49:02,180 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:03,181 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:03,829 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
dn5_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
scm2_1   | ************************************************************/
scm2_1   | 2023-06-16 10:48:10,455 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
scm3_1   | Waiting for the service scm2:9894
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
om2_1    | /************************************************************
om2_1    | STARTUP_MSG: Starting OzoneManager
om2_1    | STARTUP_MSG:   host = 56d7e9fb6d03/10.9.0.12
om2_1    | STARTUP_MSG:   args = [--init]
scm2_1   | 2023-06-16 10:48:10,822 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm2_1   | 2023-06-16 10:48:10,959 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
s3g_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
s3g_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
s3g_1    | 2023-06-16 10:47:42,322 [main] INFO http.BaseHttpServer: Starting Web-server for s3gateway at: http://0.0.0.0:9878
om2_1    | STARTUP_MSG:   version = 1.3.0
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
om2_1    | STARTUP_MSG:   java = 11.0.14.1
om2_1    | ************************************************************/
dn5_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
s3g_1    | 2023-06-16 10:47:42,334 [main] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
s3g_1    | 2023-06-16 10:47:42,549 [main] INFO util.log: Logging initialized @10537ms to org.eclipse.jetty.util.log.Slf4jLog
s3g_1    | 2023-06-16 10:47:43,858 [main] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
s3g_1    | 2023-06-16 10:47:44,149 [main] INFO http.HttpRequestLog: Http request log for http.requests.s3gateway is not defined
dn4_1    | 	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)
dn4_1    | 	at org.apache.hadoop.ipc.Client.call(Client.java:1502)
dn4_1    | 	... 12 more
om1_1    | 2023-06-16 10:50:26,819 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally -1 -> 24
dn5_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-06-16 10:47:40,964 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-06-16 10:47:44,206 [main] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
s3g_1    | 2023-06-16 10:47:44,240 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context s3gateway
s3g_1    | 2023-06-16 10:47:44,246 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
dn4_1    | 2023-06-16 10:48:34,774 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:34,774 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:35,774 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,819 [OM StateMachine ApplyTransaction Thread - 0] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Closing segment log_inprogress_0 to index: 24
dn5_1    | Caused by: java.util.concurrent.TimeoutException
om2_1    | 2023-06-16 10:47:49,737 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
om2_1    | 2023-06-16 10:47:52,889 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
om2_1    | 2023-06-16 10:47:53,390 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-16 10:47:53,402 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
om2_1    | 2023-06-16 10:47:53,405 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-06-16 10:48:35,775 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:36,775 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:36,776 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,822 [om1@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om1@group-D66704EFC61C-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0 to /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_0-24
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
om2_1    | 2023-06-16 10:47:54,520 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
om2_1    | 2023-06-16 10:47:58,767 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:00,769 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:02,770 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:04,772 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
dn4_1    | 2023-06-16 10:48:37,776 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:37,777 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:38,777 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,824 [OM StateMachine ApplyTransaction Thread - 0] INFO raftlog.RaftLog: om1@group-D66704EFC61C-SegmentedRaftLog: snapshotIndex: updateIncreasingly -1 -> 24
dn5_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
om2_1    | 2023-06-16 10:48:06,774 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:08,776 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:10,780 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:12,782 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:14,784 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
dn4_1    | 2023-06-16 10:48:38,778 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:39,778 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:39,779 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,825 [OM StateMachine ApplyTransaction Thread - 0] INFO om.OzoneManagerPrepareState: Prepare marker file written with log index 23 to file /data/metadata/current/prepareMarker
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-06-16 10:48:16,785 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 10 failover attempts. Trying to failover after sleeping for 2000ms.
dn4_1    | 2023-06-16 10:48:40,780 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:40,780 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:41,781 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:26,827 [OM StateMachine ApplyTransaction Thread - 0] INFO upgrade.OMPrepareRequest: OM om1 prepared at log index 23. Returning response txnID: 23
dn5_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
om2_1    | 2023-06-16 10:48:18,790 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 11 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
dn4_1    | 2023-06-16 10:48:41,781 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:42,782 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:42,782 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    |  with log index 23
dn5_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
om2_1    | 2023-06-16 10:48:20,792 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 12 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:22,793 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 13 failover attempts. Trying to failover after sleeping for 2000ms.
dn4_1    | 2023-06-16 10:48:43,783 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:43,783 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:44,783 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:44,784 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn5_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn5_1    | 	... 1 more
dn5_1    | 2023-06-16 10:49:04,182 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:05,171 [Command processor thread] INFO server.RaftServer: 74bc4d26-95cc-4798-833d-04dc94ff925a: addNew group-1BF7D81DC8B1:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] returns group-1BF7D81DC8B1:java.util.concurrent.CompletableFuture@56a0c18e[Not completed]
dn5_1    | 2023-06-16 10:49:05,190 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:05,279 [pool-22-thread-1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a: new RaftServerImpl for group-1BF7D81DC8B1:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn5_1    | 2023-06-16 10:49:05,280 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
dn5_1    | 2023-06-16 10:49:05,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn5_1    | 2023-06-16 10:49:05,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn5_1    | 2023-06-16 10:49:05,282 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
om1_1    | 2023-06-16 10:50:26,834 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
dn5_1    | 2023-06-16 10:49:05,285 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn5_1    | 2023-06-16 10:49:05,287 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn5_1    | 2023-06-16 10:49:05,324 [pool-22-thread-1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: ConfigurationManager, init=-1: peers:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om1_1    | 2023-06-16 10:50:26,867 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
dn5_1    | 2023-06-16 10:49:05,343 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-16 10:49:05,371 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-16 10:49:05,374 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om1_1    | 2023-06-16 10:50:26,949 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
s3g_1    | 2023-06-16 10:47:44,247 [main] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
s3g_1    | 2023-06-16 10:47:44,753 [main] INFO s3.Gateway: STARTUP_MSG: 
s3g_1    | /************************************************************
s3g_1    | STARTUP_MSG: Starting Gateway
dn5_1    | 2023-06-16 10:49:05,414 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-16 10:49:05,429 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-16 10:49:05,448 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
om1_1    | 2023-06-16 10:50:26,949 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
s3g_1    | STARTUP_MSG:   host = 3fe193aced49/10.9.0.23
s3g_1    | STARTUP_MSG:   args = []
s3g_1    | STARTUP_MSG:   version = 1.3.0
scm2_1   | 2023-06-16 10:48:10,962 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om1_1    | 2023-06-16 10:50:26,969 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 2023-06-16 10:48:11,224 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
s3g_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/hk2-utils-2.5.0.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jakarta.inject-2.6.1.jar:/opt/hadoop/share/ozone/lib/hk2-locator-2.6.1.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/aopalliance-repackaged-2.5.0.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/javax.interceptor-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/javax.el-api-3.0.0.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jakarta.ws.rs-api-2.1.6.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/jakarta.xml.bind-api-2.3.3.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-2.34.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jersey-container-servlet-core-2.34.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/cdi-api-1.2.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/activation-1.1.1.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jersey-cdi1x-2.34.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/hk2-api-2.5.0.jar:/opt/hadoop/share/ozone/lib/javax.inject-1.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/jackson-module-jaxb-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/jakarta.validation-api-2.0.2.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-2.34.jar:/opt/hadoop/share/ozone/lib/jakarta.annotation-api-1.3.5.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/osgi-resource-locator-1.0.3.jar:/opt/hadoop/share/ozone/lib/jackson-dataformat-xml-2.13.4.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-common-2.34.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/jersey-hk2-2.34.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/jersey-media-jaxb-2.34.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar
s3g_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
s3g_1    | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-06-16 10:50:26,970 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 2023-06-16 10:48:11,228 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
scm2_1   | 2023-06-16 10:48:11,889 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm2_1   | 2023-06-16 10:48:15,413 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 1 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-16 10:48:17,416 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 2 failover attempts. Trying to failover after sleeping for 2000ms.
om1_1    | 2023-06-16 10:50:26,970 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm2_1   | 2023-06-16 10:48:19,417 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 3 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-16 10:48:21,419 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm1:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 4 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-16 10:48:23,420 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 5 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-16 10:48:25,463 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:80f154b6-d6d9-4d00-b482-76c3516d18ec is not the leader. Could not determine the leader node.
om1_1    | 2023-06-16 10:50:26,982 [qtp1040023210-44] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
recon_1  | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
om1_1    | 2023-06-16 10:50:26,988 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 2023-06-16 10:50:26,988 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
s3g_1    | ************************************************************/
s3g_1    | 2023-06-16 10:47:44,811 [main] INFO s3.Gateway: registered UNIX signal handlers for [TERM, HUP, INT]
s3g_1    | 2023-06-16 10:47:45,133 [main] INFO s3.Gateway: Starting Ozone S3 gateway
s3g_1    | 2023-06-16 10:47:45,764 [main] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
s3g_1    | 2023-06-16 10:47:47,050 [main] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
s3g_1    | 2023-06-16 10:47:47,052 [main] INFO impl.MetricsSystemImpl: S3Gateway metrics system started
s3g_1    | 2023-06-16 10:47:47,372 [main] INFO http.HttpServer2: Jetty bound to port 9878
s3g_1    | 2023-06-16 10:47:47,420 [main] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
s3g_1    | 2023-06-16 10:47:47,729 [main] INFO server.session: DefaultSessionIdManager workerName=node0
s3g_1    | 2023-06-16 10:47:47,729 [main] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-06-16 10:48:24,796 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 14 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:26,844 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:80f154b6-d6d9-4d00-b482-76c3516d18ec is not the leader. Could not determine the leader node.
om2_1    | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
om2_1    | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
om2_1    | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
s3g_1    | 2023-06-16 10:47:47,788 [main] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
s3g_1    | 2023-06-16 10:47:48,080 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5a9f4771{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
s3g_1    | 2023-06-16 10:47:48,091 [main] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@6c0d7c83{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/static,AVAILABLE}
s3g_1    | WARNING: An illegal reflective access operation has occurred
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om2_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om2_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
dn5_1    | 2023-06-16 10:49:05,717 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-16 10:49:05,756 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-16 10:49:05,769 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
s3g_1    | WARNING: Illegal reflective access by org.jboss.weld.util.reflection.Formats (file:/opt/hadoop/share/ozone/lib/weld-servlet-2.4.7.Final.jar) to constructor com.sun.org.apache.bcel.internal.classfile.ClassParser(java.io.InputStream,java.lang.String)
s3g_1    | WARNING: Please consider reporting this to the maintainers of org.jboss.weld.util.reflection.Formats
s3g_1    | WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
dn4_1    | 2023-06-16 10:48:45,784 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:45,785 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm1_1   | 2023-06-16 10:47:43,163 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
s3g_1    | WARNING: All illegal access operations will be denied in a future release
s3g_1    | Jun 16, 2023 10:48:16 AM org.glassfish.jersey.internal.Errors logErrors
s3g_1    | WARNING: The following warnings have been detected: WARNING: A HTTP GET method, public javax.ws.rs.core.Response org.apache.hadoop.ozone.s3.endpoint.ObjectEndpoint.get(java.lang.String,java.lang.String,java.lang.String,int,java.lang.String,java.io.InputStream) throws java.io.IOException,org.apache.hadoop.ozone.s3.exception.OS3Exception, should not consume any entity.
dn5_1    | 2023-06-16 10:49:05,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn5_1    | 2023-06-16 10:49:05,777 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn5_1    | 2023-06-16 10:49:05,778 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1 does not exist. Creating ...
dn4_1    | 2023-06-16 10:48:46,785 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | , while invoking $Proxy31.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 15 failover attempts. Trying to failover after sleeping for 2000ms.
scm1_1   | /************************************************************
s3g_1    | 
s3g_1    | 2023-06-16 10:48:16,520 [main] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4baf997{s3gateway,/,file:///tmp/jetty-0_0_0_0-9878-ozone-s3gateway-1_3_0_jar-_-any-7527726078420616092/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-s3gateway-1.3.0.jar!/webapps/s3gateway}
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
dn4_1    | 2023-06-16 10:48:46,786 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:47,786 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:28,846 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm2:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863 after 16 failover attempts. Trying to failover after sleeping for 2000ms.
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | 2023-06-16 10:48:59,551 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | /************************************************************
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
om2_1    | 2023-06-16 10:48:30,847 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From 56d7e9fb6d03/10.9.0.12 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy31.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 17 failover attempts. Trying to failover after sleeping for 2000ms.
scm1_1   | STARTUP_MSG:   host = 9490aade52c0/10.9.0.14
scm3_1   | STARTUP_MSG:   host = 592901d477f4/10.9.0.16
scm3_1   | STARTUP_MSG:   args = [--bootstrap]
scm3_1   | STARTUP_MSG:   version = 1.3.0
om2_1    | OM initialization succeeded.Current cluster id for sd=/data/metadata/om;cid=CID-82184b77-007c-43b9-9d2d-b37b80052e22;layoutVersion=3
scm1_1   | STARTUP_MSG:   args = [--init]
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 6 failover attempts. Trying to failover after sleeping for 2000ms.
scm2_1   | 2023-06-16 10:48:27,465 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 7 failover attempts. Trying to failover after sleeping for 2000ms.
om2_1    | 2023-06-16 10:48:32,904 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
scm1_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | 2023-06-16 10:48:29,472 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdds.ratis.ServerNotLeaderException): Server:80f154b6-d6d9-4d00-b482-76c3516d18ec is not the leader. Could not determine the leader node.
scm2_1   | 	at org.apache.hadoop.hdds.ratis.ServerNotLeaderException.convertToNotLeaderException(ServerNotLeaderException.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.scm.ha.RatisUtil.checkRatisException(RatisUtil.java:246)
dn5_1    | 2023-06-16 10:49:05,824 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1/in_use.lock acquired by nodename 7@2e2f108eef4e
dn5_1    | 2023-06-16 10:49:05,879 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1 has been successfully formatted.
dn5_1    | 2023-06-16 10:49:05,901 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1BF7D81DC8B1: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
dn5_1    | 2023-06-16 10:49:05,940 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | /************************************************************
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 56d7e9fb6d03/10.9.0.12
om2_1    | ************************************************************/
om2_1    | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | ************************************************************/
scm1_1   | 2023-06-16 10:47:43,338 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
scm1_1   | 2023-06-16 10:47:44,246 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
s3g_1    | 2023-06-16 10:48:16,567 [main] INFO server.AbstractConnector: Started ServerConnector@54504ecd{HTTP/1.1, (http/1.1)}{0.0.0.0:9878}
dn5_1    | 2023-06-16 10:49:06,001 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
dn5_1    | 2023-06-16 10:49:06,003 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-16 10:49:06,005 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 	at org.apache.hadoop.hdds.scm.protocol.ScmBlockLocationProtocolServerSideTranslatorPB.send(ScmBlockLocationProtocolServerSideTranslatorPB.java:109)
scm2_1   | 	at org.apache.hadoop.hdds.protocol.proto.ScmBlockLocationProtocolProtos$ScmBlockLocationProtocolService$2.callBlockingMethod(ScmBlockLocationProtocolProtos.java:14202)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
scm1_1   | 2023-06-16 10:47:44,620 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
dn4_1    | 2023-06-16 10:48:47,787 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:48:16,567 [main] INFO server.Server: Started @44556ms
dn5_1    | 2023-06-16 10:49:06,019 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
om1_1    | 2023-06-16 10:50:27,000 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
scm2_1   | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om2_1    | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-06-16 10:47:44,665 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
dn4_1    | 2023-06-16 10:48:48,787 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:48:16,585 [main] INFO impl.MetricsSinkAdapter: Sink prometheus started
dn5_1    | 2023-06-16 10:49:06,035 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-06-16 10:50:27,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
om2_1    | 2023-06-16 10:48:38,691 [main] INFO om.OzoneManagerStarter: STARTUP_MSG: 
scm1_1   | 2023-06-16 10:47:45,039 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
dn4_1    | 2023-06-16 10:48:48,788 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:48:16,585 [main] INFO impl.MetricsSystemImpl: Registered sink prometheus
dn5_1    | 2023-06-16 10:49:06,070 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
om1_1    | 2023-06-16 10:50:27,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
scm1_1   | 2023-06-16 10:47:45,041 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
dn4_1    | 2023-06-16 10:48:49,789 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:48:16,588 [main] INFO http.BaseHttpServer: HTTP server of s3gateway listening at http://0.0.0.0:9878
dn5_1    | 2023-06-16 10:49:06,085 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
om1_1    | 2023-06-16 10:50:27,013 [qtp1040023210-44] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912626985 in 27 milliseconds
recon_1  | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm3_1   | ************************************************************/
om2_1    | /************************************************************
scm1_1   | 2023-06-16 10:47:46,630 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
dn4_1    | 2023-06-16 10:48:49,790 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:05,149 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn5_1    | 2023-06-16 10:49:06,131 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1
om1_1    | 2023-06-16 10:50:27,030 [qtp1040023210-44] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 14 milliseconds
recon_1  | 2023-06-16 10:49:02,585 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm3_1   | 2023-06-16 10:48:59,591 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | STARTUP_MSG: Starting OzoneManager
scm1_1   | 2023-06-16 10:47:48,624 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-16 10:48:50,790 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:05,168 [qtp384515747-20] INFO audit.AuditLogger: Refresh DebugCmdSet for S3GAudit to [].
dn5_1    | 2023-06-16 10:49:06,132 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om1_1    | 2023-06-16 10:50:27,054 [qtp1040023210-44] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912626985
recon_1  | 2023-06-16 10:49:02,597 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm3_1   | 2023-06-16 10:48:59,785 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | STARTUP_MSG:   host = 56d7e9fb6d03/10.9.0.12
scm1_1   | 2023-06-16 10:47:48,763 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
dn4_1    | 2023-06-16 10:48:50,791 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:05,175 [qtp384515747-20] INFO ozone.OmUtils: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn5_1    | 2023-06-16 10:49:06,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
om1_1    | 2023-06-16 10:50:27,055 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 2023-06-16 10:49:02,604 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8 to Node DB.
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm3_1   | 2023-06-16 10:48:59,850 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om2_1    | STARTUP_MSG:   args = [--]
scm1_1   | 2023-06-16 10:47:48,772 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
dn4_1    | 2023-06-16 10:48:51,791 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:05,175 [qtp384515747-20] INFO ozone.OmUtils: Using OzoneManager ServiceID 'omservice'.
dn5_1    | 2023-06-16 10:49:06,141 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-06-16 10:50:27,055 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 2023-06-16 10:49:03,125 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7e6c224f-5cdf-45b7-b597-f8bf583ee016
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm3_1   | 2023-06-16 10:48:59,872 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om2_1    | STARTUP_MSG:   version = 1.3.0
scm1_1   | 2023-06-16 10:47:48,773 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
dn4_1    | 2023-06-16 10:48:51,792 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:06,422 [qtp384515747-20] INFO rpc.RpcClient: Creating Bucket: s3v/old1-bucket, with server-side default bucket layout, dlfknslnfslf as owner, Versioning false, Storage Type set to DISK and Encryption set to false 
dn5_1    | 2023-06-16 10:49:06,142 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
om1_1    | 2023-06-16 10:50:27,064 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 2023-06-16 10:49:03,125 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | , while invoking $Proxy14.send over nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863 after 8 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1   | 2023-06-16 10:49:00,023 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
scm1_1   | 2023-06-16 10:47:48,773 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
om2_1    | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/jna-platform-5.2.0.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/orc-core-1.5.8.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-x86_64.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/httpmime-4.5.6.jar:/opt/hadoop/share/ozone/lib/proto-google-common-protos-2.9.0.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/httpasyncclient-4.1.3.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/jetty-client-9.4.31.v20200723.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugin-classloader-2.3.0.jar:/opt/hadoop/share/ozone/lib/ozone-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-lite-1.48.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/grpc-context-1.48.1.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-aarch_64.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/ozone-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/grpc-protobuf-1.48.1.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/ozone-interface-storage-1.3.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-lang-2.6.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jna-5.2.0.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/aspectjweaver-1.9.7.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/grpc-netty-1.48.1.jar:/opt/hadoop/share/ozone/lib/grpc-api-1.48.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-http2-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/annotations-4.1.1.4.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-cred-2.3.0.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/aspectjrt-1.9.7.jar:/opt/hadoop/share/ozone/lib/hppc-0.8.0.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-osx-x86_64.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-http-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-audit-2.3.0.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/ozone-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/jersey-client-1.19.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-windows-x86_64.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final-linux-aarch_64.jar:/opt/hadoop/share/ozone/lib/gethostname4j-0.0.2.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/netty-handler-proxy-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/animal-sniffer-annotations-1.21.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/grpc-core-1.48.1.jar:/opt/hadoop/share/ozone/lib/ranger-intg-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-classes-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-codec-socks-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/ranger-plugins-common-2.3.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/grpc-stub-1.48.1.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/perfmark-api-0.25.0.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/netty-tcnative-boringssl-static-2.0.53.Final.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/httpcore-nio-4.4.6.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar
dn4_1    | 2023-06-16 10:48:52,792 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm2/10.9.0.15:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:48:52,793 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn5_1    | 2023-06-16 10:49:06,151 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-16 10:50:27,064 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 2023-06-16 10:49:03,126 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 7e6c224f-5cdf-45b7-b597-f8bf583ee016 to Node DB.
scm2_1   | 2023-06-16 10:48:31,474 [main] INFO retry.RetryInvocationHandler: com.google.protobuf.ServiceException: java.net.ConnectException: Call From b1adc14b735f/10.9.0.15 to scm3:9863 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused, while invoking $Proxy14.send over nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863 after 9 failover attempts. Trying to failover after sleeping for 2000ms.
scm3_1   | 2023-06-16 10:49:00,027 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
scm1_1   | 2023-06-16 10:47:48,823 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
dn4_1    | 2023-06-16 10:48:53,794 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:07,485 [qtp384515747-16] WARN impl.MetricsSystemImpl: S3Gateway metrics system already initialized!
om2_1    | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:24Z
dn5_1    | 2023-06-16 10:49:06,154 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
om1_1    | 2023-06-16 10:50:27,064 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 2023-06-16 10:49:03,798 [IPC Server handler 99 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a27fbf0d-ae65-4185-b735-d902c8c8b71a
scm2_1   | 2023-06-16 10:48:33,559 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-82184b77-007c-43b9-9d2d-b37b80052e22, SCMID af8a130f-204e-4987-9eb3-c559054693bc
scm3_1   | 2023-06-16 10:49:00,420 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
scm1_1   | 2023-06-16 10:47:48,824 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
dn4_1    | 2023-06-16 10:48:54,795 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
s3g_1    | 2023-06-16 10:50:07,743 [qtp384515747-16] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om2_1    | STARTUP_MSG:   java = 11.0.14.1
dn5_1    | 2023-06-16 10:49:06,155 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-16 10:50:27,074 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 2023-06-16 10:49:03,799 [IPC Server handler 99 on default port 9891] INFO node.SCMNodeManager: Registered Data node : a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-16 10:48:33,560 [main] INFO server.StorageContainerManager: Primary SCM Node ID 80f154b6-d6d9-4d00-b482-76c3516d18ec
scm3_1   | 2023-06-16 10:49:01,243 [main] INFO server.StorageContainerManager: SCM BootStrap  is successful for ClusterID CID-82184b77-007c-43b9-9d2d-b37b80052e22, SCMID d5edb8ee-91fd-4f84-a835-71ed6541c25d
scm1_1   | 2023-06-16 10:47:48,826 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-16 10:48:55,796 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | ************************************************************/
dn5_1    | 2023-06-16 10:49:06,162 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn5_1    | 2023-06-16 10:49:06,192 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
recon_1  | 2023-06-16 10:49:03,800 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node a27fbf0d-ae65-4185-b735-d902c8c8b71a to Node DB.
scm2_1   | 2023-06-16 10:48:33,576 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm2_1   | /************************************************************
scm1_1   | 2023-06-16 10:47:48,891 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn4_1    | 2023-06-16 10:48:56,797 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:38,712 [main] INFO om.OzoneManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn5_1    | 2023-06-16 10:49:06,200 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1    | 2023-06-16 10:50:27,074 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 2023-06-16 10:49:03,855 [IPC Server handler 12 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74bc4d26-95cc-4798-833d-04dc94ff925a
scm2_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at b1adc14b735f/10.9.0.15
scm3_1   | 2023-06-16 10:49:01,246 [main] INFO server.StorageContainerManager: Primary SCM Node ID 80f154b6-d6d9-4d00-b482-76c3516d18ec
scm1_1   | 2023-06-16 10:47:48,891 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn4_1    | 2023-06-16 10:48:57,798 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:42,248 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for OMAudit to [].
dn5_1    | 2023-06-16 10:49:06,243 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-06-16 10:50:27,084 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 2023-06-16 10:49:03,856 [IPC Server handler 12 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | ************************************************************/
scm3_1   | 2023-06-16 10:49:01,251 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
scm1_1   | 2023-06-16 10:47:49,014 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-16 10:48:58,799 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:43,884 [main] INFO ha.OMHANodeDetails: ozone.om.internal.service.id is not defined, falling back to ozone.om.service.ids to find serviceID for OzoneManager if it is HA enabled cluster
dn5_1    | 2023-06-16 10:49:06,249 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-16 10:50:27,084 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 2023-06-16 10:49:03,858 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 74bc4d26-95cc-4798-833d-04dc94ff925a to Node DB.
scm2_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm3_1   | /************************************************************
scm1_1   | 2023-06-16 10:47:49,101 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-16 10:48:59,800 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:44,167 [main] INFO ha.OMHANodeDetails: Found matching OM address with OMServiceId: omservice, OMNodeId: om2, RPC Address: om2:9862 and Ratis port: 9872
om2_1    | 2023-06-16 10:48:44,167 [main] INFO ha.OMHANodeDetails: Setting configuration key ozone.om.address with value of key ozone.om.address.omservice.om2: om2
dn5_1    | 2023-06-16 10:49:06,257 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
recon_1  | 2023-06-16 10:49:03,993 [IPC Server handler 20 on default port 9891] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/13cc61e5-01ec-4df0-9437-cac1ff0f189e
scm2_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm3_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 592901d477f4/10.9.0.16
scm1_1   | 2023-06-16 10:47:49,119 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-16 10:49:00,777 [EndpointStateMachine task thread for scm2/10.9.0.15:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
om1_1    | 2023-06-16 10:50:27,085 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:48:44,170 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-06-16 10:49:06,319 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-16 10:49:03,993 [IPC Server handler 20 on default port 9891] INFO node.SCMNodeManager: Registered Data node : 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-16 10:48:39,315 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm3_1   | ************************************************************/
scm1_1   | 2023-06-16 10:47:53,645 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-16 10:49:00,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 11 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:27,100 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:48:44,284 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = MULTITENANCY_SCHEMA (version = 3), software layout = MULTITENANCY_SCHEMA (version = 3)
dn5_1    | 2023-06-16 10:49:06,323 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-16 10:49:03,994 [EventQueue-NewNodeForReconNewNodeHandler] INFO scm.ReconNodeManager: Adding new node 13cc61e5-01ec-4df0-9437-cac1ff0f189e to Node DB.
scm2_1   | /************************************************************
scm3_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
scm1_1   | 2023-06-16 10:47:53,771 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-16 10:49:01,801 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 12 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:27,100 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:48:45,582 [main] INFO reflections.Reflections: Reflections took 1110 ms to scan 1 urls, producing 114 keys and 335 values [using 2 cores]
dn5_1    | 2023-06-16 10:49:06,326 [pool-22-thread-1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: start as a follower, conf=-1: peers:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
recon_1  | 2023-06-16 10:49:04,307 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn4_1.ha_net
scm2_1   | STARTUP_MSG: Starting StorageContainerManager
scm3_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
scm1_1   | 2023-06-16 10:47:53,789 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
om1_1    | 2023-06-16 10:50:27,110 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:02,550 [Datanode State Machine Daemon Thread] ERROR datanode.RunningDatanodeState: Error in executing end point task.
om2_1    | 2023-06-16 10:48:45,627 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-06-16 10:49:06,333 [pool-22-thread-1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: changes role from      null to FOLLOWER at term 0 for startAsFollower
recon_1  | 2023-06-16 10:49:04,308 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=21e502a4-0e73-4435-b1e3-1e7ec9f35ce6 reported by 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | STARTUP_MSG:   host = b1adc14b735f/10.9.0.15
scm3_1   | 2023-06-16 10:49:05,825 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
scm1_1   | 2023-06-16 10:47:53,794 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om1_1    | 2023-06-16 10:50:27,111 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | java.util.concurrent.ExecutionException: java.util.concurrent.TimeoutException
om2_1    | 2023-06-16 10:48:46,572 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn5_1    | 2023-06-16 10:49:06,350 [pool-22-thread-1] INFO impl.RoleInfo: 74bc4d26-95cc-4798-833d-04dc94ff925a: start 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState
recon_1  | 2023-06-16 10:49:04,308 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]] moved to OPEN state
scm2_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   version = 1.3.0
scm3_1   | /************************************************************
scm3_1   | STARTUP_MSG: Starting StorageContainerManager
om2_1    | 2023-06-16 10:48:46,765 [main] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 3 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
dn5_1    | 2023-06-16 10:49:06,351 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
recon_1  | 2023-06-16 10:49:04,913 [IPC Server handler 20 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn4_1.ha_net
scm1_1   | 2023-06-16 10:47:53,807 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm1_1   | 2023-06-16 10:47:53,928 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | STARTUP_MSG:   host = 592901d477f4/10.9.0.16
scm2_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)
om2_1    | 2023-06-16 10:48:49,102 [main] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn5_1    | 2023-06-16 10:49:06,351 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
recon_1  | 2023-06-16 10:49:04,919 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-16 10:47:54,127 [main] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: addNew group-B37B80052E22:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] returns group-B37B80052E22:java.util.concurrent.CompletableFuture@7e242b4d[Not completed]
scm1_1   | 2023-06-16 10:47:54,542 [pool-2-thread-1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec: new RaftServerImpl for group-B37B80052E22:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|priority:0|startupRole:FOLLOWER] with SCMStateMachine:uninitialized
scm3_1   | STARTUP_MSG:   args = []
scm2_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:191)
om2_1    | 2023-06-16 10:48:49,556 [main] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn5_1    | 2023-06-16 10:49:06,389 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1BF7D81DC8B1,id=74bc4d26-95cc-4798-833d-04dc94ff925a
recon_1  | 2023-06-16 10:49:05,273 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn2_1.ha_net
scm1_1   | 2023-06-16 10:47:54,602 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om1_1    | 2023-06-16 10:50:27,111 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | STARTUP_MSG:   version = 1.3.0
scm2_1   | STARTUP_MSG:   java = 11.0.14.1
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.computeNextContainerState(RunningDatanodeState.java:199)
om2_1    | 2023-06-16 10:48:49,557 [main] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn5_1    | 2023-06-16 10:49:06,392 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn5_1    | 2023-06-16 10:49:06,394 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-16 10:47:54,663 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om1_1    | 2023-06-16 10:50:27,134 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | ************************************************************/
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:239)
scm3_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
om2_1    | 2023-06-16 10:48:49,974 [main] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
dn5_1    | 2023-06-16 10:49:06,398 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
recon_1  | 2023-06-16 10:49:05,273 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=e4ca3c21-179d-436f-b702-75b0b54839b8 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-16 10:47:54,667 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om1_1    | 2023-06-16 10:50:27,135 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:39,339 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.await(RunningDatanodeState.java:50)
scm3_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
om2_1    | 2023-06-16 10:48:50,127 [main] INFO om.OzoneManager: Created Volume s3v With Owner hadoop required for S3Gateway operations.
dn5_1    | 2023-06-16 10:49:06,402 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
recon_1  | 2023-06-16 10:49:05,274 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:47:54,667 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.StateContext.execute(StateContext.java:660)
scm2_1   | 2023-06-16 10:48:39,576 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:50:27,143 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | STARTUP_MSG:   java = 11.0.14.1
om2_1    | 2023-06-16 10:48:50,213 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn5_1    | 2023-06-16 10:49:06,488 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1
recon_1  | 2023-06-16 10:49:05,854 [IPC Server handler 12 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn2_1.ha_net
scm1_1   | 2023-06-16 10:47:54,670 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.startStateMachineThread(DatanodeStateMachine.java:298)
scm2_1   | 2023-06-16 10:48:39,670 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om1_1    | 2023-06-16 10:50:27,143 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | ************************************************************/
om2_1    | 2023-06-16 10:48:50,227 [main] WARN utils.OzoneManagerRatisUtils: ozone.om.ratis.snapshot.dir is not configured. Falling back to ozone.metadata.dirs config
dn5_1    | 2023-06-16 10:49:06,499 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1.
recon_1  | 2023-06-16 10:49:05,856 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm1_1   | 2023-06-16 10:47:54,670 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.statemachine.DatanodeStateMachine.lambda$startDaemon$0(DatanodeStateMachine.java:493)
scm2_1   | 2023-06-16 10:48:39,725 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om1_1    | 2023-06-16 10:50:27,143 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:05,892 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om2_1    | 2023-06-16 10:48:50,267 [main] INFO snapshot.OzoneManagerSnapshotProvider: Initializing OM Snapshot Provider
recon_1  | 2023-06-16 10:49:05,956 [IPC Server handler 17 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn5_1.ha_net
dn5_1    | 2023-06-16 10:49:07,201 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:54,757 [pool-2-thread-1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: ConfigurationManager, init=-1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn4_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm2_1   | 2023-06-16 10:48:39,877 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm2, RPC Address: scm2:9894 and Ratis port: 9894
om1_1    | 2023-06-16 10:50:27,154 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:06,244 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om2_1    | 2023-06-16 10:48:50,969 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1  | 2023-06-16 10:49:05,957 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1 reported by 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-16 10:49:08,202 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:54,821 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
dn4_1    | Caused by: java.util.concurrent.TimeoutException
scm2_1   | 2023-06-16 10:48:39,879 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm2: scm2
om1_1    | 2023-06-16 10:50:27,154 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:06,402 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
om2_1    | 2023-06-16 10:48:51,061 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
recon_1  | 2023-06-16 10:49:05,957 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:74bc4d26-95cc-4798-833d-04dc94ff925a, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]] moved to OPEN state
dn5_1    | 2023-06-16 10:49:09,203 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:55,016 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:204)
scm2_1   | 2023-06-16 10:48:42,175 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:50:27,160 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:06,481 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om2_1    | 2023-06-16 10:48:51,287 [main] INFO ratis.OzoneManagerRatisServer: Instantiating OM Ratis server with groupID: omservice and peers: om2:9872, om1:9872, om3:9872
recon_1  | 2023-06-16 10:49:06,090 [IPC Server handler 5 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn3_1.ha_net
dn5_1    | 2023-06-16 10:49:10,204 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:55,043 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 	at org.apache.hadoop.ozone.container.common.states.datanode.RunningDatanodeState.lambda$execute$0(RunningDatanodeState.java:157)
scm2_1   | 2023-06-16 10:48:42,904 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:50:27,160 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:06,719 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm3, RPC Address: scm3:9894 and Ratis port: 9894
om2_1    | 2023-06-16 10:48:51,364 [main] INFO ratis.OzoneManagerStateMachine: LastAppliedIndex is set from TransactionInfo from OM DB as (t:0, i:~)
dn5_1    | 2023-06-16 10:49:11,204 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:55,236 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm2_1   | 2023-06-16 10:48:43,669 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
om1_1    | 2023-06-16 10:50:27,161 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:06,730 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm3: scm3
recon_1  | 2023-06-16 10:49:06,092 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:51,814 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om2_1    | 2023-06-16 10:48:52,161 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
scm1_1   | 2023-06-16 10:47:55,271 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
dn4_1    | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
scm2_1   | 2023-06-16 10:48:43,686 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om1_1    | 2023-06-16 10:50:27,169 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:09,054 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:49:06,201 [IPC Server handler 21 on default port 9891] INFO scm.ReconNodeManager: Sending ReregisterCommand() for ha_dn1_1.ha_net
om2_1    | 2023-06-16 10:48:52,163 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9872 (fallback to raft.grpc.server.port)
dn5_1    | 2023-06-16 10:49:11,427 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO impl.FollowerState: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5077963012ns, electionTimeout:5075ms
scm1_1   | 2023-06-16 10:47:55,272 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
scm2_1   | 2023-06-16 10:48:43,941 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-16 10:50:27,169 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:09,257 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
recon_1  | 2023-06-16 10:49:06,202 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=e156c71f-4c3d-49f1-9f4c-6e4272a38d3e reported by 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,167 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om2_1    | 2023-06-16 10:48:52,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9872 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-16 10:47:55,782 [pool-2-thread-1] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
scm2_1   | 2023-06-16 10:48:44,009 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:af8a130f-204e-4987-9eb3-c559054693bc
om1_1    | 2023-06-16 10:50:27,177 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:09,565 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
recon_1  | 2023-06-16 10:49:06,203 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:13cc61e5-01ec-4df0-9437-cac1ff0f189e, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:48:52,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
dn5_1    | 2023-06-16 10:49:11,429 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO impl.RoleInfo: 74bc4d26-95cc-4798-833d-04dc94ff925a: shutdown 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState
scm1_1   | 2023-06-16 10:47:58,231 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-06-16 10:48:44,303 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-06-16 10:50:27,177 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:09,569 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
recon_1  | 2023-06-16 10:49:08,643 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,170 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9872 (custom)
dn5_1    | 2023-06-16 10:49:11,429 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-16 10:47:58,250 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 	... 1 more
scm2_1   | 2023-06-16 10:48:44,640 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-16 10:50:27,178 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:09,705 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
recon_1  | 2023-06-16 10:49:08,643 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 reported by 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-16 10:49:08,673 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn5_1    | 2023-06-16 10:49:11,433 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-16 10:47:58,259 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-16 10:49:02,802 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 13 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-16 10:48:44,652 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-16 10:50:27,213 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:09,775 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:d5edb8ee-91fd-4f84-a835-71ed6541c25d
recon_1  | 2023-06-16 10:49:08,674 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,175 [main] INFO server.GrpcService: raft.grpc.message.size.max = 33554432 (custom)
dn5_1    | 2023-06-16 10:49:11,433 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-FollowerState] INFO impl.RoleInfo: 74bc4d26-95cc-4798-833d-04dc94ff925a: start 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1
scm1_1   | 2023-06-16 10:47:58,310 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
dn4_1    | 2023-06-16 10:49:03,698 [Command processor thread] INFO server.RaftServer: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: addNew group-1E7EC9F35CE6:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] returns group-1E7EC9F35CE6:java.util.concurrent.CompletableFuture@13f57ab9[Not completed]
scm2_1   | 2023-06-16 10:48:44,657 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-16 10:50:27,213 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:10,018 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
recon_1  | 2023-06-16 10:49:08,746 [IPC Server handler 41 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn3_1.ha_net
om2_1    | 2023-06-16 10:48:52,180 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn5_1    | 2023-06-16 10:49:11,442 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO impl.LeaderElection: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:47:58,323 [pool-2-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-16 10:49:03,756 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: new RaftServerImpl for group-1E7EC9F35CE6:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
dn4_1    | 2023-06-16 10:49:03,767 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om1_1    | 2023-06-16 10:50:27,217 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:10,211 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
recon_1  | 2023-06-16 10:49:08,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,184 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn5_1    | 2023-06-16 10:49:11,443 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO impl.LeaderElection: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1 ELECTION round 0: result PASSED (term=1)
dn4_1    | 2023-06-16 10:49:03,782 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-16 10:48:44,659 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
scm1_1   | 2023-06-16 10:47:58,339 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 does not exist. Creating ...
om1_1    | 2023-06-16 10:50:27,217 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:10,222 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
recon_1  | 2023-06-16 10:49:08,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/ONE PipelineID=78655ac1-fdd1-422f-9722-5f4ec0623377 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,187 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 3000ms (default)
dn5_1    | 2023-06-16 10:49:11,444 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO impl.RoleInfo: 74bc4d26-95cc-4798-833d-04dc94ff925a: shutdown 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1
dn4_1    | 2023-06-16 10:49:03,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-16 10:48:44,661 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
scm1_1   | 2023-06-16 10:47:58,547 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/in_use.lock acquired by nodename 13@9490aade52c0
om1_1    | 2023-06-16 10:50:27,218 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:10,224 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
recon_1  | 2023-06-16 10:49:08,750 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:48:52,239 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-16 10:48:52,266 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-16 10:49:03,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-16 10:48:44,662 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
scm1_1   | 2023-06-16 10:47:58,651 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 has been successfully formatted.
om1_1    | 2023-06-16 10:50:27,225 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:10,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1  | 2023-06-16 10:49:08,866 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:52,269 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
om2_1    | 2023-06-16 10:48:53,384 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-16 10:49:03,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm2_1   | 2023-06-16 10:48:44,663 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
scm1_1   | 2023-06-16 10:47:58,687 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-16 10:50:27,226 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:10,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1  | 2023-06-16 10:49:08,866 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:48:53,406 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn5_1    | 2023-06-16 10:49:11,444 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
dn5_1    | 2023-06-16 10:49:11,445 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1BF7D81DC8B1 with new leaderId: 74bc4d26-95cc-4798-833d-04dc94ff925a
dn5_1    | 2023-06-16 10:49:11,445 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: change Leader from null to 74bc4d26-95cc-4798-833d-04dc94ff925a at term 1 for becomeLeader, leader elected after 6030ms
dn5_1    | 2023-06-16 10:49:11,500 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
dn5_1    | 2023-06-16 10:49:11,509 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
dn5_1    | 2023-06-16 10:49:11,522 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
dn5_1    | 2023-06-16 10:49:11,529 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
dn4_1    | 2023-06-16 10:49:03,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
dn4_1    | 2023-06-16 10:49:03,806 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 14 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:47:58,995 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm3_1   | 2023-06-16 10:49:10,226 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
om2_1    | 2023-06-16 10:48:53,409 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn4_1    | 2023-06-16 10:49:03,813 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: ConfigurationManager, init=-1: peers:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
dn5_1    | 2023-06-16 10:49:11,532 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm2_1   | 2023-06-16 10:48:44,666 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-16 10:48:44,672 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
om1_1    | 2023-06-16 10:50:27,233 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:47:58,996 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm3_1   | 2023-06-16 10:49:10,227 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
om2_1    | 2023-06-16 10:48:53,413 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
dn4_1    | 2023-06-16 10:49:03,820 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
dn5_1    | 2023-06-16 10:49:11,533 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm2_1   | 2023-06-16 10:48:44,681 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1  | 2023-06-16 10:49:11,476 [IPC Server handler 22 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn5_1.ha_net
om1_1    | 2023-06-16 10:50:27,233 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:47:59,012 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm3_1   | 2023-06-16 10:49:10,231 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-16 10:49:03,845 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn5_1    | 2023-06-16 10:49:11,548 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm2_1   | 2023-06-16 10:48:44,740 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 2023-06-16 10:49:11,787 [IPC Server handler 3 on default port 9891] INFO scm.ReconNodeManager: Updating nodeDB for ha_dn1_1.ha_net
om1_1    | 2023-06-16 10:50:27,234 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:47:59,039 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm3_1   | 2023-06-16 10:49:10,239 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
dn4_1    | 2023-06-16 10:49:03,851 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn5_1    | 2023-06-16 10:49:11,553 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm2_1   | 2023-06-16 10:48:44,753 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1  | 2023-06-16 10:49:18,866 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:27,244 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:47:59,041 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-16 10:49:10,239 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
dn4_1    | 2023-06-16 10:49:03,917 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
dn5_1    | 2023-06-16 10:49:11,564 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO impl.RoleInfo: 74bc4d26-95cc-4798-833d-04dc94ff925a: start 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderStateImpl
scm2_1   | 2023-06-16 10:48:44,755 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
recon_1  | 2023-06-16 10:49:18,867 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 reported by 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:27,244 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:47:59,160 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm3_1   | 2023-06-16 10:49:10,262 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
dn4_1    | 2023-06-16 10:49:03,935 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
dn5_1    | 2023-06-16 10:49:11,611 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1   | 2023-06-16 10:48:45,987 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
recon_1  | 2023-06-16 10:49:18,867 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]] moved to OPEN state
om1_1    | 2023-06-16 10:50:27,247 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:47:59,194 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-06-16 10:49:10,265 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
dn4_1    | 2023-06-16 10:49:03,939 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn5_1    | 2023-06-16 10:49:11,706 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-LeaderElection1] INFO server.RaftServer$Division: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1: set configuration 0: peers:[74bc4d26-95cc-4798-833d-04dc94ff925a|rpc:10.9.0.21:9856|admin:10.9.0.21:9857|client:10.9.0.21:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-16 10:48:46,012 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 2023-06-16 10:49:22,611 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om1_1    | 2023-06-16 10:50:27,247 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:47:59,217 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22
scm3_1   | 2023-06-16 10:49:10,270 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
dn4_1    | 2023-06-16 10:49:04,128 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn5_1    | 2023-06-16 10:49:11,758 [74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 74bc4d26-95cc-4798-833d-04dc94ff925a@group-1BF7D81DC8B1-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1/current/log_inprogress_0
scm2_1   | 2023-06-16 10:48:46,013 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1  | 2023-06-16 10:49:22,612 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
om1_1    | 2023-06-16 10:50:27,247 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:47:59,231 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-16 10:49:10,807 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
dn4_1    | 2023-06-16 10:49:04,131 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn5_1    | 2023-06-16 10:49:12,205 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-16 10:48:46,013 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1  | 2023-06-16 10:49:23,525 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1686912562612
om1_1    | 2023-06-16 10:50:27,261 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:47:59,235 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-16 10:47:59,490 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-16 10:47:59,498 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-06-16 10:49:10,812 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
dn4_1    | 2023-06-16 10:49:04,133 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn5_1    | 2023-06-16 10:49:13,206 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-16 10:48:46,014 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
recon_1  | 2023-06-16 10:49:23,528 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-16 10:50:27,262 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:47:59,542 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-16 10:49:04,135 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm3_1   | 2023-06-16 10:49:10,822 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
dn5_1    | 2023-06-16 10:49:14,207 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-16 10:48:46,055 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-16 10:48:46,157 [main] INFO server.RaftServer: af8a130f-204e-4987-9eb3-c559054693bc: addNew group-B37B80052E22:[] returns group-B37B80052E22:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
om1_1    | 2023-06-16 10:50:27,266 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:04,137 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
dn4_1    | 2023-06-16 10:49:04,139 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/21e502a4-0e73-4435-b1e3-1e7ec9f35ce6 does not exist. Creating ...
scm2_1   | 2023-06-16 10:48:46,242 [pool-16-thread-1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc: new RaftServerImpl for group-B37B80052E22:[] with SCMStateMachine:uninitialized
scm2_1   | 2023-06-16 10:48:46,245 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
dn5_1    | 2023-06-16 10:49:16,948 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
dn4_1    | 2023-06-16 10:49:04,155 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/21e502a4-0e73-4435-b1e3-1e7ec9f35ce6/in_use.lock acquired by nodename 7@ee32ea3a212f
recon_1  | 2023-06-16 10:49:23,530 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om1_1    | 2023-06-16 10:50:27,266 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,266 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,190 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/21e502a4-0e73-4435-b1e3-1e7ec9f35ce6 has been successfully formatted.
recon_1  | 2023-06-16 10:49:23,650 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1686912562612.
om1_1    | 2023-06-16 10:50:27,280 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,280 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:04,221 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-1E7EC9F35CE6: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
recon_1  | 2023-06-16 10:49:23,677 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
om1_1    | 2023-06-16 10:50:27,286 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,286 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,223 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
recon_1  | 2023-06-16 10:49:23,684 [pool-49-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
om1_1    | 2023-06-16 10:50:27,286 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,331 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:04,288 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-16 10:48:53,417 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-16 10:48:53,430 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm1_1   | 2023-06-16 10:47:59,551 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-16 10:47:59,556 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-16 10:49:04,299 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
dn4_1    | 2023-06-16 10:49:04,310 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
dn4_1    | 2023-06-16 10:49:04,312 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
dn4_1    | 2023-06-16 10:49:04,330 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
recon_1  | 2023-06-16 10:49:23,684 [pool-49-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
om1_1    | 2023-06-16 10:50:27,333 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:04,372 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
dn4_1    | 2023-06-16 10:49:04,392 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
recon_1  | 2023-06-16 10:49:23,987 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
scm1_1   | 2023-06-16 10:47:59,612 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | 2023-06-16 10:49:10,822 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om2_1    | 2023-06-16 10:48:53,460 [main] INFO server.RaftServer: om2: addNew group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] returns group-D66704EFC61C:java.util.concurrent.CompletableFuture@7015ebef[Not completed]
om2_1    | 2023-06-16 10:48:53,460 [main] INFO om.OzoneManager: OzoneManager Ratis server initialized at port 9872
om2_1    | 2023-06-16 10:48:53,543 [main] INFO om.OzoneManager: Creating RPC Server
om2_1    | 2023-06-16 10:48:53,613 [pool-26-thread-1] INFO server.RaftServer$Division: om2: new RaftServerImpl for group-D66704EFC61C:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER] with OzoneManagerStateMachine:uninitialized
dn4_1    | 2023-06-16 10:49:04,418 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/21e502a4-0e73-4435-b1e3-1e7ec9f35ce6
dn4_1    | 2023-06-16 10:49:04,420 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
scm1_1   | 2023-06-16 10:47:59,780 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-06-16 10:47:59,823 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:47:59,823 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:47:59,830 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-06-16 10:48:00,083 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | 2023-06-16 10:49:10,822 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-16 10:49:10,825 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-16 10:49:10,844 [main] INFO server.RaftServer: d5edb8ee-91fd-4f84-a835-71ed6541c25d: addNew group-B37B80052E22:[] returns group-B37B80052E22:java.util.concurrent.CompletableFuture@51dbd6e4[Not completed]
scm1_1   | 2023-06-16 10:48:00,083 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-16 10:48:00,129 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: start as a follower, conf=-1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:00,132 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-06-16 10:48:00,136 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState
scm1_1   | 2023-06-16 10:48:00,234 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-16 10:49:10,891 [pool-16-thread-1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d: new RaftServerImpl for group-B37B80052E22:[] with SCMStateMachine:uninitialized
scm3_1   | 2023-06-16 10:49:10,898 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
scm3_1   | 2023-06-16 10:49:10,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm1_1   | 2023-06-16 10:48:00,263 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B37B80052E22,id=80f154b6-d6d9-4d00-b482-76c3516d18ec
scm3_1   | 2023-06-16 10:49:10,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm3_1   | 2023-06-16 10:49:10,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
scm3_1   | 2023-06-16 10:49:10,902 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-16 10:49:10,903 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-16 10:49:10,921 [pool-16-thread-1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-06-16 10:49:10,930 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-16 10:49:10,936 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
dn4_1    | 2023-06-16 10:49:04,426 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-16 10:49:04,427 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
dn4_1    | 2023-06-16 10:49:04,457 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-16 10:49:04,458 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-16 10:49:04,464 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-16 10:49:04,469 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-16 10:49:04,470 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-16 10:49:04,503 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
dn4_1    | 2023-06-16 10:49:04,513 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-16 10:49:04,513 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-16 10:49:04,519 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
dn4_1    | 2023-06-16 10:49:04,533 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-16 10:49:04,542 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-16 10:49:04,543 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: start as a follower, conf=-1: peers:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:04,547 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1    | 2023-06-16 10:48:53,642 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm3_1   | 2023-06-16 10:49:10,942 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-16 10:48:46,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-16 10:48:46,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-16 10:48:46,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
dn4_1    | 2023-06-16 10:49:04,550 [pool-22-thread-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState
dn4_1    | 2023-06-16 10:49:04,585 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-1E7EC9F35CE6,id=2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
scm3_1   | 2023-06-16 10:49:10,971 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm1_1   | 2023-06-16 10:48:00,283 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-16 10:48:00,286 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-16 10:48:00,287 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn4_1    | 2023-06-16 10:49:04,589 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-16 10:49:04,587 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-16 10:49:10,978 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm1_1   | 2023-06-16 10:48:00,288 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-16 10:48:00,295 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-16 10:48:00,403 [main] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start RPC server
scm2_1   | 2023-06-16 10:48:46,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-16 10:50:27,338 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:10,982 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-06-16 10:49:23,987 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
dn4_1    | 2023-06-16 10:49:04,598 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-16 10:48:01,126 [main] INFO server.GrpcService: 80f154b6-d6d9-4d00-b482-76c3516d18ec: GrpcService started, listening on 9894
scm2_1   | 2023-06-16 10:48:46,261 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om1_1    | 2023-06-16 10:50:27,338 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:11,280 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 2023-06-16 10:49:23,988 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
dn4_1    | 2023-06-16 10:49:04,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-16 10:48:01,168 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-80f154b6-d6d9-4d00-b482-76c3516d18ec: Started
scm1_1   | 2023-06-16 10:48:05,488 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.FollowerState: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5352472380ns, electionTimeout:5193ms
scm1_1   | 2023-06-16 10:48:05,495 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState
scm3_1   | 2023-06-16 10:49:11,282 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 2023-06-16 10:49:23,988 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
dn4_1    | 2023-06-16 10:49:04,599 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-06-16 10:48:05,522 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
om1_1    | 2023-06-16 10:50:27,339 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:46,290 [pool-16-thread-1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-06-16 10:48:46,290 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-16 10:48:46,323 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-06-16 10:49:11,288 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 2023-06-16 10:49:24,004 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
dn4_1    | 2023-06-16 10:49:04,600 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-16 10:48:05,608 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om1_1    | 2023-06-16 10:50:27,353 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:46,323 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm2_1   | 2023-06-16 10:48:46,376 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-06-16 10:48:46,383 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm3_1   | 2023-06-16 10:49:11,292 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-06-16 10:49:24,004 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.016 seconds to process 0 keys.
dn4_1    | 2023-06-16 10:49:04,725 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=21e502a4-0e73-4435-b1e3-1e7ec9f35ce6
scm1_1   | 2023-06-16 10:48:05,613 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1
om1_1    | 2023-06-16 10:50:27,353 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:46,383 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-16 10:48:47,152 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-16 10:48:47,169 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-06-16 10:49:11,292 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-06-16 10:49:24,032 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 0 records from "FILE_COUNT_BY_SIZE"
dn4_1    | 2023-06-16 10:49:04,732 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS ONE PipelineID=21e502a4-0e73-4435-b1e3-1e7ec9f35ce6.
scm1_1   | 2023-06-16 10:48:05,780 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.LeaderElection: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,362 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:48:47,169 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-06-16 10:49:11,294 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
recon_1  | 2023-06-16 10:49:24,033 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
dn4_1    | 2023-06-16 10:49:04,732 [Command processor thread] INFO server.RaftServer: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: addNew group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-E3FF46599155:java.util.concurrent.CompletableFuture@280f9111[Not completed]
scm1_1   | 2023-06-16 10:48:05,782 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.LeaderElection: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1 ELECTION round 0: result PASSED (term=1)
om1_1    | 2023-06-16 10:50:27,362 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:47,190 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-06-16 10:48:47,191 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
recon_1  | 2023-06-16 10:49:39,744 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
dn4_1    | 2023-06-16 10:49:04,757 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: new RaftServerImpl for group-E3FF46599155:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
scm1_1   | 2023-06-16 10:48:05,790 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1
om1_1    | 2023-06-16 10:50:27,363 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:48:53,648 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
scm2_1   | 2023-06-16 10:48:47,204 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2_1   | 2023-06-16 10:48:47,211 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
dn4_1    | 2023-06-16 10:49:04,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
scm1_1   | 2023-06-16 10:48:05,843 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
om1_1    | 2023-06-16 10:50:27,381 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:48:53,648 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
scm2_1   | 2023-06-16 10:48:47,212 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2_1   | 2023-06-16 10:48:47,293 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
dn4_1    | 2023-06-16 10:49:04,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
dn4_1    | 2023-06-16 10:49:04,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
dn4_1    | 2023-06-16 10:49:04,771 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
scm2_1   | 2023-06-16 10:48:48,265 [main] INFO reflections.Reflections: Reflections took 759 ms to scan 3 urls, producing 112 keys and 252 values 
scm2_1   | 2023-06-16 10:48:48,508 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2_1   | 2023-06-16 10:48:48,513 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2_1   | 2023-06-16 10:48:48,525 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm2_1   | 2023-06-16 10:48:48,527 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm2_1   | 2023-06-16 10:48:48,665 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-06-16 10:48:48,731 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm2_1   | 2023-06-16 10:48:48,738 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-16 10:48:48,762 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm2_1   | 2023-06-16 10:48:48,853 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1   | 2023-06-16 10:48:48,854 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om2_1    | 2023-06-16 10:48:53,648 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120s (custom)
om2_1    | 2023-06-16 10:48:53,649 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-16 10:48:53,650 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-06-16 10:48:53,691 [pool-26-thread-1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: ConfigurationManager, init=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-06-16 10:48:53,694 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-16 10:48:53,743 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om1_1    | 2023-06-16 10:50:27,381 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:11,294 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm2_1   | 2023-06-16 10:48:48,878 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
recon_1  | 2023-06-16 10:49:45,539 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO scm.ReconPipelineReportHandler: Pipeline RATIS/THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 reported by a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
recon_1  | 2023-06-16 10:49:45,539 [EventQueue-PipelineReportForReconPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:48:53,768 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
dn4_1    | 2023-06-16 10:49:04,772 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om1_1    | 2023-06-16 10:50:27,388 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:11,294 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
scm2_1   | 2023-06-16 10:48:48,879 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
recon_1  | 2023-06-16 10:49:53,741 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #1 got from ha_dn2_1.ha_net.
scm1_1   | 2023-06-16 10:48:05,853 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: change Leader from null to 80f154b6-d6d9-4d00-b482-76c3516d18ec at term 1 for becomeLeader, leader elected after 10642ms
om1_1    | 2023-06-16 10:50:27,388 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:48,881 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
scm2_1   | 2023-06-16 10:48:48,912 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-06-16 10:48:06,046 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
om2_1    | 2023-06-16 10:48:53,986 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 120s (custom)
dn4_1    | 2023-06-16 10:49:04,773 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm3_1   | 2023-06-16 10:49:11,324 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
om1_1    | 2023-06-16 10:50:27,389 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:48,929 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
recon_1  | 2023-06-16 10:49:53,889 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #1 to Recon.
scm1_1   | 2023-06-16 10:48:06,189 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
om2_1    | 2023-06-16 10:48:54,039 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 300s (custom)
dn4_1    | 2023-06-16 10:49:04,774 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
scm3_1   | 2023-06-16 10:49:11,822 [main] INFO reflections.Reflections: Reflections took 440 ms to scan 3 urls, producing 112 keys and 252 values 
om1_1    | 2023-06-16 10:50:27,405 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:48,935 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
recon_1  | 2023-06-16 10:50:01,581 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: New container #2 got from ha_dn4_1.ha_net.
scm1_1   | 2023-06-16 10:48:06,207 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-16 10:48:06,326 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
dn4_1    | 2023-06-16 10:49:04,775 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
scm3_1   | 2023-06-16 10:49:11,960 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
om1_1    | 2023-06-16 10:50:27,405 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:49,114 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
scm1_1   | 2023-06-16 10:48:06,334 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-06-16 10:48:54,041 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
recon_1  | 2023-06-16 10:50:01,588 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: New container #2 got from ha_dn2_1.ha_net.
dn4_1    | 2023-06-16 10:49:04,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm3_1   | 2023-06-16 10:49:11,960 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
om1_1    | 2023-06-16 10:50:27,408 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:48:49,183 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
scm1_1   | 2023-06-16 10:48:06,353 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-16 10:48:06,435 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
recon_1  | 2023-06-16 10:50:01,601 [FixedThreadPoolWithAffinityExecutor-1-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
dn4_1    | 2023-06-16 10:49:04,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
scm3_1   | 2023-06-16 10:49:11,965 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
om1_1    | 2023-06-16 10:50:27,408 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:49,280 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
om2_1    | 2023-06-16 10:48:54,672 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
recon_1  | 2023-06-16 10:50:01,620 [FixedThreadPoolWithAffinityExecutor-0-0] INFO scm.ReconContainerManager: Successfully added container #2 to Recon.
dn4_1    | 2023-06-16 10:49:04,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
scm3_1   | 2023-06-16 10:49:11,977 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
scm3_1   | 2023-06-16 10:49:12,069 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm2_1   | 2023-06-16 10:48:49,355 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
scm1_1   | 2023-06-16 10:48:06,518 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
om2_1    | 2023-06-16 10:48:54,688 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
recon_1  | 2023-06-16 10:50:24,041 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm3_1   | 2023-06-16 10:49:12,116 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
om1_1    | 2023-06-16 10:50:27,408 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,776 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
scm2_1   | 2023-06-16 10:48:49,364 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | 2023-06-16 10:48:06,564 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl
om2_1    | 2023-06-16 10:48:54,689 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
scm3_1   | 2023-06-16 10:49:12,124 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
om1_1    | 2023-06-16 10:50:27,421 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:04,779 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-16 10:48:49,375 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm1_1   | 2023-06-16 10:48:07,104 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: Starting segment from index:0
om2_1    | 2023-06-16 10:48:54,689 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1   | 2023-06-16 10:49:12,135 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
om1_1    | 2023-06-16 10:50:27,422 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:04,783 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-16 10:48:49,378 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-16 10:48:49,400 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm2_1   | 2023-06-16 10:48:52,231 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-16 10:48:07,429 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-80f154b6-d6d9-4d00-b482-76c3516d18ec: Detected pause in JVM or host machine (eg GC): pause of approximately 101484563ns. No GCs detected.
om2_1    | 2023-06-16 10:48:54,689 [pool-26-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
om1_1    | 2023-06-16 10:50:27,426 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:04,788 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm3_1   | 2023-06-16 10:49:12,177 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm2_1   | 2023-06-16 10:48:52,358 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-16 10:48:08,011 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:48:55,924 [main] INFO reflections.Reflections: Reflections took 2211 ms to scan 8 urls, producing 23 keys and 521 values [using 2 cores]
om1_1    | 2023-06-16 10:50:27,426 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm3_1   | 2023-06-16 10:49:12,177 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm2_1   | 2023-06-16 10:48:52,615 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-16 10:48:08,802 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0
om2_1    | 2023-06-16 10:48:56,280 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-06-16 10:50:27,426 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-06-16 10:48:53,116 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-16 10:48:09,204 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-80f154b6-d6d9-4d00-b482-76c3516d18ec: Detected pause in JVM or host machine (eg GC): pause of approximately 232804700ns. No GCs detected.
om2_1    | 2023-06-16 10:48:56,315 [Socket Reader #1 for port 9862] INFO ipc.Server: Starting Socket Reader #1 for port 9862
om1_1    | 2023-06-16 10:50:27,440 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:12,192 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
dn4_1    | 2023-06-16 10:49:04,789 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-16 10:48:53,133 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm1_1   | 2023-06-16 10:48:09,258 [main] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: close
om2_1    | 2023-06-16 10:48:56,921 [Listener at om2/9862] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
om1_1    | 2023-06-16 10:50:27,441 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:12,192 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
dn4_1    | 2023-06-16 10:49:04,798 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 does not exist. Creating ...
scm2_1   | 2023-06-16 10:48:53,183 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
recon_1  | 2023-06-16 10:50:24,043 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 2 
scm1_1   | 2023-06-16 10:48:09,266 [main] INFO server.GrpcService: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown server GrpcServerProtocolService now
om2_1    | 2023-06-16 10:48:57,004 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
om1_1    | 2023-06-16 10:50:27,446 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:12,197 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
dn4_1    | 2023-06-16 10:49:04,814 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm2_1   | 2023-06-16 10:48:53,266 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
recon_1  | 2023-06-16 10:50:24,073 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 11, SequenceNumber diff: 29, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-16 10:50:24,073 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 29 records
recon_1  | 2023-06-16 10:50:24,076 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a process run of NSSummaryTaskWithFSO
recon_1  | 2023-06-16 10:50:24,077 [pool-28-thread-1] INFO tasks.NSSummaryTaskWithLegacy: Completed a process run of NSSummaryTaskWithLegacy
recon_1  | 2023-06-16 10:50:24,169 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
recon_1  | 2023-06-16 10:50:24,285 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'process' run of TableCountTask.
om1_1    | 2023-06-16 10:50:27,446 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,815 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/in_use.lock acquired by nodename 7@ee32ea3a212f
om2_1    | 2023-06-16 10:48:57,004 [Listener at om2/9862] INFO impl.MetricsSystemImpl: OzoneManager metrics system started
scm3_1   | 2023-06-16 10:49:12,199 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
scm1_1   | 2023-06-16 10:48:09,272 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: shutdown
recon_1  | 2023-06-16 10:50:24,305 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: ContainerKeyMapperTask successfully processed 3 OM DB update event(s).
om1_1    | 2023-06-16 10:50:27,447 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:53,288 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
dn4_1    | 2023-06-16 10:49:04,871 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155 has been successfully formatted.
om2_1    | 2023-06-16 10:48:57,203 [Listener at om2/9862] INFO om.OzoneManager: OzoneManager RPC server is listening at om2/10.9.0.12:9862
scm3_1   | 2023-06-16 10:49:12,214 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
recon_1  | 2023-06-16 10:50:24,360 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'process' run of FileSizeCountTask.
om1_1    | 2023-06-16 10:50:27,455 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:53,296 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
dn4_1    | 2023-06-16 10:49:04,872 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-E3FF46599155: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
om2_1    | 2023-06-16 10:48:57,210 [Listener at om2/9862] INFO ratis.OzoneManagerRatisServer: Starting OzoneManagerRatisServer om2 at port 9872
scm1_1   | 2023-06-16 10:48:09,279 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-B37B80052E22,id=80f154b6-d6d9-4d00-b482-76c3516d18ec
scm3_1   | 2023-06-16 10:49:12,215 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
recon_1  | 2023-06-16 10:51:24,370 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om1_1    | 2023-06-16 10:50:27,455 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:53,441 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
dn4_1    | 2023-06-16 10:49:04,902 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om2_1    | 2023-06-16 10:48:57,216 [om2-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c does not exist. Creating ...
scm1_1   | 2023-06-16 10:48:09,279 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl
scm3_1   | 2023-06-16 10:49:12,332 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
recon_1  | 2023-06-16 10:51:24,370 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
om1_1    | 2023-06-16 10:50:27,465 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:48:53,449 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
dn4_1    | 2023-06-16 10:49:04,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om2_1    | 2023-06-16 10:48:57,225 [om2-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/in_use.lock acquired by nodename 6@56d7e9fb6d03
scm1_1   | 2023-06-16 10:48:09,439 [main] INFO server.GrpcService: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown server GrpcServerProtocolService successfully
scm3_1   | 2023-06-16 10:49:12,386 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
recon_1  | 2023-06-16 10:51:24,371 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 31 
om1_1    | 2023-06-16 10:50:27,466 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | Container Balancer status:
scm2_1   | Key                            Value
om2_1    | 2023-06-16 10:48:57,275 [om2-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c has been successfully formatted.
om2_1    | 2023-06-16 10:48:57,285 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm3_1   | 2023-06-16 10:49:12,459 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
recon_1  | 2023-06-16 10:51:24,382 [pool-27-thread-1] WARN impl.OzoneManagerServiceProviderImpl: Unable to get and apply delta updates from OM.
recon_1  | INTERNAL_ERROR org.apache.hadoop.ozone.om.exceptions.OMException: Invalid transaction log iterator when getting updates since sequence number 31
om1_1    | 2023-06-16 10:50:27,469 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,908 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | Running                        true
om2_1    | 2023-06-16 10:48:57,315 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-16 10:48:09,463 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO impl.PendingRequests: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-PendingRequests: sendNotLeaderResponses
scm3_1   | 2023-06-16 10:49:12,522 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.handleError(OzoneManagerProtocolClientSideTranslatorPB.java:701)
om1_1    | 2023-06-16 10:50:27,490 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:04,911 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | Container Balancer Configuration values:
om2_1    | 2023-06-16 10:48:57,322 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-16 10:48:09,536 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO impl.StateMachineUpdater: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater: Took a snapshot at index 0
scm3_1   | 2023-06-16 10:49:12,522 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
recon_1  | 	at org.apache.hadoop.ozone.om.protocolPB.OzoneManagerProtocolClientSideTranslatorPB.getDBUpdates(OzoneManagerProtocolClientSideTranslatorPB.java:1822)
om1_1    | 2023-06-16 10:50:27,490 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:04,913 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | Key                                                Value
om2_1    | 2023-06-16 10:48:57,332 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-16 10:48:09,537 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO impl.StateMachineUpdater: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 0
scm3_1   | 2023-06-16 10:49:12,538 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-06-16 10:49:12,557 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:27,493 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:04,914 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm2_1   | Threshold                                          10
om2_1    | 2023-06-16 10:48:57,340 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-16 10:48:09,554 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO impl.StateMachineUpdater: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater: set stopIndex = 0
scm3_1   | 2023-06-16 10:49:12,559 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-16 10:49:13,889 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
om1_1    | 2023-06-16 10:50:27,493 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | Max Datanodes to Involve per Iteration(percent)    20
om2_1    | 2023-06-16 10:48:57,342 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-16 10:48:09,582 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: closes. applyIndex: 0
scm3_1   | 2023-06-16 10:49:13,969 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.innerGetAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:409)
om1_1    | 2023-06-16 10:50:27,493 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:04,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | Max Size to Move per Iteration                     500GB
om2_1    | 2023-06-16 10:48:57,368 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-16 10:48:09,590 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.getAndApplyDeltaUpdatesFromOM(OzoneManagerServiceProviderImpl.java:381)
om1_1    | 2023-06-16 10:50:27,511 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:04,949 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155
dn4_1    | 2023-06-16 10:49:04,949 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
om2_1    | 2023-06-16 10:48:57,369 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm3_1   | 2023-06-16 10:49:14,135 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm1_1   | 2023-06-16 10:48:09,620 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker close()
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.syncDataFromOM(OzoneManagerServiceProviderImpl.java:458)
dn4_1    | 2023-06-16 10:49:04,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
dn4_1    | 2023-06-16 10:49:04,950 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
om1_1    | 2023-06-16 10:50:27,511 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:48:57,396 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new om2@group-D66704EFC61C-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c
scm3_1   | 2023-06-16 10:49:14,321 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-16 10:48:09,634 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-80f154b6-d6d9-4d00-b482-76c3516d18ec: Stopped
recon_1  | 	at org.apache.hadoop.ozone.recon.spi.impl.OzoneManagerServiceProviderImpl.lambda$start$0(OzoneManagerServiceProviderImpl.java:248)
dn4_1    | 2023-06-16 10:49:04,956 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
dn4_1    | 2023-06-16 10:49:04,958 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-16 10:50:27,530 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:48:57,417 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm3_1   | 2023-06-16 10:49:14,324 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | 2023-06-16 10:48:09,634 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
dn4_1    | 2023-06-16 10:49:04,958 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-16 10:49:04,958 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
om1_1    | 2023-06-16 10:50:27,530 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:48:57,423 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-06-16 10:49:14,337 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm1_1   | 2023-06-16 10:48:09,682 [main] INFO server.StorageContainerManager: SCM initialization succeeded. Current cluster id for sd=/data/metadata/scm; cid=CID-82184b77-007c-43b9-9d2d-b37b80052e22; layoutVersion=4; scmId=80f154b6-d6d9-4d00-b482-76c3516d18ec
dn4_1    | 2023-06-16 10:49:04,959 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-16 10:49:04,959 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
om1_1    | 2023-06-16 10:50:27,535 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:48:57,424 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm3_1   | 2023-06-16 10:49:14,479 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-16 10:48:09,807 [shutdown-hook-0] INFO server.StorageContainerManagerStarter: SHUTDOWN_MSG: 
dn4_1    | 2023-06-16 10:49:04,960 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-16 10:49:04,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-16 10:50:27,547 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:48:57,425 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm3_1   | 2023-06-16 10:49:14,502 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm1_1   | /************************************************************
dn4_1    | 2023-06-16 10:49:04,964 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
om1_1    | 2023-06-16 10:50:27,547 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:48:57,429 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm3_1   | 2023-06-16 10:49:14,513 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm1_1   | SHUTDOWN_MSG: Shutting down StorageContainerManager at 9490aade52c0/10.9.0.14
scm2_1   | Max Size Entering Target per Iteration             26GB
dn4_1    | 2023-06-16 10:49:04,965 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
om1_1    | 2023-06-16 10:50:27,556 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:48:57,430 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm3_1   | 2023-06-16 10:49:14,653 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | ************************************************************/
scm1_1   | No '-XX:...' jvm parameters are set. Adding safer GC settings '-XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled' to the OZONE_OPTS
recon_1  | 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
dn4_1    | 2023-06-16 10:49:04,965 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
om2_1    | 2023-06-16 10:48:57,433 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm3_1   | 2023-06-16 10:49:14,657 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
scm2_1   | Max Size Leaving Source per Iteration              26GB
scm2_1   | 
recon_1  | 	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
dn4_1    | 2023-06-16 10:49:04,975 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:48:57,434 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm3_1   | Container Balancer status:
scm2_1   | 2023-06-16 10:48:53,450 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-16 10:48:53,451 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
recon_1  | 	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
dn4_1    | 2023-06-16 10:49:04,976 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: changes role from      null to FOLLOWER at term 0 for startAsFollower
om2_1    | 2023-06-16 10:48:57,473 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm3_1   | Key                            Value
scm2_1   | 2023-06-16 10:48:53,464 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-06-16 10:48:53,480 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
dn4_1    | 2023-06-16 10:49:04,976 [pool-22-thread-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState
om2_1    | 2023-06-16 10:48:57,474 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm3_1   | Running                        true
scm2_1   | 2023-06-16 10:48:53,489 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 does not exist. Creating ...
scm1_1   | OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
recon_1  | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
dn4_1    | 2023-06-16 10:49:04,993 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-E3FF46599155,id=2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:27,556 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:48:57,475 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm3_1   | Container Balancer Configuration values:
scm2_1   | 2023-06-16 10:48:53,509 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/in_use.lock acquired by nodename 8@b1adc14b735f
scm1_1   | 2023-06-16 10:48:19,409 [main] INFO server.StorageContainerManagerStarter: STARTUP_MSG: 
recon_1  | 	at java.base/java.lang.Thread.run(Thread.java:829)
dn4_1    | 2023-06-16 10:49:04,993 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om1_1    | 2023-06-16 10:50:27,556 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:48:57,476 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm3_1   | Key                                                Value
scm2_1   | 2023-06-16 10:48:53,570 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 has been successfully formatted.
scm1_1   | /************************************************************
recon_1  | 2023-06-16 10:51:24,382 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining full snapshot from Ozone Manager
dn4_1    | 2023-06-16 10:49:04,994 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
om1_1    | 2023-06-16 10:50:27,568 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:48:57,497 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm3_1   | Threshold                                          10
scm2_1   | 2023-06-16 10:48:53,573 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | STARTUP_MSG: Starting StorageContainerManager
recon_1  | 2023-06-16 10:51:24,431 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Got new checkpoint from OM : /data/metadata/om.snapshot.db_1686912684382
dn4_1    | 2023-06-16 10:49:04,994 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
om1_1    | 2023-06-16 10:50:27,568 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:48:57,498 [om2-impl-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm3_1   | Max Datanodes to Involve per Iteration(percent)    20
scm2_1   | 2023-06-16 10:48:53,605 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | STARTUP_MSG:   host = 9490aade52c0/10.9.0.14
recon_1  | 2023-06-16 10:51:24,431 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Cleaning up old OM snapshot db at /data/metadata/om.snapshot.db_1686912562612.
dn4_1    | 2023-06-16 10:49:04,996 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
om1_1    | 2023-06-16 10:50:27,571 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:48:57,511 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: start as a follower, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | Max Size to Move per Iteration                     500GB
scm3_1   | Max Size Entering Target per Iteration             26GB
scm2_1   | 2023-06-16 10:48:53,611 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | STARTUP_MSG:   args = []
recon_1  | 2023-06-16 10:51:24,439 [pool-27-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
dn4_1    | 2023-06-16 10:49:05,002 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
om1_1    | 2023-06-16 10:50:27,571 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | Max Size Leaving Source per Iteration              26GB
om2_1    | 2023-06-16 10:48:57,512 [om2-impl-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm2_1   | 2023-06-16 10:48:53,615 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | STARTUP_MSG:   version = 1.3.0
recon_1  | 2023-06-16 10:51:24,439 [pool-27-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
dn4_1    | 2023-06-16 10:49:05,030 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155
dn4_1    | 2023-06-16 10:49:05,040 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-16 10:49:05,856 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:49:06,878 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:57,513 [om2-impl-thread1] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
scm3_1   | 
scm2_1   | 2023-06-16 10:48:53,620 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | STARTUP_MSG:   classpath = /etc/hadoop:/opt/hadoop/share/ozone/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/ozone/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/ozone/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/ozone/lib/jaxb-core-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/istack-commons-runtime-3.0.5.jar:/opt/hadoop/share/ozone/lib/netty-handler-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/commons-validator-1.6.jar:/opt/hadoop/share/ozone/lib/bcpkix-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/ozone/lib/netty-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/ratis-thirdparty-misc-1.0.2.jar:/opt/hadoop/share/ozone/lib/jsr305-3.0.0.jar:/opt/hadoop/share/ozone/lib/j2objc-annotations-1.3.jar:/opt/hadoop/share/ozone/lib/jetty-webapp-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/picocli-4.6.1.jar:/opt/hadoop/share/ozone/lib/javassist-3.21.0-GA.jar:/opt/hadoop/share/ozone/lib/jaxb-api-2.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-common-2.4.0.jar:/opt/hadoop/share/ozone/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-1.6.21.jar:/opt/hadoop/share/ozone/lib/hdds-annotation-processing-1.3.0.jar:/opt/hadoop/share/ozone/lib/log4j-core-2.17.1.jar:/opt/hadoop/share/ozone/lib/commons-logging-1.2.jar:/opt/hadoop/share/ozone/lib/jetty-security-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/accessors-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-metrics-2.4.0.jar:/opt/hadoop/share/ozone/lib/txw2-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-grpc-2.4.0.jar:/opt/hadoop/share/ozone/lib/httpcore-4.4.13.jar:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar:/opt/hadoop/share/ozone/lib/guava-31.1-jre.jar:/opt/hadoop/share/ozone/lib/opentracing-tracerresolver-0.1.8.jar:/opt/hadoop/share/ozone/lib/commons-net-3.6.jar:/opt/hadoop/share/ozone/lib/netty-transport-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-auth-3.3.4.jar:/opt/hadoop/share/ozone/lib/libthrift-0.14.1.jar:/opt/hadoop/share/ozone/lib/stax-ex-1.7.8.jar:/opt/hadoop/share/ozone/lib/error_prone_annotations-2.2.0.jar:/opt/hadoop/share/ozone/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/ozone/lib/httpclient-4.5.13.jar:/opt/hadoop/share/ozone/lib/jaeger-client-1.6.0.jar:/opt/hadoop/share/ozone/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/ozone/lib/hdds-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/jackson-annotations-2.13.4.jar:/opt/hadoop/share/ozone/lib/commons-io-2.11.0.jar:/opt/hadoop/share/ozone/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/ozone/lib/hadoop-annotations-3.3.4.jar:/opt/hadoop/share/ozone/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/ratis-proto-2.4.0.jar:/opt/hadoop/share/ozone/lib/json-smart-2.4.7.jar:/opt/hadoop/share/ozone/lib/snakeyaml-1.32.jar:/opt/hadoop/share/ozone/lib/jetty-io-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/ozone/lib/jetty-server-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/ozone/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/ozone/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/ozone/lib/rocksdbjni-7.4.5.jar:/opt/hadoop/share/ozone/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-xml-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hadoop-common-3.3.4.jar:/opt/hadoop/share/ozone/lib/disruptor-3.4.2.jar:/opt/hadoop/share/ozone/lib/reflections-0.9.11.jar:/opt/hadoop/share/ozone/lib/gson-2.9.0.jar:/opt/hadoop/share/ozone/lib/annotations-13.0.jar:/opt/hadoop/share/ozone/lib/simpleclient_common-0.7.0.jar:/opt/hadoop/share/ozone/lib/jaeger-thrift-1.6.0.jar:/opt/hadoop/share/ozone/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/ozone/lib/bcprov-jdk15on-1.67.jar:/opt/hadoop/share/ozone/lib/commons-configuration2-2.1.1.jar:/opt/hadoop/share/ozone/lib/token-provider-1.0.1.jar:/opt/hadoop/share/ozone/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/ozone/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/ozone/lib/netty-buffer-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/jaeger-tracerresolver-1.6.0.jar:/opt/hadoop/share/ozone/lib/hdds-container-service-1.3.0.jar:/opt/hadoop/share/ozone/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/ozone/lib/commons-lang3-3.7.jar:/opt/hadoop/share/ozone/lib/re2j-1.1.jar:/opt/hadoop/share/ozone/lib/jackson-datatype-jsr310-2.13.4.jar:/opt/hadoop/share/ozone/lib/simpleclient_dropwizard-0.7.0.jar:/opt/hadoop/share/ozone/lib/simpleclient-0.7.0.jar:/opt/hadoop/share/ozone/lib/asm-5.0.4.jar:/opt/hadoop/share/ozone/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/ozone/lib/netty-codec-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-interface-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/failureaccess-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-server-2.4.0.jar:/opt/hadoop/share/ozone/lib/jackson-databind-2.13.4.2.jar:/opt/hadoop/share/ozone/lib/ratis-server-api-2.4.0.jar:/opt/hadoop/share/ozone/lib/okio-2.8.0.jar:/opt/hadoop/share/ozone/lib/jetty-servlet-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/hdds-server-framework-1.3.0.jar:/opt/hadoop/share/ozone/lib/jsp-api-2.1.jar:/opt/hadoop/share/ozone/lib/jsch-0.1.54.jar:/opt/hadoop/share/ozone/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/ozone/lib/reload4j-1.2.22.jar:/opt/hadoop/share/ozone/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop/share/ozone/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/ozone/lib/commons-codec-1.11.jar:/opt/hadoop/share/ozone/lib/hdds-hadoop-dependency-server-1.3.0.jar:/opt/hadoop/share/ozone/lib/commons-pool2-2.6.0.jar:/opt/hadoop/share/ozone/lib/netty-resolver-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hdds-erasurecode-1.3.0.jar:/opt/hadoop/share/ozone/lib/opentracing-api-0.33.0.jar:/opt/hadoop/share/ozone/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/ozone/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/ozone/lib/jetty-util-ajax-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/opentracing-noop-0.33.0.jar:/opt/hadoop/share/ozone/lib/netty-transport-native-unix-common-4.1.79.Final.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-client-3.3.4.jar:/opt/hadoop/share/ozone/lib/jersey-core-1.19.jar:/opt/hadoop/share/ozone/lib/commons-compress-1.21.jar:/opt/hadoop/share/ozone/lib/jaxb-runtime-2.3.0.1.jar:/opt/hadoop/share/ozone/lib/hadoop-hdfs-3.3.4.jar:/opt/hadoop/share/ozone/lib/jackson-core-2.13.4.jar:/opt/hadoop/share/ozone/lib/opentracing-util-0.33.0.jar:/opt/hadoop/share/ozone/lib/ratis-client-2.4.0.jar:/opt/hadoop/share/ozone/lib/hdds-interface-client-1.3.0.jar:/opt/hadoop/share/ozone/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/ozone/lib/commons-cli-1.2.jar:/opt/hadoop/share/ozone/lib/log4j-api-2.17.1.jar:/opt/hadoop/share/ozone/lib/javax.annotation-api-1.2.jar:/opt/hadoop/share/ozone/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/ozone/lib/okhttp-4.9.3.jar:/opt/hadoop/share/ozone/lib/checker-qual-3.12.0.jar:/opt/hadoop/share/ozone/lib/hdds-config-1.3.0.jar:/opt/hadoop/share/ozone/lib/stax2-api-3.1.4.jar:/opt/hadoop/share/ozone/lib/commons-digester-1.8.1.jar:/opt/hadoop/share/ozone/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/ozone/lib/jaeger-core-1.6.0.jar:/opt/hadoop/share/ozone/lib/commons-text-1.4.jar:/opt/hadoop/share/ozone/lib/jersey-servlet-1.19.jar:/opt/hadoop/share/ozone/lib/jetty-http-9.4.49.v20220914.jar:/opt/hadoop/share/ozone/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/ozone/lib/ratis-netty-2.4.0.jar:/opt/hadoop/share/ozone/lib/FastInfoset-1.2.13.jar:/opt/hadoop/share/ozone/lib/hdds-interface-admin-1.3.0.jar:/opt/hadoop/share/ozone/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/ozone/lib/jersey-server-1.19.jar:/opt/hadoop/share/ozone/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/ozone/web:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar
scm1_1   | STARTUP_MSG:   build = https://github.com/apache/ozone.git/d0d18a3bff64b90f5f0755edb6003301049ffb32 ; compiled by 'micahzhao' on 2022-12-10T13:23Z
recon_1  | 2023-06-16 10:51:24,490 [pool-27-thread-1] INFO recovery.ReconOmMetadataManagerImpl: Created OM DB handle from snapshot at /data/metadata/om.snapshot.db_1686912684382.
dn4_1    | 2023-06-16 10:49:07,879 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:48:57,532 [om2-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
scm3_1   | 2023-06-16 10:49:14,658 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-16 10:48:53,630 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | STARTUP_MSG:   java = 11.0.14.1
om1_1    | 2023-06-16 10:50:27,572 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 2023-06-16 10:51:24,516 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Calling reprocess on Recon tasks.
dn4_1    | 2023-06-16 10:49:08,606 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155.
om2_1    | 2023-06-16 10:48:57,561 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm3_1   | 2023-06-16 10:49:14,658 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 2023-06-16 10:48:53,654 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | ************************************************************/
om1_1    | 2023-06-16 10:50:27,581 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 2023-06-16 10:51:24,516 [pool-73-thread-1] INFO tasks.NSSummaryTaskWithFSO: Completed a reprocess run of NSSummaryTaskWithFSO
dn4_1    | 2023-06-16 10:49:08,606 [Command processor thread] INFO server.RaftServer: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: addNew group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] returns group-42EFE7C8DED3:java.util.concurrent.CompletableFuture@68cf200e[Not completed]
om2_1    | 2023-06-16 10:48:57,562 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm3_1   | 2023-06-16 10:49:14,668 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
scm2_1   | 2023-06-16 10:48:53,658 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-16 10:48:19,435 [main] INFO server.StorageContainerManagerStarter: registered UNIX signal handlers for [TERM, HUP, INT]
om1_1    | 2023-06-16 10:50:27,581 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 2023-06-16 10:51:24,518 [pool-73-thread-2] INFO tasks.NSSummaryTaskWithLegacy: Completed a reprocess run of NSSummaryTaskWithLegacy
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: new RaftServerImpl for group-42EFE7C8DED3:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER] with ContainerStateMachine:uninitialized
om2_1    | 2023-06-16 10:48:57,583 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm3_1   | 2023-06-16 10:49:14,672 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
scm2_1   | 2023-06-16 10:48:53,670 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22
scm1_1   | 2023-06-16 10:48:19,522 [main] WARN server.ServerUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-16 10:48:19,560 [main] INFO ha.SCMHANodeDetails: ServiceID for StorageContainerManager is null
recon_1  | 2023-06-16 10:51:24,611 [pool-28-thread-1] INFO tasks.TableCountTask: Completed a 'reprocess' run of TableCountTask.
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5s (custom)
om2_1    | 2023-06-16 10:48:57,584 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 400000 (default)
scm3_1   | 2023-06-16 10:49:14,675 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 does not exist. Creating ...
scm2_1   | 2023-06-16 10:48:53,707 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-16 10:48:19,582 [main] INFO ha.SCMHANodeDetails: ozone.scm.default.service.id is not defined, falling back to ozone.scm.service.ids to find serviceID for StorageContainerManager if it is HA enabled cluster
om1_1    | 2023-06-16 10:50:27,584 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 2023-06-16 10:51:24,611 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Starting a 'reprocess' run of ContainerKeyMapperTask.
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-06-16 10:48:57,585 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-06-16 10:49:14,681 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/in_use.lock acquired by nodename 6@592901d477f4
scm2_1   | 2023-06-16 10:48:53,709 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm1_1   | 2023-06-16 10:48:19,630 [main] INFO ha.SCMHANodeDetails: Found matching SCM address with SCMServiceId: scmservice, SCMNodeId: scm1, RPC Address: scm1:9894 and Ratis port: 9894
om1_1    | 2023-06-16 10:50:27,584 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 2023-06-16 10:51:24,612 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: KEY_CONTAINER Table is empty, initializing from CONTAINER_KEY Table ...
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-06-16 10:48:57,586 [om2-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = true (custom)
scm3_1   | 2023-06-16 10:49:14,715 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO storage.RaftStorage: Storage directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22 has been successfully formatted.
scm2_1   | 2023-06-16 10:48:53,713 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm1_1   | 2023-06-16 10:48:19,631 [main] INFO ha.SCMHANodeDetails: Setting configuration key ozone.scm.address with value of key ozone.scm.address.scmservice.scm1: scm1
om1_1    | 2023-06-16 10:50:27,584 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 2023-06-16 10:51:24,612 [pool-28-thread-1] INFO impl.ReconContainerMetadataManagerImpl: It took 0.0 seconds to initialized 0 records to KEY_CONTAINER table
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 300s (custom)
om2_1    | 2023-06-16 10:48:57,643 [Listener at om2/9862] INFO server.RaftServer: om2: start RPC server
scm3_1   | 2023-06-16 10:49:14,730 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm2_1   | 2023-06-16 10:48:53,727 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-16 10:48:20,605 [main] WARN utils.HAUtils: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:50:27,595 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 2023-06-16 10:51:24,637 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: Completed 'reprocess' of ContainerKeyMapperTask.
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-16 10:48:57,906 [Listener at om2/9862] INFO server.GrpcService: om2: GrpcService started, listening on 9872
scm3_1   | 2023-06-16 10:49:14,746 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm2_1   | 2023-06-16 10:48:53,734 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-16 10:48:20,862 [main] WARN db.DBStoreBuilder: ozone.scm.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
om1_1    | 2023-06-16 10:50:27,596 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 2023-06-16 10:51:24,637 [pool-28-thread-1] INFO tasks.ContainerKeyMapperTask: It took me 0.025 seconds to process 3 keys.
dn4_1    | 2023-06-16 10:49:08,609 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
om2_1    | 2023-06-16 10:48:57,908 [Listener at om2/9862] INFO om.OzoneManager: Version File has different layout version (3) than OM DB (null). That is expected if this OM has never been finalized to a newer layout version.
scm3_1   | 2023-06-16 10:49:14,747 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm2_1   | 2023-06-16 10:48:53,740 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-16 10:48:21,238 [main] INFO net.NodeSchemaLoader: Loading schema from [file:/etc/hadoop/network-topology-default.xml, jar:file:/opt/hadoop/share/ozone/lib/hdds-common-1.3.0.jar!/network-topology-default.xml]
om1_1    | 2023-06-16 10:50:27,603 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 2023-06-16 10:51:24,641 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Deleted 2 records from "FILE_COUNT_BY_SIZE"
dn4_1    | 2023-06-16 10:49:08,610 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: ConfigurationManager, init=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null, confs=<EMPTY_MAP>
om2_1    | 2023-06-16 10:48:57,921 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Started
scm3_1   | 2023-06-16 10:49:14,749 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm2_1   | 2023-06-16 10:48:53,740 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-16 10:48:21,239 [main] INFO net.NodeSchemaLoader: Loading network topology layer schema file
om1_1    | 2023-06-16 10:50:27,605 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 2023-06-16 10:51:24,646 [pool-28-thread-1] INFO tasks.FileSizeCountTask: Completed a 'reprocess' run of FileSizeCountTask.
dn4_1    | 2023-06-16 10:49:08,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/ratis] (custom)
om2_1    | 2023-06-16 10:48:58,079 [Listener at om2/9862] INFO http.BaseHttpServer: Starting Web-server for ozoneManager at: http://0.0.0.0:9874
scm3_1   | 2023-06-16 10:49:14,752 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-06-16 10:48:53,740 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-16 10:48:21,354 [main] INFO metrics.MetricRegistries: Loaded MetricRegistries class org.apache.ratis.metrics.impl.MetricRegistriesImpl
om1_1    | 2023-06-16 10:50:27,606 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 2023-06-16 10:52:24,651 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om2_1    | 2023-06-16 10:48:58,080 [Listener at om2/9862] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm3_1   | 2023-06-16 10:49:14,757 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-16 10:48:53,772 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm2_1   | 2023-06-16 10:48:53,778 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
om1_1    | 2023-06-16 10:50:27,613 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
recon_1  | 2023-06-16 10:52:24,652 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
dn4_1    | 2023-06-16 10:49:08,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
om2_1    | 2023-06-16 10:48:58,166 [Listener at om2/9862] INFO util.log: Logging initialized @24792ms to org.eclipse.jetty.util.log.Slf4jLog
scm3_1   | 2023-06-16 10:49:14,771 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-16 10:48:53,779 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:48:21,390 [main] INFO ha.SCMRatisServerImpl: starting Raft server for scm:80f154b6-d6d9-4d00-b482-76c3516d18ec
scm1_1   | 2023-06-16 10:48:21,534 [main] INFO server.RaftServer: raft.rpc.type = GRPC (default)
om1_1    | 2023-06-16 10:50:27,613 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:08,610 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-06-16 10:48:58,697 [Listener at om2/9862] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm3_1   | 2023-06-16 10:49:14,772 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-06-16 10:48:53,784 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
recon_1  | 2023-06-16 10:52:24,652 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 34 
scm1_1   | 2023-06-16 10:48:21,688 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.host = null (fallback to raft.grpc.server.host)
om1_1    | 2023-06-16 10:50:27,616 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:08,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 300s (custom)
om2_1    | 2023-06-16 10:48:58,756 [Listener at om2/9862] INFO http.HttpRequestLog: Http request log for http.requests.ozoneManager is not defined
scm3_1   | 2023-06-16 10:49:14,783 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22
scm2_1   | 2023-06-16 10:48:53,831 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-16 10:52:24,657 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
recon_1  | 2023-06-16 10:52:24,657 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
om1_1    | 2023-06-16 10:50:27,617 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 600000ms (custom)
om2_1    | 2023-06-16 10:48:58,821 [Listener at om2/9862] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm3_1   | 2023-06-16 10:49:14,783 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm2_1   | 2023-06-16 10:48:53,832 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
recon_1  | 2023-06-16 10:53:24,659 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
scm1_1   | 2023-06-16 10:48:21,700 [main] INFO grpc.GrpcConfigKeys: raft.grpc.admin.port = 9894 (fallback to raft.grpc.server.port)
om1_1    | 2023-06-16 10:50:27,617 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,611 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
dn4_1    | 2023-06-16 10:49:08,612 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
dn4_1    | 2023-06-16 10:49:08,613 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
dn4_1    | 2023-06-16 10:49:08,614 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
dn4_1    | 2023-06-16 10:49:08,614 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm1_1   | 2023-06-16 10:48:21,700 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.host = null (fallback to raft.grpc.server.host)
scm2_1   | 2023-06-16 10:48:53,851 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
om2_1    | 2023-06-16 10:48:58,843 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context ozoneManager
om1_1    | 2023-06-16 10:50:27,625 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:14,784 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
dn4_1    | 2023-06-16 10:49:08,614 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm1_1   | 2023-06-16 10:48:21,701 [main] INFO grpc.GrpcConfigKeys: raft.grpc.client.port = 9894 (fallback to raft.grpc.server.port)
recon_1  | 2023-06-16 10:53:24,660 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm2_1   | 2023-06-16 10:48:53,851 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: changes role from      null to FOLLOWER at term 0 for startInitializing
om2_1    | 2023-06-16 10:48:58,858 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om1_1    | 2023-06-16 10:50:27,625 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:14,785 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
dn4_1    | 2023-06-16 10:49:08,614 [pool-22-thread-1] INFO storage.RaftStorageDirectory: The storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 does not exist. Creating ...
scm1_1   | 2023-06-16 10:48:21,701 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.host = null (default)
recon_1  | 2023-06-16 10:53:24,660 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 34 
scm2_1   | 2023-06-16 10:48:53,855 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B37B80052E22,id=af8a130f-204e-4987-9eb3-c559054693bc
om2_1    | 2023-06-16 10:48:58,859 [Listener at om2/9862] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-06-16 10:50:27,630 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:14,795 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
dn4_1    | 2023-06-16 10:49:08,632 [pool-22-thread-1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/in_use.lock acquired by nodename 7@ee32ea3a212f
scm1_1   | 2023-06-16 10:48:21,701 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.port = 9894 (custom)
recon_1  | 2023-06-16 10:53:24,669 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
scm2_1   | 2023-06-16 10:48:53,860 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
om2_1    | 2023-06-16 10:48:59,239 [Listener at om2/9862] INFO http.HttpServer2: Jetty bound to port 9874
om1_1    | 2023-06-16 10:50:27,630 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:14,795 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
dn4_1    | 2023-06-16 10:49:08,636 [pool-22-thread-1] INFO storage.RaftStorage: Storage directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3 has been successfully formatted.
scm1_1   | 2023-06-16 10:48:21,702 [main] INFO server.GrpcService: raft.grpc.message.size.max = 32m (=33554432) (custom)
recon_1  | 2023-06-16 10:53:24,669 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
scm2_1   | 2023-06-16 10:48:53,864 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
om2_1    | 2023-06-16 10:48:59,240 [Listener at om2/9862] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-06-16 10:48:59,508 [Listener at om2/9862] INFO server.session: DefaultSessionIdManager workerName=node0
scm3_1   | 2023-06-16 10:49:14,796 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-16 10:49:08,638 [pool-22-thread-1] INFO ratis.ContainerStateMachine: group-42EFE7C8DED3: The snapshot info is null. Setting the last applied indexto:(t:0, i:~)
scm1_1   | 2023-06-16 10:48:21,704 [main] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
recon_1  | 2023-06-16 10:53:36,075 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 16 milliseconds to process 0 existing database records.
om2_1    | 2023-06-16 10:48:59,516 [Listener at om2/9862] INFO server.session: No SessionScavenger set, using defaults
om1_1    | 2023-06-16 10:50:27,631 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:53,867 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm3_1   | 2023-06-16 10:49:14,797 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-16 10:49:08,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
scm1_1   | 2023-06-16 10:48:21,705 [main] INFO server.GrpcService: raft.grpc.flow.control.window = 1MB (=1048576) (default)
recon_1  | 2023-06-16 10:53:36,080 [ContainerHealthTask] INFO fsck.ContainerHealthTask: Container Health task thread took 5 milliseconds for processing 2 containers.
om2_1    | 2023-06-16 10:48:59,522 [Listener at om2/9862] INFO server.session: node0 Scavenging every 600000ms
om1_1    | 2023-06-16 10:50:27,641 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:53,870 [af8a130f-204e-4987-9eb3-c559054693bc-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm3_1   | 2023-06-16 10:49:14,798 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
dn4_1    | 2023-06-16 10:49:08,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
scm1_1   | 2023-06-16 10:48:21,707 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
recon_1  | 2023-06-16 10:53:36,342 [PipelineSyncTask] INFO scm.ReconPipelineManager: Recon has 7 pipelines in house.
om2_1    | 2023-06-16 10:48:59,621 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5f3b84bd{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-06-16 10:50:27,641 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:53,875 [Listener at 0.0.0.0/9860] INFO server.RaftServer: af8a130f-204e-4987-9eb3-c559054693bc: start RPC server
scm3_1   | 2023-06-16 10:49:14,822 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
dn4_1    | 2023-06-16 10:49:08,638 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
scm1_1   | 2023-06-16 10:48:21,722 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
recon_1  | 2023-06-16 10:53:36,346 [PipelineSyncTask] INFO scm.PipelineSyncTask: Pipeline sync Thread took 8 milliseconds.
om2_1    | 2023-06-16 10:48:59,627 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@5323999f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,AVAILABLE}
om1_1    | 2023-06-16 10:50:27,644 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:48:53,971 [Listener at 0.0.0.0/9860] INFO server.GrpcService: af8a130f-204e-4987-9eb3-c559054693bc: GrpcService started, listening on 9894
scm3_1   | 2023-06-16 10:49:14,823 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
dn4_1    | 2023-06-16 10:49:08,639 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
scm1_1   | 2023-06-16 10:48:21,725 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.cached = true (default)
recon_1  | 2023-06-16 10:54:24,671 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om2_1    | 2023-06-16 10:49:00,091 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: receive requestVote(ELECTION, om1, group-D66704EFC61C, 1, (t:0, i:~))
om1_1    | 2023-06-16 10:50:27,644 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:54,032 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-af8a130f-204e-4987-9eb3-c559054693bc: Started
scm3_1   | 2023-06-16 10:49:14,823 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
dn4_1    | 2023-06-16 10:49:08,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm1_1   | 2023-06-16 10:48:21,730 [main] INFO grpc.GrpcConfigKeys: raft.grpc.server.async.request.thread.pool.size = 32 (default)
scm1_1   | 2023-06-16 10:48:22,198 [main] INFO impl.DataStreamServerImpl: raft.datastream.type = DISABLED (default)
om2_1    | 2023-06-16 10:49:00,152 [grpc-default-executor-0] INFO impl.VoteContext: om2@group-D66704EFC61C-FOLLOWER: accept ELECTION from om1: our priority 0 <= candidate's priority 0
om1_1    | 2023-06-16 10:50:27,645 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:54,120 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863, nodeId=scm3,nodeAddress=scm3/10.9.0.16:9863]
scm3_1   | 2023-06-16 10:49:14,825 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
dn4_1    | 2023-06-16 10:49:08,642 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-06-16 10:48:22,202 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.cached = true (default)
recon_1  | 2023-06-16 10:54:24,672 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
om2_1    | 2023-06-16 10:49:00,162 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C: changes role from  FOLLOWER to FOLLOWER at term 1 for candidate:om1
om1_1    | 2023-06-16 10:50:27,663 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:57,508 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: receive installSnapshot: 80f154b6-d6d9-4d00-b482-76c3516d18ec->af8a130f-204e-4987-9eb3-c559054693bc#0-t2,notify:(t:1, i:0)
scm3_1   | 2023-06-16 10:49:14,841 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-16 10:49:08,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm1_1   | 2023-06-16 10:48:22,207 [main] INFO server.RaftServerConfigKeys: raft.server.threadpool.proxy.size = 0 (default)
recon_1  | 2023-06-16 10:54:24,672 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 34 
om2_1    | 2023-06-16 10:49:00,162 [grpc-default-executor-0] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
scm2_1   | 2023-06-16 10:48:57,531 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
om1_1    | 2023-06-16 10:50:27,664 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:14,841 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
dn4_1    | 2023-06-16 10:49:08,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm1_1   | 2023-06-16 10:48:22,207 [main] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
recon_1  | 2023-06-16 10:54:24,679 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
om2_1    | 2023-06-16 10:49:00,163 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState was interrupted
scm2_1   | 2023-06-16 10:48:57,531 [grpc-default-executor-0] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: change Leader from null to 80f154b6-d6d9-4d00-b482-76c3516d18ec at term 2 for installSnapshot, leader elected after 11154ms
om1_1    | 2023-06-16 10:50:27,672 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:14,853 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: start with initializing state, conf=-1: peers:[]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:08,643 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: new 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3
dn4_1    | 2023-06-16 10:49:08,643 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 4294967296 (custom)
recon_1  | 2023-06-16 10:54:24,679 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
om2_1    | 2023-06-16 10:49:00,163 [grpc-default-executor-0] INFO impl.RoleInfo: om2: start om2@group-D66704EFC61C-FollowerState
scm2_1   | 2023-06-16 10:48:57,546 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: Received notification to install snapshot at index 0
om1_1    | 2023-06-16 10:50:27,672 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 1024 (custom)
scm1_1   | 2023-06-16 10:48:22,207 [main] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
scm3_1   | 2023-06-16 10:49:14,855 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: changes role from      null to FOLLOWER at term 0 for startInitializing
recon_1  | 2023-06-16 10:54:39,046 [Finalizer] WARN managed.ManagedRocksObjectUtils: RocksIterator is not closed properly
om2_1    | 2023-06-16 10:49:00,187 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-06-16 10:48:57,547 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
om1_1    | 2023-06-16 10:50:27,672 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 1048576 (custom)
scm1_1   | 2023-06-16 10:48:22,218 [main] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm3_1   | 2023-06-16 10:49:14,859 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B37B80052E22,id=d5edb8ee-91fd-4f84-a835-71ed6541c25d
recon_1  | 2023-06-16 10:55:24,681 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Syncing data from Ozone Manager.
om2_1    | 2023-06-16 10:49:00,202 [om2@group-D66704EFC61C-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-06-16 10:48:57,957 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set new configuration index: 1
om1_1    | 2023-06-16 10:50:27,678 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 16384 (custom)
scm1_1   | 2023-06-16 10:48:22,232 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: found a subdirectory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22
om2_1    | 2023-06-16 10:49:00,320 [grpc-default-executor-0] INFO server.RaftServer$Division: om2@group-D66704EFC61C replies to ELECTION vote request: om1<-om2#0:OK-t1. Peer's state: om2@group-D66704EFC61C:t1, leader=null, voted=om1, raftlog=Memoized:om2@group-D66704EFC61C-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[om1|rpc:om1:9872|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | configurationEntry {
om1_1    | 2023-06-16 10:50:27,678 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
recon_1  | 2023-06-16 10:55:24,682 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Obtaining delta updates from Ozone Manager
scm3_1   | 2023-06-16 10:49:14,866 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
scm1_1   | 2023-06-16 10:48:22,252 [main] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: addNew group-B37B80052E22:[] returns group-B37B80052E22:java.util.concurrent.CompletableFuture@2b8bd14b[Not completed]
om2_1    | 2023-06-16 10:49:01,122 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: change Leader from null to om1 at term 1 for appendEntries, leader elected after 7136ms
scm2_1   |   peers {
om1_1    | 2023-06-16 10:50:27,690 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
recon_1  | 2023-06-16 10:55:24,682 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: OriginalFromSequenceNumber : 34 
scm3_1   | 2023-06-16 10:49:14,867 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
scm1_1   | 2023-06-16 10:48:22,280 [pool-16-thread-1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec: new RaftServerImpl for group-B37B80052E22:[] with SCMStateMachine:uninitialized
om2_1    | 2023-06-16 10:49:01,158 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: set configuration 0: peers:[om1|rpc:om1:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om3|rpc:om3:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, om2|rpc:om2:9872|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   |     id: "80f154b6-d6d9-4d00-b482-76c3516d18ec"
om1_1    | 2023-06-16 10:50:27,691 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
recon_1  | 2023-06-16 10:55:24,690 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Number of updates received from OM : 0, SequenceNumber diff: 0, SequenceNumber Lag from OM 0.
scm3_1   | 2023-06-16 10:49:14,871 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.min = 5000ms (custom)
om2_1    | 2023-06-16 10:49:01,264 [om2-server-thread1] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: Starting segment from index:0
scm2_1   |     address: "scm1:9894"
om1_1    | 2023-06-16 10:50:27,704 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
recon_1  | 2023-06-16 10:55:24,691 [pool-27-thread-1] INFO impl.OzoneManagerServiceProviderImpl: Delta updates received from OM : 1 loops, 0 records
scm3_1   | 2023-06-16 10:49:14,872 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
dn4_1    | 2023-06-16 10:49:08,644 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.timeout.max = 5200ms (custom)
om2_1    | 2023-06-16 10:49:01,894 [Listener at om2/9862] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@5d7f1e59{ozoneManager,/,file:///tmp/jetty-0_0_0_0-9874-ozone-manager-1_3_0_jar-_-any-2746917904337249817/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
scm2_1   |     startupRole: FOLLOWER
om1_1    | 2023-06-16 10:50:27,706 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:14,886 [Listener at 0.0.0.0/9860] INFO server.RaftServer: d5edb8ee-91fd-4f84-a835-71ed6541c25d: start RPC server
dn4_1    | 2023-06-16 10:49:08,650 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 1048576 (custom)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.sleep.time = 25ms (default)
om2_1    | 2023-06-16 10:49:01,967 [Listener at om2/9862] INFO server.AbstractConnector: Started ServerConnector@2d4fb0d8{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
scm2_1   |   }
scm2_1   | }
scm3_1   | 2023-06-16 10:49:15,036 [Listener at 0.0.0.0/9860] INFO server.GrpcService: d5edb8ee-91fd-4f84-a835-71ed6541c25d: GrpcService started, listening on 9894
dn4_1    | 2023-06-16 10:49:08,651 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.rpc.slowness.timeout = 120000ms (custom)
om2_1    | 2023-06-16 10:49:01,967 [Listener at om2/9862] INFO server.Server: Started @28594ms
scm2_1   |  from snapshot
om1_1    | 2023-06-16 10:50:27,706 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:15,053 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-d5edb8ee-91fd-4f84-a835-71ed6541c25d: Started
dn4_1    | 2023-06-16 10:49:08,654 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.leaderelection.leader.step-down.wait-time = 10s (default)
om2_1    | 2023-06-16 10:49:02,006 [Listener at om2/9862] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm2_1   | 2023-06-16 10:48:57,971 [grpc-default-executor-0] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,712 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:15,096 [Listener at 0.0.0.0/9860] INFO proxy.SCMBlockLocationFailoverProxyProvider: Created block location fail-over proxy with 2 nodes: [nodeId=scm2,nodeAddress=scm2/10.9.0.15:9863, nodeId=scm1,nodeAddress=scm1/10.9.0.14:9863]
dn4_1    | 2023-06-16 10:49:08,654 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = true (custom)
scm1_1   | 2023-06-16 10:48:22,281 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.sleep.deviation.threshold = 300ms (default)
scm2_1   | 2023-06-16 10:48:57,996 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: reply installSnapshot: 80f154b6-d6d9-4d00-b482-76c3516d18ec<-af8a130f-204e-4987-9eb3-c559054693bc#0:OK-t0,ALREADY_INSTALLED
om2_1    | 2023-06-16 10:49:02,007 [Listener at om2/9862] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-16 10:50:27,713 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,042 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: receive installSnapshot: 80f154b6-d6d9-4d00-b482-76c3516d18ec->d5edb8ee-91fd-4f84-a835-71ed6541c25d#0-t2,notify:(t:1, i:0)
dn4_1    | 2023-06-16 10:49:08,664 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-16 10:48:22,301 [pool-16-thread-1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: ConfigurationManager, init=-1: peers:[]|listeners:[], old=null, confs=<EMPTY_MAP>
scm2_1   | 2023-06-16 10:48:58,080 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: af8a130f-204e-4987-9eb3-c559054693bc: Completed INSTALL_SNAPSHOT, lastRequest: 80f154b6-d6d9-4d00-b482-76c3516d18ec->af8a130f-204e-4987-9eb3-c559054693bc#0-t2,notify:(t:1, i:0)
om2_1    | 2023-06-16 10:49:02,008 [Listener at om2/9862] INFO http.BaseHttpServer: HTTP server of ozoneManager listening at http://0.0.0.0:9874
om1_1    | 2023-06-16 10:50:27,713 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,047 [grpc-default-executor-0] INFO ha.SCMStateMachine: leader changed, yet current SCM is still follower.
dn4_1    | 2023-06-16 10:49:08,664 [pool-22-thread-1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-16 10:48:22,318 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.dir = [/data/metadata/scm-ha] (custom)
scm2_1   | 2023-06-16 10:48:58,333 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO impl.RoleInfo: af8a130f-204e-4987-9eb3-c559054693bc: start af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-FollowerState
om2_1    | 2023-06-16 10:49:02,023 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/5cb24680-b9e7-3c90-a862-d66704efc61c/current/log_inprogress_0
om1_1    | 2023-06-16 10:50:27,718 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,718 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:08,669 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: start as a follower, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:22,321 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.corruption.policy = EXCEPTION (default)
scm2_1   | 2023-06-16 10:48:58,349 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
om2_1    | 2023-06-16 10:49:02,032 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-16 10:49:16,047 [grpc-default-executor-0] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: change Leader from null to 80f154b6-d6d9-4d00-b482-76c3516d18ec at term 2 for installSnapshot, leader elected after 5076ms
om1_1    | 2023-06-16 10:50:27,722 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:08,669 [pool-22-thread-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: changes role from      null to FOLLOWER at term 0 for startAsFollower
scm1_1   | 2023-06-16 10:48:22,323 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.storage.free-space.min = 0MB (=0) (default)
om2_1    | 2023-06-16 10:49:02,133 [IPC Server listener on 9862] INFO ipc.Server: IPC Server listener on 9862: starting
scm3_1   | 2023-06-16 10:49:16,053 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: Received notification to install snapshot at index 0
om1_1    | 2023-06-16 10:50:27,722 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,670 [pool-22-thread-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState
scm1_1   | 2023-06-16 10:48:22,342 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.notification.no-leader.timeout = 60s (default)
scm2_1   | 2023-06-16 10:48:58,357 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: inconsistency entries. Reply:80f154b6-d6d9-4d00-b482-76c3516d18ec<-af8a130f-204e-4987-9eb3-c559054693bc#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om2_1    | 2023-06-16 10:49:02,172 [Listener at om2/9862] INFO om.OzoneManager: Trash Interval set to 0. Files deleted won't move to trash
scm3_1   | 2023-06-16 10:49:16,054 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: InstallSnapshot notification result: ALREADY_INSTALLED, current snapshot index: -1
om1_1    | 2023-06-16 10:50:27,722 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,670 [pool-22-thread-1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-42EFE7C8DED3,id=2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
scm1_1   | 2023-06-16 10:48:22,348 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.expirytime = 60000ms (default)
scm2_1   | 2023-06-16 10:48:58,398 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
om2_1    | 2023-06-16 10:49:02,212 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7cc1f72c] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm3_1   | 2023-06-16 10:49:16,159 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set new configuration index: 19
om1_1    | 2023-06-16 10:50:27,729 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:08,670 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-16 10:48:22,351 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.retrycache.statistics.expirytime = 100?s (default)
scm2_1   | 2023-06-16 10:48:58,400 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: inconsistency entries. Reply:80f154b6-d6d9-4d00-b482-76c3516d18ec<-af8a130f-204e-4987-9eb3-c559054693bc#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om2_1    | 2023-06-16 10:49:05,011 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Received Configuration change notification from Ratis. New Peer list:
scm3_1   | configurationEntry {
om1_1    | 2023-06-16 10:50:27,730 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:08,670 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 10000 (custom)
scm1_1   | 2023-06-16 10:48:22,591 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
scm2_1   | 2023-06-16 10:48:58,430 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | [id: "om1"
scm3_1   |   peers {
om1_1    | 2023-06-16 10:50:27,733 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:08,670 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = 5 (custom)
scm1_1   | 2023-06-16 10:48:22,592 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.cached = true (default)
scm2_1   | 2023-06-16 10:48:58,430 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | address: "om1:9872"
scm3_1   |     id: "af8a130f-204e-4987-9eb3-c559054693bc"
om1_1    | 2023-06-16 10:50:27,733 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,675 [pool-22-thread-1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-16 10:48:22,592 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.server.size = 0 (default)
scm2_1   | 2023-06-16 10:48:58,437 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: Starting segment from index:0
om2_1    | startupRole: FOLLOWER
scm3_1   |     address: "scm2:9894"
om1_1    | 2023-06-16 10:50:27,733 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,676 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-16 10:48:22,593 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.cached = true (default)
scm2_1   | 2023-06-16 10:48:58,527 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
om2_1    | , id: "om3"
scm3_1   |     startupRole: FOLLOWER
om1_1    | 2023-06-16 10:50:27,739 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:08,679 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-16 10:48:22,594 [pool-16-thread-1] INFO server.RaftServerConfigKeys: raft.server.threadpool.client.size = 0 (default)
scm2_1   | 2023-06-16 10:48:58,584 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | address: "om3:9872"
scm3_1   |   }
om1_1    | 2023-06-16 10:50:27,739 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:08,679 [Command processor thread] INFO ratis.XceiverServerRatis: Created group PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3
scm1_1   | 2023-06-16 10:48:22,600 [main] INFO ha.SCMSnapshotProvider: Initializing SCM Snapshot Provider
scm2_1   | 2023-06-16 10:48:58,587 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread1] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | startupRole: FOLLOWER
scm3_1   |   peers {
om1_1    | 2023-06-16 10:50:27,742 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:08,890 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
scm1_1   | 2023-06-16 10:48:22,600 [main] WARN server.ServerUtils: Storage directory for Ratis is not configured. It is a good idea to map this to an SSD disk. Falling back to ozone.metadata.dirs
scm1_1   | 2023-06-16 10:48:22,600 [main] WARN ha.SCMHAUtils: SCM snapshot dir is not configured. Falling back to ozone.metadata.dirs config
om2_1    | , id: "om2"
scm3_1   |     id: "80f154b6-d6d9-4d00-b482-76c3516d18ec"
om1_1    | 2023-06-16 10:50:27,742 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:08,918 [Command processor thread] INFO commandhandler.CreatePipelineCommandHandler: Created Pipeline RATIS THREE PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3.
scm1_1   | 2023-06-16 10:48:22,628 [main] INFO upgrade.AbstractLayoutVersionManager: Initializing Layout version manager with metadata layout = DATANODE_SCHEMA_V3 (version = 4), software layout = DATANODE_SCHEMA_V3 (version = 4)
scm2_1   | 2023-06-16 10:48:58,792 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0
om2_1    | address: "om2:9872"
scm3_1   |     address: "scm1:9894"
om1_1    | 2023-06-16 10:50:27,742 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:09,693 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO impl.FollowerState: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5143697060ns, electionTimeout:5093ms
scm1_1   | 2023-06-16 10:48:22,808 [main] INFO reflections.Reflections: Reflections took 136 ms to scan 3 urls, producing 112 keys and 252 values 
scm2_1   | 2023-06-16 10:48:58,806 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0 to /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_0-0
om2_1    | startupRole: FOLLOWER
scm3_1   |     startupRole: FOLLOWER
om1_1    | 2023-06-16 10:50:27,747 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:09,693 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState
scm1_1   | 2023-06-16 10:48:22,864 [main] INFO ha.SequenceIdGenerator: upgrade localId to 111677748019200000
scm2_1   | 2023-06-16 10:48:58,882 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_1
om2_1    | ]
scm3_1   |   }
om1_1    | 2023-06-16 10:50:27,747 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:09,693 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-16 10:48:22,864 [main] INFO ha.SequenceIdGenerator: upgrade delTxnId to 0
scm2_1   | 2023-06-16 10:48:58,920 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:49:46,067 [OM StateMachine ApplyTransaction Thread - 0] INFO volume.OMVolumeCreateRequest: created volume:old1-volume for user:hadoop
om2_1    | 2023-06-16 10:49:48,670 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: old1-volume
om1_1    | 2023-06-16 10:50:27,752 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:09,696 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-16 10:48:22,866 [main] INFO ha.SequenceIdGenerator: upgrade containerId to 0
scm1_1   | 2023-06-16 10:48:22,867 [main] INFO ha.SequenceIdGenerator: Init the HA SequenceIdGenerator.
om2_1    | 2023-06-16 10:49:57,108 [OM StateMachine ApplyTransaction Thread - 0] INFO bucket.OMBucketCreateRequest: created bucket: old1-bucket of layout LEGACY in volume: s3v
om1_1    | 2023-06-16 10:50:27,752 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:58,922 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
dn4_1    | 2023-06-16 10:49:09,696 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1
scm1_1   | 2023-06-16 10:48:22,895 [main] INFO node.SCMNodeManager: Entering startup safe mode.
scm3_1   | }
om2_1    | 2023-06-16 10:50:06,473 [OM StateMachine ApplyTransaction Thread - 0] ERROR bucket.OMBucketCreateRequest: Bucket creation failed for bucket:old1-bucket in volume:s3v
om1_1    | 2023-06-16 10:50:27,752 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:58,931 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
dn4_1    | 2023-06-16 10:49:09,714 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1 ELECTION round 0: submit vote requests at term 1 for -1: peers:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:22,911 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRandom
scm3_1   |  from snapshot
om2_1    | BUCKET_ALREADY_EXISTS org.apache.hadoop.ozone.om.exceptions.OMException: Bucket already exist
scm2_1   | 2023-06-16 10:48:58,932 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
dn4_1    | 2023-06-16 10:49:09,716 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1 ELECTION round 0: result PASSED (term=1)
scm1_1   | 2023-06-16 10:48:22,915 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-06-16 10:49:16,189 [grpc-default-executor-0] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 19: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 	at org.apache.hadoop.ozone.om.request.bucket.OMBucketCreateRequest.validateAndUpdateCache(OMBucketCreateRequest.java:206)
om1_1    | 2023-06-16 10:50:27,764 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:48:58,953 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread2] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 17: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn4_1    | 2023-06-16 10:49:09,716 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1
scm1_1   | 2023-06-16 10:48:22,919 [main] INFO pipeline.PipelineStateManagerImpl: No pipeline exists in current db
scm3_1   | 2023-06-16 10:49:16,192 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: reply installSnapshot: 80f154b6-d6d9-4d00-b482-76c3516d18ec<-d5edb8ee-91fd-4f84-a835-71ed6541c25d#0:OK-t0,ALREADY_INSTALLED
om2_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleWriteRequest(OzoneManagerRequestHandler.java:311)
om1_1    | 2023-06-16 10:50:27,764 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:48:58,975 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
dn4_1    | 2023-06-16 10:49:09,717 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: changes role from CANDIDATE to LEADER at term 1 for changeToLeader
scm1_1   | 2023-06-16 10:48:22,939 [main] INFO algorithms.LeaderChoosePolicyFactory: Create leader choose policy of type org.apache.hadoop.hdds.scm.pipeline.leader.choose.algorithms.MinLeaderCountChoosePolicy
scm3_1   | 2023-06-16 10:49:16,237 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: d5edb8ee-91fd-4f84-a835-71ed6541c25d: Completed INSTALL_SNAPSHOT, lastRequest: 80f154b6-d6d9-4d00-b482-76c3516d18ec->d5edb8ee-91fd-4f84-a835-71ed6541c25d#0-t2,notify:(t:1, i:0)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.runCommand(OzoneManagerStateMachine.java:533)
om1_1    | 2023-06-16 10:50:27,767 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:48:59,299 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
dn4_1    | 2023-06-16 10:49:09,717 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-1E7EC9F35CE6 with new leaderId: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
scm1_1   | 2023-06-16 10:48:22,939 [main] INFO algorithms.ContainerPlacementPolicyFactory: Create container placement policy of type org.apache.hadoop.hdds.scm.container.placement.algorithms.SCMContainerPlacementRackScatter
scm3_1   | 2023-06-16 10:49:16,308 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO impl.RoleInfo: d5edb8ee-91fd-4f84-a835-71ed6541c25d: start d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-FollowerState
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$1(OzoneManagerStateMachine.java:324)
om1_1    | 2023-06-16 10:50:27,768 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:48:59,304 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread2] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 19: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:09,718 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: change Leader from null to 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8 at term 1 for becomeLeader, leader elected after 5800ms
scm1_1   | 2023-06-16 10:48:22,943 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineCreator.
scm3_1   | 2023-06-16 10:49:16,316 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
om1_1    | 2023-06-16 10:50:27,769 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:48:59,772 [JvmPauseMonitor0] WARN util.JvmPauseMonitor: JvmPauseMonitor-af8a130f-204e-4987-9eb3-c559054693bc: Detected pause in JVM or host machine (eg GC): pause of approximately 109227299ns.
dn4_1    | 2023-06-16 10:49:09,729 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm1_1   | 2023-06-16 10:48:22,943 [main] INFO pipeline.BackgroundPipelineCreator: Starting RatisPipelineUtilsThread.
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
scm3_1   | 2023-06-16 10:49:16,319 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: inconsistency entries. Reply:80f154b6-d6d9-4d00-b482-76c3516d18ec<-d5edb8ee-91fd-4f84-a835-71ed6541c25d#1:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
om1_1    | 2023-06-16 10:50:27,783 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | GC pool 'ParNew' had collection(s): count=1 time=118ms
scm2_1   | 2023-06-16 10:48:59,871 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861: skipped Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:35370
scm2_1   | 2023-06-16 10:48:59,871 [IPC Server handler 4 on default port 9861] INFO ipc.Server: IPC Server handler 4 on default port 9861: skipped Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:53398
scm2_1   | 2023-06-16 10:48:59,871 [IPC Server handler 6 on default port 9861] INFO ipc.Server: IPC Server handler 6 on default port 9861: skipped Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:57588
scm2_1   | 2023-06-16 10:48:59,872 [IPC Server handler 8 on default port 9861] INFO ipc.Server: IPC Server handler 8 on default port 9861: skipped Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:51426
scm2_1   | 2023-06-16 10:48:59,879 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm2 to group group-B37B80052E22:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm2_1   | 2023-06-16 10:48:59,879 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
scm2_1   | 2023-06-16 10:49:00,136 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]].
dn4_1    | 2023-06-16 10:49:09,739 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
scm3_1   | 2023-06-16 10:49:16,329 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: Failed appendEntries as previous log entry ((t:1, i:0)) is not found
scm1_1   | 2023-06-16 10:48:22,948 [main] INFO BackgroundPipelineScrubber: Starting BackgroundPipelineScrubber Service.
om1_1    | 2023-06-16 10:50:27,783 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:49:00,154 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:09,741 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 1024 M (=1073741824) (custom)
om2_1    | 2023-06-16 10:50:26,875 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:26,876 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: Received notification to install snapshot at index 25
scm1_1   | 2023-06-16 10:48:22,948 [main] INFO ha.SCMServiceManager: Registering service BackgroundPipelineScrubber.
om1_1    | 2023-06-16 10:50:27,790 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:49:00,161 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
om2_1    | 2023-06-16 10:50:26,900 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: notifyInstallSnapshot: nextIndex is 24 but the leader's first available index is 25.
om2_1    | 2023-06-16 10:50:26,900 [grpc-default-executor-0] INFO ratis.OzoneManagerStateMachine: Received install snapshot notification from OM leader: om1 with term index: (t:1, i:25)
dn4_1    | 2023-06-16 10:49:09,748 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 180s (custom)
scm1_1   | 2023-06-16 10:48:22,953 [main] INFO ExpiredContainerReplicaOpScrubber: Starting ExpiredContainerReplicaOpScrubber Service.
om1_1    | 2023-06-16 10:50:27,791 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,161 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
om2_1    | 2023-06-16 10:50:26,926 [pool-25-thread-1] INFO om.OzoneManager: Downloading checkpoint from leader OM om1 and reloading state from the checkpoint.
scm3_1   | 2023-06-16 10:49:16,329 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: inconsistency entries. Reply:80f154b6-d6d9-4d00-b482-76c3516d18ec<-d5edb8ee-91fd-4f84-a835-71ed6541c25d#0:FAIL-t2,INCONSISTENCY,nextIndex=0,followerCommit=-1,matchIndex=-1
dn4_1    | 2023-06-16 10:49:09,749 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
scm1_1   | 2023-06-16 10:48:22,953 [main] INFO ha.SCMServiceManager: Registering service ExpiredContainerReplicaOpScrubber.
om1_1    | 2023-06-16 10:50:27,791 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,210 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]].
om2_1    | 2023-06-16 10:50:26,927 [pool-25-thread-1] INFO snapshot.OzoneManagerSnapshotProvider: Downloading latest checkpoint from Leader OM om1. Checkpoint URL: http://om1:9874/dbCheckpoint?flushBeforeCheckpoint=true
scm3_1   | 2023-06-16 10:49:16,345 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:09,751 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm1_1   | 2023-06-16 10:48:22,979 [main] INFO algorithms.PipelineChoosePolicyFactory: Create pipeline choose policy of type org.apache.hadoop.hdds.scm.pipeline.choose.algorithms.RandomPipelineChoosePolicy
om1_1    | 2023-06-16 10:50:27,797 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:00,255 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:26,930 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:16,345 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:09,758 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 1024 (custom)
scm1_1   | 2023-06-16 10:48:22,989 [main] INFO ha.SCMServiceManager: Registering service SCMBlockDeletingService.
om1_1    | 2023-06-16 10:50:27,797 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:49:00,274 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]].
om2_1    | 2023-06-16 10:50:26,955 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,347 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 17: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
dn4_1    | 2023-06-16 10:49:09,772 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm1_1   | 2023-06-16 10:48:23,013 [main] INFO replication.ReplicationManager: Starting Replication Monitor Thread.
om1_1    | 2023-06-16 10:50:27,800 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:49:00,286 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:26,962 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm3_1   | 2023-06-16 10:49:16,348 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 19: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:09,782 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderStateImpl
scm1_1   | 2023-06-16 10:48:23,024 [main] INFO ha.SCMServiceManager: Registering service ReplicationManager.
om1_1    | 2023-06-16 10:50:27,800 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,308 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]].
om2_1    | 2023-06-16 10:50:26,964 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2453:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm3_1   | 2023-06-16 10:49:16,373 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-16 10:49:09,844 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-16 10:49:09,891 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om1_1    | 2023-06-16 10:50:27,801 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,308 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:09,911 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-LeaderElection1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6: set configuration 0: peers:[2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:1|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:23,024 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm3_1   | 2023-06-16 10:49:16,418 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
om1_1    | 2023-06-16 10:50:27,806 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:00,312 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]].
scm2_1   | 2023-06-16 10:49:00,346 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:10,076 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO impl.FollowerState: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5100355069ns, electionTimeout:5035ms
scm1_1   | 2023-06-16 10:48:23,030 [main] INFO safemode.ContainerSafeModeRule: containers with one replica threshold count 0
scm3_1   | 2023-06-16 10:49:16,443 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,806 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:49:00,348 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]].
om2_1    | 2023-06-16 10:50:26,980 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,093 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState
scm1_1   | 2023-06-16 10:48:23,033 [main] INFO safemode.HealthyPipelineSafeModeRule: Total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-16 10:49:16,450 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,818 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,387 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:26,981 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:10,093 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-16 10:48:23,034 [main] INFO safemode.OneReplicaPipelineSafeModeRule: Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-16 10:49:16,450 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 17: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om1_1    | 2023-06-16 10:50:27,818 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:00,390 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]].
om2_1    | 2023-06-16 10:50:26,993 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,094 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
scm1_1   | 2023-06-16 10:48:23,742 [main] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm3_1   | 2023-06-16 10:49:16,451 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread1] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 19: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,835 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:00,404 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-16 10:49:00,598 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
dn4_1    | 2023-06-16 10:49:10,095 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2
scm1_1   | 2023-06-16 10:48:23,763 [main] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm3_1   | 2023-06-16 10:49:16,539 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0
om1_1    | 2023-06-16 10:50:27,835 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:49:00,770 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#12 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.20:47544: output error
scm2_1   | 2023-06-16 10:49:00,770 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
dn4_1    | 2023-06-16 10:49:10,164 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:23,809 [Socket Reader #1 for port 9861] INFO ipc.Server: Starting Socket Reader #1 for port 9861
scm3_1   | 2023-06-16 10:49:16,554 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0 to /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_0-0
om1_1    | 2023-06-16 10:50:27,847 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,847 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:26,997 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
dn4_1    | 2023-06-16 10:49:10,212 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-16 10:48:23,930 [Listener at 0.0.0.0/9861] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-16 10:48:23,936 [Listener at 0.0.0.0/9861] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
scm2_1   | java.nio.channels.ClosedChannelException
om1_1    | 2023-06-16 10:50:27,848 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:26,998 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2454:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
dn4_1    | 2023-06-16 10:49:10,220 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-16 10:49:10,220 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2-1] INFO server.GrpcServerProtocolClient: Build channel for a27fbf0d-ae65-4185-b735-d902c8c8b71a
scm1_1   | 2023-06-16 10:48:23,937 [Socket Reader #1 for port 9863] INFO ipc.Server: Starting Socket Reader #1 for port 9863
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 2023-06-16 10:50:27,857 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,861 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:10,220 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2-2] INFO server.GrpcServerProtocolClient: Build channel for 7e6c224f-5cdf-45b7-b597-f8bf583ee016
dn4_1    | 2023-06-16 10:49:10,325 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-1E7EC9F35CE6-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/21e502a4-0e73-4435-b1e3-1e7ec9f35ce6/current/log_inprogress_0
scm1_1   | 2023-06-16 10:48:23,963 [Listener at 0.0.0.0/9863] INFO audit.AuditLogger: Refresh DebugCmdSet for SCMAudit to [].
scm1_1   | 2023-06-16 10:48:23,969 [Listener at 0.0.0.0/9863] INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 10000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false.
om1_1    | 2023-06-16 10:50:27,861 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:27,871 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,340 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2: ELECTION REJECTED received 2 response(s) and 0 exception(s):
scm3_1   | 2023-06-16 10:49:16,572 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_1
scm1_1   | 2023-06-16 10:48:23,970 [Socket Reader #1 for port 9860] INFO ipc.Server: Starting Socket Reader #1 for port 9860
scm2_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-06-16 10:50:27,872 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,042 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,345 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection:   Response 0: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-a27fbf0d-ae65-4185-b735-d902c8c8b71a#0:FAIL-t1
scm3_1   | 2023-06-16 10:49:16,594 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-16 10:48:24,017 [Listener at 0.0.0.0/9860] INFO ha.SCMServiceManager: Registering service ContainerBalancer.
scm1_1   | 2023-06-16 10:48:24,018 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: 
om1_1    | 2023-06-16 10:50:27,874 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:50:27,043 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:10,345 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection:   Response 1: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:OK-t1
scm3_1   | 2023-06-16 10:49:16,596 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread3] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 31: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
scm1_1   | Container Balancer status:
scm2_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om1_1    | 2023-06-16 10:50:27,879 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,045 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,346 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2 ELECTION round 0: result REJECTED
dn4_1    | 2023-06-16 10:49:10,346 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
scm1_1   | Key                            Value
scm2_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om1_1    | 2023-06-16 10:50:27,880 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:50:27,059 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm3_1   | 2023-06-16 10:49:16,596 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
dn4_1    | 2023-06-16 10:49:10,347 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2
scm1_1   | Running                        true
om1_1    | 2023-06-16 10:50:27,882 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:50:27,059 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2455:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm3_1   | 2023-06-16 10:49:16,600 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
dn4_1    | 2023-06-16 10:49:10,347 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-LeaderElection2] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState
scm1_1   | Container Balancer Configuration values:
om1_1    | 2023-06-16 10:50:27,882 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,069 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,602 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
dn4_1    | 2023-06-16 10:49:10,366 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | Key                                                Value
om1_1    | 2023-06-16 10:50:27,882 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | Threshold                                          10
scm1_1   | Max Datanodes to Involve per Iteration(percent)    20
om2_1    | 2023-06-16 10:50:27,070 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,073 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,081 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:27,889 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:16,606 [d5edb8ee-91fd-4f84-a835-71ed6541c25d-server-thread2] INFO server.RaftServer$Division: d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22: set configuration 33: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | Max Size to Move per Iteration                     500GB
om2_1    | 2023-06-16 10:50:27,082 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2456:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,088 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,089 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,890 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:27,892 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | Max Size Entering Target per Iteration             26GB
om2_1    | 2023-06-16 10:50:27,094 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,107 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,108 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2457:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:27,892 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,892 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | Max Size Leaving Source per Iteration              26GB
om2_1    | 2023-06-16 10:50:27,126 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,127 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,131 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,908 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,908 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 
om2_1    | 2023-06-16 10:50:27,137 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,137 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2458:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,151 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,911 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,911 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:24,018 [Listener at 0.0.0.0/9860] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
om2_1    | 2023-06-16 10:50:27,152 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,153 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,159 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:27,911 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,916 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:24,019 [Listener at 0.0.0.0/9860] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=false}.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm3_1   | 2023-06-16 10:49:16,628 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm1_1   | 2023-06-16 10:48:24,022 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: StorageContainerLocationProtocol RPC server is listening at /0.0.0.0:9860
om1_1    | 2023-06-16 10:50:27,916 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
scm3_1   | 2023-06-16 10:49:16,639 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-16 10:48:24,025 [Listener at 0.0.0.0/9860] INFO ha.SCMRatisServerImpl: starting ratis server 0.0.0.0:9894
om1_1    | 2023-06-16 10:50:27,918 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
scm2_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
scm1_1   | 2023-06-16 10:48:24,030 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO storage.RaftStorageDirectory: Lock on /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/in_use.lock acquired by nodename 7@9490aade52c0
om1_1    | 2023-06-16 10:50:27,918 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
scm2_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
scm2_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
scm1_1   | 2023-06-16 10:48:24,035 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO storage.RaftStorage: Read RaftStorageMetadata{term=1, votedFor=80f154b6-d6d9-4d00-b482-76c3516d18ec} from /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/raft-meta
om1_1    | 2023-06-16 10:50:27,918 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
scm2_1   | 2023-06-16 10:49:00,828 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm2_1   | 2023-06-16 10:49:00,828 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-06-16 10:48:24,069 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:27,927 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,927 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:27,930 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,930 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,653 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl: Successfully added SCM scm3 to group group-B37B80052E22:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-16 10:48:24,083 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.use.memory = false (default)
om1_1    | 2023-06-16 10:50:27,930 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,933 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,933 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:27,937 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:16,653 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-06-16 10:48:24,108 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.gap = 1000000 (custom)
om1_1    | 2023-06-16 10:50:27,937 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:10,376 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-16 10:49:10,892 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:49:11,893 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:50:27,159 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2459:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:24,109 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om1_1    | 2023-06-16 10:50:27,937 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:02,082 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-06-16 10:49:16,803 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]].
dn4_1    | 2023-06-16 10:49:12,894 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:49:13,700 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5030036602ns, electionTimeout:5020ms
scm1_1   | 2023-06-16 10:48:24,110 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.read.timeout = 1000ms (default)
om1_1    | 2023-06-16 10:50:27,940 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,942 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:16,812 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:13,700 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState
dn4_1    | 2023-06-16 10:49:13,700 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: changes role from  FOLLOWER to CANDIDATE at term 0 for changeToCandidate
scm1_1   | 2023-06-16 10:48:24,110 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.preservation.log.num = 0 (default)
scm2_1   | 2023-06-16 10:49:02,084 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-16 10:50:27,944 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:16,813 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
dn4_1    | 2023-06-16 10:49:13,700 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
dn4_1    | 2023-06-16 10:49:13,700 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3
scm1_1   | 2023-06-16 10:48:24,114 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-16 10:49:02,085 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
om1_1    | 2023-06-16 10:50:27,944 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,813 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
dn4_1    | 2023-06-16 10:49:13,706 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3 ELECTION round 0: submit vote requests at term 1 for -1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
dn4_1    | 2023-06-16 10:49:13,707 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-16 10:48:24,127 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.num.max = 2 (custom)
scm2_1   | 2023-06-16 10:49:02,122 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
om1_1    | 2023-06-16 10:50:27,944 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,836 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]].
dn4_1    | 2023-06-16 10:49:13,708 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
dn4_1    | 2023-06-16 10:49:13,736 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3: ELECTION REJECTED received 1 response(s) and 0 exception(s):
scm1_1   | 2023-06-16 10:48:24,128 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.cache.size.max = 200MB (=209715200) (default)
scm2_1   | 2023-06-16 10:49:02,132 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om1_1    | 2023-06-16 10:50:27,951 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:13,737 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.LeaderElection:   Response 0: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8<-7e6c224f-5cdf-45b7-b597-f8bf583ee016#0:FAIL-t1
dn4_1    | 2023-06-16 10:49:13,737 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.LeaderElection: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3 ELECTION round 0: result REJECTED
scm1_1   | 2023-06-16 10:48:24,134 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: new 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker for RaftStorageImpl:Storage Directory /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22
scm2_1   | 2023-06-16 10:49:02,133 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-16 10:50:27,951 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:27,953 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om1_1    | 2023-06-16 10:50:27,953 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:24,142 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.byte-limit = 64MB (=67108864) (default)
scm1_1   | 2023-06-16 10:48:24,144 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.queue.element-limit = 4096 (default)
scm3_1   | 2023-06-16 10:49:16,837 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:27,953 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:27,956 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,957 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:24,144 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.segment.size.max = 4194304 (custom)
scm2_1   | 2023-06-16 10:49:02,133 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
om1_1    | 2023-06-16 10:50:27,959 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:50:27,166 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,166 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:02,302 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@11826398] INFO util.JvmPauseMonitor: Starting JVM pause monitor
scm2_1   | 2023-06-16 10:49:02,320 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm2_1   | 2023-06-16 10:49:02,320 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-06-16 10:50:27,167 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:13,737 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: changes role from CANDIDATE to FOLLOWER at term 1 for REJECTED
om1_1    | 2023-06-16 10:50:27,959 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,174 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,174 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2460:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm3_1   | 2023-06-16 10:49:16,839 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]].
dn4_1    | 2023-06-16 10:49:13,739 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3
dn4_1    | 2023-06-16 10:49:13,739 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-LeaderElection3] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState
om2_1    | 2023-06-16 10:50:27,187 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,187 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:16,840 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:13,744 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-16 10:49:13,747 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-16 10:50:27,211 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,216 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm3_1   | 2023-06-16 10:49:16,841 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]].
dn4_1    | 2023-06-16 10:49:13,895 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
dn4_1    | 2023-06-16 10:49:14,896 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ipc.Client: Retrying connect to server: scm3/10.9.0.16:9861. Already tried 10 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=15, sleepTime=1000 MILLISECONDS)
om2_1    | 2023-06-16 10:50:27,216 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2461:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,221 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,842 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:15,483 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: receive requestVote(ELECTION, a27fbf0d-ae65-4185-b735-d902c8c8b71a, group-E3FF46599155, 2, (t:0, i:0))
dn4_1    | 2023-06-16 10:49:15,484 [grpc-default-executor-1] INFO impl.VoteContext: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FOLLOWER: accept ELECTION from a27fbf0d-ae65-4185-b735-d902c8c8b71a: our priority 0 <= candidate's priority 1
om2_1    | 2023-06-16 10:50:27,223 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,224 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,843 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]].
dn4_1    | 2023-06-16 10:49:15,484 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:a27fbf0d-ae65-4185-b735-d902c8c8b71a
scm2_1   | 2023-06-16 10:49:02,388 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @28001ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-06-16 10:50:27,230 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,230 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2462:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm3_1   | 2023-06-16 10:49:16,845 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:15,485 [grpc-default-executor-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState
dn4_1    | 2023-06-16 10:49:15,485 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO impl.FollowerState: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState was interrupted
om2_1    | 2023-06-16 10:50:27,237 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,238 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:16,846 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]].
dn4_1    | 2023-06-16 10:49:15,485 [grpc-default-executor-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState
dn4_1    | 2023-06-16 10:49:15,503 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155 replies to ELECTION vote request: a27fbf0d-ae65-4185-b735-d902c8c8b71a<-2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8#0:OK-t2. Peer's state: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155:t2, leader=null, voted=a27fbf0d-ae65-4185-b735-d902c8c8b71a, raftlog=Memoized:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:50:27,243 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,245 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm3_1   | 2023-06-16 10:49:16,848 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:15,513 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
dn4_1    | 2023-06-16 10:49:15,517 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
om2_1    | 2023-06-16 10:50:27,246 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2463:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,259 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:16,855 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]].
scm2_1   | 2023-06-16 10:49:02,591 [IPC Server handler 3 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:27,959 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:15,704 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-E3FF46599155 with new leaderId: a27fbf0d-ae65-4185-b735-d902c8c8b71a
dn4_1    | 2023-06-16 10:49:15,706 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: change Leader from null to a27fbf0d-ae65-4185-b735-d902c8c8b71a at term 2 for appendEntries, leader elected after 10927ms
dn4_1    | 2023-06-16 10:49:15,707 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:1|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:0|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-16 10:49:02,600 [IPC Server handler 3 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:27,962 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:15,708 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker: Starting segment from index:0
dn4_1    | 2023-06-16 10:49:15,709 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-E3FF46599155-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/47baacd4-0b41-4a2f-af12-e3ff46599155/current/log_inprogress_0
dn4_1    | 2023-06-16 10:49:16,918 [EndpointStateMachine task thread for scm3/10.9.0.16:9861 - 0 ] INFO ozoneimpl.OzoneContainer: Ignore. OzoneContainer already started.
scm2_1   | 2023-06-16 10:49:02,640 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm3_1   | 2023-06-16 10:49:16,855 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
dn4_1    | 2023-06-16 10:49:18,810 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: receive requestVote(ELECTION, 7e6c224f-5cdf-45b7-b597-f8bf583ee016, group-42EFE7C8DED3, 2, (t:0, i:0))
dn4_1    | 2023-06-16 10:49:18,810 [grpc-default-executor-1] INFO impl.VoteContext: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FOLLOWER: accept ELECTION from 7e6c224f-5cdf-45b7-b597-f8bf583ee016: our priority 0 <= candidate's priority 1
dn4_1    | 2023-06-16 10:49:18,810 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: changes role from  FOLLOWER to FOLLOWER at term 2 for candidate:7e6c224f-5cdf-45b7-b597-f8bf583ee016
om1_1    | 2023-06-16 10:50:27,962 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm2_1   | 2023-06-16 10:49:02,661 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-16 10:48:24,148 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.preallocated.size = 4194304 (custom)
scm1_1   | 2023-06-16 10:48:24,150 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.force.sync.num = 128 (default)
om1_1    | 2023-06-16 10:50:27,966 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:18,810 [grpc-default-executor-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: shutdown 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState
scm2_1   | 2023-06-16 10:49:02,696 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm2_1   | 2023-06-16 10:49:02,706 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm2_1   | 2023-06-16 10:49:02,718 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om1_1    | 2023-06-16 10:50:27,966 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
dn4_1    | 2023-06-16 10:49:18,810 [grpc-default-executor-1] INFO impl.RoleInfo: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8: start 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState
scm2_1   | 2023-06-16 10:49:02,724 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm2_1   | 2023-06-16 10:49:02,724 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm2_1   | 2023-06-16 10:49:02,725 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om1_1    | 2023-06-16 10:50:27,966 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
dn4_1    | 2023-06-16 10:49:18,810 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO impl.FollowerState: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState was interrupted
scm2_1   | 2023-06-16 10:49:02,868 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm2_1   | 2023-06-16 10:49:02,872 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm2_1   | 2023-06-16 10:49:02,970 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
om1_1    | 2023-06-16 10:50:27,970 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
dn4_1    | 2023-06-16 10:49:18,816 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5s (fallback to raft.server.rpc.timeout.min)
scm2_1   | 2023-06-16 10:49:02,970 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm2_1   | 2023-06-16 10:49:02,972 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 660000ms
scm2_1   | 2023-06-16 10:49:03,017 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@2c579202{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om1_1    | 2023-06-16 10:50:27,971 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
dn4_1    | 2023-06-16 10:49:18,816 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm2_1   | 2023-06-16 10:49:03,020 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@766b6d02{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
scm2_1   | 2023-06-16 10:49:03,109 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7e6c224f-5cdf-45b7-b597-f8bf583ee016
scm2_1   | 2023-06-16 10:49:03,132 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:27,974 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
dn4_1    | 2023-06-16 10:49:18,817 [grpc-default-executor-1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3 replies to ELECTION vote request: 7e6c224f-5cdf-45b7-b597-f8bf583ee016<-2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8#0:OK-t2. Peer's state: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3:t2, leader=null, voted=7e6c224f-5cdf-45b7-b597-f8bf583ee016, raftlog=Memoized:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLog:OPENED:c-1, conf=-1: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-16 10:49:16,859 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm2_1   | 2023-06-16 10:49:03,133 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-16 10:49:03,134 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm1_1   | 2023-06-16 10:48:24,151 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync = true (default)
dn4_1    | 2023-06-16 10:49:19,114 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO ratis.XceiverServerRatis: Leader change notification received for group: group-42EFE7C8DED3 with new leaderId: 7e6c224f-5cdf-45b7-b597-f8bf583ee016
scm3_1   | 2023-06-16 10:49:16,859 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,259 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,260 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,265 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
dn4_1    | 2023-06-16 10:49:19,114 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread1] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: change Leader from null to 7e6c224f-5cdf-45b7-b597-f8bf583ee016 at term 2 for appendEntries, leader elected after 10502ms
scm3_1   | 2023-06-16 10:49:16,862 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,265 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2464:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,278 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:24,154 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout = 10s (default)
dn4_1    | 2023-06-16 10:49:19,169 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread2] INFO server.RaftServer$Division: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3: set configuration 0: peers:[a27fbf0d-ae65-4185-b735-d902c8c8b71a|rpc:10.9.0.19:9856|admin:10.9.0.19:9857|client:10.9.0.19:9858|dataStream:|priority:0|startupRole:FOLLOWER, 7e6c224f-5cdf-45b7-b597-f8bf583ee016|rpc:10.9.0.18:9856|admin:10.9.0.18:9857|client:10.9.0.18:9858|dataStream:|priority:1|startupRole:FOLLOWER, 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8|rpc:10.9.0.20:9856|admin:10.9.0.20:9857|client:10.9.0.20:9858|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-16 10:49:16,865 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,278 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,279 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,283 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:24,157 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.sync.timeout.retry = -1 (default)
scm1_1   | 2023-06-16 10:48:24,181 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.write.buffer.size = 64KB (=65536) (default)
scm1_1   | 2023-06-16 10:48:24,181 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.unsafe-flush.enabled = false (default)
scm1_1   | 2023-06-16 10:48:24,181 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.async-flush.enabled = false (default)
om1_1    | 2023-06-16 10:50:27,974 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,748 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a27fbf0d-ae65-4185-b735-d902c8c8b71a
scm2_1   | 2023-06-16 10:49:03,753 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-16 10:49:16,869 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm1_1   | 2023-06-16 10:48:24,182 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.statemachine.data.caching.enabled = false (default)
scm1_1   | 2023-06-16 10:48:24,219 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm2_1   | 2023-06-16 10:49:03,753 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm2_1   | 2023-06-16 10:49:03,768 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm3_1   | 2023-06-16 10:49:16,869 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-06-16 10:48:24,239 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.LogSegment: Successfully read 1 entries from segment file /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0
scm1_1   | 2023-06-16 10:48:24,243 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: flushIndex: setUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:27,974 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,769 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-06-16 10:49:03,769 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm1_1   | 2023-06-16 10:48:24,243 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: safeCacheEvictIndex: setUnconditionally 0 -> -1
scm1_1   | 2023-06-16 10:48:24,292 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: start as a follower, conf=0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm1_1   | 2023-06-16 10:48:24,292 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from      null to FOLLOWER at term 1 for startAsFollower
scm1_1   | 2023-06-16 10:48:24,294 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState
scm1_1   | 2023-06-16 10:48:24,298 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO util.JmxRegister: Successfully registered JMX Bean with object name Ratis:service=RaftServer,group=group-B37B80052E22,id=80f154b6-d6d9-4d00-b482-76c3516d18ec
scm1_1   | 2023-06-16 10:48:24,299 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.enabled = true (custom)
scm1_1   | 2023-06-16 10:48:24,299 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.auto.trigger.threshold = 1000 (custom)
scm1_1   | 2023-06-16 10:48:24,300 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.snapshot.retention.file.num = -1 (default)
scm1_1   | 2023-06-16 10:48:24,300 [80f154b6-d6d9-4d00-b482-76c3516d18ec-impl-thread1] INFO server.RaftServerConfigKeys: raft.server.log.purge.upto.snapshot.index = false (default)
scm1_1   | 2023-06-16 10:48:24,303 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.min = 5000ms (fallback to raft.server.rpc.timeout.min)
scm1_1   | 2023-06-16 10:48:24,303 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.rpc.first-election.timeout.max = 5200ms (fallback to raft.server.rpc.timeout.max)
scm1_1   | 2023-06-16 10:48:24,303 [Listener at 0.0.0.0/9860] INFO server.RaftServer: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start RPC server
scm1_1   | 2023-06-16 10:48:24,343 [Listener at 0.0.0.0/9860] INFO server.GrpcService: 80f154b6-d6d9-4d00-b482-76c3516d18ec: GrpcService started, listening on 9894
scm1_1   | 2023-06-16 10:48:24,353 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-80f154b6-d6d9-4d00-b482-76c3516d18ec: Started
scm1_1   | 2023-06-16 10:48:24,356 [Listener at 0.0.0.0/9860] INFO ha.SCMHAManagerImpl:  scm role is FOLLOWER peers [80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]
scm1_1   | 2023-06-16 10:48:24,356 [Listener at 0.0.0.0/9860] INFO ha.InterSCMGrpcService: Starting SCM Grpc Service at port 9895
scm1_1   | 2023-06-16 10:48:24,421 [Listener at 0.0.0.0/9860] INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties
scm1_1   | 2023-06-16 10:48:24,437 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm1_1   | 2023-06-16 10:48:24,439 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-06-16 10:48:24,778 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm1_1   | 2023-06-16 10:48:24,779 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om2_1    | 2023-06-16 10:50:27,285 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2465:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:27,299 [pool-25-thread-1] INFO snapshot.OzoneManagerSnapshotProvider: Successfully downloaded latest checkpoint from leader OM: om1
om2_1    | 2023-06-16 10:50:27,305 [pool-25-thread-1] INFO om.OzoneManager: Downloaded checkpoint from Leader om1 to the location /data/metadata/snapshot/om.db-om1-1686912626927
scm3_1   | 2023-06-16 10:49:16,898 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
scm3_1   | 2023-06-16 10:49:16,899 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: StorageContainerManager metrics system started
scm1_1   | 2023-06-16 10:48:24,780 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
om2_1    | 2023-06-16 10:50:27,308 [pool-25-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-16 10:50:27,309 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-16 10:50:27,313 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm3_1   | 2023-06-16 10:49:17,276 [Listener at 0.0.0.0/9860] INFO server.SCMClientProtocolServer: RPC server for Client  is listening at /0.0.0.0:9860
scm3_1   | 2023-06-16 10:49:17,276 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-16 10:48:24,814 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
scm1_1   | 2023-06-16 10:48:24,815 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
scm1_1   | 2023-06-16 10:48:24,822 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm1_1   | 2023-06-16 10:48:24,827 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
scm3_1   | 2023-06-16 10:49:17,276 [IPC Server listener on 9860] INFO ipc.Server: IPC Server listener on 9860: starting
om1_1    | 2023-06-16 10:50:27,979 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:27,980 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:50:27,313 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-06-16 10:49:03,769 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
dn4_1    | 2023-06-16 10:49:19,170 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8-server-thread2] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker: Starting segment from index:0
scm3_1   | 2023-06-16 10:49:17,325 [Listener at 0.0.0.0/9860] INFO server.StorageContainerManager: ScmBlockLocationProtocol RPC server is listening at /0.0.0.0:9863
om1_1    | 2023-06-16 10:50:27,983 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:24,919 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@30b3d899] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om2_1    | 2023-06-16 10:50:27,313 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-06-16 10:49:03,774 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
dn4_1    | 2023-06-16 10:49:19,172 [2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8@group-42EFE7C8DED3-SegmentedRaftLogWorker: created new log segment /data/metadata/ratis/69c8023f-ca19-4575-8453-42efe7c8ded3/current/log_inprogress_0
scm3_1   | 2023-06-16 10:49:17,326 [Listener at 0.0.0.0/9860] INFO server.SCMBlockProtocolServer: RPC server for Block Protocol is listening at /0.0.0.0:9863
om1_1    | 2023-06-16 10:50:27,984 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:24,932 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
om2_1    | 2023-06-16 10:50:27,314 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm2_1   | 2023-06-16 10:49:03,776 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm3_1   | 2023-06-16 10:49:17,327 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
om1_1    | 2023-06-16 10:50:27,984 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:24,932 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
om2_1    | 2023-06-16 10:50:27,323 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,861 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74bc4d26-95cc-4798-833d-04dc94ff925a
scm3_1   | 2023-06-16 10:49:17,333 [IPC Server listener on 9863] INFO ipc.Server: IPC Server listener on 9863: starting
om1_1    | 2023-06-16 10:50:27,991 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:24,951 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @11555ms to org.eclipse.jetty.util.log.Slf4jLog
om2_1    | 2023-06-16 10:50:27,323 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:03,862 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm3_1   | 2023-06-16 10:49:17,374 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1894fa9f] INFO util.JvmPauseMonitor: Starting JVM pause monitor
om1_1    | 2023-06-16 10:50:27,992 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:25,268 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
om2_1    | 2023-06-16 10:50:27,329 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,862 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om1_1    | 2023-06-16 10:50:27,995 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:17,378 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Starting Web-server for scm at: http://0.0.0.0:9876
scm1_1   | 2023-06-16 10:48:25,284 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
om2_1    | 2023-06-16 10:50:27,337 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:03,927 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@27896d3b{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-9232720046276863569/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
om1_1    | 2023-06-16 10:50:27,995 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,378 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: Hadoop Security Enabled: false Ozone Security Enabled: false Ozone HTTP Security Enabled: false 
scm1_1   | 2023-06-16 10:48:25,318 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
om2_1    | 2023-06-16 10:50:27,337 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2466:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm2_1   | 2023-06-16 10:49:03,978 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@99c4993{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
om1_1    | 2023-06-16 10:50:27,995 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,396 [Listener at 0.0.0.0/9860] INFO util.log: Logging initialized @15648ms to org.eclipse.jetty.util.log.Slf4jLog
scm1_1   | 2023-06-16 10:48:25,320 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
om2_1    | 2023-06-16 10:50:27,347 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,979 [Listener at 0.0.0.0/9860] INFO server.Server: Started @29592ms
om1_1    | 2023-06-16 10:50:27,998 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:17,479 [Listener at 0.0.0.0/9860] WARN server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets. Reason: Could not read signature secret file: /opt/hadoop/hadoop-http-auth-signature-secret
scm1_1   | 2023-06-16 10:48:25,325 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
om2_1    | 2023-06-16 10:50:27,348 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:03,990 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/13cc61e5-01ec-4df0-9437-cac1ff0f189e
om1_1    | 2023-06-16 10:50:27,998 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:17,484 [Listener at 0.0.0.0/9860] INFO http.HttpRequestLog: Http request log for http.requests.scm is not defined
scm1_1   | 2023-06-16 10:48:25,326 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
om2_1    | 2023-06-16 10:50:27,349 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:03,990 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:28,000 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:17,491 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.hdds.server.http.HttpServer2$QuotingInputFilter)
scm1_1   | 2023-06-16 10:48:25,353 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
om2_1    | 2023-06-16 10:50:27,359 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:03,991 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om1_1    | 2023-06-16 10:50:28,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,493 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context scm
scm1_1   | 2023-06-16 10:48:25,354 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
om2_1    | 2023-06-16 10:50:27,359 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2467:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm2_1   | 2023-06-16 10:49:04,001 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
om1_1    | 2023-06-16 10:50:28,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,493 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
scm1_1   | 2023-06-16 10:48:25,390 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
om2_1    | 2023-06-16 10:50:27,378 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:04,001 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
om1_1    | 2023-06-16 10:50:28,004 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:17,493 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
scm1_1   | 2023-06-16 10:48:25,390 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
om2_1    | 2023-06-16 10:50:27,378 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:04,010 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm2_1   | 2023-06-16 10:49:04,418 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-16 10:49:17,529 [Listener at 0.0.0.0/9860] INFO http.HttpServer2: Jetty bound to port 9876
scm1_1   | 2023-06-16 10:48:25,392 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
om2_1    | 2023-06-16 10:50:27,382 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:04,460 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,004 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om1_1    | 2023-06-16 10:50:28,007 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:25,402 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@7b3cbe6e{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
om2_1    | 2023-06-16 10:50:27,387 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:04,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,007 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,530 [Listener at 0.0.0.0/9860] INFO server.Server: jetty-9.4.49.v20220914; built: 2022-09-14T01:07:36.601Z; git: 4231a3b2e4cb8548a412a789936d640a97b1aa0a; jvm 11.0.14.1+1-LTS
scm1_1   | 2023-06-16 10:48:25,402 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@57b9389f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-06-16 10:50:27,387 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2468:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm2_1   | 2023-06-16 10:49:05,249 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]] moved to OPEN state
om1_1    | 2023-06-16 10:50:28,007 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,578 [Listener at 0.0.0.0/9860] INFO server.session: DefaultSessionIdManager workerName=node0
scm1_1   | 2023-06-16 10:48:25,703 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@4c063cb9{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-17675464668042122387/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
om2_1    | 2023-06-16 10:50:27,398 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:05,452 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,010 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:17,578 [Listener at 0.0.0.0/9860] INFO server.session: No SessionScavenger set, using defaults
scm1_1   | 2023-06-16 10:48:25,710 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@60b616c8{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
om2_1    | 2023-06-16 10:50:27,399 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:05,898 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,010 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:17,580 [Listener at 0.0.0.0/9860] INFO server.session: node0 Scavenging every 600000ms
scm1_1   | 2023-06-16 10:48:25,710 [Listener at 0.0.0.0/9860] INFO server.Server: Started @12315ms
om2_1    | 2023-06-16 10:50:27,400 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:06,002 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,013 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:17,593 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@653a5967{logs,/logs,file:///var/log/hadoop/,AVAILABLE}
scm3_1   | 2023-06-16 10:49:17,594 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.s.ServletContextHandler@63485d7{static,/static,jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/static,AVAILABLE}
om2_1    | 2023-06-16 10:50:27,407 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:06,008 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,912 [Listener at 0.0.0.0/9860] INFO handler.ContextHandler: Started o.e.j.w.WebAppContext@6e9a10cd{scm,/,file:///tmp/jetty-0_0_0_0-9876-hdds-server-scm-1_3_0_jar-_-any-3152111308313864744/webapp/,AVAILABLE}{jar:file:/opt/hadoop/share/ozone/lib/hdds-server-scm-1.3.0.jar!/webapps/scm}
scm1_1   | 2023-06-16 10:48:25,718 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
om2_1    | 2023-06-16 10:50:27,407 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2469:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm2_1   | 2023-06-16 10:49:06,259 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,920 [Listener at 0.0.0.0/9860] INFO server.AbstractConnector: Started ServerConnector@36463b09{HTTP/1.1, (http/1.1)}{0.0.0.0:9876}
scm1_1   | 2023-06-16 10:48:25,718 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
om2_1    | 2023-06-16 10:50:27,418 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:06,283 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,015 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:17,920 [Listener at 0.0.0.0/9860] INFO server.Server: Started @16172ms
scm1_1   | 2023-06-16 10:48:25,719 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
om2_1    | 2023-06-16 10:50:27,419 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:08,771 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,016 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:17,924 [Listener at 0.0.0.0/9860] INFO impl.MetricsSinkAdapter: Sink prometheus started
scm1_1   | 2023-06-16 10:48:29,427 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.FollowerState: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState: change to CANDIDATE, lastRpcElapsedTime:5133579690ns, electionTimeout:5123ms
om2_1    | 2023-06-16 10:50:27,419 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:08,810 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,019 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:17,924 [Listener at 0.0.0.0/9860] INFO impl.MetricsSystemImpl: Registered sink prometheus
scm1_1   | 2023-06-16 10:48:29,428 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState
om2_1    | 2023-06-16 10:50:27,425 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:11,506 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,019 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:17,927 [Listener at 0.0.0.0/9860] INFO http.BaseHttpServer: HTTP server of scm listening at http://0.0.0.0:9876
scm1_1   | 2023-06-16 10:48:29,429 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from  FOLLOWER to CANDIDATE at term 1 for changeToCandidate
om2_1    | 2023-06-16 10:50:27,425 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2470:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm2_1   | 2023-06-16 10:49:11,799 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,019 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:18,887 [IPC Server handler 0 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7e6c224f-5cdf-45b7-b597-f8bf583ee016
scm3_1   | 2023-06-16 10:49:18,891 [IPC Server handler 0 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,436 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:16,589 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread2] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 31: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om1_1    | 2023-06-16 10:50:28,022 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:18,935 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
scm1_1   | 2023-06-16 10:48:29,431 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO server.RaftServerConfigKeys: raft.server.leaderelection.pre-vote = false (custom)
om2_1    | 2023-06-16 10:50:27,437 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,438 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:16,599 [af8a130f-204e-4987-9eb3-c559054693bc-server-thread2] INFO server.RaftServer$Division: af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22: set configuration 33: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:50:27,445 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,022 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:18,935 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
scm1_1   | 2023-06-16 10:48:29,431 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-FollowerState] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1
om2_1    | 2023-06-16 10:50:27,445 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2471:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,024 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:18,973 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-16 10:49:16,923 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:48:29,450 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.LeaderElection: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1 ELECTION round 0: submit vote requests at term 2 for 0: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:50:27,452 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,024 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:19,067 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
scm2_1   | 2023-06-16 10:49:16,926 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-16 10:49:16,926 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:50:27,453 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,024 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:39,763 [IPC Server handler 4 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
scm2_1   | 2023-06-16 10:49:16,931 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-16 10:48:29,451 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.LeaderElection: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1 ELECTION round 0: result PASSED (term=2)
om2_1    | 2023-06-16 10:50:27,454 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,029 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:39,771 [IPC Server handler 4 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
scm2_1   | 2023-06-16 10:49:16,940 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:48:29,451 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: shutdown 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1
om2_1    | 2023-06-16 10:50:27,464 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,029 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:39,772 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-16 10:49:16,948 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm1_1   | 2023-06-16 10:48:29,452 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: changes role from CANDIDATE to LEADER at term 2 for changeToLeader
om2_1    | 2023-06-16 10:50:27,464 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2472:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,032 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm3_1   | 2023-06-16 10:49:39,775 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
scm2_1   | 2023-06-16 10:49:18,855 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:48:29,452 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO ha.SCMStateMachine: current SCM becomes leader of term 2.
om2_1    | 2023-06-16 10:50:27,482 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,033 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,033 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:18,857 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:48:29,452 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO ha.SCMContext: update <isLeader,term> from <false,0> to <true,2>
scm3_1   | 2023-06-16 10:49:39,776 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-06-16 10:50:27,483 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm2_1   | 2023-06-16 10:49:18,857 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,039 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:29,453 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: change Leader from null to 80f154b6-d6d9-4d00-b482-76c3516d18ec at term 2 for becomeLeader, leader elected after 7109ms
scm3_1   | 2023-06-16 10:49:41,460 [IPC Server handler 5 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74bc4d26-95cc-4798-833d-04dc94ff925a
om2_1    | 2023-06-16 10:50:27,487 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:49:18,975 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,039 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:29,459 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.staging.catchup.gap = 1000 (default)
scm3_1   | 2023-06-16 10:49:41,460 [IPC Server handler 5 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,492 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm2_1   | 2023-06-16 10:49:41,830 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,043 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:29,464 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm3_1   | 2023-06-16 10:49:41,460 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-06-16 10:50:27,492 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2473:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm3_1   | 2023-06-16 10:49:41,468 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
scm2_1   | 2023-06-16 10:49:45,516 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
scm2_1   | 2023-06-16 10:49:45,517 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,506 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,043 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm3_1   | 2023-06-16 10:49:41,468 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
scm2_1   | 2023-06-16 10:49:45,517 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
scm1_1   | 2023-06-16 10:48:29,465 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.byte-limit = 64MB (=67108864) (default)
om2_1    | 2023-06-16 10:50:27,507 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,043 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm3_1   | 2023-06-16 10:49:41,468 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
scm2_1   | 2023-06-16 10:49:45,517 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
scm1_1   | 2023-06-16 10:48:29,483 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout = 10s (default)
om2_1    | 2023-06-16 10:50:27,510 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,047 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm3_1   | 2023-06-16 10:49:41,468 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
scm2_1   | 2023-06-16 10:49:45,517 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
scm1_1   | 2023-06-16 10:48:29,484 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.timeout.denomination = 1s (default)
om2_1    | 2023-06-16 10:50:27,529 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,047 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm3_1   | 2023-06-16 10:49:41,468 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
scm2_1   | 2023-06-16 10:49:45,518 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
scm2_1   | 2023-06-16 10:49:45,518 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
om2_1    | 2023-06-16 10:50:27,529 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2474:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,053 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm2_1   | 2023-06-16 10:49:45,518 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
scm1_1   | 2023-06-16 10:48:29,486 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.watch.element-limit = 65536 (default)
scm3_1   | 2023-06-16 10:49:41,475 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
om2_1    | 2023-06-16 10:50:27,542 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,054 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm2_1   | 2023-06-16 10:49:51,500 [af8a130f-204e-4987-9eb3-c559054693bc@group-B37B80052E22-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-06-16 10:48:29,496 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.element-limit = 4096 (default)
scm3_1   | 2023-06-16 10:49:41,476 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-06-16 10:50:27,544 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,054 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm2_1   | 2023-06-16 10:53:49,372 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
scm1_1   | 2023-06-16 10:48:29,498 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServerConfigKeys: raft.server.write.follower.gap.ratio.max = -1.0 (default)
scm3_1   | 2023-06-16 10:49:41,835 [IPC Server handler 1 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/13cc61e5-01ec-4df0-9437-cac1ff0f189e
om2_1    | 2023-06-16 10:50:27,550 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,057 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:29,505 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO impl.RoleInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec: start 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl
scm1_1   | 2023-06-16 10:48:29,517 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: Rolling segment log-0_0 to index:0
om2_1    | 2023-06-16 10:50:27,554 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,554 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2475:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:29,523 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: Rolled log segment from /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_0 to /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_0-0
scm3_1   | 2023-06-16 10:49:41,835 [IPC Server handler 1 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,566 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,057 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:29,535 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderElection1] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 1: peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
scm3_1   | 2023-06-16 10:49:41,836 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-06-16 10:50:27,566 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,060 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:29,550 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-SegmentedRaftLogWorker: created new log segment /data/metadata/scm-ha/82184b77-007c-43b9-9d2d-b37b80052e22/current/log_inprogress_1
scm3_1   | 2023-06-16 10:49:41,837 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,567 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,060 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,553 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO ha.SCMContext: update <isLeaderReady> from <false> to <true>
scm3_1   | 2023-06-16 10:49:45,543 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a27fbf0d-ae65-4185-b735-d902c8c8b71a
om2_1    | 2023-06-16 10:50:27,570 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,060 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,554 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.BackgroundPipelineCreator: Service BackgroundPipelineCreator transitions to RUNNING.
scm3_1   | 2023-06-16 10:49:45,543 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,570 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2476:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,063 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:29,558 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
scm3_1   | 2023-06-16 10:49:45,544 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: ignore, not leader SCM.
om2_1    | 2023-06-16 10:50:27,578 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,063 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:29,559 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.ContainerSafeModeRule: Refreshed one replica container threshold 0, currentThreshold 0
scm3_1   | 2023-06-16 10:49:45,556 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:50:27,578 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,070 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:29,559 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.OneReplicaPipelineSafeModeRule: Refreshed Total pipeline count is 0, pipeline's with at least one datanode reported threshold count is 0
scm3_1   | 2023-06-16 10:49:45,569 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 2, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,579 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,070 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,559 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO server.SCMDatanodeProtocolServer: ScmDatanodeProtocol RPC server for DataNodes is listening at /0.0.0.0:9861
scm3_1   | 2023-06-16 10:49:45,734 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,582 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,070 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,564 [IPC Server listener on 9861] INFO ipc.Server: IPC Server listener on 9861: starting
scm3_1   | 2023-06-16 10:49:45,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,582 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2477:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,077 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:29,587 [IPC Server Responder] INFO ipc.Server: IPC Server Responder: starting
scm3_1   | 2023-06-16 10:49:45,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
om2_1    | 2023-06-16 10:50:27,591 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,077 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 2023-06-16 10:48:29,724 [IPC Server handler 1 on default port 9861] WARN ipc.Server: IPC Server handler 1 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.18:47300: output error
scm3_1   | 2023-06-16 10:49:45,738 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
om2_1    | 2023-06-16 10:50:27,591 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,082 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 2023-06-16 10:48:29,749 [IPC Server handler 3 on default port 9861] WARN ipc.Server: IPC Server handler 3 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.17:56944: output error
scm3_1   | 2023-06-16 10:49:45,739 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
om2_1    | 2023-06-16 10:50:27,592 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,082 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,755 [IPC Server handler 3 on default port 9861] INFO ipc.Server: IPC Server handler 3 on default port 9861 caught an exception
scm3_1   | 2023-06-16 10:49:45,739 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
om2_1    | 2023-06-16 10:50:27,600 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,082 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | java.nio.channels.ClosedChannelException
scm3_1   | 2023-06-16 10:49:45,739 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
om2_1    | 2023-06-16 10:50:27,600 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2478:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,089 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm3_1   | 2023-06-16 10:49:51,497 [d5edb8ee-91fd-4f84-a835-71ed6541c25d@group-B37B80052E22-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
om2_1    | 2023-06-16 10:50:27,609 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,089 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
scm3_1   | 2023-06-16 10:54:12,528 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Manager is not ready to run until 5000ms after safemode exit
om2_1    | 2023-06-16 10:50:27,610 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,091 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om2_1    | 2023-06-16 10:50:27,612 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,615 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om2_1    | 2023-06-16 10:50:27,615 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2479:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,092 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om2_1    | 2023-06-16 10:50:27,619 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,092 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om2_1    | 2023-06-16 10:50:27,620 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,095 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om2_1    | 2023-06-16 10:50:27,623 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,627 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:27,628 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2480:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,095 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om2_1    | 2023-06-16 10:50:27,633 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,099 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om2_1    | 2023-06-16 10:50:27,633 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,099 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om2_1    | 2023-06-16 10:50:27,638 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,099 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om2_1    | 2023-06-16 10:50:27,643 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,104 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om1_1    | 2023-06-16 10:50:28,104 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:50:27,644 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2481:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 2023-06-16 10:50:28,107 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 24
om2_1    | 2023-06-16 10:50:27,655 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-06-16 10:50:28,107 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,655 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-06-16 10:50:28,107 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,661 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 2023-06-16 10:50:28,112 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: received a reply om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:27,670 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-06-16 10:50:28,113 [grpc-default-executor-3] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: InstallSnapshot in progress.
om2_1    | 2023-06-16 10:50:27,670 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2482:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:29,760 [IPC Server handler 1 on default port 9861] INFO ipc.Server: IPC Server handler 1 on default port 9861 caught an exception
om1_1    | 2023-06-16 10:50:28,123 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 24 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,675 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | java.nio.channels.ClosedChannelException
om1_1    | 2023-06-16 10:50:28,123 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,676 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 2023-06-16 10:50:28,129 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
om2_1    | 2023-06-16 10:50:27,677 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-06-16 10:50:28,131 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 24 -> 23
om2_1    | 2023-06-16 10:50:27,686 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om1_1    | 2023-06-16 10:50:28,138 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: RST_STREAM closed stream. HTTP/2 error code: CANCEL
om2_1    | 2023-06-16 10:50:27,686 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2483:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om1_1    | 2023-06-16 10:50:28,138 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 23 -> 22
om2_1    | 2023-06-16 10:50:27,701 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om1_1    | 2023-06-16 10:50:28,139 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: Connection closed after GOAWAY. HTTP/2 error code: NO_ERROR, debug data: app_requested
om2_1    | 2023-06-16 10:50:27,702 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om1_1    | 2023-06-16 10:50:28,139 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 22 -> 21
om2_1    | 2023-06-16 10:50:27,703 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om1_1    | 2023-06-16 10:50:28,145 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,709 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om1_1    | 2023-06-16 10:50:28,146 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 21 -> 20
om2_1    | 2023-06-16 10:50:27,709 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2484:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om1_1    | 2023-06-16 10:50:28,150 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,716 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om1_1    | 2023-06-16 10:50:28,151 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 20 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,716 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om1_1    | 2023-06-16 10:50:28,152 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,717 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om1_1    | 2023-06-16 10:50:28,152 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,721 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 2023-06-16 10:50:28,162 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 20 -> 19
om2_1    | 2023-06-16 10:50:27,721 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2485:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-06-16 10:50:27,725 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,164 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 19 -> 18
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 2023-06-16 10:50:27,725 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,168 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,168 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 18 -> 17
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om2_1    | 2023-06-16 10:50:27,727 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,172 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om2_1    | 2023-06-16 10:50:27,731 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,175 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 17 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,764 [IPC Server handler 2 on default port 9861] WARN ipc.Server: IPC Server handler 2 on default port 9861, call Call#1 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.21:51836: output error
om2_1    | 2023-06-16 10:50:27,732 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2486:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,175 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:29,767 [IPC Server handler 2 on default port 9861] INFO ipc.Server: IPC Server handler 2 on default port 9861 caught an exception
om2_1    | 2023-06-16 10:50:27,738 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,175 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | java.nio.channels.ClosedChannelException
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-06-16 10:50:28,177 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 17 -> 16
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om2_1    | 2023-06-16 10:50:27,738 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,178 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 16 -> 15
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om2_1    | 2023-06-16 10:50:27,738 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,179 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om2_1    | 2023-06-16 10:50:27,741 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,180 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 15 -> 14
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om2_1    | 2023-06-16 10:50:27,741 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2487:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,181 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om2_1    | 2023-06-16 10:50:27,745 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,181 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 14 -> 13
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om2_1    | 2023-06-16 10:50:27,746 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,189 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 13 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om2_1    | 2023-06-16 10:50:27,746 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,189 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om2_1    | 2023-06-16 10:50:27,749 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,189 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om2_1    | 2023-06-16 10:50:27,749 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2488:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,191 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 13 -> 12
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om2_1    | 2023-06-16 10:50:27,762 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,192 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om2_1    | 2023-06-16 10:50:27,763 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,192 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om2_1    | 2023-06-16 10:50:27,763 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,193 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 12 -> 11
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om2_1    | 2023-06-16 10:50:27,766 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 2023-06-16 10:50:28,194 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 11 -> 10
om2_1    | 2023-06-16 10:50:27,766 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2489:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-06-16 10:50:28,202 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 10 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,767 [pool-25-thread-1] INFO om.OzoneManager: Installing checkpoint with OMTransactionInfo 1#24
scm1_1   | 2023-06-16 10:48:29,786 [IPC Server handler 0 on default port 9861] WARN ipc.Server: IPC Server handler 0 on default port 9861, call Call#3 Retry#0 org.apache.hadoop.ozone.protocol.StorageContainerDatanodeProtocol.submitRequest from 10.9.0.19:49050: output error
om1_1    | 2023-06-16 10:50:28,203 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,778 [pool-25-thread-1] INFO utils.BackgroundService: Shutting down service KeyDeletingService
scm1_1   | 2023-06-16 10:48:29,791 [IPC Server handler 0 on default port 9861] INFO ipc.Server: IPC Server handler 0 on default port 9861 caught an exception
om1_1    | 2023-06-16 10:50:28,203 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | java.nio.channels.ClosedChannelException
om2_1    | 2023-06-16 10:50:27,777 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,204 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 10 -> 9
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om2_1    | 2023-06-16 10:50:27,782 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,205 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om2_1    | 2023-06-16 10:50:27,785 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,206 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 9 -> 8
scm1_1   | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om2_1    | 2023-06-16 10:50:27,787 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,207 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om2_1    | 2023-06-16 10:50:27,787 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2490:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,209 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 8 -> 7
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
om2_1    | 2023-06-16 10:50:27,795 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,214 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 7 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
om2_1    | 2023-06-16 10:50:27,796 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,215 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
om1_1    | 2023-06-16 10:50:28,215 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,796 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om1_1    | 2023-06-16 10:50:28,216 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 7 -> 6
om2_1    | 2023-06-16 10:50:27,799 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om1_1    | 2023-06-16 10:50:28,218 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,799 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2491:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om1_1    | 2023-06-16 10:50:28,219 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,800 [pool-25-thread-1] INFO utils.BackgroundService: Shutting down service DirectoryDeletingService
scm1_1   | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om1_1    | 2023-06-16 10:50:28,219 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 6 -> 5
om2_1    | 2023-06-16 10:50:27,801 [pool-25-thread-1] INFO utils.BackgroundService: Shutting down service OpenKeyCleanupService
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om1_1    | 2023-06-16 10:50:28,219 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 5 -> 4
om2_1    | 2023-06-16 10:50:27,803 [pool-25-thread-1] INFO ratis.OzoneManagerStateMachine: OzoneManagerStateMachine is pausing
scm1_1   | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 2023-06-16 10:50:28,226 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 4 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,803 [pool-25-thread-1] INFO ratis.OzoneManagerDoubleBuffer: Stopping OMDoubleBuffer flush thread
scm1_1   | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-06-16 10:50:28,227 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,804 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-06-16 10:50:28,227 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,805 [grpc-default-executor-3] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 2023-06-16 10:50:28,228 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 4 -> 3
om2_1    | 2023-06-16 10:50:27,805 [grpc-default-executor-3] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-06-16 10:50:28,228 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,821 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:30,618 [IPC Server handler 99 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:28,229 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 3 -> 2
om2_1    | 2023-06-16 10:50:27,827 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2492:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:30,620 [IPC Server handler 99 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:28,231 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,804 [OMDoubleBufferFlushThread] INFO ratis.OzoneManagerDoubleBuffer: OMDoubleBuffer flush thread OMDoubleBufferFlushThread is interrupted and will exit.
scm1_1   | 2023-06-16 10:48:30,627 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om1_1    | 2023-06-16 10:50:28,231 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 2 -> 1
om2_1    | 2023-06-16 10:50:27,828 [pool-25-thread-1] INFO ipc.Server: Stopping server on 9862
scm1_1   | 2023-06-16 10:48:30,632 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 1 DataNodes registered, 3 required.
om1_1    | 2023-06-16 10:50:28,238 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 1 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,834 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:30,641 [EventQueue-NodeRegistrationContainerReportForContainerSafeModeRule] INFO safemode.SCMSafeModeManager: ContainerSafeModeRule rule is successfully validated
om1_1    | 2023-06-16 10:50:28,239 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,834 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:30,642 [EventQueue-PipelineReportForOneReplicaPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: AtleastOneDatanodeReportedRule rule is successfully validated
om1_1    | 2023-06-16 10:50:28,239 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,834 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:30,660 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=21e502a4-0e73-4435-b1e3-1e7ec9f35ce6 to datanode:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:28,240 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 1 -> 0
om2_1    | 2023-06-16 10:50:27,856 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,240 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:30,801 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]].
scm1_1   | 2023-06-16 10:48:30,803 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,241 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,123 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/7e6c224f-5cdf-45b7-b597-f8bf583ee016
om2_1    | 2023-06-16 10:50:27,856 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2493:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,242 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,124 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,859 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,243 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,125 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om2_1    | 2023-06-16 10:50:27,859 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,250 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,125 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 2 DataNodes registered, 3 required.
om2_1    | 2023-06-16 10:50:27,859 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,250 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,127 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e4ca3c21-179d-436f-b702-75b0b54839b8 to datanode:7e6c224f-5cdf-45b7-b597-f8bf583ee016
om2_1    | 2023-06-16 10:50:27,867 [IPC Server listener on 9862] INFO ipc.Server: Stopping IPC Server listener on 9862
om1_1    | 2023-06-16 10:50:28,251 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,133 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]].
om2_1    | 2023-06-16 10:50:27,868 [IPC Server Responder] INFO ipc.Server: Stopping IPC Server Responder
om1_1    | 2023-06-16 10:50:28,251 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,136 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,873 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,252 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,781 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/74bc4d26-95cc-4798-833d-04dc94ff925a
om2_1    | 2023-06-16 10:50:27,873 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2494:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,252 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,784 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,876 [pool-25-thread-1] INFO om.OzoneManager: RPC server is stopped. Spend 48 ms.
om1_1    | 2023-06-16 10:50:28,254 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,784 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om2_1    | 2023-06-16 10:50:27,877 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,254 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,787 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1 to datanode:74bc4d26-95cc-4798-833d-04dc94ff925a
om2_1    | 2023-06-16 10:50:27,877 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,262 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,793 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. 3 DataNodes registered, 3 required.
om2_1    | 2023-06-16 10:50:27,878 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,262 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,810 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: DataNodeSafeModeRule rule is successfully validated
om2_1    | 2023-06-16 10:50:27,881 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,262 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,814 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO safemode.SCMSafeModeManager: All SCM safe mode pre check rules have passed
om2_1    | 2023-06-16 10:50:27,881 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2495:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,263 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,814 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]].
om2_1    | 2023-06-16 10:50:27,887 [pool-25-thread-1] INFO om.OzoneManager: metadataManager is stopped. Spend 11 ms.
om1_1    | 2023-06-16 10:50:28,264 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:31,823 [IPC Server handler 6 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/a27fbf0d-ae65-4185-b735-d902c8c8b71a
om2_1    | 2023-06-16 10:50:27,887 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,264 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:31,825 [IPC Server handler 6 on default port 9861] INFO node.SCMNodeManager: Registered Data node : a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om2_1    | 2023-06-16 10:50:27,887 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:31,827 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om1_1    | 2023-06-16 10:50:28,266 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,888 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,814 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
om1_1    | 2023-06-16 10:50:28,266 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,891 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:31,832 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=false} to SafeModeStatus{safeModeStatus=true, preCheckPassed=true}.
om1_1    | 2023-06-16 10:50:28,274 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,891 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2496:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:31,833 [EventQueue-NodeRegistrationContainerReportForDataNodeSafeModeRule] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om1_1    | 2023-06-16 10:50:28,274 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,905 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,274 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,906 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:31,833 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,275 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,906 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,859 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 to datanode:7e6c224f-5cdf-45b7-b597-f8bf583ee016
om1_1    | 2023-06-16 10:50:28,275 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,909 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:31,859 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 to datanode:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:28,276 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,909 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2497:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:31,860 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 to datanode:a27fbf0d-ae65-4185-b735-d902c8c8b71a
om1_1    | 2023-06-16 10:50:28,277 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,277 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,914 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,286 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,869 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]].
om2_1    | 2023-06-16 10:50:27,914 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,286 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,869 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,286 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,915 [pool-25-thread-1] INFO om.OzoneManager: Replaced DB with checkpoint from OM: om1, term: 1, index: 24, time: 27 ms
scm1_1   | 2023-06-16 10:48:31,875 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=78655ac1-fdd1-422f-9722-5f4ec0623377 to datanode:a27fbf0d-ae65-4185-b735-d902c8c8b71a
om1_1    | 2023-06-16 10:50:28,287 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,915 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,880 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]].
om1_1    | 2023-06-16 10:50:28,288 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,917 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:31,881 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,288 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,917 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2498:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:31,883 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 to datanode:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8
om1_1    | 2023-06-16 10:50:28,289 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,920 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,887 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 to datanode:7e6c224f-5cdf-45b7-b597-f8bf583ee016
om1_1    | 2023-06-16 10:50:28,289 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,920 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:31,887 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 to datanode:a27fbf0d-ae65-4185-b735-d902c8c8b71a
om1_1    | 2023-06-16 10:50:28,298 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,920 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:31,890 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]].
om1_1    | 2023-06-16 10:50:28,298 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,929 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:31,890 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,302 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,929 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2499:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:31,892 [RatisPipelineUtilsThread - 0] INFO pipeline.PipelineManagerImpl: Pipeline: PipelineID=69c8023f-ca19-4575-8453-42efe7c8ded3 contains same datanodes as previous pipelines: PipelineID=47baacd4-0b41-4a2f-af12-e3ff46599155 nodeIds: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, 7e6c224f-5cdf-45b7-b597-f8bf583ee016, a27fbf0d-ae65-4185-b735-d902c8c8b71a
om1_1    | 2023-06-16 10:50:28,304 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,932 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:32,015 [IPC Server handler 10 on default port 9861] INFO net.NetworkTopologyImpl: Added a new node: /default-rack/13cc61e5-01ec-4df0-9437-cac1ff0f189e
om1_1    | 2023-06-16 10:50:28,304 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,932 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:48:32,015 [IPC Server handler 10 on default port 9861] INFO node.SCMNodeManager: Registered Data node : 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: null, persistedOpStateExpiryEpochSec: 0}
om1_1    | 2023-06-16 10:50:28,305 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,932 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:32,015 [EventQueue-NewNodeForNewNodeHandler] INFO pipeline.BackgroundPipelineCreator: trigger a one-shot run on RatisPipelineUtilsThread.
om1_1    | 2023-06-16 10:50:28,306 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,936 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:48:32,017 [RatisPipelineUtilsThread - 0] INFO pipeline.RatisPipelineProvider: Sending CreatePipelineCommand for pipeline:PipelineID=e156c71f-4c3d-49f1-9f4c-6e4272a38d3e to datanode:13cc61e5-01ec-4df0-9437-cac1ff0f189e
om1_1    | 2023-06-16 10:50:28,306 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,936 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2500:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:32,022 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO pipeline.PipelineStateManagerImpl: Created pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]].
om1_1    | 2023-06-16 10:50:28,314 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,937 [IPC Server handler 0 on default port 9862] WARN ipc.Server: IPC Server handler 0 on default port 9862, call Call#3 Retry#0 org.apache.hadoop.ozone.om.protocol.OzoneManagerProtocol.submitRequest from 10.9.0.14:48730: output error
scm1_1   | 2023-06-16 10:48:32,025 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,314 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,937 [IPC Server handler 0 on default port 9862] INFO ipc.Server: IPC Server handler 0 on default port 9862 caught an exception
scm1_1   | 2023-06-16 10:48:54,496 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: 80f154b6-d6d9-4d00-b482-76c3516d18ec: Submitting SetConfiguration request to Ratis server with new SCM peers list: [80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER]
om2_1    | java.nio.channels.ClosedChannelException
om1_1    | 2023-06-16 10:50:28,315 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:54,515 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: receive setConfiguration SetConfigurationRequest:client-DD09E793C77B->80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
om2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.ensureOpenAndConnected(SocketChannelImpl.java:180)
om1_1    | 2023-06-16 10:50:28,315 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:54,529 [IPC Server handler 1 on default port 9863] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-DD09E793C77B->80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22, cid=7, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER], listeners:[]
om2_1    | 	at java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:452)
om1_1    | 2023-06-16 10:50:28,316 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:54,674 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 	at org.apache.hadoop.ipc.Server.channelWrite(Server.java:3619)
om1_1    | 2023-06-16 10:50:28,316 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:54,687 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 	at org.apache.hadoop.ipc.Server.access$1700(Server.java:144)
om1_1    | 2023-06-16 10:50:28,319 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 	at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:1672)
scm1_1   | 2023-06-16 10:48:54,690 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om1_1    | 2023-06-16 10:50:28,319 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 	at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:1742)
scm1_1   | 2023-06-16 10:48:54,722 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om1_1    | 2023-06-16 10:50:28,326 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 	at org.apache.hadoop.ipc.Server$Connection.sendResponse(Server.java:2847)
scm1_1   | 2023-06-16 10:48:54,723 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om1_1    | 2023-06-16 10:50:28,326 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:54,723 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 	at org.apache.hadoop.ipc.Server$Connection.access$300(Server.java:1814)
om1_1    | 2023-06-16 10:50:28,326 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:54,725 [IPC Server handler 1 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.doResponse(Server.java:1125)
om1_1    | 2023-06-16 10:50:28,327 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:54,726 [IPC Server handler 1 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 	at org.apache.hadoop.ipc.Server$Call.doResponse(Server.java:917)
om1_1    | 2023-06-16 10:50:28,328 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:54,786 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
om2_1    | 	at org.apache.hadoop.ipc.Server$Call.sendResponse(Server.java:903)
om1_1    | 2023-06-16 10:50:28,329 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:54,932 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-GrpcLogAppender: send 80f154b6-d6d9-4d00-b482-76c3516d18ec->af8a130f-204e-4987-9eb3-c559054693bc#0-t2,notify:(t:1, i:0)
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1060)
om1_1    | 2023-06-16 10:50:28,330 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:55,033 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for af8a130f-204e-4987-9eb3-c559054693bc
om2_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 2023-06-16 10:50:28,331 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:58,116 [grpc-default-executor-0] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-InstallSnapshotResponseHandler: received the first reply 80f154b6-d6d9-4d00-b482-76c3516d18ec<-af8a130f-204e-4987-9eb3-c559054693bc#0:OK-t0,ALREADY_INSTALLED
om2_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 2023-06-16 10:50:28,338 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:48:58,144 [grpc-default-executor-0] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
om2_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 2023-06-16 10:50:28,338 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:58,156 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: snapshotIndex: setUnconditionally 0 -> 0
om2_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 2023-06-16 10:50:28,339 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:58,157 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: matchIndex: setUnconditionally 0 -> 0
om2_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-06-16 10:50:28,340 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:58,157 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: nextIndex: setUnconditionally 0 -> 1
om2_1    | 2023-06-16 10:50:27,939 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,340 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:48:58,157 [grpc-default-executor-0] INFO leader.FollowerInfo: Follower 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc acknowledged installing snapshot
om2_1    | 2023-06-16 10:50:27,939 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,341 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:48:58,157 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: nextIndex: updateToMax old=1, new=1, updated? false
om1_1    | 2023-06-16 10:50:28,342 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,939 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:58,392 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: nextIndex: updateUnconditionally 17 -> 0
om1_1    | 2023-06-16 10:50:28,343 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,932 [pool-25-thread-1] WARN server.ServerUtils: ozone.om.db.dirs is not configured. We recommend adding this setting. Falling back to ozone.metadata.dirs instead.
scm1_1   | 2023-06-16 10:48:58,416 [grpc-default-executor-0] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->af8a130f-204e-4987-9eb3-c559054693bc: nextIndex: updateUnconditionally 17 -> 0
om1_1    | 2023-06-16 10:50:28,353 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,941 [pool-25-thread-1] INFO codec.OmKeyInfoCodec: OmKeyInfoCodec ignorePipeline = true
scm1_1   | 2023-06-16 10:48:58,928 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 17: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om1_1    | 2023-06-16 10:50:28,353 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,941 [pool-25-thread-1] INFO codec.RepeatedOmKeyInfoCodec: RepeatedOmKeyInfoCodec ignorePipeline = true
om2_1    | 2023-06-16 10:50:27,943 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,354 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,943 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2501:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:48:59,014 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 19: peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om1_1    | 2023-06-16 10:50:28,355 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,947 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:48:59,338 [IPC Server handler 1 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: af8a130f-204e-4987-9eb3-c559054693bc.
om1_1    | 2023-06-16 10:50:28,359 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,947 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:49:04,335 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 21e502a4-0e73-4435-b1e3-1e7ec9f35ce6, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8, CreationTimestamp2023-06-16T10:48:30.659Z[UTC]] moved to OPEN state
om1_1    | 2023-06-16 10:50:28,360 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,948 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:04,396 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,361 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,952 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:49:04,435 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,361 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,952 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2502:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:49:04,917 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,369 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:27,955 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:05,224 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e4ca3c21-179d-436f-b702-75b0b54839b8, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.127Z[UTC]] moved to OPEN state
om1_1    | 2023-06-16 10:50:28,370 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:05,378 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,370 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,955 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:49:05,381 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,371 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:27,956 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:05,866 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,372 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:27,958 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,372 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:05,944 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 11af8f3f-9d7f-4a69-aa88-1bf7d81dc8b1, Nodes: 74bc4d26-95cc-4798-833d-04dc94ff925a{ip: 10.9.0.21, host: ha_dn5_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:74bc4d26-95cc-4798-833d-04dc94ff925a, CreationTimestamp2023-06-16T10:48:31.787Z[UTC]] moved to OPEN state
om1_1    | 2023-06-16 10:50:28,373 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:05,983 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,958 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2503:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,375 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:05,985 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,961 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,382 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:49:06,205 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: e156c71f-4c3d-49f1-9f4c-6e4272a38d3e, Nodes: 13cc61e5-01ec-4df0-9437-cac1ff0f189e{ip: 10.9.0.17, host: ha_dn1_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:13cc61e5-01ec-4df0-9437-cac1ff0f189e, CreationTimestamp2023-06-16T10:48:32.017Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:50:27,961 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,382 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:06,239 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,961 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,383 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:06,255 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,964 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,383 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:08,648 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,965 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2504:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,384 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:08,688 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,969 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,385 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:08,748 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 78655ac1-fdd1-422f-9722-5f4ec0623377, Nodes: a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/ONE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.875Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:50:27,969 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,385 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:08,757 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 0, healthy pipeline threshold count is 1
om2_1    | 2023-06-16 10:50:27,970 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,385 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:08,759 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,973 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,394 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:49:08,889 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,973 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2505:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,395 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:11,502 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,978 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,395 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:11,791 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om2_1    | 2023-06-16 10:50:27,979 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,396 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:15,425 [IPC Server handler 4 on default port 9863] INFO ha.SCMRatisServerImpl: 80f154b6-d6d9-4d00-b482-76c3516d18ec: Submitting SetConfiguration request to Ratis server with new SCM peers list: [af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|priority:0|startupRole:FOLLOWER]
om2_1    | 2023-06-16 10:50:27,979 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,397 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:15,427 [IPC Server handler 4 on default port 9863] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: receive setConfiguration SetConfigurationRequest:client-DD09E793C77B->80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
om2_1    | 2023-06-16 10:50:27,981 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,398 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:15,427 [IPC Server handler 4 on default port 9863] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl: startSetConfiguration SetConfigurationRequest:client-DD09E793C77B->80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22, cid=13, seq=0, RW, null, SET_UNCONDITIONALLY, servers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER, d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|priority:0|startupRole:FOLLOWER], listeners:[]
om2_1    | 2023-06-16 10:50:27,982 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2506:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,398 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:15,428 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.snapshot.chunk.size.max = 16MB (=16777216) (default)
om2_1    | 2023-06-16 10:50:27,989 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,399 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:15,432 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.byte-limit = 33554432 (custom)
om2_1    | 2023-06-16 10:50:27,990 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,407 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:49:15,432 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.buffer.element-limit = 1024 (custom)
om2_1    | 2023-06-16 10:50:27,990 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,407 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:15,434 [IPC Server handler 4 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.leader.outstanding.appends.max = 128 (default)
om2_1    | 2023-06-16 10:50:27,993 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,407 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:15,445 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.rpc.request.timeout = 30000ms (custom)
om2_1    | 2023-06-16 10:50:27,994 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2507:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,408 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:15,445 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.install.snapshot.enabled = false (custom)
om2_1    | 2023-06-16 10:50:27,997 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,409 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:15,445 [IPC Server handler 4 on default port 9863] INFO grpc.GrpcConfigKeys: raft.grpc.server.heartbeat.channel = true (default)
om2_1    | 2023-06-16 10:50:27,997 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,410 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:15,445 [IPC Server handler 4 on default port 9863] INFO server.RaftServerConfigKeys: raft.server.log.appender.wait-time.min = 10ms (default)
om2_1    | 2023-06-16 10:50:27,998 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,411 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:15,464 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = 0, notify follower to install snapshot-(t:1, i:0)
om2_1    | 2023-06-16 10:50:28,000 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,411 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,000 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2508:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,419 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:49:15,466 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-GrpcLogAppender: send 80f154b6-d6d9-4d00-b482-76c3516d18ec->d5edb8ee-91fd-4f84-a835-71ed6541c25d#0-t2,notify:(t:1, i:0)
om2_1    | 2023-06-16 10:50:28,003 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,420 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:15,467 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcServerProtocolClient: Build channel for d5edb8ee-91fd-4f84-a835-71ed6541c25d
om2_1    | 2023-06-16 10:50:28,003 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,420 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,004 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:16,244 [grpc-default-executor-1] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-InstallSnapshotResponseHandler: received the first reply 80f154b6-d6d9-4d00-b482-76c3516d18ec<-d5edb8ee-91fd-4f84-a835-71ed6541c25d#0:OK-t0,ALREADY_INSTALLED
om1_1    | 2023-06-16 10:50:28,421 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,006 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:49:16,245 [grpc-default-executor-1] INFO server.GrpcLogAppender: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d-InstallSnapshotResponseHandler: Follower snapshot is already at index 0.
scm1_1   | 2023-06-16 10:49:16,245 [grpc-default-executor-1] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: snapshotIndex: setUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:16,250 [grpc-default-executor-1] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: matchIndex: setUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,421 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:16,250 [grpc-default-executor-1] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: nextIndex: setUnconditionally 0 -> 1
om2_1    | 2023-06-16 10:50:28,006 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2509:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,422 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:16,251 [grpc-default-executor-1] INFO leader.FollowerInfo: Follower 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d acknowledged installing snapshot
om2_1    | 2023-06-16 10:50:28,008 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,422 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:16,251 [grpc-default-executor-1] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: nextIndex: updateToMax old=1, new=1, updated? false
om2_1    | 2023-06-16 10:50:28,009 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,423 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:16,332 [grpc-default-executor-2] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: nextIndex: updateUnconditionally 31 -> 0
om2_1    | 2023-06-16 10:50:28,009 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,431 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
scm1_1   | 2023-06-16 10:49:16,340 [grpc-default-executor-2] INFO leader.FollowerInfo: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22->d5edb8ee-91fd-4f84-a835-71ed6541c25d: nextIndex: updateUnconditionally 31 -> 0
om2_1    | 2023-06-16 10:50:28,011 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,432 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:16,586 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 31: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=peers:[af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[]
om2_1    | 2023-06-16 10:50:28,012 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2510:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,432 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:16,595 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-LeaderStateImpl] INFO server.RaftServer$Division: 80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22: set configuration 33: peers:[d5edb8ee-91fd-4f84-a835-71ed6541c25d|rpc:scm3:9894|priority:0|startupRole:FOLLOWER, af8a130f-204e-4987-9eb3-c559054693bc|rpc:scm2:9894|priority:0|startupRole:FOLLOWER, 80f154b6-d6d9-4d00-b482-76c3516d18ec|rpc:scm1:9894|admin:|client:|dataStream:|priority:0|startupRole:FOLLOWER]|listeners:[], old=null
om2_1    | 2023-06-16 10:50:28,014 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,433 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:16,608 [IPC Server handler 4 on default port 9863] INFO ha.SCMRatisServerImpl: Successfully added new SCM: d5edb8ee-91fd-4f84-a835-71ed6541c25d.
om2_1    | 2023-06-16 10:50:28,015 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,434 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:18,871 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 69c8023f-ca19-4575-8453-42efe7c8ded3, Nodes: 2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:7e6c224f-5cdf-45b7-b597-f8bf583ee016, CreationTimestamp2023-06-16T10:48:31.883Z[UTC]] moved to OPEN state
om2_1    | 2023-06-16 10:50:28,016 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,434 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,018 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
scm1_1   | 2023-06-16 10:49:18,871 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 0, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,435 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,018 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2511:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
scm1_1   | 2023-06-16 10:49:18,956 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] INFO safemode.HealthyPipelineSafeModeRule: Refreshed total pipeline count is 1, healthy pipeline threshold count is 1
om1_1    | 2023-06-16 10:50:28,436 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,020 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:19,009 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM in safe mode. Healthy pipelines reported count is 1, required healthy pipeline reported count is 1
om1_1    | 2023-06-16 10:50:28,444 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,444 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:19,012 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: HealthyPipelineSafeModeRule rule is successfully validated
om1_1    | 2023-06-16 10:50:28,444 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,445 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:19,012 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: ScmSafeModeManager, all rules are successfully validated
om1_1    | 2023-06-16 10:50:28,446 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,447 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:19,012 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO safemode.SCMSafeModeManager: SCM exiting safe mode.
om1_1    | 2023-06-16 10:50:28,447 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,448 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:19,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN events.EventQueue: No event handler registered for event TypedEvent{payloadType=SafeModeStatus, name='Safe mode status'}
om1_1    | 2023-06-16 10:50:28,456 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,021 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
scm1_1   | 2023-06-16 10:49:19,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ha.SCMContext: Update SafeModeStatus from SafeModeStatus{safeModeStatus=true, preCheckPassed=true} to SafeModeStatus{safeModeStatus=false, preCheckPassed=true}.
om1_1    | 2023-06-16 10:50:28,456 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,456 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:19,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO BackgroundPipelineScrubber: Service BackgroundPipelineScrubber transitions to RUNNING.
om1_1    | 2023-06-16 10:50:28,457 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,458 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:19,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO ExpiredContainerReplicaOpScrubber: Service ExpiredContainerReplicaOpScrubber transitions to RUNNING.
om1_1    | 2023-06-16 10:50:28,459 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,460 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
scm1_1   | 2023-06-16 10:49:19,033 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] INFO replication.ReplicationManager: Service ReplicationManager transitions to RUNNING.
om2_1    | 2023-06-16 10:50:28,021 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,460 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
scm1_1   | 2023-06-16 10:49:19,053 [EventQueue-OpenPipelineForHealthyPipelineSafeModeRule] WARN balancer.ContainerBalancer: Could not find persisted configuration for ContainerBalancer when checking if ContainerBalancer should run. ContainerBalancer should not run now.
om2_1    | 2023-06-16 10:50:28,023 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,468 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,468 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
scm1_1   | 2023-06-16 10:49:45,510 [EventQueue-PipelineReportForPipelineReportHandler] INFO pipeline.PipelineManagerImpl: Pipeline Pipeline[ Id: 47baacd4-0b41-4a2f-af12-e3ff46599155, Nodes: 7e6c224f-5cdf-45b7-b597-f8bf583ee016{ip: 10.9.0.18, host: ha_dn2_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}2d56f2fc-15a5-495c-86fe-d6dd2eb7a5e8{ip: 10.9.0.20, host: ha_dn4_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}a27fbf0d-ae65-4185-b735-d902c8c8b71a{ip: 10.9.0.19, host: ha_dn3_1.ha_net, ports: [REPLICATION=9886, RATIS=9858, RATIS_ADMIN=9857, RATIS_SERVER=9856, STANDALONE=9859], networkLocation: /default-rack, certSerialId: null, persistedOpState: IN_SERVICE, persistedOpStateExpiryEpochSec: 0}, ReplicationConfig: RATIS/THREE, State:ALLOCATED, leaderId:a27fbf0d-ae65-4185-b735-d902c8c8b71a, CreationTimestamp2023-06-16T10:48:31.859Z[UTC]] moved to OPEN state
scm1_1   | 2023-06-16 10:49:51,386 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for containerId, change lastId from 0 to 1000.
scm1_1   | 2023-06-16 10:49:51,505 [80f154b6-d6d9-4d00-b482-76c3516d18ec@group-B37B80052E22-StateMachineUpdater] WARN ha.SequenceIdGenerator: Failed to allocate a batch for localId, expected lastId is 0, actual lastId is 111677748019200000.
scm1_1   | 2023-06-16 10:49:51,531 [IPC Server handler 1 on default port 9863] INFO ha.SequenceIdGenerator: Allocate a batch for localId, change lastId from 111677748019200000 to 111677748019201000.
scm1_1   | 2023-06-16 10:49:53,024 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:49:53,035 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:50:23,024 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:50:23,035 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:50:53,025 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:50:53,035 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:51:23,025 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:51:23,036 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:51:53,026 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:51:53,036 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:52:23,026 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:52:23,036 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:52:53,026 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:52:53,036 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:53:23,027 [ReplicationMonitor] INFO replication.ReplicationManager: Replication Monitor Thread took 1 milliseconds for processing 2 containers.
scm1_1   | 2023-06-16 10:53:23,027 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:53:23,037 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:53:53,027 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:53:53,037 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:54:23,028 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:54:23,037 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:54:53,028 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:54:53,038 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:55:23,028 [Under Replicated Processor] INFO replication.UnderReplicatedProcessor: Processed 0 under replicated containers, failed processing 0
scm1_1   | 2023-06-16 10:55:23,038 [Over Replicated Processor] INFO replication.OverReplicatedProcessor: Processed 0 over replicated containers, failed processing 0
om2_1    | 2023-06-16 10:50:28,023 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2512:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,028 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,028 [grpc-default-executor-0] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,028 [grpc-default-executor-0] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,031 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,032 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2513:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,037 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,037 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,038 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,041 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,042 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2514:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,046 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,047 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,047 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,049 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,049 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2515:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,051 [pool-25-thread-1] INFO om.OzoneManager: S3 Multi-Tenancy is disabled
om2_1    | 2023-06-16 10:50:28,052 [pool-25-thread-1] ERROR om.OzoneManager: Terminating with exit status 1: Failed to reload OM state and instantiate services.
om2_1    | org.apache.hadoop.metrics2.MetricsException: Metrics source OMPerformanceMetrics already exists!
om2_1    | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152)
om2_1    | 	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125)
om2_1    | 	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229)
om2_1    | 	at org.apache.hadoop.ozone.om.OMPerformanceMetrics.register(OMPerformanceMetrics.java:33)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.instantiateServices(OzoneManager.java:726)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.reloadOMState(OzoneManager.java:3981)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.installCheckpoint(OzoneManager.java:3835)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.installCheckpoint(OzoneManager.java:3747)
om2_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.installSnapshotFromLeader(OzoneManager.java:3724)
om2_1    | 	at org.apache.hadoop.ozone.om.ratis.OzoneManagerStateMachine.lambda$6(OzoneManagerStateMachine.java:478)
om2_1    | 	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om2_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om2_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om2_1    | 2023-06-16 10:50:28,055 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,055 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,056 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,059 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,059 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2516:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,062 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,469 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,473 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,473 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,474 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,475 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,476 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,483 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,484 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,484 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,485 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,486 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,486 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,487 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,487 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,496 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,496 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,496 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,497 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,062 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,062 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,498 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,069 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,499 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,069 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2517:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,499 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,067 [shutdown-hook-0] INFO om.OzoneManagerStarter: SHUTDOWN_MSG: 
om1_1    | 2023-06-16 10:50:28,499 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | /************************************************************
om1_1    | 2023-06-16 10:50:28,508 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | SHUTDOWN_MSG: Shutting down OzoneManager at 56d7e9fb6d03/10.9.0.12
om1_1    | 2023-06-16 10:50:28,508 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | ************************************************************/
om1_1    | 2023-06-16 10:50:28,508 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,509 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,075 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,510 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,075 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,511 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,075 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,511 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,081 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,513 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,081 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2518:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,520 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,084 [shutdown-hook-0] INFO om.OzoneManager: om2[om2:9862]: Stopping Ozone Manager
om1_1    | 2023-06-16 10:50:28,520 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,084 [shutdown-hook-0] INFO ipc.Server: Stopping server on 9862
om1_1    | 2023-06-16 10:50:28,521 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,087 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,521 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,088 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om1_1    | 2023-06-16 10:50:28,522 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,523 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,524 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,524 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,532 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,532 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,532 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,088 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,533 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,091 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om1_1    | 2023-06-16 10:50:28,533 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,091 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2519:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om1_1    | 2023-06-16 10:50:28,534 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:28,094 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,534 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:28,094 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,094 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,098 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,098 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2520:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,092 [shutdown-hook-0] INFO server.RaftServer: om2: close
om2_1    | 2023-06-16 10:50:28,102 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,103 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,103 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,106 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: Failed appendEntries as snapshot (25) installation is in progress
om2_1    | 2023-06-16 10:50:28,106 [om2-server-thread1] INFO server.RaftServer$Division: om2@group-D66704EFC61C: inconsistency entries. Reply:om1<-om2#2521:FAIL-t1,INCONSISTENCY,nextIndex=24,followerCommit=22,matchIndex=-1
om2_1    | 2023-06-16 10:50:28,110 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: receive installSnapshot: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,111 [grpc-default-executor-2] INFO impl.SnapshotInstallationHandler: om2@group-D66704EFC61C: reply installSnapshot: om1<-om2#0:FAIL-t1,IN_PROGRESS
om2_1    | 2023-06-16 10:50:28,111 [om2-impl-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: shutdown
om2_1    | 2023-06-16 10:50:28,111 [shutdown-hook-0] INFO server.GrpcService: om2: shutdown server GrpcServerProtocolService now
om2_1    | 2023-06-16 10:50:28,112 [grpc-default-executor-2] INFO server.GrpcServerProtocolService: om2: Completed INSTALL_SNAPSHOT, lastRequest: om1->om2#0-t1,notify:(t:1, i:25)
om2_1    | 2023-06-16 10:50:28,119 [om2-impl-thread2] INFO util.JmxRegister: Successfully un-registered JMX Bean with object name Ratis:service=RaftServer,group=group-D66704EFC61C,id=om2
om2_1    | 2023-06-16 10:50:28,120 [om2-impl-thread2] INFO impl.RoleInfo: om2: shutdown om2@group-D66704EFC61C-FollowerState
om2_1    | 2023-06-16 10:50:28,120 [om2@group-D66704EFC61C-FollowerState] INFO impl.FollowerState: om2@group-D66704EFC61C-FollowerState was interrupted
om2_1    | 2023-06-16 10:50:28,122 [om2-impl-thread2] INFO impl.StateMachineUpdater: om2@group-D66704EFC61C-StateMachineUpdater: set stopIndex = 22
om2_1    | 2023-06-16 10:50:28,126 [grpc-default-executor-0] WARN server.GrpcServerProtocolService: om2: installSnapshot onError, lastRequest: null: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
om2_1    | 2023-06-16 10:50:28,126 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: Current Snapshot Index (t:1, i:22)
om2_1    | 2023-06-16 10:50:28,134 [om2@group-D66704EFC61C-StateMachineUpdater] INFO impl.StateMachineUpdater: om2@group-D66704EFC61C-StateMachineUpdater: Took a snapshot at index 22
om2_1    | 2023-06-16 10:50:28,152 [om2@group-D66704EFC61C-StateMachineUpdater] INFO impl.StateMachineUpdater: om2@group-D66704EFC61C-StateMachineUpdater: snapshotIndex: updateIncreasingly -1 -> 22
om2_1    | 2023-06-16 10:50:28,153 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerStateMachine: StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
om2_1    | 2023-06-16 10:50:28,153 [om2@group-D66704EFC61C-StateMachineUpdater] INFO ratis.OzoneManagerDoubleBuffer: OMDoubleBuffer flush thread is not running.
om2_1    | 2023-06-16 10:50:28,150 [shutdown-hook-0] INFO server.GrpcService: om2: shutdown server GrpcServerProtocolService successfully
om2_1    | 2023-06-16 10:50:28,156 [grpc-default-executor-2] WARN server.GrpcServerProtocolService: om2: installSnapshot onError, lastRequest: om1->om2#2452-t1,previous=(t:1, i:22),leaderCommit=22,initializing? true,entries: size=1, first=(t:1, i:23), STATEMACHINELOGENTRY, 0@client-A16F78D745B6: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: CANCELLED: client cancelled
om2_1    | 2023-06-16 10:50:38,153 [om2@group-D66704EFC61C-StateMachineUpdater] ERROR ratis.OzoneManagerStateMachine: Unable to shutdown executor service after timeout 10 SECONDS
om2_1    | 2023-06-16 10:50:38,154 [om2-impl-thread2] INFO server.RaftServer$Division: om2@group-D66704EFC61C: closes. applyIndex: 22
om2_1    | 2023-06-16 10:50:38,155 [om2@group-D66704EFC61C-SegmentedRaftLogWorker] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker was interrupted, exiting. There are 0 tasks remaining in the queue.
om2_1    | 2023-06-16 10:50:38,158 [om2-impl-thread2] INFO segmented.SegmentedRaftLogWorker: om2@group-D66704EFC61C-SegmentedRaftLogWorker close()
om1_1    | 2023-06-16 10:50:28,535 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:38,160 [JvmPauseMonitor0] INFO util.JvmPauseMonitor: JvmPauseMonitor-om2: Stopped
om1_1    | 2023-06-16 10:50:28,543 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:38,160 [shutdown-hook-0] INFO ratis.OzoneManagerStateMachine: StateMachine has shutdown. Shutdown OzoneManager if not already shutdown.
om1_1    | 2023-06-16 10:50:28,544 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,544 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:38,160 [shutdown-hook-0] INFO ratis.OzoneManagerDoubleBuffer: OMDoubleBuffer flush thread is not running.
om1_1    | 2023-06-16 10:50:28,544 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:48,161 [shutdown-hook-0] ERROR ratis.OzoneManagerStateMachine: Unable to shutdown executor service after timeout 10 SECONDS
om1_1    | 2023-06-16 10:50:28,545 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,546 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:48,174 [shutdown-hook-0] INFO handler.ContextHandler: Stopped o.e.j.w.WebAppContext@5d7f1e59{ozoneManager,/,null,STOPPED}{jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/ozoneManager}
om1_1    | 2023-06-16 10:50:28,546 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om2_1    | 2023-06-16 10:50:48,177 [shutdown-hook-0] INFO server.AbstractConnector: Stopped ServerConnector@2d4fb0d8{HTTP/1.1, (http/1.1)}{0.0.0.0:9874}
om1_1    | 2023-06-16 10:50:28,547 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om2_1    | 2023-06-16 10:50:48,177 [shutdown-hook-0] INFO server.session: node0 Stopped scavenging
om1_1    | 2023-06-16 10:50:28,555 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om2_1    | 2023-06-16 10:50:48,177 [shutdown-hook-0] INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5323999f{static,/static,jar:file:/opt/hadoop/share/ozone/lib/ozone-manager-1.3.0.jar!/webapps/static,STOPPED}
om2_1    | 2023-06-16 10:50:48,178 [shutdown-hook-0] INFO handler.ContextHandler: Stopped o.e.j.s.ServletContextHandler@5f3b84bd{logs,/logs,file:///var/log/hadoop/,STOPPED}
om1_1    | 2023-06-16 10:50:28,555 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,555 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,556 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,557 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,557 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,558 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,558 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,566 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,566 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,567 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,567 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,568 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,568 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,569 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,570 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,578 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,578 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,578 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,579 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,579 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,579 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,580 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,581 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,589 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,589 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,589 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,590 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,591 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,591 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,593 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,594 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,601 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,601 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,602 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,602 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,603 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,603 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,604 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,604 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,613 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,613 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,613 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,614 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,616 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,616 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,617 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,617 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,624 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,625 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,625 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,625 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,626 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,626 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,627 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,628 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,636 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,636 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,636 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,637 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,637 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,638 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,638 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,638 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,647 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,648 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,648 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,649 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,649 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,649 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,650 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,650 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,659 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,659 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,659 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,660 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,660 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,661 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,661 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,662 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,670 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,671 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,671 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,672 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,673 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,673 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,673 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,674 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,682 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,682 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,682 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,683 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,684 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,684 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,684 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,684 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,694 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,694 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,694 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,695 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,695 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,695 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,695 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,696 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,705 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,705 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,706 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,706 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,707 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,707 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,708 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,708 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,717 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,717 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,717 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,718 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,718 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,719 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,719 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,719 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,728 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,728 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,728 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,729 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,729 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,729 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,730 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,730 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,739 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,739 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,740 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,740 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,741 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,741 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,741 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,741 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,751 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,751 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,751 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,752 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,752 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,753 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,753 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,753 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,762 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,762 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,762 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,763 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,764 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,765 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,765 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,765 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,774 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,774 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,774 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,775 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,776 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,776 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,777 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,777 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,786 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,786 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,786 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,787 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,788 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,788 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,788 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,788 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,797 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,798 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,798 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,799 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,800 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,800 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,801 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,801 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,809 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,810 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,810 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,810 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,811 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,811 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,812 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,812 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,821 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,821 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,821 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,822 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,823 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,823 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,824 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,824 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,833 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,833 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,833 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,834 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,834 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,835 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,835 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,836 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,845 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,845 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,845 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,846 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,847 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,847 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,847 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,847 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,857 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,857 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,857 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,857 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,859 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,860 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,859 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,860 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,868 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,868 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,869 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,869 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,875 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,875 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,877 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,877 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,880 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,880 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,880 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,881 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,883 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,884 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,884 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,884 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,893 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,894 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,894 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,894 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,895 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,895 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,896 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,897 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,905 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,906 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,906 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,907 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,908 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,908 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,908 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,908 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,918 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,918 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,918 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,918 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,919 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,920 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,920 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,920 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,930 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,930 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,930 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,931 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,932 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,932 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,932 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,933 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,942 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,942 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,942 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,944 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,944 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,944 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,945 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,946 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,954 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,954 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,955 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,955 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,956 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,957 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,956 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,959 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,967 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,967 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,967 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,968 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,968 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,968 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,968 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,969 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,978 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,978 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,978 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,979 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,979 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,980 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,980 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,980 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,989 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,989 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:28,990 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,990 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,991 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,991 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:28,993 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:28,995 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,001 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,001 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,003 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,003 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,003 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,004 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,004 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,013 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,014 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,014 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,015 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,015 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,016 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,025 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,025 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,025 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,025 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,028 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,028 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,029 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,029 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,036 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,036 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,036 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,037 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,038 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,038 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,038 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,038 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,048 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,048 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,048 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,049 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,049 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,050 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,051 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,051 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,059 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,059 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,060 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,060 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,061 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,062 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,063 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,063 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,073 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,073 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,074 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,074 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,075 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,076 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,077 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,077 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,085 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,085 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,086 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,088 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,090 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,091 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,092 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,092 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,120 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,120 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,121 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,122 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,123 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,123 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,127 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,128 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,132 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,132 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,133 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,133 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,138 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,139 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,140 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,140 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,145 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,145 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,145 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,146 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,147 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,147 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,152 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,153 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,157 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,157 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,157 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,158 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,159 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,159 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,160 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,160 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,169 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,169 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,169 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,170 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,171 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,172 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,173 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,173 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,181 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,182 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,182 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,182 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,183 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,184 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,185 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,186 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,193 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,194 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,194 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,194 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,195 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,196 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,196 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,197 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,205 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,206 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,206 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,206 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,207 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,207 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,208 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,209 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,217 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,217 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,217 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,218 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,218 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,218 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,219 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,219 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,228 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,228 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,228 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,229 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,229 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,229 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,230 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,230 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,240 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,240 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,240 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,241 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,243 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,243 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,250 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,251 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,251 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,252 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,252 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,252 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,253 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,253 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,253 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,254 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,263 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,263 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,264 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,264 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,265 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,265 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,265 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,266 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,275 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,275 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,275 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,276 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,276 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,277 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,279 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,279 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,286 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,287 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,287 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,287 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,288 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,289 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,289 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,289 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,298 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,298 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,299 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,299 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,300 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,301 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,301 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,302 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,310 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,310 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,311 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,311 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,312 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,313 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,313 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,314 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,317 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,317 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,317 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,318 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,318 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,318 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,319 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,320 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,328 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,328 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,329 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,329 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,330 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,331 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,331 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,332 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,340 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,340 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,340 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,341 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,342 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,342 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,343 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,344 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,352 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,352 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,352 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,354 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,354 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,355 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,355 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,355 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,364 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,364 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,365 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,365 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,366 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,367 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,367 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,368 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,376 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,376 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,377 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,377 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,378 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,379 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,379 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,380 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,388 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,388 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,388 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,389 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,390 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,391 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,392 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,393 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,400 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,400 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,401 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,401 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,403 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,403 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,404 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,404 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,413 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,413 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,413 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,414 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,415 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,415 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,416 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,416 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,425 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,425 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,425 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,426 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,427 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,427 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,427 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,427 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,437 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,437 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,437 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,438 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,438 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,438 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,439 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,439 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,448 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,448 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,449 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,449 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,450 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,450 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,450 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,451 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,460 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,460 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,460 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,460 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,461 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,461 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,461 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,461 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,473 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,473 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,473 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,475 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,475 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,475 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,476 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,476 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,476 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,476 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,477 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,477 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,478 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,478 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,479 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,479 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,488 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,488 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,488 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,489 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,489 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,489 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,490 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,491 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,499 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,500 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,500 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,501 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,502 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,502 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,503 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,503 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,512 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,512 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,512 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,513 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,514 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,514 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,514 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,514 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,524 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,524 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,524 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,525 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,525 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,525 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,526 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,526 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,535 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,535 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,535 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,536 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,536 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,536 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,537 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,537 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,546 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,546 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,546 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,547 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,547 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,548 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,548 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,548 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,557 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,558 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,558 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,558 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,558 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,559 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,559 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,559 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,569 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,569 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,569 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,569 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,570 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,570 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,570 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,571 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,580 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,580 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,580 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,580 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,581 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,581 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,582 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,583 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,591 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,591 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,591 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,592 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,593 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,593 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,594 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,595 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,602 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,603 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,603 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,603 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,604 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,604 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,605 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,605 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,614 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,614 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,614 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,615 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,616 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,616 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,616 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,617 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,626 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,626 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,626 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,627 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,627 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,628 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,628 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,628 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,637 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,637 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,638 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,638 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,638 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,639 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,639 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,639 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,648 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,649 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,649 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,649 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,650 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,650 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,650 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,650 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,660 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,660 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,660 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,660 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,661 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,661 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,661 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,662 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,671 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,671 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,671 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,672 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,673 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,673 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,675 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,676 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,682 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,683 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,683 [grpc-default-executor-7] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,683 [grpc-default-executor-7] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,684 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,684 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,685 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,685 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,694 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,694 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,694 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,695 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,696 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,696 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,697 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,703 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,705 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,706 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,706 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,707 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,707 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,707 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,708 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,708 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,717 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,717 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,717 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,718 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,718 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,718 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,719 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,719 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,728 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,728 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,728 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,729 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,729 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,729 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,730 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,731 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,739 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,740 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,740 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,740 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,740 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,741 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,741 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,741 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,751 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,751 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,751 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,751 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,752 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,752 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,752 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,752 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,762 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,762 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,762 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,763 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,763 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,763 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,763 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,763 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,773 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,773 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,773 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,774 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,774 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,775 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,775 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,776 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,784 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,785 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,785 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,785 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,786 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,786 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,786 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,786 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,796 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,796 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,796 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,797 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,797 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,797 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,797 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,797 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,837 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,837 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,837 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,838 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,839 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,839 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,843 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,843 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,849 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,849 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,850 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,850 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,852 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,852 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,854 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,854 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,861 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,861 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,861 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,862 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,862 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,863 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,863 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,863 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,872 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,873 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,873 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,874 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,875 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,876 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,877 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,878 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,886 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,887 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,888 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,888 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,889 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,889 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,889 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,890 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,899 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,899 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,899 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,899 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,900 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,900 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,900 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,900 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,911 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,911 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,911 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,912 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,912 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,913 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,912 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,914 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,923 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,923 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,924 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,924 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,925 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,925 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,926 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,927 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,935 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,935 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,935 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,936 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,936 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,936 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,937 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,937 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,946 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,946 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,947 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,947 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,948 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,948 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,948 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,948 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,958 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,958 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,958 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,959 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,959 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,959 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,959 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,960 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,969 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,969 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,969 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,970 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,970 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,970 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,971 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,971 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,980 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,980 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,981 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,981 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,981 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,982 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,982 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,982 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,991 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,991 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:29,991 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,992 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,992 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,993 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:29,993 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:29,993 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,002 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,002 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,003 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,003 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,003 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,003 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,004 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,004 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,013 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,014 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,014 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,014 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,015 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,015 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,015 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,024 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,024 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,025 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,025 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,025 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,026 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,026 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,026 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,035 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,036 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,036 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,036 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,036 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,037 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,037 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,037 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,046 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,047 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,047 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,047 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,048 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,048 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,049 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,050 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,058 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,058 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,058 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,059 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,059 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,059 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,064 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,064 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,069 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,069 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,070 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,071 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,071 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,072 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,072 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,072 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,081 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,081 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,081 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,082 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,083 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,083 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,083 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,084 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,093 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,093 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,093 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,094 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,095 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,096 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,096 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,097 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,105 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,105 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,105 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,106 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,106 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,107 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,107 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,108 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,116 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,117 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,117 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,117 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,118 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,118 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,119 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,119 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,128 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,128 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,128 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,128 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,129 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,129 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,129 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,129 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,139 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,139 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,139 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,139 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,140 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,140 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,140 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,140 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,150 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,150 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,150 [grpc-default-executor-3] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,150 [grpc-default-executor-3] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,151 [grpc-default-executor-6] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,151 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-AppendLogResponseHandler: Failed appendEntries: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,152 [grpc-default-executor-6] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,152 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | 2023-06-16 10:50:30,161 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: followerNextIndex = 0 but logStartIndex = -1, notify follower to install snapshot-(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,161 [om1@group-D66704EFC61C->om2-GrpcLogAppender-LogAppenderDaemon] INFO server.GrpcLogAppender: om1@group-D66704EFC61C->om2-GrpcLogAppender: send om1->om2#0-t1,notify:(t:1, i:25)
om1_1    | 2023-06-16 10:50:30,161 [grpc-default-executor-1] WARN server.GrpcLogAppender: om1@group-D66704EFC61C->om2-InstallSnapshotResponseHandler: Failed InstallSnapshot: org.apache.ratis.thirdparty.io.grpc.StatusRuntimeException: UNAVAILABLE: io exception
om1_1    | 2023-06-16 10:50:30,162 [grpc-default-executor-1] INFO leader.FollowerInfo: om1@group-D66704EFC61C->om2: nextIndex: updateUnconditionally 0 -> 0
om1_1    | Jun 16, 2023 10:51:07 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:08 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:09 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:12 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:12 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:13 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:15 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:15 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:15 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:16 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:18 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:20 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:22 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | 2023-06-16 10:51:24,379 [IPC Server handler 32 on default port 9862] WARN db.RDBStore: Unable to get delta updates since sequenceNumber 31. This exception will be thrown to the client
om1_1    | org.apache.hadoop.hdds.utils.db.SequenceNumberNotFoundException: Invalid transaction log iterator when getting updates since sequence number 31
om1_1    | 	at org.apache.hadoop.hdds.utils.db.RDBStore.getUpdatesSince(RDBStore.java:301)
om1_1    | 	at org.apache.hadoop.ozone.om.OzoneManager.getDBUpdates(OzoneManager.java:4145)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.getOMDBUpdates(OzoneManagerRequestHandler.java:328)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerRequestHandler.handleReadRequest(OzoneManagerRequestHandler.java:226)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitReadRequestToOM(OzoneManagerProtocolServerSideTranslatorPB.java:226)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.processRequest(OzoneManagerProtocolServerSideTranslatorPB.java:175)
om1_1    | 	at org.apache.hadoop.hdds.server.OzoneProtocolMessageDispatcher.processRequest(OzoneProtocolMessageDispatcher.java:87)
om1_1    | 	at org.apache.hadoop.ozone.protocolPB.OzoneManagerProtocolServerSideTranslatorPB.submitRequest(OzoneManagerProtocolServerSideTranslatorPB.java:147)
om1_1    | 	at org.apache.hadoop.ozone.protocol.proto.OzoneManagerProtocolProtos$OzoneManagerService$2.callBlockingMethod(OzoneManagerProtocolProtos.java)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server.processCall(ProtobufRpcEngine.java:465)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:578)
om1_1    | 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)
om1_1    | 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)
om1_1    | 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)
om1_1    | 	at java.base/java.security.AccessController.doPrivileged(Native Method)
om1_1    | 	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
om1_1    | 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
om1_1    | 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)
om1_1    | 2023-06-16 10:51:24,395 [qtp1040023210-49] INFO utils.DBCheckpointServlet: Received request to obtain DB checkpoint snapshot
om1_1    | 2023-06-16 10:51:24,398 [qtp1040023210-49] INFO db.RDBCheckpointManager: Created checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912684395 in 2 milliseconds
om1_1    | 2023-06-16 10:51:24,413 [qtp1040023210-49] INFO utils.DBCheckpointServlet: Time taken to write the checkpoint to response output stream: 14 milliseconds
om1_1    | 2023-06-16 10:51:24,413 [qtp1040023210-49] INFO db.RocksDBCheckpoint: Cleaning up RocksDB checkpoint at /data/metadata/db.checkpoints/om.db_checkpoint_1686912684395
om1_1    | Jun 16, 2023 10:51:24 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:31 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:33 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:40 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:49 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:51:57 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:52:20 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:52:26 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:53:02 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:53:05 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2
om1_1    | 	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:797)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:54:02 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<5>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
om1_1    | Jun 16, 2023 10:54:24 AM org.apache.ratis.thirdparty.io.grpc.internal.ManagedChannelImpl$NameResolverListener handleErrorInSyncContext
om1_1    | WARNING: [Channel<8>: (om2:9872)] Failed to resolve name. status=Status{code=UNAVAILABLE, description=Unable to resolve host om2, cause=java.lang.RuntimeException: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:223)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.doResolve(DnsNameResolver.java:282)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$Resolve.run(DnsNameResolver.java:318)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
om1_1    | 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
om1_1    | 	at java.base/java.lang.Thread.run(Thread.java:829)
om1_1    | Caused by: java.net.UnknownHostException: om2: Name or service not known
om1_1    | 	at java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
om1_1    | 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
om1_1    | 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
om1_1    | 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
om1_1    | 	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver$JdkAddressResolver.resolveAddress(DnsNameResolver.java:631)
om1_1    | 	at org.apache.ratis.thirdparty.io.grpc.internal.DnsNameResolver.resolveAddresses(DnsNameResolver.java:219)
om1_1    | 	... 5 more
om1_1    | }
